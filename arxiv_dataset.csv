Title,Category,Category Description,Published,Summary
MM-Ego: Towards Building Egocentric Multimodal LLMs,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"This research aims to comprehensively explore building a multimodal
foundation model for egocentric video understanding. To achieve this goal, we
work on three fronts. First, as there is a lack of QA data for egocentric video
understanding, we develop a data engine that efficiently generates 7M
high-quality QA samples for egocentric videos ranging from 30 seconds to one
hour long, based on human-annotated data. This is currently the largest
egocentric QA dataset. Second, we contribute a challenging egocentric QA
benchmark with 629 videos and 7,026 questions to evaluate the models' ability
in recognizing and memorizing visual details across videos of varying lengths.
We introduce a new de-biasing evaluation method to help mitigate the
unavoidable language bias present in the models being evaluated. Third, we
propose a specialized multimodal architecture featuring a novel ""Memory Pointer
Prompting"" mechanism. This design includes a global glimpse step to gain an
overarching understanding of the entire video and identify key visual
information, followed by a fallback step that utilizes the key visual
information to generate responses. This enables the model to more effectively
comprehend extended video content. With the data, benchmark, and model, we
successfully build MM-Ego, an egocentric multimodal LLM that shows powerful
performance on egocentric video understanding."
Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Retrieval-Augmented Generation (RAG), while effective in integrating external
knowledge to address the limitations of large language models (LLMs), can be
undermined by imperfect retrieval, which may introduce irrelevant, misleading,
or even malicious information. Despite its importance, previous studies have
rarely explored the behavior of RAG through joint analysis on how errors from
imperfect retrieval attribute and propagate, and how potential conflicts arise
between the LLMs' internal knowledge and external sources. We find that
imperfect retrieval augmentation might be inevitable and quite harmful, through
controlled analysis under realistic conditions. We identify the knowledge
conflicts between LLM-internal and external knowledge from retrieval as a
bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs
resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach
that adaptively elicits essential information from LLMs' internal knowledge,
iteratively consolidates internal and external knowledge with source-awareness,
and finalizes the answer according to information reliability. Our experiments
using Gemini and Claude demonstrate that Astute RAG significantly outperforms
previous robustness-enhanced RAG methods. Notably, Astute RAG is the only
approach that matches or exceeds the performance of LLMs without RAG under
worst-case scenarios. Further analysis reveals that Astute RAG effectively
resolves knowledge conflicts, improving the reliability and trustworthiness of
RAG systems."
Do better language models have crisper vision?,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"How well do text-only Large Language Models (LLMs) grasp the visual world? As
LLMs are increasingly used in computer vision, addressing this question becomes
both fundamental and pertinent. However, existing studies have primarily
focused on limited scenarios, such as their ability to generate visual content
or cluster multimodal data. To this end, we propose the Visual Text
Representation Benchmark (ViTeRB) to isolate key properties that make language
models well-aligned with the visual world. With this, we identify large-scale
decoder-based LLMs as ideal candidates for representing text in vision-centric
contexts, counter to the current practice of utilizing text encoders. Building
on these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.
By leveraging precomputable frozen features from strong vision and language
models, ShareLock achieves an impressive 51% accuracy on ImageNet despite
utilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU
hour (or 10 hours including the precomputation of features) - orders of
magnitude less than prior methods. Code will be released."
Glider: Global and Local Instruction-Driven Expert Router,cs.LG,Machine Learning,2024-10-09,"The availability of performant pre-trained models has led to a proliferation
of fine-tuned expert models that are specialized to particular domains. This
has enabled the creation of powerful and adaptive routing-based ""Model
MoErging"" methods with the goal of using expert modules to create an aggregate
system with improved performance or generalization. However, existing MoErging
methods often prioritize generalization to unseen tasks at the expense of
performance on held-in tasks, which limits its practical applicability in
real-world deployment scenarios. We observe that current token-level routing
mechanisms neglect the global semantic context of the input task. This
token-wise independence hinders effective expert selection for held-in tasks,
as routing decisions fail to incorporate the semantic properties of the task.
To address this, we propose, Global and Local Instruction Driven Expert Router
(GLIDER) that integrates a multi-scale routing mechanism, encompassing a
semantic global router and a learned local router. The global router leverages
LLM's advanced reasoning capabilities for semantic-related contexts to enhance
expert selection. Given the input query and LLM, the router generates semantic
task instructions that guide the retrieval of the most relevant experts across
all layers. This global guidance is complemented by a local router that
facilitates token-level routing decisions within each module, enabling finer
control and enhanced performance on unseen tasks. Our experiments using
T5-based models for T0 and FLAN tasks demonstrate that GLIDER achieves
substantially improved held-in performance while maintaining strong
generalization on held-out tasks. We also perform ablations experiments to dive
deeper into the components of GLIDER. Our experiments highlight the importance
of our multi-scale routing that leverages LLM-driven semantic reasoning for
MoErging methods."
IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made
notable strides in compositional text-to-image generation. However, these
methods typically exhibit distinct strengths for compositional generation, with
some excelling in handling attribute binding and others in spatial
relationships. This disparity highlights the need for an approach that can
leverage the complementary strengths of various models to comprehensively
improve the composition capability. To this end, we introduce IterComp, a novel
framework that aggregates composition-aware model preferences from multiple
models and employs an iterative feedback learning approach to enhance
compositional generation. Specifically, we curate a gallery of six powerful
open-source diffusion models and evaluate their three key compositional
metrics: attribute binding, spatial relationships, and non-spatial
relationships. Based on these metrics, we develop a composition-aware model
preference dataset comprising numerous image-rank pairs to train
composition-aware reward models. Then, we propose an iterative feedback
learning method to enhance compositionality in a closed-loop manner, enabling
the progressive self-refinement of both the base diffusion model and reward
models over multiple iterations. Theoretical proof demonstrates the
effectiveness and extensive experiments show our significant superiority over
previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category
object composition and complex semantic alignment. IterComp opens new research
avenues in reward feedback learning for diffusion models and compositional
generation. Code: https://github.com/YangLing0818/IterComp"
VIRT: Vision Instructed Transformer for Robotic Manipulation,cs.RO,Robotics,2024-10-09,"Robotic manipulation, owing to its multi-modal nature, often faces
significant training ambiguity, necessitating explicit instructions to clearly
delineate the manipulation details in tasks. In this work, we highlight that
vision instruction is naturally more comprehensible to recent robotic policies
than the commonly adopted text instruction, as these policies are born with
some vision understanding ability like human infants. Building on this premise
and drawing inspiration from cognitive science, we introduce the robotic
imagery paradigm, which realizes large-scale robotic data pre-training without
text annotations. Additionally, we propose the robotic gaze strategy that
emulates the human eye gaze mechanism, thereby guiding subsequent actions and
focusing the attention of the policy on the manipulated object. Leveraging
these innovations, we develop VIRT, a fully Transformer-based policy. We design
comprehensive tasks using both a physical robot and simulated environments to
assess the efficacy of VIRT. The results indicate that VIRT can complete very
competitive tasks like ``opening the lid of a tightly sealed bottle'', and the
proposed techniques boost the success rates of the baseline policy on diverse
challenging tasks from nearly 0% to more than 65%."
One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation,cs.LG,Machine Learning,2024-10-09,"Foundation models (FMs) are pre-trained on large-scale datasets and then
fine-tuned on a downstream task for a specific application. The most successful
and most commonly used fine-tuning method is to update the pre-trained weights
via a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are
usually initialized at random with a uniform rank distribution across model
weights. Recent works focus on weight-driven initialization or learning of
adaptive ranks during training. Both approaches have only been investigated in
isolation, resulting in slow convergence or a uniform rank distribution, in
turn leading to sub-optimal performance. We propose to enhance LoRA by
initializing the new weights in a data-driven manner by computing singular
value decomposition on minibatches of activation vectors. Then, we initialize
the LoRA matrices with the obtained right-singular vectors and re-distribute
ranks among all weight matrices to explain the maximal amount of variance and
continue the standard LoRA fine-tuning procedure. This results in our new
method Explained Variance Adaptation (EVA). We apply EVA to a variety of
fine-tuning tasks ranging from language generation and understanding to image
classification and reinforcement learning. EVA exhibits faster convergence than
competitors and attains the highest average score across a multitude of tasks
per domain."
Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"We present the Modality Integration Rate (MIR), an effective, robust, and
generalized metric to indicate the multi-modal pre-training quality of Large
Vision Language Models (LVLMs). Large-scale pre-training plays a critical role
in building capable LVLMs, while evaluating its training quality without the
costly supervised fine-tuning stage is under-explored. Loss, perplexity, and
in-context evaluation results are commonly used pre-training metrics for Large
Language Models (LLMs), while we observed that these metrics are less
indicative when aligning a well-trained LLM with a new modality. Due to the
lack of proper metrics, the research of LVLMs in the critical pre-training
stage is hindered greatly, including the training data choice, efficient module
design, etc. In this paper, we propose evaluating the pre-training quality from
the inter-modal distribution distance perspective and present MIR, the Modality
Integration Rate, which is 1) \textbf{Effective} to represent the pre-training
quality and show a positive relation with the benchmark performance after
supervised fine-tuning. 2) \textbf{Robust} toward different training/evaluation
data. 3) \textbf{Generalize} across training configurations and architecture
choices. We conduct a series of pre-training experiments to explore the
effectiveness of MIR and observe satisfactory results that MIR is indicative
about training data selection, training strategy schedule, and model
architecture design to get better pre-training results. We hope MIR could be a
helpful metric for building capable LVLMs and inspire the following research
about modality alignment in different areas. Our code is at:
https://github.com/shikiw/Modality-Integration-Rate."
Sylber: Syllabic Embedding Representation of Speech from Raw Audio,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Syllables are compositional units of spoken language that play a crucial role
in human speech perception and production. However, current neural speech
representations lack structure, resulting in dense token sequences that are
costly to process. To bridge this gap, we propose a new model, Sylber, that
produces speech representations with clean and robust syllabic structure.
Specifically, we propose a self-supervised model that regresses features on
syllabic segments distilled from a teacher model which is an exponential moving
average of the model in training. This results in a highly structured
representation of speech features, offering three key benefits: 1) a fast,
linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization
with an average of 4.27 tokens per second, and 3) syllabic units better suited
for lexical and syntactic understanding. We also train token-to-speech
generative models with our syllabic units and show that fully intelligible
speech can be reconstructed from these tokens. Lastly, we observe that
categorical perception, a linguistic phenomenon of speech perception, emerges
naturally in our model, making the embedding space more categorical and sparse
than previous self-supervised learning approaches. Together, we present a novel
self-supervised approach for representing speech as syllables, with significant
potential for efficient speech tokenization and spoken language modeling."
Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"We aim to evaluate Large Language Models (LLMs) for embodied decision making.
While a significant body of work has been leveraging LLMs for decision making
in embodied environments, we still lack a systematic understanding of their
performance because they are usually applied in different domains, for
different purposes, and built based on different inputs and outputs.
Furthermore, existing evaluations tend to rely solely on a final success rate,
making it difficult to pinpoint what ability is missing in LLMs and where the
problem lies, which in turn blocks embodied agents from leveraging LLMs
effectively and selectively. To address these limitations, we propose a
generalized interface (Embodied Agent Interface) that supports the
formalization of various types of tasks and input-output specifications of
LLM-based modules. Specifically, it allows us to unify 1) a broad set of
embodied decision-making tasks involving both state and temporally extended
goals, 2) four commonly-used LLM-based modules for decision making: goal
interpretation, subgoal decomposition, action sequencing, and transition
modeling, and 3) a collection of fine-grained metrics which break down
evaluation into various types of errors, such as hallucination errors,
affordance errors, various types of planning errors, etc. Overall, our
benchmark offers a comprehensive assessment of LLMs' performance for different
subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI
systems, and providing insights for effective and selective use of LLMs in
embodied decision making."
AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Recent advancements in diffusion models have led to significant improvements
in the generation and animation of 4D full-body human-object interactions
(HOI). Nevertheless, existing methods primarily focus on SMPL-based motion
generation, which is limited by the scarcity of realistic large-scale
interaction data. This constraint affects their ability to create everyday HOI
scenes. This paper addresses this challenge using a zero-shot approach with a
pre-trained diffusion model. Despite this potential, achieving our goals is
difficult due to the diffusion model's lack of understanding of ''where'' and
''how'' objects interact with the human body. To tackle these issues, we
introduce AvatarGO, a novel framework designed to generate animatable 4D HOI
scenes directly from textual inputs. Specifically, 1) for the ''where''
challenge, we propose LLM-guided contact retargeting, which employs Lang-SAM to
identify the contact body part from text prompts, ensuring precise
representation of human-object spatial relations. 2) For the ''how'' challenge,
we introduce correspondence-aware motion optimization that constructs motion
fields for both human and object models using the linear blend skinning
function from SMPL-X. Our framework not only generates coherent compositional
motions, but also exhibits greater robustness in handling penetration issues.
Extensive experiments with existing methods validate AvatarGO's superior
generation and animation capabilities on a variety of human-object pairs and
diverse poses. As the first attempt to synthesize 4D avatars with object
interactions, we hope AvatarGO could open new doors for human-centric 4D
content creation."
Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"In this work, we address the problem of large language model (LLM)
unlearning, aiming to remove unwanted data influences and associated model
capabilities (e.g., copyrighted data or harmful content generation) while
preserving essential model utilities, without the need for retraining from
scratch. Despite the growing need for LLM unlearning, a principled optimization
framework remains lacking. To this end, we revisit the state-of-the-art
approach, negative preference optimization (NPO), and identify the issue of
reference model bias, which could undermine NPO's effectiveness, particularly
when unlearning forget data of varying difficulty. Given that, we propose a
simple yet effective unlearning optimization framework, called SimNPO, showing
that 'simplicity' in removing the reliance on a reference model (through the
lens of simple preference optimization) benefits unlearning. We also provide
deeper insights into SimNPO's advantages, supported by analysis using mixtures
of Markov chains. Furthermore, we present extensive experiments validating
SimNPO's superiority over existing unlearning baselines in benchmarks like TOFU
and MUSE, and robustness against relearning attacks. Codes are available at
https://github.com/OPTML-Group/Unlearn-Simple."
Quanda: An Interpretability Toolkit for Training Data Attribution Evaluation and Beyond,cs.LG,Machine Learning,2024-10-09,"In recent years, training data attribution (TDA) methods have emerged as a
promising direction for the interpretability of neural networks. While research
around TDA is thriving, limited effort has been dedicated to the evaluation of
attributions. Similar to the development of evaluation metrics for traditional
feature attribution approaches, several standalone metrics have been proposed
to evaluate the quality of TDA methods across various contexts. However, the
lack of a unified framework that allows for systematic comparison limits trust
in TDA methods and stunts their widespread adoption. To address this research
gap, we introduce Quanda, a Python toolkit designed to facilitate the
evaluation of TDA methods. Beyond offering a comprehensive set of evaluation
metrics, Quanda provides a uniform interface for seamless integration with
existing TDA implementations across different repositories, thus enabling
systematic benchmarking. The toolkit is user-friendly, thoroughly tested,
well-documented, and available as an open-source library on PyPi and under
https://github.com/dilyabareeva/quanda."
InstructG2I: Synthesizing Images from Multimodal Attributed Graphs,cs.AI,Artificial Intelligence,2024-10-09,"In this paper, we approach an overlooked yet critical task Graph2Image:
generating images from multimodal attributed graphs (MMAGs). This task poses
significant challenges due to the explosion in graph size, dependencies among
graph entities, and the need for controllability in graph conditions. To
address these challenges, we propose a graph context-conditioned diffusion
model called InstructG2I. InstructG2I first exploits the graph structure and
multimodal information to conduct informative neighbor sampling by combining
personalized page rank and re-ranking based on vision-language features. Then,
a Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary
set of graph prompts to guide the denoising process of diffusion. Finally, we
propose graph classifier-free guidance, enabling controllable generation by
varying the strength of graph guidance and multiple connected edges to a node.
Extensive experiments conducted on three datasets from different domains
demonstrate the effectiveness and controllability of our approach. The code is
available at https://github.com/PeterGriffinJin/InstructG2I."
Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Recent advances in diffusion models have demonstrated exceptional
capabilities in image and video generation, further improving the effectiveness
of 4D synthesis. Existing 4D generation methods can generate high-quality 4D
objects or scenes based on user-friendly conditions, benefiting the gaming and
video industries. However, these methods struggle to synthesize significant
object deformation of complex 4D transitions and interactions within scenes. To
address this challenge, we propose Trans4D, a novel text-to-4D synthesis
framework that enables realistic complex scene transitions. Specifically, we
first use multi-modal large language models (MLLMs) to produce a physic-aware
scene description for 4D scene initialization and effective transition timing
planning. Then we propose a geometry-aware 4D transition network to realize a
complex scene-level 4D transition based on the plan, which involves expressive
geometrical object deformation. Extensive experiments demonstrate that Trans4D
consistently outperforms existing state-of-the-art methods in generating 4D
scenes with accurate and high-quality transitions, validating its
effectiveness. Code: https://github.com/YangLing0818/Trans4D"
CHASE: Learning Convex Hull Adaptive Shift for Skeleton-based Multi-Entity Action Recognition,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Skeleton-based multi-entity action recognition is a challenging task aiming
to identify interactive actions or group activities involving multiple diverse
entities. Existing models for individuals often fall short in this task due to
the inherent distribution discrepancies among entity skeletons, leading to
suboptimal backbone optimization. To this end, we introduce a Convex Hull
Adaptive Shift based multi-Entity action recognition method (CHASE), which
mitigates inter-entity distribution gaps and unbiases subsequent backbones.
Specifically, CHASE comprises a learnable parameterized network and an
auxiliary objective. The parameterized network achieves plausible,
sample-adaptive repositioning of skeleton sequences through two key components.
First, the Implicit Convex Hull Constrained Adaptive Shift ensures that the new
origin of the coordinate system is within the skeleton convex hull. Second, the
Coefficient Learning Block provides a lightweight parameterization of the
mapping from skeleton sequences to their specific coefficients in convex
combinations. Moreover, to guide the optimization of this network for
discrepancy minimization, we propose the Mini-batch Pair-wise Maximum Mean
Discrepancy as the additional objective. CHASE operates as a sample-adaptive
normalization method to mitigate inter-entity distribution discrepancies,
thereby reducing data bias and improving the subsequent classifier's
multi-entity action recognition performance. Extensive experiments on six
datasets, including NTU Mutual 11/26, H2O, Assembly101, Collective Activity and
Volleyball, consistently verify our approach by seamlessly adapting to
single-entity backbones and boosting their performance in multi-entity
scenarios. Our code is publicly available at https://github.com/Necolizer/CHASE ."
Towards Interpreting Visual Information Processing in Vision-Language Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Vision-Language Models (VLMs) are powerful tools for processing and
understanding text and images. We study the processing of visual tokens in the
language model component of LLaVA, a prominent VLM. Our approach focuses on
analyzing the localization of object information, the evolution of visual token
representations across layers, and the mechanism of integrating visual
information for predictions. Through ablation studies, we demonstrated that
object identification accuracy drops by over 70\% when object-specific tokens
are removed. We observed that visual token representations become increasingly
interpretable in the vocabulary space across layers, suggesting an alignment
with textual tokens corresponding to image content. Finally, we found that the
model extracts object information from these refined representations at the
last token position for prediction, mirroring the process in text-only language
models for factual association tasks. These findings provide crucial insights
into how VLMs process and integrate visual information, bridging the gap
between our understanding of language and vision models, and paving the way for
more interpretable and controllable multimodal systems."
Taking a turn for the better: Conversation redirection throughout the course of mental-health therapy,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Mental-health therapy involves a complex conversation flow in which patients
and therapists continuously negotiate what should be talked about next. For
example, therapists might try to shift the conversation's direction to keep the
therapeutic process on track and avoid stagnation, or patients might push the
discussion towards issues they want to focus on.
  How do such patient and therapist redirections relate to the development and
quality of their relationship? To answer this question, we introduce a
probabilistic measure of the extent to which a certain utterance immediately
redirects the flow of the conversation, accounting for both the intention and
the actual realization of such a change. We apply this new measure to
characterize the development of patient-therapist relationships over multiple
sessions in a very large, widely-used online therapy platform. Our analysis
reveals that (1) patient control of the conversation's direction generally
increases relative to that of the therapist as their relationship progresses;
and (2) patients who have less control in the first few sessions are
significantly more likely to eventually express dissatisfaction with their
therapist and terminate the relationship."
Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"One essential advantage of recurrent neural networks (RNNs) over
transformer-based language models is their linear computational complexity
concerning the sequence length, which makes them much faster in handling long
sequences during inference. However, most publicly available RNNs (e.g., Mamba
and RWKV) are trained on sequences with less than 10K tokens, and their
effectiveness in longer contexts remains largely unsatisfying so far. In this
paper, we study the cause of the inability to process long context for RNNs and
suggest critical mitigations. We examine two practical concerns when applying
state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to
inputs longer than the training length and (2) the upper bound of memory
capacity. Addressing the first concern, we first investigate *state collapse*
(SC), a phenomenon that causes severe performance degradation on sequence
lengths not encountered during training. With controlled experiments, we
attribute this to overfitting due to the recurrent state being
overparameterized for the training length. For the second concern, we train a
series of Mamba-2 models on long documents to empirically estimate the
recurrent state capacity in language modeling and passkey retrieval. Then,
three SC mitigation methods are proposed to improve Mamba-2's length
generalizability, allowing the model to process more than 1M tokens without SC.
We also find that the recurrent state capacity in passkey retrieval scales
exponentially to the state size, and we empirically train a Mamba-2 370M with
near-perfect passkey retrieval accuracy on 256K context length. This suggests a
promising future for RNN-based long-context modeling."
Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and
MT-Bench, have become popular for evaluating language models due to their
cost-effectiveness and scalability compared to human evaluation. Achieving high
win rates on these benchmarks can significantly boost the promotional impact of
newly released language models. This promotional benefit may motivate tricks,
such as manipulating model output length or style to game win rates, even
though several mechanisms have been developed to control length and disentangle
style to reduce gameability. Nonetheless, we show that even a ""null model"" that
always outputs a constant response (irrelevant to input instructions) can cheat
automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on
AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.
Moreover, the crafted cheating outputs are transferable because we assume that
the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are
private and cannot be accessed. While our experiments are primarily
proof-of-concept, an adversary could use LLMs to generate more imperceptible
cheating responses, unethically benefiting from high win rates and promotional
impact. Our findings call for the development of anti-cheating mechanisms for
reliable automatic benchmarks. The code is available at
https://github.com/sail-sg/Cheating-LLM-Benchmarks."
EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Recent advancements in generation models have showcased remarkable
capabilities in generating fantastic content. However, most of them are trained
on proprietary high-quality data, and some models withhold their parameters and
only provide accessible application programming interfaces (APIs), limiting
their benefits for downstream tasks. To explore the feasibility of training a
text-to-image generation model comparable to advanced models using publicly
available resources, we introduce EvolveDirector. This framework interacts with
advanced models through their public APIs to obtain text-image data pairs to
train a base model. Our experiments with extensive data indicate that the model
trained on generated data of the advanced model can approximate its generation
capability. However, it requires large-scale samples of 10 million or more.
This incurs significant expenses in time, computational resources, and
especially the costs associated with calling fee-based APIs. To address this
problem, we leverage pre-trained large vision-language models (VLMs) to guide
the evolution of the base model. VLM continuously evaluates the base model
during training and dynamically updates and refines the training dataset by the
discrimination, expansion, deletion, and mutation operations. Experimental
results show that this paradigm significantly reduces the required data volume.
Furthermore, when approaching multiple advanced models, EvolveDirector can
select the best samples generated by them to learn powerful and balanced
abilities. The final trained model Edgen is demonstrated to outperform these
advanced models. The code and model weights are available at
https://github.com/showlab/EvolveDirector."
Mental Disorders Detection in the Era of Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"This paper compares the effectiveness of traditional machine learning
methods, encoder-based models, and large language models (LLMs) on the task of
detecting depression and anxiety. Five datasets were considered, each differing
in format and the method used to define the target pathology class. We tested
AutoML models based on linguistic features, several variations of encoder-based
Transformers such as BERT, and state-of-the-art LLMs as pathology
classification models. The results demonstrated that LLMs outperform
traditional methods, particularly on noisy and small datasets where training
examples vary significantly in text length and genre. However, psycholinguistic
features and encoder-based models can achieve performance comparable to
language models when trained on texts from individuals with clinically
confirmed depression, highlighting their potential effectiveness in targeted
clinical applications."
Exploring the Readiness of Prominent Small Language Models for the Democratization of Financial Literacy,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"The use of small language models (SLMs), herein defined as models with less
than three billion parameters, is increasing across various domains and
applications. Due to their ability to run on more accessible hardware and
preserve user privacy, SLMs possess the potential to democratize access to
language models for individuals of different socioeconomic status and with
different privacy preferences. This study assesses several state-of-the-art
SLMs (e.g., Apple's OpenELM, Microsoft's Phi, Google's Gemma, and the Tinyllama
project) for use in the financial domain to support the development of
financial literacy LMs. Democratizing access to quality financial information
for those who are financially under educated is greatly needed in society,
particularly as new financial markets and products emerge and participation in
financial markets increases due to ease of access. We are the first to examine
the use of open-source SLMs to democratize access to financial question
answering capabilities for individuals and students. To this end, we provide an
analysis of the memory usage, inference time, similarity comparisons to
ground-truth answers, and output readability of prominent SLMs to determine
which models are most accessible and capable of supporting access to financial
information. We analyze zero-shot and few-shot learning variants of the models.
The results suggest that some off-the-shelf SLMs merit further exploration and
fine-tuning to prepare them for individual use, while others may have limits to
their democratization."
Personalized Visual Instruction Tuning,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Recent advancements in multimodal large language models (MLLMs) have
demonstrated significant progress; however, these models exhibit a notable
limitation, which we refer to as ""face blindness"". Specifically, they can
engage in general conversations but fail to conduct personalized dialogues
targeting at specific individuals. This deficiency hinders the application of
MLLMs in personalized settings, such as tailored visual assistants on mobile
devices, or domestic robots that need to recognize members of the family. In
this paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel
data curation and training framework designed to enable MLLMs to identify
target individuals within an image and engage in personalized and coherent
dialogues. Our approach involves the development of a sophisticated pipeline
that autonomously generates training data containing personalized
conversations. This pipeline leverages the capabilities of various visual
experts, image generation models, and (multi-modal) large language models. To
evaluate the personalized potential of MLLMs, we present a benchmark called
P-Bench, which encompasses various question types with different levels of
difficulty. The experiments demonstrate a substantial personalized performance
enhancement after fine-tuning with our curated dataset."
VHELM: A Holistic Evaluation of Vision Language Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Current benchmarks for assessing vision-language models (VLMs) often focus on
their perception or problem-solving capabilities and neglect other critical
aspects such as fairness, multilinguality, or toxicity. Furthermore, they
differ in their evaluation procedures and the scope of the evaluation, making
it difficult to compare models. To address these issues, we extend the HELM
framework to VLMs to present the Holistic Evaluation of Vision Language Models
(VHELM). VHELM aggregates various datasets to cover one or more of the 9
aspects: visual perception, knowledge, reasoning, bias, fairness,
multilinguality, robustness, toxicity, and safety. In doing so, we produce a
comprehensive, multi-dimensional view of the capabilities of the VLMs across
these important factors. In addition, we standardize the standard inference
parameters, methods of prompting, and evaluation metrics to enable fair
comparisons across models. Our framework is designed to be lightweight and
automatic so that evaluation runs are cheap and fast. Our initial run evaluates
22 VLMs on 21 existing datasets to provide a holistic snapshot of the models.
We uncover new key findings, such as the fact that efficiency-focused models
(e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than
their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark
but not when evaluated on the other aspects. For transparency, we release the
raw model generations and complete results on our website
(https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living
benchmark, and we hope to continue adding new datasets and models over time."
I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"As Large Language Model (LLM)-based agents become increasingly autonomous and
will more freely interact with each other, studying interactions between them
becomes crucial to anticipate emergent phenomena and potential risks. Drawing
inspiration from the widely popular Stanford Prison Experiment, we contribute
to this line of research by studying interaction patterns of LLM agents in a
context characterized by strict social hierarchy. We do so by specifically
studying two types of phenomena: persuasion and anti-social behavior in
simulated scenarios involving a guard and a prisoner agent who seeks to achieve
a specific goal (i.e., obtaining additional yard time or escape from prison).
Leveraging 200 experimental scenarios for a total of 2,000 machine-machine
conversations across five different popular LLMs, we provide a set of
noteworthy findings. We first document how some models consistently fail in
carrying out a conversation in our multi-agent setup where power dynamics are
at play. Then, for the models that were able to engage in successful
interactions, we empirically show how the goal that an agent is set to achieve
impacts primarily its persuasiveness, while having a negligible effect with
respect to the agent's anti-social behavior. Third, we highlight how agents'
personas, and particularly the guard's personality, drive both the likelihood
of successful persuasion from the prisoner and the emergence of anti-social
behaviors. Fourth, we show that even without explicitly prompting for specific
personalities, anti-social behavior emerges by simply assigning agents' roles.
These results bear implications for the development of interactive LLM agents
as well as the debate on their societal impact."
"Continual Learning: Less Forgetting, More OOD Generalization via Adaptive Contrastive Replay",cs.LG,Machine Learning,2024-10-09,"Machine learning models often suffer from catastrophic forgetting of
previously learned knowledge when learning new classes. Various methods have
been proposed to mitigate this issue. However, rehearsal-based learning, which
retains samples from previous classes, typically achieves good performance but
tends to memorize specific instances, struggling with Out-of-Distribution (OOD)
generalization. This often leads to high forgetting rates and poor
generalization. Surprisingly, the OOD generalization capabilities of these
methods have been largely unexplored. In this paper, we highlight this issue
and propose a simple yet effective strategy inspired by contrastive learning
and data-centric principles to address it. We introduce Adaptive Contrastive
Replay (ACR), a method that employs dual optimization to simultaneously train
both the encoder and the classifier. ACR adaptively populates the replay buffer
with misclassified samples while ensuring a balanced representation of classes
and tasks. By refining the decision boundary in this way, ACR achieves a
balance between stability and plasticity. Our method significantly outperforms
previous approaches in terms of OOD generalization, achieving an improvement of
13.41\% on Split CIFAR-100, 9.91\% on Split Mini-ImageNet, and 5.98\% on Split
Tiny-ImageNet."
Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Multi-hop reasoning, which requires multi-step reasoning based on the
supporting documents within a given context, remains challenging for large
language models (LLMs). LLMs often struggle to filter out irrelevant documents
within the context, and their performance is sensitive to the position of
supporting documents within that context. In this paper, we identify an
additional challenge: LLMs' performance is also sensitive to the order in which
the supporting documents are presented. We refer to this as the misordered
context problem. To address this issue, we propose a simple yet effective
method called context repetition (CoRe), which involves prompting the model by
repeatedly presenting the context to ensure the supporting documents are
presented in the optimal order for the model. Using CoRe, we improve the F1
score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p
on a synthetic task. Additionally, CoRe helps mitigate the well-known
""lost-in-the-middle"" problem in LLMs and can be effectively combined with
retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning."
Identifying and Addressing Delusions for Target-Directed Decision-Making,cs.AI,Artificial Intelligence,2024-10-09,"We are interested in target-directed agents, which produce targets during
decision-time planning, to guide their behaviors and achieve better
generalization during evaluation. Improper training of these agents can result
in delusions: the agent may come to hold false beliefs about the targets, which
cannot be properly rejected, leading to unwanted behaviors and damaging
out-of-distribution generalization. We identify different types of delusions by
using intuitive examples in carefully controlled environments, and investigate
their causes. We demonstrate how delusions can be addressed for agents trained
by hindsight relabeling, a mainstream approach in for training target-directed
RL agents. We validate empirically the effectiveness of the proposed solutions
in correcting delusional behaviors and improving out-of-distribution
generalization."
MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"We introduce MLE-bench, a benchmark for measuring how well AI agents perform
at machine learning engineering. To this end, we curate 75 ML
engineering-related competitions from Kaggle, creating a diverse set of
challenging tasks that test real-world ML engineering skills such as training
models, preparing datasets, and running experiments. We establish human
baselines for each competition using Kaggle's publicly available leaderboards.
We use open-source agent scaffolds to evaluate several frontier language models
on our benchmark, finding that the best-performing setup--OpenAI's o1-preview
with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in
16.9% of competitions. In addition to our main results, we investigate various
forms of resource scaling for AI agents and the impact of contamination from
pre-training. We open-source our benchmark code (github.com/openai/mle-bench/)
to facilitate future research in understanding the ML engineering capabilities
of AI agents."
"LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning",cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Language plays a vital role in the realm of human motion. Existing methods
have largely depended on CLIP text embeddings for motion generation, yet they
fall short in effectively aligning language and motion due to CLIP's
pretraining on static image-text pairs. This work introduces LaMP, a novel
Language-Motion Pretraining model, which transitions from a language-vision to
a more suitable language-motion latent space. It addresses key limitations by
generating motion-informative text embeddings, significantly enhancing the
relevance and semantics of generated motion sequences. With LaMP, we advance
three key tasks: text-to-motion generation, motion-text retrieval, and motion
captioning through aligned language-motion representation learning. For
generation, we utilize LaMP to provide the text condition instead of CLIP, and
an autoregressive masked prediction is designed to achieve mask modeling
without rank collapse in transformers. For retrieval, motion features from
LaMP's motion transformer interact with query tokens to retrieve text features
from the text transformer, and vice versa. For captioning, we finetune a large
language model with the language-informative motion features to develop a
strong motion captioning model. In addition, we introduce the LaMP-BertScore
metric to assess the alignment of generated motions with textual descriptions.
Extensive experimental results on multiple datasets demonstrate substantial
improvements over previous methods across all three tasks. The code of our
method will be made public."
"Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology",cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Developing agents capable of navigating to a target location based on
language instructions and visual information, known as vision-language
navigation (VLN), has attracted widespread interest. Most research has focused
on ground-based agents, while UAV-based VLN remains relatively underexplored.
Recent efforts in UAV vision-language navigation predominantly adopt
ground-based VLN settings, relying on predefined discrete action spaces and
neglecting the inherent disparities in agent movement dynamics and the
complexity of navigation tasks between ground and aerial environments. To
address these disparities and challenges, we propose solutions from three
perspectives: platform, benchmark, and methodology. To enable realistic UAV
trajectory simulation in VLN tasks, we propose the OpenUAV platform, which
features diverse environments, realistic flight control, and extensive
algorithmic support. We further construct a target-oriented VLN dataset
consisting of approximately 12k trajectories on this platform, serving as the
first dataset specifically designed for realistic UAV VLN tasks. To tackle the
challenges posed by complex aerial environments, we propose an assistant-guided
UAV object search benchmark called UAV-Need-Help, which provides varying levels
of guidance information to help UAVs better accomplish realistic VLN tasks. We
also propose a UAV navigation LLM that, given multi-view images, task
descriptions, and assistant instructions, leverages the multimodal
understanding capabilities of the MLLM to jointly process visual and textual
information, and performs hierarchical trajectory generation. The evaluation
results of our method significantly outperform the baseline models, while there
remains a considerable gap between our results and those achieved by human
operators, underscoring the challenge presented by the UAV-Need-Help task."
Stanceformer: Target-Aware Transformer for Stance Detection,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"The task of Stance Detection involves discerning the stance expressed in a
text towards a specific subject or target. Prior works have relied on existing
transformer models that lack the capability to prioritize targets effectively.
Consequently, these models yield similar performance regardless of whether we
utilize or disregard target information, undermining the task's significance.
To address this challenge, we introduce Stanceformer, a target-aware
transformer model that incorporates enhanced attention towards the targets
during both training and inference. Specifically, we design a \textit{Target
Awareness} matrix that increases the self-attention scores assigned to the
targets. We demonstrate the efficacy of the Stanceformer with various
BERT-based models, including state-of-the-art models and Large Language Models
(LLMs), and evaluate its performance across three stance detection datasets,
alongside a zero-shot dataset. Our approach Stanceformer not only provides
superior performance but also generalizes even to other domains, such as
Aspect-based Sentiment Analysis. We make the code publicly
available.\footnote{\scriptsize\url{https://github.com/kgarg8/Stanceformer}}"
JPEG Inspired Deep Learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Although it is traditionally believed that lossy image compression, such as
JPEG compression, has a negative impact on the performance of deep neural
networks (DNNs), it is shown by recent works that well-crafted JPEG compression
can actually improve the performance of deep learning (DL). Inspired by this,
we propose JPEG-DL, a novel DL framework that prepends any underlying DNN
architecture with a trainable JPEG compression layer. To make the quantization
operation in JPEG compression trainable, a new differentiable soft quantizer is
employed at the JPEG layer, and then the quantization operation and underlying
DNN are jointly trained. Extensive experiments show that in comparison with the
standard DL, JPEG-DL delivers significant accuracy improvements across various
datasets and model architectures while enhancing robustness against adversarial
attacks. Particularly, on some fine-grained image classification datasets,
JPEG-DL can increase prediction accuracy by as much as 20.9%. Our code is
available on https://github.com/JpegInspiredDl/JPEG-Inspired-DL.git."
FlowBotHD: History-Aware Diffuser Handling Ambiguities in Articulated Objects Manipulation,cs.RO,Robotics,2024-10-09,"We introduce a novel approach to manipulate articulated objects with
ambiguities, such as opening a door, in which multi-modality and occlusions
create ambiguities about the opening side and direction. Multi-modality occurs
when the method to open a fully closed door (push, pull, slide) is uncertain,
or the side from which it should be opened is uncertain. Occlusions further
obscure the door's shape from certain angles, creating further ambiguities
during the occlusion. To tackle these challenges, we propose a history-aware
diffusion network that models the multi-modal distribution of the articulated
object and uses history to disambiguate actions and make stable predictions
under occlusions. Experiments and analysis demonstrate the state-of-art
performance of our method and specifically improvements in ambiguity-caused
failure modes. Our project website is available at
https://flowbothd.github.io/."
MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Scientific discovery contributes largely to human society's prosperity, and
recent progress shows that LLMs could potentially catalyze this process.
However, it is still unclear whether LLMs can discover novel and valid
hypotheses in chemistry. In this work, we investigate this central research
question: Can LLMs automatically discover novel and valid chemistry research
hypotheses given only a chemistry research background (consisting of a research
question and/or a background survey), without limitation on the domain of the
research question? After extensive discussions with chemistry experts, we
propose an assumption that a majority of chemistry hypotheses can be resulted
from a research background and several inspirations. With this key insight, we
break the central question into three smaller fundamental questions. In brief,
they are: (1) given a background question, whether LLMs can retrieve good
inspirations; (2) with background and inspirations, whether LLMs can lead to
hypothesis; and (3) whether LLMs can identify good hypotheses to rank them
higher. To investigate these questions, we construct a benchmark consisting of
51 chemistry papers published in Nature, Science, or a similar level in 2024
(all papers are only available online since 2024). Every paper is divided by
chemistry PhD students into three components: background, inspirations, and
hypothesis. The goal is to rediscover the hypothesis, given only the background
and a large randomly selected chemistry literature corpus consisting the ground
truth inspiration papers, with LLMs trained with data up to 2023. We also
develop an LLM-based multi-agent framework that leverages the assumption,
consisting of three stages reflecting the three smaller questions. The proposed
method can rediscover many hypotheses with very high similarity with the ground
truth ones, covering the main innovations."
Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning,cs.LG,Machine Learning,2024-10-09,"Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world
systems, yet leveraging large language models (LLMs) for TAGs presents unique
challenges due to the gap between sequential text processing and
graph-structured data. We introduce AskGNN, a novel approach that bridges this
gap by leveraging In-Context Learning (ICL) to integrate graph data and
task-specific information into LLMs. AskGNN employs a Graph Neural Network
(GNN)-powered structure-enhanced retriever to select labeled nodes across
graphs, incorporating complex graph structures and their supervision signals.
Our learning-to-retrieve algorithm optimizes the retriever to select example
nodes that maximize LLM performance on graph. Experiments across three tasks
and seven LLMs demonstrate AskGNN's superior effectiveness in graph task
performance, opening new avenues for applying LLMs to graph-structured data
without extensive fine-tuning."
Pixtral 12B,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.
Pixtral-12B is trained to understand both natural images and documents,
achieving leading performance on various multimodal benchmarks, surpassing a
number of larger models. Unlike many open-source models, Pixtral is also a
cutting-edge text model for its size, and does not compromise on natural
language performance to excel in multimodal tasks. Pixtral uses a new vision
encoder trained from scratch, which allows it to ingest images at their natural
resolution and aspect ratio. This gives users flexibility on the number of
tokens used to process an image. Pixtral is also able to process any number of
images in its long context window of 128K tokens. Pixtral 12B substanially
outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B).
It also outperforms much larger open models like Llama-3.2 90B while being 7x
smaller. We further contribute an open-source benchmark, MM-MT-Bench, for
evaluating vision-language models in practical scenarios, and provide detailed
analysis and code for standardized evaluation protocols for multimodal LLMs.
Pixtral-12B is released under Apache 2.0 license."
Retrieval-Augmented Decision Transformer: External Memory for In-context RL,cs.LG,Machine Learning,2024-10-09,"In-context learning (ICL) is the ability of a model to learn a new task by
observing a few exemplars in its context. While prevalent in NLP, this
capability has recently also been observed in Reinforcement Learning (RL)
settings. Prior in-context RL methods, however, require entire episodes in the
agent's context. Given that complex environments typically lead to long
episodes with sparse rewards, these methods are constrained to simple
environments with short episodes. To address these challenges, we introduce
Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external
memory mechanism to store past experiences from which it retrieves only
sub-trajectories relevant for the current situation. The retrieval component in
RA-DT does not require training and can be entirely domain-agnostic. We
evaluate the capabilities of RA-DT on grid-world environments, robotics
simulations, and procedurally-generated video games. On grid-worlds, RA-DT
outperforms baselines, while using only a fraction of their context length.
Furthermore, we illuminate the limitations of current in-context RL methods on
complex environments and discuss future directions. To facilitate future
research, we release datasets for four of the considered environments."
ReIFE: Re-evaluating Instruction-Following Evaluation,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"The automatic evaluation of instruction following typically involves using
large language models (LLMs) to assess response quality. However, there is a
lack of comprehensive evaluation of these LLM-based evaluators across two
dimensions: the base LLMs and the evaluation protocols. Therefore, we present a
thorough meta-evaluation of instruction following, including 25 base LLMs and
15 recently proposed evaluation protocols, on 4 human-annotated datasets,
assessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows
us to identify the best-performing base LLMs and evaluation protocols with a
high degree of robustness. Moreover, our large-scale evaluation reveals: (1)
Base LLM performance ranking remains largely consistent across evaluation
protocols, with less capable LLMs showing greater improvement from protocol
enhancements; (2) Robust evaluation of evaluation protocols requires many base
LLMs with varying capability levels, as protocol effectiveness can depend on
the base LLM used; (3) Evaluation results on different datasets are not always
consistent, so a rigorous evaluation requires multiple datasets with
distinctive features. We release our meta-evaluation suite ReIFE, which
provides the codebase and evaluation result collection for more than 500
LLM-evaluator configurations, to support future research in
instruction-following evaluation."
A Gentle Introduction and Tutorial on Deep Generative Models in Transportation Research,cs.LG,Machine Learning,2024-10-09,"Deep Generative Models (DGMs) have rapidly advanced in recent years, becoming
essential tools in various fields due to their ability to learn complex data
distributions and generate synthetic data. Their importance in transportation
research is increasingly recognized, particularly for applications like traffic
data generation, prediction, and feature extraction. This paper offers a
comprehensive introduction and tutorial on DGMs, with a focus on their
applications in transportation. It begins with an overview of generative
models, followed by detailed explanations of fundamental models, a systematic
review of the literature, and practical tutorial code to aid implementation.
The paper also discusses current challenges and opportunities, highlighting how
these models can be effectively utilized and further developed in
transportation research. This paper serves as a valuable reference, guiding
researchers and practitioners from foundational knowledge to advanced
applications of DGMs in transportation research."
Data Selection via Optimal Control for Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"This work investigates the selection of high-quality pre-training data from
massive corpora to enhance LMs' capabilities for downstream usage. We formulate
data selection as a generalized Optimal Control problem, which can be solved
theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of
necessary conditions that characterize the relationship between optimal data
selection and LM training dynamics. Based on these theoretical results, we
introduce PMP-based Data Selection (PDS), a framework that approximates optimal
data selection by solving the PMP conditions. In our experiments, we adopt PDS
to select data from CommmonCrawl and show that the PDS-selected corpus
accelerates the learning of LMs and constantly boosts their performance on a
wide range of downstream tasks across various model sizes. Moreover, the
benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by
the extrapolation of the test loss curves according to the Scaling Laws. PDS
also improves data utilization when the pre-training data is limited, by
reducing the data demand by 1.8 times, which mitigates the quick exhaustion of
available web-crawled corpora. Our code, data, and model checkpoints can be
found in https://github.com/microsoft/LMOps/tree/main/data_selection."
InAttention: Linear Context Scaling for Transformers,cs.LG,Machine Learning,2024-10-09,"VRAM requirements for transformer models scale quadratically with context
length due to the self-attention mechanism. In this paper we modify the
decoder-only transformer, replacing self-attention with InAttention, which
scales linearly with context length during inference by having tokens attend
only to initial states. Benchmarking shows that InAttention significantly
reduces VRAM usage during inference, enabling handling of long sequences on
consumer GPUs. We corroborate that fine-tuning extends context length
efficiently, improving performance on long sequences without high training
costs. InAttention offers a scalable solution for long-range dependencies in
transformer models, paving the way for further optimization."
TinyEmo: Scaling down Emotional Reasoning via Metric Projection,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"This paper introduces TinyEmo, a family of small multi-modal language models
for emotional reasoning and classification. Our approach features: (1) a
synthetic emotional instruct dataset for both pre-training and fine-tuning
stages, (2) a Metric Projector that delegates classification from the language
model allowing for more efficient training and inference, (3) a multi-modal
large language model (MM-LLM) for emotional reasoning, and (4) a semi-automated
framework for bias detection. TinyEmo is able to perform emotion classification
and emotional reasoning, all while using substantially fewer parameters than
comparable models. This efficiency allows us to freely incorporate more diverse
emotional datasets, enabling strong performance on classification tasks, with
our smallest model (700M parameters) outperforming larger state-of-the-art
models based on general-purpose MM-LLMs with over 7B parameters. Additionally,
the Metric Projector allows for interpretability and indirect bias detection in
large models without additional training, offering an approach to understand
and improve AI systems.
  We release code, models, and dataset at https://github.com/ggcr/TinyEmo"
Online Epsilon Net and Piercing Set for Geometric Concepts,cs.LG,Machine Learning,2024-10-09,"VC-dimension and $\varepsilon$-nets are key concepts in Statistical Learning
Theory. Intuitively, VC-dimension is a measure of the size of a class of sets.
The famous $\varepsilon$-net theorem, a fundamental result in Discrete
Geometry, asserts that if the VC-dimension of a set system is bounded, then a
small sample exists that intersects all sufficiently large sets.
  In online learning scenarios where data arrives sequentially, the
VC-dimension helps to bound the complexity of the set system, and
$\varepsilon$-nets ensure the selection of a small representative set. This
sampling framework is crucial in various domains, including spatial data
analysis, motion planning in dynamic environments, optimization of sensor
networks, and feature extraction in computer vision, among others. Motivated by
these applications, we study the online $\varepsilon$-net problem for geometric
concepts with bounded VC-dimension. While the offline version of this problem
has been extensively studied, surprisingly, there are no known theoretical
results for the online version to date. We present the first deterministic
online algorithm with an optimal competitive ratio for intervals in
$\mathbb{R}$. Next, we give a randomized online algorithm with a near-optimal
competitive ratio for axis-aligned boxes in $\mathbb{R}^d$, for $d\le 3$.
Furthermore, we introduce a novel technique to analyze similar-sized objects of
constant description complexity in $\mathbb{R}^d$, which may be of independent
interest. Next, we focus on the continuous version of this problem, where
ranges of the set system are geometric concepts in $\mathbb{R}^d$ arriving in
an online manner, but the universe is the entire space, and the objective is to
choose a small sample that intersects all the ranges."
Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Large Language Models (LLMs) have recently revolutionized the NLP field,
while they still fall short in some specific down-stream tasks. In the work, we
focus on utilizing LLMs to perform machine translation, where we observe that
two patterns of errors frequently occur and drastically affect the translation
quality: language mismatch and repetition. The work sets out to explore the
potential for mitigating these two issues by leveraging model editing methods,
e.g., by locating Feed-Forward Network (FFN) neurons or something that are
responsible for the errors and deactivating them in the inference time. We find
that directly applying such methods either limited effect on the targeted
errors or has significant negative side-effect on the general translation
quality, indicating that the located components may also be crucial for
ensuring machine translation with LLMs on the rails. To this end, we propose to
refine the located components by fetching the intersection of the locating
results under different language settings, filtering out the aforementioned
information that is irrelevant to targeted errors. The experiment results
empirically demonstrate that our methods can effectively reduce the language
mismatch and repetition ratios and meanwhile enhance or keep the general
translation quality in most cases."
S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in Pruning,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Recently, differentiable mask pruning methods optimize the continuous
relaxation architecture (soft network) as the proxy of the pruned discrete
network (hard network) for superior sub-architecture search. However, due to
the agnostic impact of the discretization process, the hard network struggles
with the equivalent representational capacity as the soft network, namely
discretization gap, which severely spoils the pruning performance. In this
paper, we first investigate the discretization gap and propose a novel
structural differentiable mask pruning framework named S2HPruner to bridge the
discretization gap in a one-stage manner. In the training procedure, SH2Pruner
forwards both the soft network and its corresponding hard network, then
distills the hard network under the supervision of the soft network. To
optimize the mask and prevent performance degradation, we propose a decoupled
bidirectional knowledge distillation. It blocks the weight updating from the
hard to the soft network while maintaining the gradient corresponding to the
mask. Compared with existing pruning arts, S2HPruner achieves surpassing
pruning performance without fine-tuning on comprehensive benchmarks, including
CIFAR-100, Tiny ImageNet, and ImageNet with a variety of network architectures.
Besides, investigation and analysis experiments explain the effectiveness of
S2HPruner. Codes will be released soon."
Emergent properties with repeated examples,cs.LG,Machine Learning,2024-10-09,"We study the performance of transformers as a function of the number of
repetitions of training examples with algorithmically generated datasets. On
three problems of mathematics: the greatest common divisor, modular
multiplication, and matrix eigenvalues, we show that for a fixed number of
training steps, models trained on smaller sets of repeated examples outperform
models trained on larger sets of single-use examples. We also demonstrate that
two-set training - repeated use of a small random subset of examples, along
normal sampling on the rest of the training set - provides for faster learning
and better performance. This highlights that the benefits of repetition can
outweigh those of data diversity. These datasets and problems provide a
controlled setting to shed light on the still poorly understood interplay
between generalization and memorization in deep learning."
Distributionally Robust Clustered Federated Learning: A Case Study in Healthcare,cs.LG,Machine Learning,2024-10-09,"In this paper, we address the challenge of heterogeneous data distributions
in cross-silo federated learning by introducing a novel algorithm, which we
term Cross-silo Robust Clustered Federated Learning (CS-RCFL). Our approach
leverages the Wasserstein distance to construct ambiguity sets around each
client's empirical distribution that capture possible distribution shifts in
the local data, enabling evaluation of worst-case model performance. We then
propose a model-agnostic integer fractional program to determine the optimal
distributionally robust clustering of clients into coalitions so that possible
biases in the local models caused by statistically heterogeneous client
datasets are avoided, and analyze our method for linear and logistic regression
models. Finally, we discuss a federated learning protocol that ensures the
privacy of client distributions, a critical consideration, for instance, when
clients are healthcare institutions. We evaluate our algorithm on synthetic and
real-world healthcare data."
"PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness",cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Large Language Models (LLMs) demonstrate impressive capabilities across
various domains, including role-playing, creative writing, mathematical
reasoning, and coding. Despite these advancements, LLMs still encounter
challenges with length control, frequently failing to adhere to specific length
constraints due to their token-level operations and insufficient training on
data with strict length limitations. We identify this issue as stemming from a
lack of positional awareness and propose novel approaches--PositionID Prompting
and PositionID Fine-Tuning--to address it. These methods enhance the model's
ability to continuously monitor and manage text length during generation.
Additionally, we introduce PositionID CP Prompting to enable LLMs to perform
copy and paste operations accurately. Furthermore, we develop two benchmarks
for evaluating length control and copy-paste abilities. Our experiments
demonstrate that our methods significantly improve the model's adherence to
length constraints and copy-paste accuracy without compromising response
quality."
Clean Evaluations on Contaminated Visual Language Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"How to evaluate large language models (LLMs) cleanly has been established as
an important research era to genuinely report the performance of possibly
contaminated LLMs. Yet, how to cleanly evaluate the visual language models
(VLMs) is an under-studied problem. We propose a novel approach to achieve such
goals through data augmentation methods on the visual input information. We
then craft a new visual clean evaluation benchmark with thousands of data
instances. Through extensive experiments, we found that the traditional visual
data augmentation methods are useful, but they are at risk of being used as a
part of the training data as a workaround. We further propose using BGR
augmentation to switch the colour channel of the visual information. We found
that it is a simple yet effective method for reducing the effect of data
contamination and fortunately, it is also harmful to be used as a data
augmentation method during training. It means that it is hard to integrate such
data augmentation into training by malicious trainers and it could be a
promising technique to cleanly evaluate visual LLMs. Our code, data, and model
weights will be released upon publication."
Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation Models Without Human Feedback,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Radiologists play a crucial role by translating medical images into medical
reports. However, the field faces staffing shortages and increasing workloads.
While automated approaches using vision-language models (VLMs) show promise as
assistants, they require exceptionally high accuracy. Most current VLMs in
radiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the
general domain, additional preference fine-tuning has become standard practice.
The challenge in radiology lies in the prohibitive cost of obtaining
radiologist feedback. We propose a scalable automated preference alignment
technique for VLMs in radiology, focusing on chest X-ray (CXR) report
generation. Our method leverages publicly available datasets with an
LLM-as-a-Judge mechanism, eliminating the need for additional expert
radiologist feedback. We evaluate and benchmark five direct alignment
algorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN
scores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in
an average across six metrics (domain specific and general), compared to the
SFT baseline. We study reward overoptimization via length exploitation, with
reports lengthening by up to 3.2x. To assess a potential alignment tax, we
benchmark on six additional diverse tasks, finding no significant degradations.
A reader study involving four board-certified radiologists indicates win rates
of up to 0.62 over the SFT baseline, while significantly penalizing verbosity.
Our analysis provides actionable insights for the development of VLMs in
high-stakes fields like radiology."
Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization,cs.LG,Machine Learning,2024-10-09,"Out-of-Distribution (OOD) generalization in machine learning is a burgeoning
area of study. Its primary goal is to enhance the adaptability and resilience
of machine learning models when faced with new, unseen, and potentially
adversarial data that significantly diverges from their original training
datasets. In this paper, we investigate time series OOD generalization via
pre-trained Large Language Models (LLMs). We first propose a novel
\textbf{T}ri-level learning framework for \textbf{T}ime \textbf{S}eries
\textbf{O}OD generalization, termed TTSO, which considers both sample-level and
group-level uncertainties. This formula offers a fresh theoretic perspective
for formulating and analyzing OOD generalization problem. In addition, we
provide a theoretical analysis to justify this method is well motivated. We
then develop a stratified localization algorithm tailored for this tri-level
optimization problem, theoretically demonstrating the guaranteed convergence of
the proposed algorithm. Our analysis also reveals that the iteration complexity
to obtain an $\epsilon$-stationary point is bounded by
O($\frac{1}{\epsilon^{2}}$). Extensive experiments on real-world datasets have
been conducted to elucidate the effectiveness of the proposed method."
Optimizing Estimators of Squared Calibration Errors in Classification,cs.LG,Machine Learning,2024-10-09,"In this work, we propose a mean-squared error-based risk that enables the
comparison and optimization of estimators of squared calibration errors in
practical settings. Improving the calibration of classifiers is crucial for
enhancing the trustworthiness and interpretability of machine learning models,
especially in sensitive decision-making scenarios. Although various calibration
(error) estimators exist in the current literature, there is a lack of guidance
on selecting the appropriate estimator and tuning its hyperparameters. By
leveraging the bilinear structure of squared calibration errors, we reformulate
calibration estimation as a regression problem with independent and identically
distributed (i.i.d.) input pairs. This reformulation allows us to quantify the
performance of different estimators even for the most challenging calibration
criterion, known as canonical calibration. Our approach advocates for a
training-validation-testing pipeline when estimating a calibration error on an
evaluation dataset. We demonstrate the effectiveness of our pipeline by
optimizing existing calibration estimators and comparing them with novel kernel
ridge regression-based estimators on standard image classification tasks."
Causal Representation Learning in Temporal Data via Single-Parent Decoding,cs.LG,Machine Learning,2024-10-09,"Scientific research often seeks to understand the causal structure underlying
high-level variables in a system. For example, climate scientists study how
phenomena, such as El Ni\~no, affect other climate processes at remote
locations across the globe. However, scientists typically collect low-level
measurements, such as geographically distributed temperature readings. From
these, one needs to learn both a mapping to causally-relevant latent variables,
such as a high-level representation of the El Ni\~no phenomenon and other
processes, as well as the causal model over them. The challenge is that this
task, called causal representation learning, is highly underdetermined from
observational data alone, requiring other constraints during learning to
resolve the indeterminacies. In this work, we consider a temporal model with a
sparsity assumption, namely single-parent decoding: each observed low-level
variable is only affected by a single latent variable. Such an assumption is
reasonable in many scientific applications that require finding groups of
low-level variables, such as extracting regions from geographically gridded
measurement data in climate research or capturing brain regions from neural
activity data. We demonstrate the identifiability of the resulting model and
propose a differentiable method, Causal Discovery with Single-parent Decoding
(CDSD), that simultaneously learns the underlying latents and a causal graph
over them. We assess the validity of our theoretical results using simulated
data and showcase the practical validity of our method in an application to
real-world data from the climate science field."
Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based Outline-guided Generation,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"The patent domain is gaining attention in natural language processing
research, offering practical applications in streamlining the patenting process
and providing challenging benchmarks for large language models (LLMs). However,
the generation of the description sections of patents, which constitute more
than 90% of the patent document, has not been studied to date. We address this
gap by introducing the task of outline-guided paper-to-patent generation, where
an academic paper provides the technical specification of the invention and an
outline conveys the desired patent structure. We present PAP2PAT, a new
challenging benchmark of 1.8k patent-paper pairs with document outlines,
collected using heuristics that reflect typical research lab practices. Our
experiments with current open-weight LLMs and outline-guided chunk-based
generation show that they can effectively use information from the paper but
struggle with repetitions, likely due to the inherent repetitiveness of patent
language. We release our data and code."
Through the Looking Glass: Mirror Schrdinger Bridges,cs.LG,Machine Learning,2024-10-09,"Resampling from a target measure whose density is unknown is a fundamental
problem in mathematical statistics and machine learning. A setting that
dominates the machine learning literature consists of learning a map from an
easy-to-sample prior, such as the Gaussian distribution, to a target measure.
Under this model, samples from the prior are pushed forward to generate a new
sample on the target measure, which is often difficult to sample from directly.
In this paper, we propose a new model for conditional resampling called mirror
Schr\""odinger bridges. Our key observation is that solving the Schr\""odinger
bridge problem between a distribution and itself provides a natural way to
produce new samples from conditional distributions, giving in-distribution
variations of an input data point. We show how to efficiently solve this
largely overlooked version of the Schr\""odinger bridge problem. We prove that
our proposed method leads to significant algorithmic simplifications over
existing alternatives, in addition to providing control over in-distribution
variation. Empirically, we demonstrate how these benefits can be leveraged to
produce proximal samples in a number of application domains."
CursorCore: Assist Programming through Aligning Anything,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Large language models have been successfully applied to programming
assistance tasks, such as code completion, code insertion, and instructional
code editing. However, these applications remain insufficiently automated and
struggle to effectively integrate various types of information during the
programming process, including coding history, current code, and user
instructions. In this work, we propose a new conversational framework that
comprehensively integrates these information sources, collect data to train our
models and evaluate their performance. Firstly, to thoroughly evaluate how well
models align with different types of information and the quality of their
outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to
comprehensively assess the performance of models in programming assistance
tasks. Then, for data collection, we develop a data generation pipeline,
Programming-Instruct, which synthesizes training data from diverse sources,
such as GitHub and online judge platforms. This pipeline can automatically
generate various types of messages throughout the programming process. Finally,
using this pipeline, we generate 219K samples, fine-tune multiple models, and
develop the CursorCore series. We show that CursorCore outperforms other models
of comparable size. This framework unifies applications such as inline chat and
automated editing, contributes to the advancement of coding assistants. Code,
models and data are freely available at
https://github.com/TechxGenus/CursorCore."
Efficient Distribution Matching of Representations via Noise-Injected Deep InfoMax,cs.LG,Machine Learning,2024-10-09,"Deep InfoMax (DIM) is a well-established method for self-supervised
representation learning (SSRL) based on maximization of the mutual information
between the input and the output of a deep neural network encoder. Despite the
DIM and contrastive SSRL in general being well-explored, the task of learning
representations conforming to a specific distribution (i.e., distribution
matching, DM) is still under-addressed. Motivated by the importance of DM to
several downstream tasks (including generative modeling, disentanglement,
outliers detection and other), we enhance DIM to enable automatic matching of
learned representations to a selected prior distribution. To achieve this, we
propose injecting an independent noise into the normalized outputs of the
encoder, while keeping the same InfoMax training objective. We show that such
modification allows for learning uniformly and normally distributed
representations, as well as representations of other absolutely continuous
distributions. Our approach is tested on various downstream tasks. The results
indicate a moderate trade-off between the performance on the downstream tasks
and quality of DM."
Diffusion Density Estimators,cs.LG,Machine Learning,2024-10-09,"We investigate the use of diffusion models as neural density estimators. The
current approach to this problem involves converting the generative process to
a smooth flow, known as the Probability Flow ODE. The log density at a given
sample can be obtained by solving the ODE with a black-box solver. We introduce
a new, highly parallelizable method that computes log densities without the
need to solve a flow. Our approach is based on estimating a path integral by
Monte Carlo, in a manner identical to the simulation-free training of diffusion
models. We also study how different training parameters affect the accuracy of
the density calculation, and offer insights into how these models can be made
more scalable and efficient."
Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Multi-view consistency remains a challenge for image diffusion models. Even
within the Text-to-Texture problem, where perfect geometric correspondences are
known a priori, many methods fail to yield aligned predictions across views,
necessitating non-trivial fusion methods to incorporate the results onto the
original mesh. We explore this issue for a Collaborative Control workflow
specifically in PBR Text-to-Texture. Collaborative Control directly models PBR
image probability distributions, including normal bump maps; to our knowledge,
the only diffusion model to directly output full PBR stacks. We discuss the
design decisions involved in making this model multi-view consistent, and
demonstrate the effectiveness of our approach in ablation studies, as well as
practical applications."
Structure-Centric Robust Monocular Depth Estimation via Knowledge Distillation,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Monocular depth estimation, enabled by self-supervised learning, is a key
technique for 3D perception in computer vision. However, it faces significant
challenges in real-world scenarios, which encompass adverse weather variations,
motion blur, as well as scenes with poor lighting conditions at night. Our
research reveals that we can divide monocular depth estimation into three
sub-problems: depth structure consistency, local texture disambiguation, and
semantic-structural correlation. Our approach tackles the non-robustness of
existing self-supervised monocular depth estimation models to interference
textures by adopting a structure-centered perspective and utilizing the scene
structure characteristics demonstrated by semantics and illumination. We devise
a novel approach to reduce over-reliance on local textures, enhancing
robustness against missing or interfering patterns. Additionally, we
incorporate a semantic expert model as the teacher and construct inter-model
feature dependencies via learnable isomorphic graphs to enable aggregation of
semantic structural knowledge. Our approach achieves state-of-the-art
out-of-distribution monocular depth estimation performance across a range of
public adverse scenario datasets. It demonstrates notable scalability and
compatibility, without necessitating extensive model engineering. This
showcases the potential for customizing models for diverse industrial
applications."
Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models,cs.LG,Machine Learning,2024-10-09,"We investigate feature universality in large language models (LLMs), a
research field that aims to understand how different models similarly represent
concepts in the latent spaces of their intermediate layers. Demonstrating
feature universality allows discoveries about latent representations to
generalize across several models. However, comparing features across LLMs is
challenging due to polysemanticity, in which individual neurons often
correspond to multiple features rather than distinct ones. This makes it
difficult to disentangle and match features across different models. To address
this issue, we employ a method known as dictionary learning by using sparse
autoencoders (SAEs) to transform LLM activations into more interpretable spaces
spanned by neurons corresponding to individual features. After matching feature
neurons across models via activation correlation, we apply representational
space similarity metrics like Singular Value Canonical Correlation Analysis to
analyze these SAE features across different LLMs. Our experiments reveal
significant similarities in SAE feature spaces across various LLMs, providing
new evidence for feature universality."
Adaptive High-Frequency Transformer for Diverse Wildlife Re-Identification,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Wildlife ReID involves utilizing visual technology to identify specific
individuals of wild animals in different scenarios, holding significant
importance for wildlife conservation, ecological research, and environmental
monitoring. Existing wildlife ReID methods are predominantly tailored to
specific species, exhibiting limited applicability. Although some approaches
leverage extensively studied person ReID techniques, they struggle to address
the unique challenges posed by wildlife. Therefore, in this paper, we present a
unified, multi-species general framework for wildlife ReID. Given that
high-frequency information is a consistent representation of unique features in
various species, significantly aiding in identifying contours and details such
as fur textures, we propose the Adaptive High-Frequency Transformer model with
the goal of enhancing high-frequency information learning. To mitigate the
inevitable high-frequency interference in the wilderness environment, we
introduce an object-aware high-frequency selection strategy to adaptively
capture more valuable high-frequency components. Notably, we unify the
experimental settings of multiple wildlife datasets for ReID, achieving
superior performance over state-of-the-art ReID methods. In domain
generalization scenarios, our approach demonstrates robust generalization to
unknown species."
AdaRC: Mitigating Graph Structure Shifts during Test-Time,cs.LG,Machine Learning,2024-10-09,"Powerful as they are, graph neural networks (GNNs) are known to be vulnerable
to distribution shifts. Recently, test-time adaptation (TTA) has attracted
attention due to its ability to adapt a pre-trained model to a target domain
without re-accessing the source domain. However, existing TTA algorithms are
primarily designed for attribute shifts in vision tasks, where samples are
independent. These methods perform poorly on graph data that experience
structure shifts, where node connectivity differs between source and target
graphs. We attribute this performance gap to the distinct impact of node
attribute shifts versus graph structure shifts: the latter significantly
degrades the quality of node representations and blurs the boundaries between
different node categories. To address structure shifts in graphs, we propose
AdaRC, an innovative framework designed for effective and efficient adaptation
to structure shifts by adjusting the hop-aggregation parameters in GNNs. To
enhance the representation quality, we design a prediction-informed clustering
loss to encourage the formation of distinct clusters for different node
categories. Additionally, AdaRC seamlessly integrates with existing TTA
algorithms, allowing it to handle attribute shifts effectively while improving
overall performance under combined structure and attribute shifts. We validate
the effectiveness of AdaRC on both synthetic and real-world datasets,
demonstrating its robustness across various combinations of structure and
attribute shifts."
Personal Intelligence System UniLM: Hybrid On-Device Small Language Model and Server-Based Large Language Model for Malay Nusantara,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"In contexts with limited computational and data resources, high-resource
language models often prove inadequate, particularly when addressing the
specific needs of Malay languages. This paper introduces a Personal
Intelligence System designed to efficiently integrate both on-device and
server-based models. The system incorporates SLiM-34M for on-device processing,
optimized for low memory and power usage, and MANYAK-1.3B for server-based
tasks, allowing for scalable, high-performance language processing. The models
achieve significant results across various tasks, such as machine translation,
question-answering, and translate IndoMMLU. Particularly noteworthy is
SLiM-34M's ability to achieve a high improvement in accuracy compared to other
LLMs while using 2 times fewer pre-training tokens. This work challenges the
prevailing assumption that large-scale computational resources are necessary to
build effective language models, contributing to the development of
resource-efficient models for the Malay language with the unique orchestration
between SLiM-34M and MANYAK-1.3B."
DLGNet: Hyperedge Classification through Directed Line Graphs for Chemical Reactions,cs.LG,Machine Learning,2024-10-09,"Graphs and hypergraphs provide powerful abstractions for modeling
interactions among a set of entities of interest and have been attracting a
growing interest in the literature thanks to many successful applications in
several fields. In particular, they are rapidly expanding in domains such as
chemistry and biology, especially in the areas of drug discovery and molecule
generation. One of the areas witnessing the fasted growth is the chemical
reactions field, where chemical reactions can be naturally encoded as directed
hyperedges of a hypergraph. In this paper, we address the chemical reaction
classification problem by introducing the notation of a Directed Line Graph
(DGL) associated with a given directed hypergraph. On top of it, we build the
Directed Line Graph Network (DLGNet), the first spectral-based Graph Neural
Network (GNN) expressly designed to operate on a hypergraph via its DLG
transformation. The foundation of DLGNet is a novel Hermitian matrix, the
Directed Line Graph Laplacian, which compactly encodes the directionality of
the interactions taking place within the directed hyperedges of the hypergraph
thanks to the DLG representation. The Directed Line Graph Laplacian enjoys many
desirable properties, including admitting an eigenvalue decomposition and being
positive semidefinite, which make it well-suited for its adoption within a
spectral-based GNN. Through extensive experiments on chemical reaction
datasets, we show that DGLNet significantly outperforms the existing
approaches, achieving on a collection of real-world datasets an average
relative-percentage-difference improvement of 33.01%, with a maximum
improvement of 37.71%."
RM4D: A Combined Reachability and Inverse Reachability Map for Common 6-/7-axis Robot Arms by Dimensionality Reduction to 4D,cs.RO,Robotics,2024-10-09,"Knowledge of a manipulator's workspace is fundamental for a variety of tasks
including robot design, grasp planning and robot base placement. Consequently,
workspace representations are well studied in robotics. Two important
representations are reachability maps and inverse reachability maps. The former
predicts whether a given end-effector pose is reachable from where the robot
currently is, and the latter suggests suitable base positions for a desired
end-effector pose. Typically, the reachability map is built by discretizing the
6D space containing the robot's workspace and determining, for each cell,
whether it is reachable or not. The reachability map is subsequently inverted
to build the inverse map. This is a cumbersome process which restricts the
applications of such maps. In this work, we exploit commonalities of existing
six and seven axis robot arms to reduce the dimension of the discretization
from 6D to 4D. We propose Reachability Map 4D (RM4D), a map that only requires
a single 4D data structure for both forward and inverse queries. This gives a
much more compact map that can be constructed by an order of magnitude faster
than existing maps, with no inversion overheads and no loss in accuracy. Our
experiments showcase the usefulness of RM4D for grasp planning with a mobile
manipulator."
$\texttt{ModSCAN}$: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities,cs.CR,Cryptography and Security,2024-10-09,"Large vision-language models (LVLMs) have been rapidly developed and widely
used in various fields, but the (potential) stereotypical bias in the model is
largely unexplored. In this study, we present a pioneering measurement
framework, $\texttt{ModSCAN}$, to $\underline{SCAN}$ the stereotypical bias
within LVLMs from both vision and language $\underline{Mod}$alities.
$\texttt{ModSCAN}$ examines stereotypical biases with respect to two typical
stereotypical attributes (gender and race) across three kinds of scenarios:
occupations, descriptors, and persona traits. Our findings suggest that 1) the
currently popular LVLMs show significant stereotype biases, with CogVLM
emerging as the most biased model; 2) these stereotypical biases may stem from
the inherent biases in the training dataset and pre-trained models; 3) the
utilization of specific prompt prefixes (from both vision and language
modalities) performs well in reducing stereotypical biases. We believe our work
can serve as the foundation for understanding and addressing stereotypical bias
in LVLMs."
Uncovering Factor Level Preferences to Improve Human-Model Alignment,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Despite advancements in Large Language Model (LLM) alignment, understanding
the reasons behind LLM preferences remains crucial for bridging the gap between
desired and actual behavior. LLMs often exhibit biases or tendencies that
diverge from human preferences, such as favoring certain writing styles or
producing overly verbose outputs. However, current methods for evaluating
preference alignment often lack explainability, relying on coarse-grained
comparisons. To address this, we introduce PROFILE (PRObing Factors of
InfLuence for Explainability), a novel framework that uncovers and quantifies
the influence of specific factors driving preferences. PROFILE's factor level
analysis explains the 'why' behind human-model alignment and misalignment,
offering insights into the direction of model improvement. We apply PROFILE to
analyze human and LLM preferences across three tasks: summarization, helpful
response generation, and document-based question-answering. Our factor level
analysis reveals a substantial discrepancy between human and LLM preferences in
generation tasks, whereas LLMs show strong alignment with human preferences in
evaluation tasks. We demonstrate how leveraging factor level insights,
including addressing misaligned factors or exploiting the generation-evaluation
gap, can improve alignment with human preferences. This work underscores the
importance of explainable preference analysis and highlights PROFILE's
potential to provide valuable training signals, driving further improvements in
human-model alignment."
Bridge the Points: Graph-based Few-shot Segment Anything Semantically,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"The recent advancements in large-scale pre-training techniques have
significantly enhanced the capabilities of vision foundation models, notably
the Segment Anything Model (SAM), which can generate precise masks based on
point and box prompts. Recent studies extend SAM to Few-shot Semantic
Segmentation (FSS), focusing on prompt generation for SAM-based automatic
semantic segmentation. However, these methods struggle with selecting suitable
prompts, require specific hyperparameter settings for different scenarios, and
experience prolonged one-shot inference times due to the overuse of SAM,
resulting in low efficiency and limited automation ability. To address these
issues, we propose a simple yet effective approach based on graph analysis. In
particular, a Positive-Negative Alignment module dynamically selects the point
prompts for generating masks, especially uncovering the potential of the
background context as the negative reference. Another subsequent Point-Mask
Clustering module aligns the granularity of masks and selected points as a
directed graph, based on mask coverage over points. These points are then
aggregated by decomposing the weakly connected components of the directed graph
in an efficient manner, constructing distinct natural clusters. Finally, the
positive and overshooting gating, benefiting from graph-based granularity
alignment, aggregate high-confident masks and filter out the false-positive
masks for final prediction, reducing the usage of additional hyperparameters
and redundant mask generation. Extensive experimental analysis across standard
FSS, One-shot Part Segmentation, and Cross Domain FSS datasets validate the
effectiveness and efficiency of the proposed approach, surpassing
state-of-the-art generalist models with a mIoU of 58.7% on COCO-20i and 35.2%
on LVIS-92i. The code is available in https://andyzaq.github.io/GF-SAM/."
Self-Boosting Large Language Models with Synthetic Preference Data,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Through alignment with human preferences, Large Language Models (LLMs) have
advanced significantly in generating honest, harmless, and helpful responses.
However, collecting high-quality preference data is a resource-intensive and
creativity-demanding process, especially for the continual improvement of LLMs.
We introduce SynPO, a self-boosting paradigm that leverages synthetic
preference data for model alignment. SynPO employs an iterative mechanism
wherein a self-prompt generator creates diverse prompts, and a response
improver refines model responses progressively. This approach trains LLMs to
autonomously learn the generative rewards for their own outputs and eliminates
the need for large-scale annotation of prompts and human preferences. After
four SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements
in instruction-following abilities, achieving over 22.1% win rate improvements
on AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general
performance of LLMs on various tasks, validated by a 3.2 to 5.0 average score
increase on the well-recognized Open LLM leaderboard."
Support Vector Boosting Machine (SVBM): Enhancing Classification Performance with AdaBoost and Residual Connections,cs.LG,Machine Learning,2024-10-09,"In traditional boosting algorithms, the focus on misclassified training
samples emphasizes their importance based on difficulty during the learning
process. While using a standard Support Vector Machine (SVM) as a weak learner
in an AdaBoost framework can enhance model performance by concentrating on
error samples, this approach introduces significant challenges. Specifically,
SVMs, characterized by their stability and robustness, may require
destabilization to fit the boosting paradigm, which in turn can constrain
performance due to reliance on the weighted results from preceding iterations.
To address these challenges, we propose the Support Vector Boosting Machine
(SVBM), which integrates a novel subsampling process with SVM algorithms and
residual connection techniques. This method updates sample weights by
considering both the current model's predictions and the outputs from prior
rounds, allowing for effective sparsity control. The SVBM framework enhances
the ability to form complex decision boundaries, thereby improving
classification performance. The MATLAB source code for SVBM can be accessed at
https://github.com/junbolian/SVBM."
Control System Design and Experiments for Autonomous Underwater Helicopter Docking Procedure Based on Acoustic-inertial-optical Guidance,cs.RO,Robotics,2024-10-09,"A control system structure for the underwater docking procedure of an
Autonomous Underwater Helicopter (AUH) is proposed in this paper, which
utilizes acoustic-inertial-optical guidance. Unlike conventional Autonomous
Underwater Vehicles (AUVs), the maneuverability requirements for AUHs are more
stringent during the docking procedure, requiring it to remain stationary or
have minimal horizontal movement while moving vertically. The docking procedure
is divided into two stages: Homing and Landing, each stage utilizing different
guidance methods. Additionally, a segmented aligning strategy operating at
various altitudes and a linear velocity decision are both adopted in Landing
stage. Due to the unique structure of the Subsea Docking System (SDS), the AUH
is required to dock onto the SDS in a fixed orientation with specific attitude
and altitude. Therefore, a particular criterion is proposed to determine
whether the AUH has successfully docked onto the SDS. Furthermore, the
effectiveness and robustness of the proposed control method in AUH's docking
procedure are demonstrated through pool experiments and sea trials."
Faithful Interpretation for Graph Neural Networks,cs.LG,Machine Learning,2024-10-09,"Currently, attention mechanisms have garnered increasing attention in Graph
Neural Networks (GNNs), such as Graph Attention Networks (GATs) and Graph
Transformers (GTs). It is not only due to the commendable boost in performance
they offer but also its capacity to provide a more lucid rationale for model
behaviors, which are often viewed as inscrutable. However, Attention-based GNNs
have demonstrated instability in interpretability when subjected to various
sources of perturbations during both training and testing phases, including
factors like additional edges or nodes. In this paper, we propose a solution to
this problem by introducing a novel notion called Faithful Graph
Attention-based Interpretation (FGAI). In particular, FGAI has four crucial
properties regarding stability and sensitivity to interpretation and final
output distribution. Built upon this notion, we propose an efficient
methodology for obtaining FGAI, which can be viewed as an ad hoc modification
to the canonical Attention-based GNNs. To validate our proposed solution, we
introduce two novel metrics tailored for graph interpretation assessment.
Experimental results demonstrate that FGAI exhibits superior stability and
preserves the interpretability of attention under various forms of
perturbations and randomness, which makes FGAI a more faithful and reliable
explanation tool."
A Trilogy of AI Safety Frameworks: Paths from Facts and Knowledge Gaps to Reliable Predictions and New Knowledge,cs.AI,Artificial Intelligence,2024-10-09,"AI Safety has become a vital front-line concern of many scientists within and
outside the AI community. There are many immediate and long term anticipated
risks that range from existential risk to human existence to deep fakes and
bias in machine learning systems [1-5]. In this paper, we reduce the full scope
and immense complexity of AI safety concerns to a trilogy of three important
but tractable opportunities for advances that have the short-term potential to
improve AI safety and reliability without reducing AI innovation in critical
domains. In this perspective, we discuss this vision based on several case
studies that already produced proofs of concept in critical ML applications in
biomedical science."
CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Neural dependency parsing has achieved remarkable performance for low
resource morphologically rich languages. It has also been well-studied that
morphologically rich languages exhibit relatively free word order. This prompts
a fundamental investigation: Is there a way to enhance dependency parsing
performance, making the model robust to word order variations utilizing the
relatively free word order nature of morphologically rich languages? In this
work, we examine the robustness of graph-based parsing architectures on 7
relatively free word order languages. We focus on scrutinizing essential
modifications such as data augmentation and the removal of position encoding
required to adapt these architectures accordingly. To this end, we propose a
contrastive self-supervised learning method to make the model robust to word
order variations. Furthermore, our proposed modification demonstrates a
substantial average gain of 3.03/2.95 points in 7 relatively free word order
languages, as measured by the UAS/LAS Score metric when compared to the best
performing baseline."
Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Recent studies have shown that the denoising process in (generative)
diffusion models can induce meaningful (discriminative) representations inside
the model, though the quality of these representations still lags behind those
learned through recent self-supervised learning methods. We argue that one main
bottleneck in training large-scale diffusion models for generation lies in
effectively learning these representations. Moreover, training can be made
easier by incorporating high-quality external visual representations, rather
than relying solely on the diffusion models to learn them independently. We
study this by introducing a straightforward regularization called
REPresentation Alignment (REPA), which aligns the projections of noisy input
hidden states in denoising networks with clean image representations obtained
from external, pretrained visual encoders. The results are striking: our simple
strategy yields significant improvements in both training efficiency and
generation quality when applied to popular diffusion and flow-based
transformers, such as DiTs and SiTs. For instance, our method can speed up SiT
training by over 17.5$\times$, matching the performance (without
classifier-free guidance) of a SiT-XL model trained for 7M steps in less than
400K steps. In terms of final generation quality, our approach achieves
state-of-the-art results of FID=1.42 using classifier-free guidance with the
guidance interval."
Predicting Bitcoin Market Trends with Enhanced Technical Indicator Integration and Classification Models,cs.LG,Machine Learning,2024-10-09,"Thanks to the high potential for profit, trading has become increasingly
attractive to investors as the cryptocurrency and stock markets rapidly expand.
However, because financial markets are intricate and dynamic, accurately
predicting prices remains a significant challenge. The volatile nature of the
cryptocurrency market makes it even harder for traders and investors to make
decisions. This study presents a machine learning model based on classification
to forecast the direction of the cryptocurrency market, i.e., whether prices
will increase or decrease. The model is trained using historical data and
important technical indicators such as the Moving Average Convergence
Divergence, the Relative Strength Index, and Bollinger Bands. We illustrate our
approach with an empirical study of the closing price of Bitcoin. Several
simulations, including a confusion matrix and Receiver Operating Characteristic
curve, are used to assess the model's performance, and the results show a
buy/sell signal accuracy of over 92%. These findings demonstrate how machine
learning models can assist investors and traders of cryptocurrencies in making
wise/informed decisions in a very volatile market."
SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Speculative decoding (SD) has emerged as a widely used paradigm to accelerate
the inference of large language models (LLMs) without compromising generation
quality. It works by first employing a compact model to draft multiple tokens
efficiently and then using the target LLM to verify them in parallel. While
this technique has achieved notable speedups, most existing approaches
necessitate either additional parameters or extensive training to construct
effective draft models, thereby restricting their applicability across
different LLMs and tasks. To address this limitation, we explore a novel
plug-and-play SD solution with layer-skipping, which skips intermediate layers
of the target LLM as the compact draft model. Our analysis reveals that LLMs
exhibit great potential for self-acceleration through layer sparsity and the
task-specific nature of this sparsity. Building on these insights, we introduce
SWIFT, an on-the-fly self-speculative decoding algorithm that adaptively
selects intermediate layers of LLMs to skip during inference. SWIFT does not
require auxiliary models or additional training, making it a plug-and-play
solution for accelerating LLM inference across diverse input data streams. Our
extensive experiments across a wide range of models and downstream tasks
demonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving
the original distribution of the generated text."
Utilize the Flow before Stepping into the Same River Twice: Certainty Represented Knowledge Flow for Refusal-Aware Instruction Tuning,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)
to refuse to answer unknown questions. By modifying responses of unknown
questions in the training data to refusal responses such as ""I don't know"",
RAIT enhances the reliability of LLMs and reduces their hallucination.
Generally, RAIT modifies training samples based on the correctness of the
initial LLM's response. However, this crude approach can cause LLMs to
excessively refuse answering questions they could have correctly answered, the
problem we call over-refusal. In this paper, we explore two primary causes of
over-refusal: Static conflict emerges when the RAIT data is constructed solely
on correctness criteria, causing similar samples in the LLM's feature space to
be assigned different labels (original vs. modified ""I don't know""). Dynamic
conflict occurs due to the changes of LLM's knowledge state during fine-tuning,
which transforms previous unknown questions into knowns, while the training
data, which is constructed based on the initial LLM, remains unchanged. These
conflicts cause the trained LLM to misclassify known questions as unknown,
resulting in over-refusal. To address this issue, we introduce Certainty
Represented Knowledge Flow for Refusal-Aware Instructions Construction (CRaFT).
CRaFT centers on two main contributions: First, we additionally incorporate
response certainty to selectively filter and modify data, reducing static
conflicts. Second, we implement preliminary rehearsal training to characterize
changes in the LLM's knowledge state, which helps mitigate dynamic conflicts
during the fine-tuning process. We conducted extensive experiments on
open-ended question answering and multiple-choice question task. Experiment
results show that CRaFT can improve LLM's overall performance during the RAIT
process. Source code and training data will be released at Github."
Compositional Entailment Learning for Hyperbolic Vision-Language Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Image-text representation learning forms a cornerstone in vision-language
models, where pairs of images and textual descriptions are contrastively
aligned in a shared embedding space. Since visual and textual concepts are
naturally hierarchical, recent work has shown that hyperbolic space can serve
as a high-potential manifold to learn vision-language representation with
strong downstream performance. In this work, for the first time we show how to
fully leverage the innate hierarchical nature of hyperbolic embeddings by
looking beyond individual image-text pairs. We propose Compositional Entailment
Learning for hyperbolic vision-language models. The idea is that an image is
not only described by a sentence but is itself a composition of multiple object
boxes, each with their own textual description. Such information can be
obtained freely by extracting nouns from sentences and using openly available
localized grounding models. We show how to hierarchically organize images,
image boxes, and their textual descriptions through contrastive and
entailment-based objectives. Empirical evaluation on a hyperbolic
vision-language model trained with millions of image-text pairs shows that the
proposed compositional learning approach outperforms conventional Euclidean
CLIP learning, as well as recent hyperbolic alternatives, with better zero-shot
and retrieval generalization and clearly stronger hierarchical performance."
Combining Planning and Diffusion for Mobility with Unknown Dynamics,cs.RO,Robotics,2024-10-09,"Manipulation of large objects over long horizons (such as carts in a
warehouse) is an essential skill for deployable robotic systems. Large objects
require mobile manipulation which involves simultaneous manipulation,
navigation, and movement with the object in tow. In many real-world situations,
object dynamics are incredibly complex, such as the interaction of an office
chair (with a rotating base and five caster wheels) and the ground. We present
a hierarchical algorithm for long-horizon robot manipulation problems in which
the dynamics are partially unknown. We observe that diffusion-based behavior
cloning is highly effective for short-horizon problems with unknown dynamics,
so we decompose the problem into an abstract high-level, obstacle-aware
motion-planning problem that produces a waypoint sequence. We use a
short-horizon, relative-motion diffusion policy to achieve the waypoints in
sequence. We train mobile manipulation policies on a Spot robot that has to
push and pull an office chair. Our hierarchical manipulation policy performs
consistently better, especially when the horizon increases, compared to a
diffusion policy trained on long-horizon demonstrations or motion planning
assuming a rigidly-attached object (success rate of 8 (versus 0 and 5
respectively) out of 10 runs). Importantly, our learned policy generalizes to
new layouts, grasps, chairs, and flooring that induces more friction, without
any further training, showing promise for other complex mobile manipulation
problems. Project Page: https://yravan.github.io/plannerorderedpolicy/"
Reliable Probabilistic Human Trajectory Prediction for Autonomous Applications,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Autonomous systems, like vehicles or robots, require reliable, accurate,
fast, resource-efficient, scalable, and low-latency trajectory predictions to
get initial knowledge about future locations and movements of surrounding
objects for safe human-machine interaction. Furthermore, they need to know the
uncertainty of the predictions for risk assessment to provide safe path
planning. This paper presents a lightweight method to address these
requirements, combining Long Short-Term Memory and Mixture Density Networks.
Our method predicts probability distributions, including confidence level
estimations for positional uncertainty to support subsequent risk management
applications and runs on a low-power embedded platform. We discuss essential
requirements for human trajectory prediction in autonomous vehicle applications
and demonstrate our method's performance using multiple traffic-related
datasets. Furthermore, we explain reliability and sharpness metrics and show
how important they are to guarantee the correctness and robustness of a model's
predictions and uncertainty assessments. These essential evaluations have so
far received little attention for no good reason. Our approach focuses entirely
on real-world applicability. Verifying prediction uncertainties and a model's
reliability are central to autonomous real-world applications. Our framework
and code are available at:
https://github.com/kav-institute/mdn_trajectory_forecasting."
Generative Model for Less-Resourced Language with 1 billion parameters,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Large language models (LLMs) are a basic infrastructure for modern natural
language processing. Many commercial and open-source LLMs exist for English,
e.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on
mostly English texts, their fluency and knowledge of low-resource languages and
societies are superficial. We present the development of large generative
language models for a less-resourced language. GaMS 1B - Generative Model for
Slovene with 1 billion parameters was created by continuing pretraining of the
existing English OPT model. We developed a new tokenizer adapted to Slovene,
Croatian, and English languages and used embedding initialization methods FOCUS
and WECHSEL to transfer the embeddings from the English OPT model. We evaluate
our models on several classification datasets from the Slovene suite of
benchmarks and generative sentence simplification task SENTA. We only used a
few-shot in-context learning of our models, which are not yet
instruction-tuned. For classification tasks, in this mode, the generative
models lag behind the existing Slovene BERT-type models fine-tuned for specific
tasks. On a sentence simplification task, the GaMS models achieve comparable or
better performance than the GPT-3.5-Turbo model."
Average Certified Radius is a Poor Metric for Randomized Smoothing,cs.LG,Machine Learning,2024-10-09,"Randomized smoothing is a popular approach for providing certified robustness
guarantees against adversarial attacks, and has become a very active area of
research. Over the past years, the average certified radius (ACR) has emerged
as the single most important metric for comparing methods and tracking progress
in the field. However, in this work, we show that ACR is an exceptionally poor
metric for evaluating robustness guarantees provided by randomized smoothing.
We theoretically show not only that a trivial classifier can have arbitrarily
large ACR, but also that ACR is much more sensitive to improvements on easy
samples than on hard ones. Empirically, we confirm that existing training
strategies that improve ACR reduce the model's robustness on hard samples.
Further, we show that by focusing on easy samples, we can effectively replicate
the increase in ACR. We develop strategies, including explicitly discarding
hard samples, reweighing the dataset with certified radius, and extreme
optimization for easy samples, to achieve state-of-the-art ACR, although these
strategies ignore robustness for the general data distribution. Overall, our
results suggest that ACR has introduced a strong undesired bias to the field,
and better metrics are required to holistically evaluate randomized smoothing."
Learning from Spatio-temporal Correlation for Semi-Supervised LiDAR Semantic Segmentation,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"We address the challenges of the semi-supervised LiDAR segmentation (SSLS)
problem, particularly in low-budget scenarios. The two main issues in
low-budget SSLS are the poor-quality pseudo-labels for unlabeled data, and the
performance drops due to the significant imbalance between ground-truth and
pseudo-labels. This imbalance leads to a vicious training cycle. To overcome
these challenges, we leverage the spatio-temporal prior by recognizing the
substantial overlap between temporally adjacent LiDAR scans. We propose a
proximity-based label estimation, which generates highly accurate pseudo-labels
for unlabeled data by utilizing semantic consistency with adjacent labeled
data. Additionally, we enhance this method by progressively expanding the
pseudo-labels from the nearest unlabeled scans, which helps significantly
reduce errors linked to dynamic classes. Additionally, we employ a dual-branch
structure to mitigate performance degradation caused by data imbalance.
Experimental results demonstrate remarkable performance in low-budget settings
(i.e., <= 5%) and meaningful improvements in normal budget settings (i.e., 5 -
50%). Finally, our method has achieved new state-of-the-art results on
SemanticKITTI and nuScenes in semi-supervised LiDAR segmentation. With only 5%
labeled data, it offers competitive results against fully-supervised
counterparts. Moreover, it surpasses the performance of the previous
state-of-the-art at 100% labeled data (75.2%) using only 20% of labeled data
(76.0%) on nuScenes. The code is available on https://github.com/halbielee/PLE."
FltLM: An Intergrated Long-Context Large Language Model for Effective Context Filtering and Understanding,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"The development of Long-Context Large Language Models (LLMs) has markedly
advanced natural language processing by facilitating the process of textual
data across long documents and multiple corpora. However, Long-Context LLMs
still face two critical challenges: The lost in the middle phenomenon, where
crucial middle-context information is likely to be missed, and the distraction
issue that the models lose focus due to overly extended contexts. To address
these challenges, we propose the Context Filtering Language Model (FltLM), a
novel integrated Long-Context LLM which enhances the ability of the model on
multi-document question-answering (QA) tasks. Specifically, FltLM innovatively
incorporates a context filter with a soft mask mechanism, identifying and
dynamically excluding irrelevant content to concentrate on pertinent
information for better comprehension and reasoning. Our approach not only
mitigates these two challenges, but also enables the model to operate
conveniently in a single forward pass. Experimental results demonstrate that
FltLM significantly outperforms supervised fine-tuning and retrieval-based
methods in complex QA scenarios, suggesting a promising solution for more
accurate and reliable long-context natural language understanding applications."
Adaptive Refinement Protocols for Distributed Distribution Estimation under $\ell^p$-Losses,cs.LG,Machine Learning,2024-10-09,"Consider the communication-constrained estimation of discrete distributions
under $\ell^p$ losses, where each distributed terminal holds multiple
independent samples and uses limited number of bits to describe the samples. We
obtain the minimax optimal rates of the problem in most parameter regimes. An
elbow effect of the optimal rates at $p=2$ is clearly identified. To show the
optimal rates, we first design estimation protocols to achieve them. The key
ingredient of these protocols is to introduce adaptive refinement mechanisms,
which first generate rough estimate by partial information and then establish
refined estimate in subsequent steps guided by the rough estimate. The
protocols leverage successive refinement, sample compression and thresholding
methods to achieve the optimal rates in different parameter regimes. The
optimality of the protocols is shown by deriving compatible minimax lower
bounds."
Degree Distribution based Spiking Graph Networks for Domain Adaptation,cs.LG,Machine Learning,2024-10-09,"Spiking Graph Networks (SGNs) have garnered significant attraction from both
researchers and industry due to their ability to address energy consumption
challenges in graph classification. However, SGNs are only effective for
in-distribution data and cannot tackle out-of-distribution data. In this paper,
we first propose the domain adaptation problem in SGNs, and introduce a novel
framework named Degree-aware Spiking Graph Domain Adaptation for
Classification. The proposed DeSGDA addresses the spiking graph domain
adaptation problem by three aspects: node degree-aware personalized spiking
representation, adversarial feature distribution alignment, and pseudo-label
distillation. First, we introduce the personalized spiking representation
method for generating degree-dependent spiking signals. Specifically, the
threshold of triggering a spike is determined by the node degree, allowing this
personalized approach to capture more expressive information for
classification. Then, we propose the graph feature distribution alignment
module that is adversarially trained using membrane potential against a domain
discriminator. Such an alignment module can efficiently maintain high
performance and low energy consumption in the case of inconsistent
distribution. Additionally, we extract consistent predictions across two spaces
to create reliable pseudo-labels, effectively leveraging unlabeled data to
enhance graph classification performance. Extensive experiments on benchmark
datasets validate the superiority of the proposed DeSGDA compared with
competitive baselines."
Privately Counting Partially Ordered Data,cs.CR,Cryptography and Security,2024-10-09,"We consider differentially private counting when each data point consists of
$d$ bits satisfying a partial order. Our main technical contribution is a
problem-specific $K$-norm mechanism that runs in time $O(d^2)$. Experiments
show that, depending on the partial order in question, our solution dominates
existing pure differentially private mechanisms, and can reduce their error by
an order of magnitude or more."
Evaluating Model Performance with Hard-Swish Activation Function Adjustments,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"In the field of pattern recognition, achieving high accuracy is essential.
While training a model to recognize different complex images, it is vital to
fine-tune the model to achieve the highest accuracy possible. One strategy for
fine-tuning a model involves changing its activation function. Most pre-trained
models use ReLU as their default activation function, but switching to a
different activation function like Hard-Swish could be beneficial. This study
evaluates the performance of models using ReLU, Swish and Hard-Swish activation
functions across diverse image datasets. Our results show a 2.06% increase in
accuracy for models on the CIFAR-10 dataset and a 0.30% increase in accuracy
for models on the ATLAS dataset. Modifying the activation functions in
architecture of pre-trained models lead to improved overall accuracy."
Noise is All You Need: Private Second-Order Convergence of Noisy SGD,cs.LG,Machine Learning,2024-10-09,"Private optimization is a topic of major interest in machine learning, with
differentially private stochastic gradient descent (DP-SGD) playing a key role
in both theory and practice. Furthermore, DP-SGD is known to be a powerful tool
in contexts beyond privacy, including robustness, machine unlearning, etc.
Existing analyses of DP-SGD either make relatively strong assumptions (e.g.,
Lipschitz continuity of the loss function, or even convexity) or prove only
first-order convergence (and thus might end at a saddle point in the non-convex
setting). At the same time, there has been progress in proving second-order
convergence of the non-private version of ``noisy SGD'', as well as progress in
designing algorithms that are more complex than DP-SGD and do guarantee
second-order convergence. We revisit DP-SGD and show that ``noise is all you
need'': the noise necessary for privacy already implies second-order
convergence under the standard smoothness assumptions, even for non-Lipschitz
loss functions. Hence, we get second-order convergence essentially for free:
DP-SGD, the workhorse of modern private optimization, under minimal assumptions
can be used to find a second-order stationary point."
Secure Video Quality Assessment Resisting Adversarial Attacks,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"The exponential surge in video traffic has intensified the imperative for
Video Quality Assessment (VQA). Leveraging cutting-edge architectures, current
VQA models have achieved human-comparable accuracy. However, recent studies
have revealed the vulnerability of existing VQA models against adversarial
attacks. To establish a reliable and practical assessment system, a secure VQA
model capable of resisting such malicious attacks is urgently demanded.
Unfortunately, no attempt has been made to explore this issue. This paper first
attempts to investigate general adversarial defense principles, aiming at
endowing existing VQA models with security. Specifically, we first introduce
random spatial grid sampling on the video frame for intra-frame defense. Then,
we design pixel-wise randomization through a guardian map, globally
neutralizing adversarial perturbations. Meanwhile, we extract temporal
information from the video sequence as compensation for inter-frame defense.
Building upon these principles, we present a novel VQA framework from the
security-oriented perspective, termed SecureVQA. Extensive experiments indicate
that SecureVQA sets a new benchmark in security while achieving competitive VQA
performance compared with state-of-the-art models. Ablation studies delve
deeper into analyzing the principles of SecureVQA, demonstrating their
generalization and contributions to the security of leading VQA models."
On Wagner's k-Tree Algorithm Over Integers,cs.CR,Cryptography and Security,2024-10-09,"The k-Tree algorithm [Wagner 02] is a non-trivial algorithm for the
average-case k-SUM problem that has found widespread use in cryptanalysis. Its
input consists of k lists, each containing n integers from a range of size m.
Wagner's original heuristic analysis suggested that this algorithm succeeds
with constant probability if n = m^{1/(\log{k}+1)}, and that in this case it
runs in time O(kn). Subsequent rigorous analysis of the algorithm [Lyubashevsky
05, Shallue 08, Joux-Kippen-Loss 24] has shown that it succeeds with high
probability if the input list sizes are significantly larger than this.
  We present a broader rigorous analysis of the k-Tree algorithm, showing upper
and lower bounds on its success probability and complexity for any size of the
input lists. Our results confirm Wagner's heuristic conclusions, and also give
meaningful bounds for a wide range of list sizes that are not covered by
existing analyses. We present analytical bounds that are asymptotically tight,
as well as an efficient algorithm that computes (provably correct) bounds for a
wide range of concrete parameter settings. We also do the same for the k-Tree
algorithm over Z_m. Finally, we present experimental evaluation of the
tightness of our results."
Safe Reinforcement Learning Filter for Multicopter Collision-Free Tracking under disturbances,cs.RO,Robotics,2024-10-09,"This paper proposes a safe reinforcement learning filter (SRLF) to realize
multicopter collision-free trajectory tracking with input disturbance. A novel
robust control barrier function (RCBF) with its analysis techniques is
introduced to avoid collisions with unknown disturbances during tracking. To
ensure the system state remains within the safe set, the RCBF gain is designed
in control action. A safety filter is introduced to transform unsafe
reinforcement learning (RL) control inputs into safe ones, allowing RL training
to proceed without explicitly considering safety constraints. The SRLF obtains
rigorous guaranteed safe control action by solving a quadratic programming (QP)
problem that incorporates forward invariance of RCBF and input saturation
constraints. Both simulation and real-world experiments on multicopters
demonstrate the effectiveness and excellent performance of SRLF in achieving
collision-free tracking under input disturbances and saturation."
Understanding Model Ensemble in Transferable Adversarial Attack,cs.LG,Machine Learning,2024-10-09,"Model ensemble adversarial attack has become a powerful method for generating
transferable adversarial examples that can target even unknown models, but its
theoretical foundation remains underexplored. To address this gap, we provide
early theoretical insights that serve as a roadmap for advancing model ensemble
adversarial attack. We first define transferability error to measure the error
in adversarial transferability, alongside concepts of diversity and empirical
model ensemble Rademacher complexity. We then decompose the transferability
error into vulnerability, diversity, and a constant, which rigidly explains the
origin of transferability error in model ensemble attack: the vulnerability of
an adversarial example to ensemble components, and the diversity of ensemble
components. Furthermore, we apply the latest mathematical tools in information
theory to bound the transferability error using complexity and generalization
terms, contributing to three practical guidelines for reducing transferability
error: (1) incorporating more surrogate models, (2) increasing their diversity,
and (3) reducing their complexity in cases of overfitting. Finally, extensive
experiments with 54 models validate our theoretical framework, representing a
significant step forward in understanding transferable model ensemble
adversarial attacks."
On the Security and Design of Cryptosystems Using Gabidulin-Kronecker Product Codes,cs.CR,Cryptography and Security,2024-10-09,"This paper is a preliminary study on the security and design of cryptosystems
using Gabidulin-Kronecker Product Codes. In particular, we point out the design
impracticality of the system, and propose ways to improve it."
Forgetting Through Transforming: Enabling Federated Unlearning via Class-Aware Representation Transformation,cs.LG,Machine Learning,2024-10-09,"Federated Unlearning (FU) enables clients to selectively remove the influence
of specific data from a trained federated learning model, addressing privacy
concerns and regulatory requirements. However, existing FU methods often
struggle to balance effective erasure with model utility preservation,
especially for class-level unlearning in non-IID settings. We propose Federated
Unlearning via Class-aware Representation Transformation (FUCRT), a novel
method that achieves unlearning through class-aware representation
transformation. FUCRT employs two key components: (1) a transformation class
selection strategy to identify optimal forgetting directions, and (2) a
transformation alignment technique using dual class-aware contrastive learning
to ensure consistent transformations across clients. Extensive experiments on
four datasets demonstrate FUCRT's superior performance in terms of erasure
guarantee, model utility preservation, and efficiency. FUCRT achieves complete
(100\%) erasure of unlearning classes while maintaining or improving
performance on remaining classes, outperforming state-of-the-art baselines
across both IID and Non-IID settings. Analysis of the representation space
reveals FUCRT's ability to effectively merge unlearning class representations
with the transformation class from remaining classes, closely mimicking the
model retrained from scratch."
A Safety Modulator Actor-Critic Method in Model-Free Safe Reinforcement Learning and Application in UAV Hovering,cs.AI,Artificial Intelligence,2024-10-09,"This paper proposes a safety modulator actor-critic (SMAC) method to address
safety constraint and overestimation mitigation in model-free safe
reinforcement learning (RL). A safety modulator is developed to satisfy safety
constraints by modulating actions, allowing the policy to ignore safety
constraint and focus on maximizing reward. Additionally, a distributional
critic with a theoretical update rule for SMAC is proposed to mitigate the
overestimation of Q-values with safety constraints. Both simulation and
real-world scenarios experiments on Unmanned Aerial Vehicles (UAVs) hovering
confirm that the SMAC can effectively maintain safety constraints and
outperform mainstream baseline algorithms."
Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Architectures such as Linformer and Mamba have recently emerged as
competitive linear time replacements for transformers. However, corresponding
large pretrained models are often unavailable, especially in non-text domains.
To remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)
approach that jointly converts a transformer model to a linear time substitute
and fine-tunes it to a target task. We also compare several means to guide the
fine-tuning to optimally retain the desired inference capability from the
original model. The methods differ in their use of the target model and the
trajectory of the parameters. In a series of empirical studies on language
processing, language modeling, and speech processing, we show that CALD can
effectively recover the result of the original model, and that the guiding
strategy contributes to the result. Some reasons for the variation are
suggested."
MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Mental health disorders are one of the most serious diseases in the world.
Most people with such a disease lack access to adequate care, which highlights
the importance of training models for the diagnosis and treatment of mental
health disorders. However, in the mental health domain, privacy concerns limit
the accessibility of personalized treatment data, making it challenging to
build powerful models. In this paper, we introduce MentalArena, a self-play
framework to train language models by generating domain-specific personalized
data, where we obtain a better model capable of making a personalized diagnosis
and treatment (as a therapist) and providing information (as a patient). To
accurately model human-like mental health patients, we devise Symptom Encoder,
which simulates a real patient from both cognition and behavior perspectives.
To address intent bias during patient-therapist interactions, we propose
Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and
dynamically manage the dialogue between patient and therapist according to the
identified deviations. We evaluated MentalArena against 6 benchmarks, including
biomedicalQA and mental health tasks, compared to 6 advanced models. Our
models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform
their counterparts, including GPT-4o. We hope that our work can inspire future
research on personalized care. Code is available in
https://github.com/Scarelette/MentalArena/tree/main"
SurANet: Surrounding-Aware Network for Concealed Object Detection via Highly-Efficient Interactive Contrastive Learning Strategy,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Concealed object detection (COD) in cluttered scenes is significant for
various image processing applications. However, due to that concealed objects
are always similar to their background, it is extremely hard to distinguish
them. Here, the major obstacle is the tiny feature differences between the
inside and outside object boundary region, which makes it trouble for existing
COD methods to achieve accurate results. In this paper, considering that the
surrounding environment information can be well utilized to identify the
concealed objects, and thus, we propose a novel deep Surrounding-Aware Network,
namely SurANet, for COD tasks, which introduces surrounding information into
feature extraction and loss function to improve the discrimination. First, we
enhance the semantics of feature maps using differential fusion of surrounding
features to highlight concealed objects. Next, a Surrounding-Aware Contrastive
Loss is applied to identify the concealed object via learning surrounding
feature maps contrastively. Then, SurANet can be trained end-to-end with high
efficiency via our proposed Spatial-Compressed Correlation Transmission
strategy after our investigation of feature dynamics, and extensive experiments
improve that such features can be well reserved respectively. Finally,
experimental results demonstrate that the proposed SurANet outperforms
state-of-the-art COD methods on multiple real datasets. Our source code will be
available at https://github.com/kyh433/SurANet."
Boosting Few-Shot Detection with Large Language Models and Layout-to-Image Synthesis,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Recent advancements in diffusion models have enabled a wide range of works
exploiting their ability to generate high-volume, high-quality data for use in
various downstream tasks. One subclass of such models, dubbed Layout-to-Image
Synthesis (LIS), learns to generate images conditioned on a spatial layout
(bounding boxes, masks, poses, etc.) and has shown a promising ability to
generate realistic images, albeit with limited layout-adherence. Moreover, the
question of how to effectively transfer those models for scalable augmentation
of few-shot detection data remains unanswered. Thus, we propose a collaborative
framework employing a Large Language Model (LLM) and an LIS model for enhancing
few-shot detection beyond state-of-the-art generative augmentation approaches.
We leverage LLM's reasoning ability to extrapolate the spatial prior of the
annotation space by generating new bounding boxes given only a few example
annotations. Additionally, we introduce our novel layout-aware CLIP score for
sample ranking, enabling tight coupling between generated layouts and images.
Significant improvements on COCO few-shot benchmarks are observed. With our
approach, a YOLOX-S baseline is boosted by more than 140%, 50%, 35% in mAP on
the COCO 5-,10-, and 30-shot settings, respectively."
Dynamic metastability in the self-attention model,cs.LG,Machine Learning,2024-10-09,"We consider the self-attention model - an interacting particle system on the
unit sphere, which serves as a toy model for Transformers, the deep neural
network architecture behind the recent successes of large language models. We
prove the appearance of dynamic metastability conjectured in [GLPR23] -
although particles collapse to a single cluster in infinite time, they remain
trapped near a configuration of several clusters for an exponentially long
period of time. By leveraging a gradient flow interpretation of the system, we
also connect our result to an overarching framework of slow motion of gradient
flows proposed by Otto and Reznikoff [OR07] in the context of coarsening and
the Allen-Cahn equation. We finally probe the dynamics beyond the exponentially
long period of metastability, and illustrate that, under an appropriate
time-rescaling, the energy reaches its global maximum in finite time and has a
staircase profile, with trajectories manifesting saddle-to-saddle-like
behavior, reminiscent of recent works in the analysis of training dynamics via
gradient descent for two-layer neural networks."
Transfer Learning for a Class of Cascade Dynamical Systems,cs.LG,Machine Learning,2024-10-09,"This work considers the problem of transfer learning in the context of
reinforcement learning. Specifically, we consider training a policy in a
reduced order system and deploying it in the full state system. The motivation
for this training strategy is that running simulations in the full-state system
may take excessive time if the dynamics are complex. While transfer learning
alleviates the computational issue, the transfer guarantees depend on the
discrepancy between the two systems. In this work, we consider a class of
cascade dynamical systems, where the dynamics of a subset of the state-space
influence the rest of the states but not vice-versa. The reinforcement learning
policy learns in a model that ignores the dynamics of these states and treats
them as commanded inputs. In the full-state system, these dynamics are handled
using a classic controller (e.g., a PID). These systems have vast applications
in the control literature and their structure allows us to provide transfer
guarantees that depend on the stability of the inner loop controller. Numerical
experiments on a quadrotor support the theoretical findings."
Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed Methods,cs.LG,Machine Learning,2024-10-09,"Physics-informed deep learning often faces optimization challenges due to the
complexity of solving partial differential equations (PDEs), which involve
exploring large solution spaces, require numerous iterations, and can lead to
unstable training. These challenges arise particularly from the
ill-conditioning of the optimization problem, caused by the differential terms
in the loss function. To address these issues, we propose learning a solver,
i.e., solving PDEs using a physics-informed iterative algorithm trained on
data. Our method learns to condition a gradient descent algorithm that
automatically adapts to each PDE instance, significantly accelerating and
stabilizing the optimization process and enabling faster convergence of
physics-aware models. Furthermore, while traditional physics-informed methods
solve for a single PDE instance, our approach addresses parametric PDEs.
Specifically, our method integrates the physical loss gradient with the PDE
parameters to solve over a distribution of PDE parameters, including
coefficients, initial conditions, or boundary conditions. We demonstrate the
effectiveness of our method through empirical experiments on multiple datasets,
comparing training and test-time optimization performance."
Dynamic Neural Potential Field: Online Trajectory Optimization in Presence of Moving Obstacles,cs.RO,Robotics,2024-10-09,"We address a task of local trajectory planning for the mobile robot in the
presence of static and dynamic obstacles. Local trajectory is obtained as a
numerical solution of the Model Predictive Control (MPC) problem. Collision
avoidance may be provided by adding repulsive potential of the obstacles to the
cost function of MPC. We develop an approach, where repulsive potential is
estimated by the neural model. We propose and explore three possible strategies
of handling dynamic obstacles. First, environment with dynamic obstacles is
considered as a sequence of static environments. Second, the neural model
predict a sequence of repulsive potential at once. Third, the neural model
predict future repulsive potential step by step in autoregressive mode. We
implement these strategies and compare it with CIAO* and MPPI using BenchMR
framework. First two strategies showed higher performance than CIAO* and MPPI
while preserving safety constraints. The third strategy was a bit slower,
however it still satisfy time limits. We deploy our approach on Husky UGV
mobile platform, which move through the office corridors under proposed MPC
local trajectory planner. The code and trained models are available at
\url{https://github.com/CognitiveAISystems/Dynamic-Neural-Potential-Field}."
An Improved Approach for Cardiac MRI Segmentation based on 3D UNet Combined with Papillary Muscle Exclusion,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Left ventricular ejection fraction (LVEF) is the most important clinical
parameter of cardiovascular function. The accuracy in estimating this parameter
is highly dependent upon the precise segmentation of the left ventricle (LV)
structure at the end diastole and systole phases. Therefore, it is crucial to
develop robust algorithms for the precise segmentation of the heart structure
during different phases. Methodology: In this work, an improved 3D UNet model
is introduced to segment the myocardium and LV, while excluding papillary
muscles, as per the recommendation of the Society for Cardiovascular Magnetic
Resonance. For the practical testing of the proposed framework, a total of
8,400 cardiac MRI images were collected and analysed from the military hospital
in Tunis (HMPIT), as well as the popular ACDC public dataset. As performance
metrics, we used the Dice coefficient and the F1 score for validation/testing
of the LV and the myocardium segmentation. Results: The data was split into
70%, 10%, and 20% for training, validation, and testing, respectively. It is
worth noting that the proposed segmentation model was tested across three axis
views: basal, medio basal and apical at two different cardiac phases: end
diastole and end systole instances. The experimental results showed a Dice
index of 0.965 and 0.945, and an F1 score of 0.801 and 0.799, at the end
diastolic and systolic phases, respectively. Additionally, clinical evaluation
outcomes revealed a significant difference in the LVEF and other clinical
parameters when the papillary muscles were included or excluded."
Multi-Neuron Unleashes Expressivity of ReLU Networks Under Convex Relaxation,cs.LG,Machine Learning,2024-10-09,"Neural work certification has established itself as a crucial tool for
ensuring the robustness of neural networks. Certification methods typically
rely on convex relaxations of the feasible output set to provide sound bounds.
However, complete certification requires exact bounds, which strongly limits
the expressivity of ReLU networks: even for the simple ``$\max$'' function in
$\mathbb{R}^2$, there does not exist a ReLU network that expresses this
function and can be exactly bounded by single-neuron relaxation methods. This
raises the question whether there exists a convex relaxation that can provide
exact bounds for general continuous piecewise linear functions in
$\mathbb{R}^n$. In this work, we answer this question affirmatively by showing
that (layer-wise) multi-neuron relaxation provides complete certification for
general ReLU networks. Based on this novel result, we show that the
expressivity of ReLU networks is no longer limited under multi-neuron
relaxation. To the best of our knowledge, this is the first positive result on
the completeness of convex relaxations, shedding light on the practice of
certified robustness."
Shap-Select: Lightweight Feature Selection Using SHAP Values and Regression,cs.LG,Machine Learning,2024-10-09,"Feature selection is an essential process in machine learning, especially
when dealing with high-dimensional datasets. It helps reduce the complexity of
machine learning models, improve performance, mitigate overfitting, and
decrease computation time. This paper presents a novel feature selection
framework, shap-select. The framework conducts a linear or logistic regression
of the target on the Shapley values of the features, on the validation set, and
uses the signs and significance levels of the regression coefficients to
implement an efficient heuristic for feature selection in tabular regression
and classification tasks. We evaluate shap-select on the Kaggle credit card
fraud dataset, demonstrating its effectiveness compared to established methods
such as Recursive Feature Elimination (RFE), HISEL (a mutual information-based
feature selection method), Boruta and a simpler Shapley value-based method. Our
findings show that shap-select combines interpretability, computational
efficiency, and performance, offering a robust solution for feature selection."
Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning,cs.LG,Machine Learning,2024-10-09,"Over-parameterized models are typically vulnerable to membership inference
attacks, which aim to determine whether a specific sample is included in the
training of a given model. Previous Weight regularizations (e.g., L1
regularization) typically impose uniform penalties on all parameters, leading
to a suboptimal tradeoff between model utility and privacy. In this work, we
first show that only a small fraction of parameters substantially impact the
privacy risk. In light of this, we propose Privacy-aware Sparsity Tuning
(PAST), a simple fix to the L1 Regularization, by employing adaptive penalties
to different parameters. Our key idea behind PAST is to promote sparsity in
parameters that significantly contribute to privacy leakage. In particular, we
construct the adaptive weight for each parameter based on its privacy
sensitivity, i.e., the gradient of the loss gap with respect to the parameter.
Using PAST, the network shrinks the loss gap between members and non-members,
leading to strong resistance to privacy attacks. Extensive experiments
demonstrate the superiority of PAST, achieving a state-of-the-art balance in
the privacy-utility trade-off."
Rethinking the Evaluation of Visible and Infrared Image Fusion,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Visible and Infrared Image Fusion (VIF) has garnered significant interest
across a wide range of high-level vision tasks, such as object detection and
semantic segmentation. However, the evaluation of VIF methods remains
challenging due to the absence of ground truth. This paper proposes a
Segmentation-oriented Evaluation Approach (SEA) to assess VIF methods by
incorporating the semantic segmentation task and leveraging segmentation labels
available in latest VIF datasets. Specifically, SEA utilizes universal
segmentation models, capable of handling diverse images and classes, to predict
segmentation outputs from fused images and compare these outputs with
segmentation labels. Our evaluation of recent VIF methods using SEA reveals
that their performance is comparable or even inferior to using visible images
only, despite nearly half of the infrared images demonstrating better
performance than visible images. Further analysis indicates that the two
metrics most correlated to our SEA are the gradient-based fusion metric
$Q_{\text{ABF}}$ and the visual information fidelity metric $Q_{\text{VIFF}}$
in conventional VIF evaluation metrics, which can serve as proxies when
segmentation labels are unavailable. We hope that our evaluation will guide the
development of novel and practical VIF methods. The code has been released in
\url{https://github.com/Yixuan-2002/SEA/}."
Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Large language models (LLMs) have demonstrated immense utility across various
industries. However, as LLMs advance, the risk of harmful outputs increases due
to incorrect or malicious instruction prompts. While current methods
effectively address jailbreak risks, they share common limitations: 1) Judging
harmful responses from the prefill-level lacks utilization of the model's
decoding outputs, leading to relatively lower effectiveness and robustness. 2)
Rejecting potentially harmful responses based on a single evaluation can
significantly impair the model's helpfulness.This paper examines the LLMs'
capability to recognize harmful outputs, revealing and quantifying their
proficiency in assessing the danger of previous tokens. Motivated by pilot
experiment results, we design a robust defense mechanism at the decoding level.
Our novel decoder-oriented, step-by-step defense architecture corrects harmful
queries directly rather than rejecting them outright. We introduce speculative
decoding to enhance usability and facilitate deployment to boost secure
decoding speed. Extensive experiments demonstrate that our approach improves
model security without compromising reasoning speed. Notably, our method
leverages the model's ability to discern hazardous information, maintaining its
helpfulness compared to existing methods."
QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space Model,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Recent advancements in State Space Models, notably Mamba, have demonstrated
superior performance over the dominant Transformer models, particularly in
reducing the computational complexity from quadratic to linear. Yet,
difficulties in adapting Mamba from language to vision tasks arise due to the
distinct characteristics of visual data, such as the spatial locality and
adjacency within images and large variations in information granularity across
visual tokens. Existing vision Mamba approaches either flatten tokens into
sequences in a raster scan fashion, which breaks the local adjacency of images,
or manually partition tokens into windows, which limits their long-range
modeling and generalization capabilities. To address these limitations, we
present a new vision Mamba model, coined QuadMamba, that effectively captures
local dependencies of varying granularities via quadtree-based image partition
and scan. Concretely, our lightweight quadtree-based scan module learns to
preserve the 2D locality of spatial regions within learned window quadrants.
The module estimates the locality score of each token from their features,
before adaptively partitioning tokens into window quadrants. An omnidirectional
window shifting scheme is also introduced to capture more intact and
informative features across different local regions. To make the discretized
quadtree partition end-to-end trainable, we further devise a sequence masking
strategy based on Gumbel-Softmax and its straight-through gradient estimator.
Extensive experiments demonstrate that QuadMamba achieves state-of-the-art
performance in various vision tasks, including image classification, object
detection, instance segmentation, and semantic segmentation. The code is in
https://github.com/VISIONSJTU/QuadMamba."
Seg2Act: Global Context-aware Action Generation for Document Logical Structuring,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Document logical structuring aims to extract the underlying hierarchical
structure of documents, which is crucial for document intelligence. Traditional
approaches often fall short in handling the complexity and the variability of
lengthy documents. To address these issues, we introduce Seg2Act, an
end-to-end, generation-based method for document logical structuring,
revisiting logical structure extraction as an action generation task.
Specifically, given the text segments of a document, Seg2Act iteratively
generates the action sequence via a global context-aware generative model, and
simultaneously updates its global context and current logical structure based
on the generated actions. Experiments on ChCatExt and HierDoc datasets
demonstrate the superior performance of Seg2Act in both supervised and transfer
learning settings."
Efficient Weight-Space Laplace-Gaussian Filtering and Smoothing for Sequential Deep Learning,cs.LG,Machine Learning,2024-10-09,"Efficiently learning a sequence of related tasks, such as in continual
learning, poses a significant challenge for neural nets due to the delicate
trade-off between catastrophic forgetting and loss of plasticity. We address
this challenge with a grounded framework for sequentially learning related
tasks based on Bayesian inference. Specifically, we treat the model's
parameters as a nonlinear Gaussian state-space model and perform efficient
inference using Gaussian filtering and smoothing. This general formalism
subsumes existing continual learning approaches, while also offering a clearer
conceptual understanding of its components. Leveraging Laplace approximations
during filtering, we construct Gaussian posterior measures on the weight space
of a neural network for each task. We use it as an efficient regularizer by
exploiting the structure of the generalized Gauss-Newton matrix (GGN) to
construct diagonal plus low-rank approximations. The dynamics model allows
targeted control of the learning process and the incorporation of
domain-specific knowledge, such as modeling the type of shift between tasks.
Additionally, using Bayesian approximate smoothing can enhance the performance
of task-specific models without needing to re-access any data."
Diffuse or Confuse: A Diffusion Deepfake Speech Dataset,cs.CR,Cryptography and Security,2024-10-09,"Advancements in artificial intelligence and machine learning have
significantly improved synthetic speech generation. This paper explores
diffusion models, a novel method for creating realistic synthetic speech. We
create a diffusion dataset using available tools and pretrained models.
Additionally, this study assesses the quality of diffusion-generated deepfakes
versus non-diffusion ones and their potential threat to current deepfake
detection systems. Findings indicate that the detection of diffusion-based
deepfakes is generally comparable to non-diffusion deepfakes, with some
variability based on detector architecture. Re-vocoding with diffusion vocoders
shows minimal impact, and the overall speech quality is comparable to
non-diffusion methods."
From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Hallucinations in large vision-language models (LVLMs) are a significant
challenge, i.e., generating objects that are not presented in the visual input,
which impairs their reliability. Recent studies often attribute hallucinations
to a lack of understanding of visual input, yet ignore a more fundamental
issue: the model's inability to effectively extract or decouple visual
features. In this paper, we revisit the hallucinations in LVLMs from an
architectural perspective, investigating whether the primary cause lies in the
visual encoder (feature extraction) or the modal alignment module (feature
decoupling). Motivated by our findings on the preliminary investigation, we
propose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs.
This plug-and-play method can be integrated into various LVLMs, utilizing
adaptive virtual tokens to extract object features from bounding boxes, thereby
addressing hallucinations caused by insufficient decoupling of visual features.
PATCH achieves state-of-the-art performance on multiple multi-modal
hallucination datasets. We hope this approach provides researchers with deeper
insights into the underlying causes of hallucinations in LVLMs, fostering
further advancements and innovation in this field."
Discrete time model predictive control for humanoid walking with step adjustment,cs.RO,Robotics,2024-10-09,"This paper presents a Discrete-Time Model Predictive Controller (MPC) for
humanoid walking with online footstep adjustment. The proposed controller
utilizes a hierarchical control approach. The high-level controller uses a
low-dimensional Linear Inverted Pendulum Model (LIPM) to determine desired foot
placement and Center of Mass (CoM) motion, to prevent falls while maintaining
the desired velocity. A Task Space Controller (TSC) then tracks the desired
motion obtained from the high-level controller, exploiting the whole-body
dynamics of the humanoid. Our approach differs from existing MPC methods for
walking pattern generation by not relying on a predefined foot-plan or a
reference center of pressure (CoP) trajectory. The overall approach is tested
in simulation on a torque-controlled Humanoid Robot. Results show that proposed
control approach generates stable walking and prevents fall against push
disturbances."
Deep End-to-End Survival Analysis with Temporal Consistency,cs.LG,Machine Learning,2024-10-09,"In this study, we present a novel Survival Analysis algorithm designed to
efficiently handle large-scale longitudinal data. Our approach draws
inspiration from Reinforcement Learning principles, particularly the Deep
Q-Network paradigm, extending Temporal Learning concepts to Survival
Regression. A central idea in our method is temporal consistency, a hypothesis
that past and future outcomes in the data evolve smoothly over time. Our
framework uniquely incorporates temporal consistency into large datasets by
providing a stable training signal that captures long-term temporal
relationships and ensures reliable updates. Additionally, the method supports
arbitrarily complex architectures, enabling the modeling of intricate temporal
dependencies, and allows for end-to-end training. Through numerous experiments
we provide empirical evidence demonstrating our framework's ability to exploit
temporal consistency across datasets of varying sizes. Moreover, our algorithm
outperforms benchmarks on datasets with long sequences, demonstrating its
ability to capture long-term patterns. Finally, ablation studies show how our
method enhances training stability."
Mind Your Questions Towards Backdoor Attacks on Text-to-Visualization Models,cs.CR,Cryptography and Security,2024-10-09,"Text-to-visualization (text-to-vis) models have become valuable tools in the
era of big data, enabling users to generate data visualizations and make
informed decisions through natural language queries (NLQs). Despite their
widespread application, the security vulnerabilities of these models have been
largely overlooked. To address this gap, we propose VisPoison, a novel
framework designed to identify these vulnerabilities of current text-to-vis
models systematically. VisPoison introduces two types of triggers that activate
three distinct backdoor attacks, potentially leading to data exposure,
misleading visualizations, or denial-of-service (DoS) incidents. The framework
features both proactive and passive attack mechanisms: proactive attacks
leverage rare-word triggers to access confidential data, while passive attacks,
triggered unintentionally by users, exploit a first-word trigger method,
causing errors or DoS events in visualizations. Through extensive experiments
on both trainable and in-context learning (ICL)-based text-to-vis models,
\textit{VisPoison} achieves attack success rates of over 90\%, highlighting the
security problem of current text-to-vis models. Additionally, we explore two
types of defense mechanisms against these attacks, but the results show that
existing countermeasures are insufficient, underscoring the pressing need for
more robust security solutions in text-to-vis systems."
HERM: Benchmarking and Enhancing Multimodal LLMs for Human-Centric Understanding,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"The significant advancements in visual understanding and instruction
following from Multimodal Large Language Models (MLLMs) have opened up more
possibilities for broader applications in diverse and universal human-centric
scenarios. However, existing image-text data may not support the precise
modality alignment and integration of multi-grained information, which is
crucial for human-centric visual understanding. In this paper, we introduce
HERM-Bench, a benchmark for evaluating the human-centric understanding
capabilities of MLLMs. Our work reveals the limitations of existing MLLMs in
understanding complex human-centric scenarios. To address these challenges, we
present HERM-100K, a comprehensive dataset with multi-level human-centric
annotations, aimed at enhancing MLLMs' training. Furthermore, we develop
HERM-7B, a MLLM that leverages enhanced training data from HERM-100K.
Evaluations on HERM-Bench demonstrate that HERM-7B significantly outperforms
existing MLLMs across various human-centric dimensions, reflecting the current
inadequacy of data annotations used in MLLM training for human-centric visual
understanding. This research emphasizes the importance of specialized datasets
and benchmarks in advancing the MLLMs' capabilities for human-centric
understanding."
To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"In recent years, multimodal large language models (MLLMs) have garnered
significant attention from both industry and academia. However, there is still
considerable debate on constructing MLLM architectures, particularly regarding
the selection of appropriate connectors for perception tasks of varying
granularities. This paper systematically investigates the impact of connectors
on MLLM performance. Specifically, we classify connectors into
feature-preserving and feature-compressing types. Utilizing a unified
classification standard, we categorize sub-tasks from three comprehensive
benchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained
perception, fine-grained perception, and reasoning, and evaluate the
performance. Our findings reveal that feature-preserving connectors excel in
\emph{fine-grained perception} tasks due to their ability to retain detailed
visual information. In contrast, feature-compressing connectors, while less
effective in fine-grained perception tasks, offer significant speed advantages
and perform comparably in \emph{coarse-grained perception} and \emph{reasoning}
tasks. These insights are crucial for guiding MLLM architecture design and
advancing the optimization of MLLM architectures."
DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh Hybrid Representation,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Recent advancements in 2D/3D generative techniques have facilitated the
generation of dynamic 3D objects from monocular videos. Previous methods mainly
rely on the implicit neural radiance fields (NeRF) or explicit Gaussian
Splatting as the underlying representation, and struggle to achieve
satisfactory spatial-temporal consistency and surface appearance. Drawing
inspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a
novel framework combining mesh representation with geometric skinning technique
to generate high-quality 4D object from a monocular video. Instead of utilizing
classical texture map for appearance, we bind Gaussian splats to triangle face
of mesh for differentiable optimization of both the texture and mesh vertices.
In particular, DreamMesh4D begins with a coarse mesh obtained through an
image-to-3D generation procedure. Sparse points are then uniformly sampled
across the mesh surface, and are used to build a deformation graph to drive the
motion of the 3D object for the sake of computational efficiency and providing
additional constraint. For each step, transformations of sparse control points
are predicted using a deformation network, and the mesh vertices as well as the
surface Gaussians are deformed via a novel geometric skinning algorithm, which
is a hybrid approach combining LBS (linear blending skinning) and DQS
(dual-quaternion skinning), mitigating drawbacks associated with both
approaches. The static surface Gaussians and mesh vertices as well as the
deformation network are learned via reference view photometric loss, score
distillation loss as well as other regularizers in a two-stage manner.
Extensive experiments demonstrate superior performance of our method.
Furthermore, our method is compatible with modern graphic pipelines, showcasing
its potential in the 3D gaming and film industry."
Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention,cs.LG,Machine Learning,2024-10-09,"In the realm of graph learning, there is a category of methods that
conceptualize graphs as hierarchical structures, utilizing node clustering to
capture broader structural information. While generally effective, these
methods often rely on a fixed graph coarsening routine, leading to overly
homogeneous cluster representations and loss of node-level information. In this
paper, we envision the graph as a network of interconnected node sets without
compressing each cluster into a single embedding. To enable effective
information transfer among these node sets, we propose the Node-to-Cluster
Attention (N2C-Attn) mechanism. N2C-Attn incorporates techniques from Multiple
Kernel Learning into the kernelized attention framework, effectively capturing
information at both node and cluster levels. We then devise an efficient form
for N2C-Attn using the cluster-wise message-passing framework, achieving linear
time complexity. We further analyze how N2C-Attn combines bi-level feature maps
of queries and keys, demonstrating its capability to merge dual-granularity
information. The resulting architecture, Cluster-wise Graph Transformer
(Cluster-GT), which uses node clusters as tokens and employs our proposed
N2C-Attn module, shows superior performance on various graph-level tasks. Code
is available at https://github.com/LUMIA-Group/Cluster-wise-Graph-Transformer."
Utilizing Transfer Learning and pre-trained Models for Effective Forest Fire Detection: A Case Study of Uttarakhand,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Forest fires pose a significant threat to the environment, human life, and
property. Early detection and response are crucial to mitigating the impact of
these disasters. However, traditional forest fire detection methods are often
hindered by our reliability on manual observation and satellite imagery with
low spatial resolution. This paper emphasizes the role of transfer learning in
enhancing forest fire detection in India, particularly in overcoming data
collection challenges and improving model accuracy across various regions. We
compare traditional learning methods with transfer learning, focusing on the
unique challenges posed by regional differences in terrain, climate, and
vegetation. Transfer learning can be categorized into several types based on
the similarity between the source and target tasks, as well as the type of
knowledge transferred. One key method is utilizing pre-trained models for
efficient transfer learning, which significantly reduces the need for extensive
labeled data. We outline the transfer learning process, demonstrating how
researchers can adapt pre-trained models like MobileNetV2 for specific tasks
such as forest fire detection. Finally, we present experimental results from
training and evaluating a deep learning model using the Uttarakhand forest fire
dataset, showcasing the effectiveness of transfer learning in this context."
"Inference over Unseen Entities, Relations and Literals on Knowledge Graphs",cs.LG,Machine Learning,2024-10-09,"In recent years, knowledge graph embedding models have been successfully
applied in the transductive setting to tackle various challenging tasks
including link prediction, and query answering. Yet, the transductive setting
does not allow for reasoning over unseen entities, relations, let alone
numerical or non-numerical literals. Although increasing efforts are put into
exploring inductive scenarios, inference over unseen entities, relations, and
literals has yet to come. This limitation prohibits the existing methods from
handling real-world dynamic knowledge graphs involving heterogeneous
information about the world. Here, we propose a remedy to this limitation. We
propose the attentive byte-pair encoding layer (BytE) to construct a triple
embedding from a sequence of byte-pair encoded subword units of entities and
relations. Compared to the conventional setting, BytE leads to massive feature
reuse via weight tying, since it forces a knowledge graph embedding model to
learn embeddings for subword units instead of entities and relations directly.
Consequently, the size of the embedding matrices are not anymore bound to the
unique number of entities and relations of a knowledge graph. Experimental
results show that BytE improves the link prediction performance of 4 knowledge
graph embedding models on datasets where the syntactic representations of
triples are semantically meaningful. However, benefits of training a knowledge
graph embedding model with BytE dissipate on knowledge graphs where entities
and relations are represented with plain numbers or URIs. We provide an open
source implementation of BytE to foster reproducible research."
CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Multi-task learning (MTL) benefits the fine-tuning of large language models
(LLMs) by providing a single model with improved performance and generalization
ability across tasks, presenting a resource-efficient alternative to developing
separate models for each task. Yet, existing MTL strategies for LLMs often fall
short by either being computationally intensive or failing to ensure
simultaneous task convergence. This paper presents CoBa, a new MTL approach
designed to effectively manage task convergence balance with minimal
computational overhead. Utilizing Relative Convergence Scores (RCS), Absolute
Convergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically
adjusts task weights during the training process, ensuring that the validation
loss of all tasks progress towards convergence at an even pace while mitigating
the issue of individual task divergence. The results of our experiments
involving three disparate datasets underscore that this approach not only
fosters equilibrium in task improvement but enhances the LLMs' performance by
up to 13% relative to the second-best baselines. Code is open-sourced at
https://github.com/codefuse-ai/MFTCoder."
Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Recent large language models (LLMs) have demonstrated remarkable
generalization abilities in mathematics and logical reasoning tasks. Prior
research indicates that LLMs pre-trained with programming language data exhibit
high mathematical and reasoning abilities; however, this causal relationship
has not been rigorously tested. Our research aims to verify which programming
languages and features during pre-training affect logical inference
performance. Specifically, we pre-trained decoder-based language models from
scratch using datasets from ten programming languages (e.g., Python, C, Java)
and three natural language datasets (Wikipedia, Fineweb, C4) under identical
conditions. Thereafter, we evaluated the trained models in a few-shot
in-context learning setting on logical reasoning tasks: FLD and bAbi, which do
not require commonsense or world knowledge. The results demonstrate that nearly
all models trained with programming languages consistently outperform those
trained with natural languages, indicating that programming languages contain
factors that elicit logic inference performance. In addition, we found that
models trained with programming languages exhibit a better ability to follow
instructions compared to those trained with natural languages. Further analysis
reveals that the depth of Abstract Syntax Trees representing parsed results of
programs also affects logical reasoning performance. These findings will offer
insights into the essential elements of pre-training for acquiring the
foundational abilities of LLMs."
MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Talking face generation (TFG) aims to animate a target identity's face to
create realistic talking videos. Personalized TFG is a variant that emphasizes
the perceptual identity similarity of the synthesized result (from the
perspective of appearance and talking style). While previous works typically
solve this problem by learning an individual neural radiance field (NeRF) for
each identity to implicitly store its static and dynamic information, we find
it inefficient and non-generalized due to the per-identity-per-training
framework and the limited training data. To this end, we propose MimicTalk, the
first attempt that exploits the rich knowledge from a NeRF-based
person-agnostic generic model for improving the efficiency and robustness of
personalized TFG. To be specific, (1) we first come up with a person-agnostic
3D TFG model as the base model and propose to adapt it into a specific
identity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help
the model learn the personalized static appearance and facial dynamic features;
(3) To generate the facial motion of the personalized talking style, we propose
an in-context stylized audio-to-motion model that mimics the implicit talking
style provided in the reference video without information loss by an explicit
style representation. The adaptation process to an unseen identity can be
performed in 15 minutes, which is 47 times faster than previous
person-dependent methods. Experiments show that our MimicTalk surpasses
previous baselines regarding video quality, efficiency, and expressiveness.
Source code and video samples are available at https://mimictalk.github.io ."
Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"While advancements in NLP have significantly improved the performance of
Large Language Models (LLMs) on tasks requiring vertical thinking, their
lateral thinking capabilities remain under-explored and challenging to measure
due to the complexity of assessing creative thought processes and the scarcity
of relevant data. To address these challenges, we introduce SPLAT, a benchmark
leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.
This benchmark, containing 975 graded situation puzzles across three difficulty
levels, employs a new multi-turn player-judge framework instead of the
traditional model-based evaluation, which often necessitates a stronger
evaluation model. This framework simulates an interactive game where the model
(player) asks the evaluation model (judge) questions about an incomplete story
to infer the full scenario. The judge answers based on a detailed reference
scenario or evaluates if the player's predictions align with the reference one.
This approach lessens dependence on more robust evaluation models, enabling the
assessment of state-of-the-art LLMs. The experiments demonstrate that a robust
evaluation model, such as WizardLM-2, closely matches human judgements in both
intermediate question-answering and final scenario accuracy, achieving over 80%
agreement-similar to the agreement levels among humans. Furthermore, applying
data and reasoning processes from our benchmark to other lateral
thinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to
performance enhancements. This suggests that our benchmark effectively
evaluates and elicits the lateral thinking abilities of LLMs. Code is available
at: https://github.com/chenqi008/LateralThinking."
Evaluating the Impact of Point Cloud Colorization on Semantic Segmentation Accuracy,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Point cloud semantic segmentation, the process of classifying each point into
predefined categories, is essential for 3D scene understanding. While
image-based segmentation is widely adopted due to its maturity, methods relying
solely on RGB information often suffer from degraded performance due to color
inaccuracies. Recent advancements have incorporated additional features such as
intensity and geometric information, yet RGB channels continue to negatively
impact segmentation accuracy when errors in colorization occur. Despite this,
previous studies have not rigorously quantified the effects of erroneous
colorization on segmentation performance. In this paper, we propose a novel
statistical approach to evaluate the impact of inaccurate RGB information on
image-based point cloud segmentation. We categorize RGB inaccuracies into two
types: incorrect color information and similar color information. Our results
demonstrate that both types of color inaccuracies significantly degrade
segmentation accuracy, with similar color errors particularly affecting the
extraction of geometric features. These findings highlight the critical need to
reassess the role of RGB information in point cloud segmentation and its
implications for future algorithm design."
Scaling Laws for Mixed quantization in Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Post-training quantization of Large Language Models (LLMs) has proven
effective in reducing the computational requirements for running inference on
these models. In this study, we focus on a straightforward question: When
aiming for a specific accuracy or perplexity target for low-precision
quantization, how many high-precision numbers or calculations are required to
preserve as we scale LLMs to larger sizes? We first introduce a critical metric
named the quantization ratio, which compares the number of parameters quantized
to low-precision arithmetic against the total parameter count. Through
extensive and carefully controlled experiments across different model families,
arithmetic types, and quantization granularities (e.g. layer-wise,
matmul-wise), we identify two central phenomenons. 1) The larger the models,
the better they can preserve performance with an increased quantization ratio,
as measured by perplexity in pre-training tasks or accuracy in downstream
tasks. 2) The finer the granularity of mixed-precision quantization (e.g.,
matmul-wise), the more the model can increase the quantization ratio. We
believe these observed phenomena offer valuable insights for future AI hardware
design and the development of advanced Efficient AI algorithms."
Collective perception for tracking people with a robot swarm,cs.RO,Robotics,2024-10-09,"Swarm perception refers to the ability of a robot swarm to utilize the
perception capabilities of each individual robot, forming a collective
understanding of the environment. Their distributed nature enables robot swarms
to continuously monitor dynamic environments by maintaining a constant presence
throughout the space.In this study, we present a preliminary experiment on the
collective tracking of people using a robot swarm. The experiment was conducted
in simulation across four different office environments, with swarms of varying
sizes. The robots were provided with images sampled from a dataset of
real-world office environment pictures.We measured the time distribution
required for a robot to detect a person changing location and to propagate this
information to increasing fractions of the swarm. The results indicate that
robot swarms show significant promise in monitoring dynamic environments."
Suppress Content Shift: Better Diffusion Features via Off-the-Shelf Generation Techniques,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Diffusion models are powerful generative models, and this capability can also
be applied to discrimination. The inner activations of a pre-trained diffusion
model can serve as features for discriminative tasks, namely, diffusion
feature. We discover that diffusion feature has been hindered by a hidden yet
universal phenomenon that we call content shift. To be specific, there are
content differences between features and the input image, such as the exact
shape of a certain object. We locate the cause of content shift as one inherent
characteristic of diffusion models, which suggests the broad existence of this
phenomenon in diffusion feature. Further empirical study also indicates that
its negative impact is not negligible even when content shift is not visually
perceivable. Hence, we propose to suppress content shift to enhance the overall
quality of diffusion features. Specifically, content shift is related to the
information drift during the process of recovering an image from the noisy
input, pointing out the possibility of turning off-the-shelf generation
techniques into tools for content shift suppression. We further propose a
practical guideline named GATE to efficiently evaluate the potential benefit of
a technique and provide an implementation of our methodology. Despite the
simplicity, the proposed approach has achieved superior results on various
tasks and datasets, validating its potential as a generic booster for diffusion
features. Our code is available at
https://github.com/Darkbblue/diffusion-content-shift."
MatMamba: A Matryoshka State Space Model,cs.LG,Machine Learning,2024-10-09,"State Space Models (SSMs) like Mamba2 are a promising alternative to
Transformers, with faster theoretical training and inference times --
especially for long context lengths. Recent work on Matryoshka Representation
Learning -- and its application to Transformer backbones in works like
MatFormer -- showed how to introduce nested granularities of smaller submodels
in one universal elastic model. In this work, we present MatMamba: a state
space model which combines Matryoshka-style learning with Mamba2, by modifying
the block to contain nested dimensions to enable joint training and adaptive
inference. MatMamba allows for efficient and adaptive deployment across various
model sizes. We train a single large MatMamba model and are able to get a
number of smaller nested models for free -- while maintaining or improving upon
the performance of a baseline smaller model trained from scratch. We train
language and image models at a variety of parameter sizes from 35M to 1.4B. Our
results on ImageNet and FineWeb show that MatMamba models scale comparably to
Transformers, while having more efficient inference characteristics. This makes
MatMamba a practically viable option for deploying large-scale models in an
elastic way based on the available inference compute. Code and models are open
sourced at \url{https://github.com/ScaledFoundations/MatMamba}"
Guaranteed Generation from Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"As large language models (LLMs) are increasingly used across various
applications, there is a growing need to control text generation to satisfy
specific constraints or requirements. This raises a crucial question: Is it
possible to guarantee strict constraint satisfaction in generated outputs while
preserving the distribution of the original model as much as possible? We first
define the ideal distribution - the one closest to the original model, which
also always satisfies the expressed constraint - as the ultimate goal of
guaranteed generation. We then state a fundamental limitation, namely that it
is impossible to reach that goal through autoregressive training alone. This
motivates the necessity of combining training-time and inference-time methods
to enforce such guarantees. Based on this insight, we propose GUARD, a simple
yet effective approach that combines an autoregressive proposal distribution
with rejection sampling. Through GUARD's theoretical properties, we show how
controlling the KL divergence between a specific proposal and the target ideal
distribution simultaneously optimizes inference speed and distributional
closeness. To validate these theoretical concepts, we conduct extensive
experiments on two text generation settings with hard-to-satisfy constraints: a
lexical constraint scenario and a sentiment reversal scenario. These
experiments show that GUARD achieves perfect constraint satisfaction while
almost preserving the ideal distribution with highly improved inference
efficiency. GUARD provides a principled approach to enforcing strict guarantees
for LLMs without compromising their generative capabilities."
Analysis of different disparity estimation techniques on aerial stereo image datasets,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"With the advent of aerial image datasets, dense stereo matching has gained
tremendous progress. This work analyses dense stereo correspondence analysis on
aerial images using different techniques. Traditional methods, optimization
based methods and learning based methods have been implemented and compared
here for aerial images. For traditional methods, we implemented the
architecture of Stereo SGBM while using different cost functions to get an
understanding of their performance on aerial datasets. Analysis of most of the
methods in standard datasets has shown good performance, however in case of
aerial dataset, not much benchmarking is available. Visual qualitative and
quantitative analysis has been carried out for two stereo aerial datasets in
order to compare different cost functions and techniques for the purpose of
depth estimation from stereo images. Using existing pre-trained models, recent
learning based architectures have also been tested on stereo pairs along with
different cost functions in SGBM. The outputs and given ground truth are
compared using MSE, SSIM and other error metrics."
Calibrating Verbalized Probabilities for Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Calibrating verbalized probabilities presents a novel approach for reliably
assessing and leveraging outputs from black-box Large Language Models (LLMs).
Recent methods have demonstrated improved calibration by applying techniques
like Platt scaling or temperature scaling to the confidence scores generated by
LLMs. In this paper, we explore the calibration of verbalized probability
distributions for discriminative tasks. First, we investigate the capability of
LLMs to generate probability distributions over categorical labels. We
theoretically and empirically identify the issue of re-softmax arising from the
scaling of verbalized probabilities, and propose using the invert softmax trick
to approximate the ""logit"" by inverting verbalized probabilities. Through
extensive evaluation on three public datasets, we demonstrate: (1) the robust
capability of LLMs in generating class distributions, and (2) the effectiveness
of the invert softmax trick in estimating logits, which, in turn, facilitates
post-calibration adjustments."
MERGE: Matching Electronic Results with Genuine Evidence for verifiable voting in person at remote locations,cs.CR,Cryptography and Security,2024-10-09,"Overseas military personnel often face significant challenges in
participating in elections due to the slow pace of traditional mail systems,
which can result in ballots missing crucial deadlines. While internet-based
voting offers a faster alternative, it introduces serious risks to the
integrity and privacy of the voting process. We introduce the MERGE protocol to
address these issues by combining the speed of electronic ballot delivery with
the reliability of paper returns. This protocol allows voters to submit an
electronic record of their vote quickly while simultaneously mailing a paper
ballot for verification. The electronic record can be used for preliminary
results, but the paper ballot is used in a Risk Limiting Audit (RLA) if
received in time, ensuring the integrity of the election. This approach extends
the time window for ballot arrival without undermining the security and
accuracy of the vote count."
PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"In this work, we introduce PII-Scope, a comprehensive benchmark designed to
evaluate state-of-the-art methodologies for PII extraction attacks targeting
LLMs across diverse threat settings. Our study provides a deeper understanding
of these attacks by uncovering several hyperparameters (e.g., demonstration
selection) crucial to their effectiveness. Building on this understanding, we
extend our study to more realistic attack scenarios, exploring PII attacks that
employ advanced adversarial strategies, including repeated and diverse
querying, and leveraging iterative learning for continual PII extraction.
Through extensive experimentation, our results reveal a notable underestimation
of PII leakage in existing single-query attacks. In fact, we show that with
sophisticated adversarial capabilities and a limited query budget, PII
extraction rates can increase by up to fivefold when targeting the pretrained
model. Moreover, we evaluate PII leakage on finetuned models, showing that they
are more vulnerable to leakage than pretrained models. Overall, our work
establishes a rigorous empirical benchmark for PII extraction attacks in
realistic threat scenarios and provides a strong foundation for developing
effective mitigation strategies."
ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness in Web Agents,cs.AI,Artificial Intelligence,2024-10-09,"Recent advancements in LLM-based web agents have introduced novel
architectures and benchmarks showcasing progress in autonomous web navigation
and interaction. However, most existing benchmarks prioritize effectiveness and
accuracy, overlooking crucial factors like safety and trustworthiness which are
essential for deploying web agents in enterprise settings. The risks of unsafe
web agent behavior, such as accidentally deleting user accounts or performing
unintended actions in critical business operations, pose significant barriers
to widespread adoption.In this paper, we present ST-WebAgentBench, a new online
benchmark specifically designed to evaluate the safety and trustworthiness of
web agents in enterprise contexts. This benchmark is grounded in a detailed
framework that defines safe and trustworthy (ST) agent behavior, outlines how
ST policies should be structured and introduces the Completion under Policies
metric to assess agent performance. Our evaluation reveals that current SOTA
agents struggle with policy adherence and cannot yet be relied upon for
critical business applications. Additionally, we propose architectural
principles aimed at improving policy awareness and compliance in web agents. We
open-source this benchmark and invite the community to contribute, with the
goal of fostering a new generation of safer, more trustworthy AI agents."
Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Large vision-language models (LVLMs) integrate visual information into large
language models, showcasing remarkable multi-modal conversational capabilities.
However, the visual modules introduces new challenges in terms of robustness
for LVLMs, as attackers can craft adversarial images that are visually clean
but may mislead the model to generate incorrect answers. In general, LVLMs rely
on vision encoders to transform images into visual tokens, which are crucial
for the language models to perceive image contents effectively. Therefore, we
are curious about one question: Can LVLMs still generate correct responses when
the encoded visual tokens are attacked and disrupting the visual information?
To this end, we propose a non-targeted attack method referred to as VT-Attack
(Visual Tokens Attack), which constructs adversarial examples from multiple
perspectives, with the goal of comprehensively disrupting feature
representations and inherent relationships as well as the semantic properties
of visual tokens output by image encoders. Using only access to the image
encoder in the proposed attack, the generated adversarial examples exhibit
transferability across diverse LVLMs utilizing the same image encoder and
generality across different tasks. Extensive experiments validate the superior
attack performance of the VT-Attack over baseline methods, demonstrating its
effectiveness in attacking LVLMs with image encoders, which in turn can provide
guidance on the robustness of LVLMs, particularly in terms of the stability of
the visual feature space."
Fourier-based Action Recognition for Wildlife Behavior Quantification with Event Cameras,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Event cameras are novel bio-inspired vision sensors that measure pixel-wise
brightness changes asynchronously instead of images at a given frame rate. They
offer promising advantages, namely a high dynamic range, low latency, and
minimal motion blur. Modern computer vision algorithms often rely on artificial
neural network approaches, which require image-like representations of the data
and cannot fully exploit the characteristics of event data. We propose
approaches to action recognition based on the Fourier Transform. The approaches
are intended to recognize oscillating motion patterns commonly present in
nature. In particular, we apply our approaches to a recent dataset of breeding
penguins annotated for ""ecstatic display"", a behavior where the observed
penguins flap their wings at a certain frequency. We find that our approaches
are both simple and effective, producing slightly lower results than a deep
neural network (DNN) while relying just on a tiny fraction of the parameters
compared to the DNN (five orders of magnitude fewer parameters). They work well
despite the uncontrolled, diverse data present in the dataset. We hope this
work opens a new perspective on event-based processing and action recognition."
OmniPose6D: Towards Short-Term Object Pose Tracking in Dynamic Scenes from Monocular RGB,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"To address the challenge of short-term object pose tracking in dynamic
environments with monocular RGB input, we introduce a large-scale synthetic
dataset OmniPose6D, crafted to mirror the diversity of real-world conditions.
We additionally present a benchmarking framework for a comprehensive comparison
of pose tracking algorithms. We propose a pipeline featuring an
uncertainty-aware keypoint refinement network, employing probabilistic modeling
to refine pose estimation. Comparative evaluations demonstrate that our
approach achieves performance superior to existing baselines on real datasets,
underscoring the effectiveness of our synthetic dataset and refinement
technique in enhancing tracking precision in dynamic contexts. Our
contributions set a new precedent for the development and assessment of object
pose tracking methodologies in complex scenes."
Autonomous localization of multiple ionizing radiation sources using miniature single-layer Compton cameras onboard a group of micro aerial vehicles,cs.RO,Robotics,2024-10-09,"A novel method for autonomous localization of multiple sources of gamma
radiation using a group of Micro Aerial Vehicles (MAVs) is presented in this
paper. The method utilizes an extremely lightweight (44 g) Compton camera
MiniPIX TPX3. The compact size of the detector allows for deployment onboard
safe and agile small-scale Unmanned Aerial Vehicles (UAVs). The proposed
radiation mapping approach fuses measurements from multiple distributed Compton
camera sensors to accurately estimate the positions of multiple radioactive
sources in real time. Unlike commonly used intensity-based detectors, the
Compton camera reconstructs the set of possible directions towards a radiation
source from just a single ionizing particle. Therefore, the proposed approach
can localize radiation sources without having to estimate the gradient of a
radiation field or contour lines, which require longer measurements. The
instant estimation is able to fully exploit the potential of highly mobile
MAVs. The radiation mapping method is combined with an active search strategy,
which coordinates the future actions of the MAVs in order to improve the
quality of the estimate of the sources' positions, as well as to explore the
area of interest faster. The proposed solution is evaluated in simulation and
real world experiments with multiple Cesium-137 radiation sources."
How hard can it be? Quantifying MITRE attack campaigns with attack trees and cATM logic,cs.CR,Cryptography and Security,2024-10-09,"The landscape of cyber threats grows more complex by the day. Advanced
Persistent Threats carry out systematic attack campaigns against which
cybersecurity practitioners must defend. Examples of such organized attacks are
operations Dream Job, Wocao, WannaCry or the SolarWinds Compromise. To evaluate
which risks are most threatening, and which campaigns to prioritize against
when defending, cybersecurity experts must be equipped with the right toolbox.
In particular, they must be able to (a) obtain likelihood values for each
attack campaign recorded in the wild and (b) reliably and transparently
operationalize these values to carry out quantitative comparisons among
campaigns. This will allow security experts to perform quantitatively-informed
decision making that is transparent and accountable. In this paper we construct
such a framework by: (1) quantifying the likelihood of attack campaigns via
data-driven procedures on the MITRE knowledge base and (2) introducing a
methodology for automatic modelling of MITRE intelligence data: this is
complete in the sense that it captures any attack campaign via template attack
tree models. (3) We further propose a computational framework to carry out this
comparisons based on the cATM formal logic, and implement this into an
open-source Python tool. Finally, we validate our approach by quantifying the
likelihood of all MITRE campaigns, and comparing the likelihood of the Wocao
and Dream Job MITRE campaigns -- generated with our proposed approach --
against ""ad hoc"" traditionally-built attack tree models, demonstrating how our
methodology is substantially lighter in modelling effort, and still capable of
capturing all the quantitative relevant data."
Perceptual Quality Assessment of Trisoup-Lifting Encoded 3D Point Clouds,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"No-reference bitstream-layer point cloud quality assessment (PCQA) can be
deployed without full decoding at any network node to achieve real-time quality
monitoring. In this work, we develop the first PCQA model dedicated to
Trisoup-Lifting encoded 3D point clouds by analyzing bitstreams without full
decoding. Specifically, we investigate the relationship among texture bitrate
per point (TBPP), texture complexity (TC) and texture quantization parameter
(TQP) while geometry encoding is lossless. Subsequently, we estimate TC by
utilizing TQP and TBPP. Then, we establish a texture distortion evaluation
model based on TC, TBPP and TQP. Ultimately, by integrating this texture
distortion model with a geometry attenuation factor, a function of
trisoupNodeSizeLog2 (tNSL), we acquire a comprehensive NR bitstream-layer PCQA
model named streamPCQ-TL. In addition, this work establishes a database named
WPC6.0, the first and largest PCQA database dedicated to Trisoup-Lifting
encoding mode, encompassing 400 distorted point clouds with both 4 geometric
multiplied by 5 texture distortion levels. Experiment results on M-PCCD,
ICIP2020 and the proposed WPC6.0 database suggest that the proposed
streamPCQ-TL model exhibits robust and notable performance in contrast to
existing advanced PCQA metrics, particularly in terms of computational cost.
The dataset and source code will be publicly released at
\href{https://github.com/qdushl/Waterloo-Point-Cloud-Database-6.0}{\textit{https://github.com/qdushl/Waterloo-Point-Cloud-Database-6.0}}"
Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Videos contain a wealth of information, and generating detailed and accurate
descriptions in natural language is a key aspect of video understanding. In
this paper, we present video-SALMONN 2, an advanced audio-visual large language
model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with
paired audio) captioning through directed preference optimization (DPO). We
propose new metrics to evaluate the completeness and accuracy of video
descriptions, which are optimized using DPO. To further improve training, we
introduce a novel multi-round DPO (mrDPO) approach, which involves periodically
updating the DPO reference model, merging and re-initializing the LoRA module
as a proxy for parameter updates after each training round (1,000 steps), and
incorporating guidance from ground-truth video captions to stabilize the
process. To address potential catastrophic forgetting of non-captioning
abilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO
LLM by using the captions generated by the mrDPO-trained model as supervised
labels. Experiments show that mrDPO significantly enhances video-SALMONN 2's
captioning accuracy, reducing global and local error rates by 40\% and 20\%,
respectively, while decreasing the repetition rate by 35\%. The final
video-SALMONN 2 model, with just 7 billion parameters, surpasses leading models
such as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining
competitive performance to the state-of-the-art on widely used video
question-answering benchmark among models of similar size. Upon acceptance, we
will release the code, model checkpoints, and training and test data. Demos are
available at
\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}."
M${}^{3}$Bench: Benchmarking Whole-body Motion Generation for Mobile Manipulation in 3D Scenes,cs.RO,Robotics,2024-10-09,"We propose M^3Bench, a new benchmark for whole-body motion generation for
mobile manipulation tasks. Given a 3D scene context, M^3Bench requires an
embodied agent to understand its configuration, environmental constraints and
task objectives, then generate coordinated whole-body motion trajectories for
object rearrangement tasks. M^3Bench features 30k object rearrangement tasks
across 119 diverse scenes, providing expert demonstrations generated by our
newly developed M^3BenchMaker. This automatic data generation tool produces
coordinated whole-body motion trajectories from high-level task instructions,
requiring only basic scene and robot information. Our benchmark incorporates
various task splits to assess generalization across different dimensions and
leverages realistic physics simulation for trajectory evaluation. Through
extensive experimental analyses, we reveal that state-of-the-art models still
struggle with coordinated base-arm motion while adhering to environment-context
and task-specific constraints, highlighting the need to develop new models that
address this gap. Through M^3Bench, we aim to facilitate future robotics
research towards more adaptive and capable mobile manipulation in diverse,
real-world environments."
Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"The hypothesis of Universality in interpretability suggests that different
neural networks may converge to implement similar algorithms on similar tasks.
In this work, we investigate two mainstream architectures for language
modeling, namely Transformers and Mambas, to explore the extent of their
mechanistic similarity. We propose to use Sparse Autoencoders (SAEs) to isolate
interpretable features from these models and show that most features are
similar in these two models. We also validate the correlation between feature
similarity and Universality. We then delve into the circuit-level analysis of
Mamba models and find that the induction circuits in Mamba are structurally
analogous to those in Transformers. We also identify a nuanced difference we
call \emph{Off-by-One motif}: The information of one token is written into the
SSM state in its next position. Whilst interaction between tokens in
Transformers does not exhibit such trend."
GLA-DA: Global-Local Alignment Domain Adaptation for Multivariate Time Series,cs.LG,Machine Learning,2024-10-09,"Unlike images and natural language tokens, time series data is highly
semantically sparse, resulting in labor-intensive label annotations.
Unsupervised and Semi-supervised Domain Adaptation (UDA and SSDA) have
demonstrated efficiency in addressing this issue by utilizing pre-labeled
source data to train on unlabeled or partially labeled target data. However, in
domain adaptation methods designed for downstream classification tasks,
directly adapting labeled source samples with unlabelled target samples often
results in similar distributions across various classes, thereby compromising
the performance of the target classification task.
  To tackle this challenge, we proposed a Global-Local Alignment Domain
Adaptation (GLA-DA) method for multivariate time series data. Data from two
domains were initially encoded to align in an intermediate feature space
adversarially, achieving Global Feature Alignment (GFA). Subsequently, GLA-DA
leveraged the consistency between similarity-based and deep learning-based
models to assign pseudo labels to unlabeled target data. This process aims to
preserve differences among data with distinct labels by aligning the samples
with the same class labels together, achieving Local Class Alignment (LCA). We
implemented GLA-DA in both UDA and SSDA scenarios, showcasing its superiority
over state-of-the-art methods through extensive experiments on various public
datasets. Ablation experiments underscored the significance of key components
within GLA-DA."
Large Language Models as Code Executors: An Exploratory Study,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"The capabilities of Large Language Models (LLMs) have significantly evolved,
extending from natural language processing to complex tasks like code
understanding and generation. We expand the scope of LLMs' capabilities to a
broader context, using LLMs to execute code snippets to obtain the output. This
paper pioneers the exploration of LLMs as code executors, where code snippets
are directly fed to the models for execution, and outputs are returned. We are
the first to comprehensively examine this feasibility across various LLMs,
including OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the
o1 model achieved over 90% accuracy in code execution, while others
demonstrated lower accuracy levels. Furthermore, we introduce an Iterative
Instruction Prompting (IIP) technique that processes code snippets line by
line, enhancing the accuracy of weaker models by an average of 7.22% (with the
highest improvement of 18.96%) and an absolute average improvement of 3.86%
against CoT prompting (with the highest improvement of 19.46%). Our study not
only highlights the transformative potential of LLMs in coding but also lays
the groundwork for future advancements in automated programming and the
completion of complex tasks."
Revisiting Multi-Permutation Equivariance through the Lens of Irreducible Representations,cs.LG,Machine Learning,2024-10-09,"This paper explores the characterization of equivariant linear layers for
representations of permutations and related groups. Unlike traditional
approaches, which address these problems using parameter-sharing, we consider
an alternative methodology based on irreducible representations and Schur's
lemma. Using this methodology, we obtain an alternative derivation for existing
models like DeepSets, 2-IGN graph equivariant networks, and Deep Weight Space
(DWS) networks. The derivation for DWS networks is significantly simpler than
that of previous results.
  Next, we extend our approach to unaligned symmetric sets, where equivariance
to the wreath product of groups is required. Previous works have addressed this
problem in a rather restrictive setting, in which almost all wreath equivariant
layers are Siamese. In contrast, we give a full characterization of layers in
this case and show that there is a vast number of additional non-Siamese layers
in some settings. We also show empirically that these additional non-Siamese
layers can improve performance in tasks like graph anomaly detection, weight
space alignment, and learning Wasserstein distances. Our code is available at
\href{https://github.com/yonatansverdlov/Irreducible-Representations-of-Deep-Weight-Spaces}{GitHub}."
Decouple-Then-Merge: Towards Better Training for Diffusion Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Diffusion models are trained by learning a sequence of models that reverse
each step of noise corruption. Typically, the model parameters are fully shared
across multiple timesteps to enhance training efficiency. However, since the
denoising tasks differ at each timestep, the gradients computed at different
timesteps may conflict, potentially degrading the overall performance of image
generation. To solve this issue, this work proposes a Decouple-then-Merge
(DeMe) framework, which begins with a pretrained model and finetunes separate
models tailored to specific timesteps. We introduce several improved techniques
during the finetuning stage to promote effective knowledge sharing while
minimizing training interference across timesteps. Finally, after finetuning,
these separate models can be merged into a single model in the parameter space,
ensuring efficient and practical inference. Experimental results show
significant generation quality improvements upon 6 benchmarks including Stable
Diffusion on COCO30K, ImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN
Bedroom, and CIFAR10."
WardropNet: Traffic Flow Predictions via Equilibrium-Augmented Learning,cs.LG,Machine Learning,2024-10-09,"When optimizing transportation systems, anticipating traffic flows is a
central element. Yet, computing such traffic equilibria remains computationally
expensive. Against this background, we introduce a novel combinatorial
optimization augmented neural network architecture that allows for fast and
accurate traffic flow predictions. We propose WardropNet, a neural network that
combines classical layers with a subsequent equilibrium layer: the first ones
inform the latter by predicting the parameterization of the equilibrium
problem's latency functions. Using supervised learning we minimize the
difference between the actual traffic flow and the predicted output. We show
how to leverage a Bregman divergence fitting the geometry of the equilibria,
which allows for end-to-end learning. WardropNet outperforms pure
learning-based approaches in predicting traffic equilibria for realistic and
stylized traffic scenarios. On realistic scenarios, WardropNet improves on
average for time-invariant predictions by up to 72% and for time-variant
predictions by up to 23% over pure learning-based approaches."
Task-oriented Time Series Imputation Evaluation via Generalized Representers,cs.LG,Machine Learning,2024-10-09,"Time series analysis is widely used in many fields such as power energy,
economics, and transportation, including different tasks such as forecasting,
anomaly detection, classification, etc. Missing values are widely observed in
these tasks, and often leading to unpredictable negative effects on existing
methods, hindering their further application. In response to this situation,
existing time series imputation methods mainly focus on restoring sequences
based on their data characteristics, while ignoring the performance of the
restored sequences in downstream tasks. Considering different requirements of
downstream tasks (e.g., forecasting), this paper proposes an efficient
downstream task-oriented time series imputation evaluation approach. By
combining time series imputation with neural network models used for downstream
tasks, the gain of different imputation strategies on downstream tasks is
estimated without retraining, and the most favorable imputation value for
downstream tasks is given by combining different imputation strategies
according to the estimated gain."
Toward Physics-guided Time Series Embedding,cs.LG,Machine Learning,2024-10-09,"In various scientific and engineering fields, the primary research areas have
revolved around physics-based dynamical systems modeling and data-driven time
series analysis. According to the embedding theory, dynamical systems and time
series can be mutually transformed using observation functions and physical
reconstruction techniques. Based on this, we propose Embedding Duality Theory,
where the parameterized embedding layer essentially provides a linear
estimation of the non-linear time series dynamics. This theory enables us to
bypass the parameterized embedding layer and directly employ physical
reconstruction techniques to acquire a data embedding representation. Utilizing
physical priors results in a 10X reduction in parameters, a 3X increase in
speed, and maximum performance boosts of 18% in expert, 22% in few-shot, and
53\% in zero-shot tasks without any hyper-parameter tuning. All methods are
encapsulated as a plug-and-play module"
Q-WSL:Leveraging Dynamic Programming for Weighted Supervised Learning in Goal-conditioned RL,cs.LG,Machine Learning,2024-10-09,"A novel class of advanced algorithms, termed Goal-Conditioned Weighted
Supervised Learning (GCWSL), has recently emerged to tackle the challenges
posed by sparse rewards in goal-conditioned reinforcement learning (RL). GCWSL
consistently delivers strong performance across a diverse set of goal-reaching
tasks due to its simplicity, effectiveness, and stability. However, GCWSL
methods lack a crucial capability known as trajectory stitching, which is
essential for learning optimal policies when faced with unseen skills during
testing. This limitation becomes particularly pronounced when the replay buffer
is predominantly filled with sub-optimal trajectories. In contrast, traditional
TD-based RL methods, such as Q-learning, which utilize Dynamic Programming, do
not face this issue but often experience instability due to the inherent
difficulties in value function approximation. In this paper, we propose
Q-learning Weighted Supervised Learning (Q-WSL), a novel framework designed to
overcome the limitations of GCWSL by incorporating the strengths of Dynamic
Programming found in Q-learning. Q-WSL leverages Dynamic Programming results to
output the optimal action of (state, goal) pairs across different trajectories
within the replay buffer. This approach synergizes the strengths of both
Q-learning and GCWSL, effectively mitigating their respective weaknesses and
enhancing overall performance. Empirical evaluations on challenging
goal-reaching tasks demonstrate that Q-WSL surpasses other goal-conditioned
approaches in terms of both performance and sample efficiency. Additionally,
Q-WSL exhibits notable robustness in environments characterized by binary
reward structures and environmental stochasticity."
Continual Learning in the Frequency Domain,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Continual learning (CL) is designed to learn new tasks while preserving
existing knowledge. Replaying samples from earlier tasks has proven to be an
effective method to mitigate the forgetting of previously acquired knowledge.
However, the current research on the training efficiency of rehearsal-based
methods is insufficient, which limits the practical application of CL systems
in resource-limited scenarios. The human visual system (HVS) exhibits varying
sensitivities to different frequency components, enabling the efficient
elimination of visually redundant information. Inspired by HVS, we propose a
novel framework called Continual Learning in the Frequency Domain (CLFD). To
our knowledge, this is the first study to utilize frequency domain features to
enhance the performance and efficiency of CL training on edge devices. For the
input features of the feature extractor, CLFD employs wavelet transform to map
the original input image into the frequency domain, thereby effectively
reducing the size of input feature maps. Regarding the output features of the
feature extractor, CLFD selectively utilizes output features for distinct
classes for classification, thereby balancing the reusability and interference
of output features based on the frequency domain similarity of the classes
across various tasks. Optimizing only the input and output features of the
feature extractor allows for seamless integration of CLFD with various
rehearsal-based methods. Extensive experiments conducted in both cloud and edge
environments demonstrate that CLFD consistently improves the performance of
state-of-the-art (SOTA) methods in both precision and training efficiency.
Specifically, CLFD can increase the accuracy of the SOTA CL method by up to
6.83% and reduce the training time by 2.6$\times$."
Subtle Errors Matter: Preference Learning via Error-injected Self-editing,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Large Language Models (LLMs) have exhibited strong mathematical reasoning and
computational prowess, tackling tasks ranging from basic arithmetic to advanced
competition-level problems. However, frequently occurring subtle errors, such
as miscalculations or incorrect substitutions, limit the models' full
mathematical potential. Existing studies to improve mathematical ability
typically involve distilling reasoning skills from stronger LLMs or applying
preference learning to step-wise response pairs. Although these methods
leverage samples of varying granularity to mitigate reasoning errors, they
overlook the frequently occurring subtle errors. A major reason is that sampled
preference pairs involve differences unrelated to the errors, which may
distract the model from focusing on subtle errors. In this work, we propose a
novel preference learning framework called eRror-Injected Self-Editing (RISE),
which injects predefined subtle errors into partial tokens of correct solutions
to construct hard pairs for error mitigation. In detail, RISE uses the model
itself to edit a small number of tokens in the solution, injecting designed
subtle errors. Then, pairs composed of self-edited solutions and their
corresponding correct ones, along with pairs of correct and incorrect solutions
obtained through sampling, are used together for subtle error-aware DPO
training. Compared with other preference learning methods, RISE further refines
the training objective to focus on predefined errors and their tokens, without
requiring fine-grained sampling or preference annotation. Extensive experiments
validate the effectiveness of RISE, with preference learning on
Qwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on
MATH."
Tree of Problems: Improving structured problem solving with compositionality,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Large Language Models (LLMs) have demonstrated remarkable performance across
multiple tasks through in-context learning. For complex reasoning tasks that
require step-by-step thinking, Chain-of-Thought (CoT) prompting has given
impressive results, especially when combined with self-consistency.
Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree
of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing
the complex problem into paths of subproblems. In this paper, we propose Tree
of Problems (ToP), a simpler version of ToT, which we hypothesise can work
better for complex tasks that can be divided into identical subtasks. Our
empirical results show that our approach outperforms ToT and GoT, and in
addition performs better than CoT on complex reasoning tasks. All code for this
paper is publicly available here:
https://github.com/ArmelRandy/tree-of-problems."
Open-RGBT: Open-vocabulary RGB-T Zero-shot Semantic Segmentation in Open-world Environments,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Semantic segmentation is a critical technique for effective scene
understanding. Traditional RGB-T semantic segmentation models often struggle to
generalize across diverse scenarios due to their reliance on pretrained models
and predefined categories. Recent advancements in Visual Language Models (VLMs)
have facilitated a shift from closed-set to open-vocabulary semantic
segmentation methods. However, these models face challenges in dealing with
intricate scenes, primarily due to the heterogeneity between RGB and thermal
modalities. To address this gap, we present Open-RGBT, a novel open-vocabulary
RGB-T semantic segmentation model. Specifically, we obtain instance-level
detection proposals by incorporating visual prompts to enhance category
understanding. Additionally, we employ the CLIP model to assess image-text
similarity, which helps correct semantic consistency and mitigates ambiguities
in category identification. Empirical evaluations demonstrate that Open-RGBT
achieves superior performance in diverse and challenging real-world scenarios,
even in the wild, significantly advancing the field of RGB-T semantic
segmentation."
ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Vision Language Models (VLMs) have become essential backbones for multimodal
intelligence, yet significant safety challenges limit their real-world
application. While textual inputs are often effectively safeguarded,
adversarial visual inputs can easily bypass VLM defense mechanisms. Existing
defense methods are either resource-intensive, requiring substantial data and
compute, or fail to simultaneously ensure safety and usefulness in responses.
To address these limitations, we propose a novel two-phase inference-time
alignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual
contents and output responses to establish a robust safety awareness in
multimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep
levels by conditioning the VLMs' generative distribution with an interference
prefix and performing sentence-level best-of-N to search the most harmless and
helpful generation paths. Extensive experiments show that ETA outperforms
baseline methods in terms of harmlessness, helpfulness, and efficiency,
reducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6%
win-ties in GPT-4 helpfulness evaluation. The code is publicly available at
https://github.com/DripNowhy/ETA."
Effective Exploration Based on the Structural Information Principles,cs.LG,Machine Learning,2024-10-09,"Traditional information theory provides a valuable foundation for
Reinforcement Learning, particularly through representation learning and
entropy maximization for agent exploration. However, existing methods primarily
concentrate on modeling the uncertainty associated with RL's random variables,
neglecting the inherent structure within the state and action spaces. In this
paper, we propose a novel Structural Information principles-based Effective
Exploration framework, namely SI2E. Structural mutual information between two
variables is defined to address the single-variable limitation in structural
information, and an innovative embedding principle is presented to capture
dynamics-relevant state-action representations. The SI2E analyzes value
differences in the agent's policy between state-action pairs and minimizes
structural entropy to derive the hierarchical state-action structure, referred
to as the encoding tree. Under this tree structure, value-conditional
structural entropy is defined and maximized to design an intrinsic reward
mechanism that avoids redundant transitions and promotes enhanced coverage in
the state-action space. Theoretical connections are established between SI2E
and classical information-theoretic methodologies, highlighting our framework's
rationality and advantage. Comprehensive evaluations in the MiniGrid,
MetaWorld, and DeepMind Control Suite benchmarks demonstrate that SI2E
significantly outperforms state-of-the-art exploration baselines regarding
final performance and sample efficiency, with maximum improvements of 37.63%
and 60.25%, respectively."
Task Coordination and Trajectory Optimization for Multi-Aerial Systems via Signal Temporal Logic: A Wind Turbine Inspection Study,cs.RO,Robotics,2024-10-09,"This paper presents a method for task allocation and trajectory generation in
cooperative inspection missions using a fleet of multirotor drones, with a
focus on wind turbine inspection. The approach generates safe, feasible flight
paths that adhere to time-sensitive constraints and vehicle limitations by
formulating an optimization problem based on Signal Temporal Logic (STL)
specifications. An event-triggered replanning mechanism addresses unexpected
events and delays, while a generalized robustness scoring method incorporates
user preferences and minimizes task conflicts. The approach is validated
through simulations in MATLAB and Gazebo, as well as field experiments in a
mock-up scenario."
Decomposing Relationship from 1-to-N into N 1-to-1 for Text-Video Retrieval,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Text-video retrieval (TVR) has seen substantial advancements in recent years,
fueled by the utilization of pre-trained models and large language models
(LLMs). Despite these advancements, achieving accurate matching in TVR remains
challenging due to inherent disparities between video and textual modalities
and irregularities in data representation. In this paper, we propose
Text-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the
conventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.
By replacing a single text query with a series of text proxies, TV-ProxyNet not
only broadens the query scope but also achieves a more precise expansion. Each
text proxy is crafted through a refined iterative process, controlled by
mechanisms we term as the director and dash, which regulate the proxy's
direction and distance relative to the original text query. This setup not only
facilitates more precise semantic alignment but also effectively manages the
disparities and noise inherent in multimodal data. Our experiments on three
representative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet
Captions, demonstrate the effectiveness of TV-ProxyNet. The results show an
improvement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved
state-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%
improvement on DiDeMo compared to existing methods, validating our approach's
ability to enhance semantic mapping and reduce error propensity."
Learning Evolving Tools for Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Tool learning enables large language models (LLMs) to interact with external
tools and APIs, greatly expanding the application scope of LLMs. However, due
to the dynamic nature of external environments, these tools and APIs may become
outdated over time, preventing LLMs from correctly invoking tools. Existing
research primarily focuses on static environments and overlooks this issue,
limiting the adaptability of LLMs in real-world applications. In this paper, we
propose ToolEVO, a novel framework designed to enhance the adaptive and
reflective capabilities of LLMs against tool variability. By leveraging Monte
Carlo Tree Search, ToolEVO facilitates active exploration and interaction of
LLMs within dynamic environments, allowing for autonomous self-reflection and
self-updating of tool usage based on environmental feedback. Additionally, we
introduce ToolQA-D, a benchmark specifically designed to evaluate the impact of
tool variability. Extensive experiments demonstrate the effectiveness and
stability of our approach, highlighting the importance of adaptability to tool
variability for effective tool learning."
$$-calibration of Language Model Confidence Scores for Generative QA,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"To use generative question-and-answering (QA) systems for decision-making and
in any critical application, these systems need to provide well-calibrated
confidence scores that reflect the correctness of their answers. Existing
calibration methods aim to ensure that the confidence score is on average
indicative of the likelihood that the answer is correct. We argue, however,
that this standard (average-case) notion of calibration is difficult to
interpret for decision-making in generative QA. To address this, we generalize
the standard notion of average calibration and introduce $\beta$-calibration,
which ensures calibration holds across different question-and-answer groups. We
then propose discretized posthoc calibration schemes for achieving
$\beta$-calibration."
Pair-VPR: Place-Aware Pre-training and Contrastive Pair Classification for Visual Place Recognition with Vision Transformers,cs.RO,Robotics,2024-10-09,"In this work we propose a novel joint training method for Visual Place
Recognition (VPR), which simultaneously learns a global descriptor and a pair
classifier for re-ranking. The pair classifier can predict whether a given pair
of images are from the same place or not. The network only comprises Vision
Transformer components for both the encoder and the pair classifier, and both
components are trained using their respective class tokens. In existing VPR
methods, typically the network is initialized using pre-trained weights from a
generic image dataset such as ImageNet. In this work we propose an alternative
pre-training strategy, by using Siamese Masked Image Modelling as a
pre-training task. We propose a Place-aware image sampling procedure from a
collection of large VPR datasets for pre-training our model, to learn visual
features tuned specifically for VPR. By re-using the Mask Image Modelling
encoder and decoder weights in the second stage of training, Pair-VPR can
achieve state-of-the-art VPR performance across five benchmark datasets with a
ViT-B encoder, along with further improvements in localization recall with
larger encoders. The Pair-VPR website is:
https://csiro-robotics.github.io/Pair-VPR."
ES-Gaussian: Gaussian Splatting Mapping via Error Space-Based Gaussian Completion,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Accurate and affordable indoor 3D reconstruction is critical for effective
robot navigation and interaction. Traditional LiDAR-based mapping provides high
precision but is costly, heavy, and power-intensive, with limited ability for
novel view rendering. Vision-based mapping, while cost-effective and capable of
capturing visual data, often struggles with high-quality 3D reconstruction due
to sparse point clouds. We propose ES-Gaussian, an end-to-end system using a
low-altitude camera and single-line LiDAR for high-quality 3D indoor
reconstruction. Our system features Visual Error Construction (VEC) to enhance
sparse point clouds by identifying and correcting areas with insufficient
geometric detail from 2D error maps. Additionally, we introduce a novel 3DGS
initialization method guided by single-line LiDAR, overcoming the limitations
of traditional multi-view setups and enabling effective reconstruction in
resource-constrained environments. Extensive experimental results on our new
Dreame-SR dataset and a publicly available dataset demonstrate that ES-Gaussian
outperforms existing methods, particularly in challenging scenarios. The
project page is available at https://chenlu-china.github.io/ES-Gaussian/."
Dissecting Fine-Tuning Unlearning in Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Fine-tuning-based unlearning methods prevail for preventing targeted harmful,
sensitive, or copyrighted information within large language models while
preserving overall capabilities. However, the true effectiveness of these
methods is unclear. In this paper, we delve into the limitations of
fine-tuning-based unlearning through activation patching and parameter
restoration experiments. Our findings reveal that these methods alter the
model's knowledge retrieval process, rather than genuinely erasing the
problematic knowledge embedded in the model parameters. Furthermore, behavioral
tests demonstrate that the unlearning mechanisms inevitably impact the global
behavior of the models, affecting unrelated knowledge or capabilities. Our work
advocates the development of more resilient unlearning techniques for truly
erasing knowledge. Our code is released at
https://github.com/yihuaihong/Dissecting-FT-Unlearning."
DDRN:a Data Distribution Reconstruction Network for Occluded Person Re-Identification,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"In occluded person re-identification(ReID), severe occlusions lead to a
significant amount of irrelevant information that hinders the accurate
identification of individuals. These irrelevant cues primarily stem from
background interference and occluding interference, adversely affecting the
final retrieval results. Traditional discriminative models, which rely on the
specific content and positions of the images, often misclassify in cases of
occlusion. To address these limitations, we propose the Data Distribution
Reconstruction Network (DDRN), a generative model that leverages data
distribution to filter out irrelevant details, enhancing overall feature
perception ability and reducing irrelevant feature interference. Additionally,
severe occlusions lead to the complexity of the feature space. To effectively
handle this, we design a multi-center approach through the proposed
Hierarchical SubcenterArcface (HS-Arcface) loss function, which can better
approximate complex feature spaces. On the Occluded-Duke dataset, we achieved a
mAP of 62.4\% (+1.1\%) and a rank-1 accuracy of 71.3\% (+0.6\%), surpassing the
latest state-of-the-art methods(FRT) significantly."
Towards Natural Image Matting in the Wild via Real-Scenario Prior,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Recent approaches attempt to adapt powerful interactive segmentation models,
such as SAM, to interactive matting and fine-tune the models based on synthetic
matting datasets. However, models trained on synthetic data fail to generalize
to complex and occlusion scenes. We address this challenge by proposing a new
matting dataset based on the COCO dataset, namely COCO-Matting. Specifically,
the construction of our COCO-Matting includes accessory fusion and
mask-to-matte, which selects real-world complex images from COCO and converts
semantic segmentation masks to matting labels. The built COCO-Matting comprises
an extensive collection of 38,251 human instance-level alpha mattes in complex
natural scenarios. Furthermore, existing SAM-based matting methods extract
intermediate features and masks from a frozen SAM and only train a lightweight
matting decoder by end-to-end matting losses, which do not fully exploit the
potential of the pre-trained SAM. Thus, we propose SEMat which revamps the
network architecture and training objectives. For network architecture, the
proposed feature-aligned transformer learns to extract fine-grained edge and
transparency features. The proposed matte-aligned decoder aims to segment
matting-specific objects and convert coarse masks into high-precision mattes.
For training objectives, the proposed regularization and trimap loss aim to
retain the prior from the pre-trained model and push the matting logits
extracted from the mask decoder to contain trimap-based semantic information.
Extensive experiments across seven diverse datasets demonstrate the superior
performance of our method, proving its efficacy in interactive natural image
matting. We open-source our code, models, and dataset at
https://github.com/XiaRho/SEMat."
Bots can Snoop: Uncovering and Mitigating Privacy Risks of Bots in Group Chats,cs.CR,Cryptography and Security,2024-10-09,"New privacy concerns arise with chatbots on group messaging platforms.
Chatbots may access information beyond their intended functionalities, such as
messages unintended for chatbots or sender's identities. Chatbot operators may
exploit such information to infer personal information and link users across
groups, potentially leading to personal data breaches, pervasive tracking, and
targeted advertising. Our analysis of conversation datasets shows that (1)
chatbots often access far more messages than needed, and (2) when a user joins
a new group with chatbots, there is a 3.4% chance that at least one of the
chatbots can recognize and associate the user with their previous interactions
in other groups. Although state-of-the-art group messaging protocols provide
robust end-to-end security and some platforms have implemented policies to
limit chatbot access, no platforms successfully combine these features. This
paper introduces SnoopGuard, a secure group messaging protocol that ensures
user privacy against chatbots while maintaining strong end-to-end security. Our
method offers selective message access, preventing chatbots from accessing
unrelated messages, and ensures sender anonymity within the group. SnoopGuard
achieves $O(\log n + m)$ message-sending complexity for a group of $n$ users
and $m$ chatbots, compared to $O(\log(n + m))$ in state-of-the-art protocols,
with acceptable overhead for enhanced privacy. Our prototype implementation
shows that sending a message in a group of 50 users and 10 chatbots takes about
30 milliseconds when integrated with Message Layer Security (MLS)."
Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Recent advancements in Transformer-based large language models (LLMs) have
set new standards in natural language processing. However, the classical
softmax attention incurs significant computational costs, leading to a $O(T)$
complexity for per-token generation, where $T$ represents the context length.
This work explores reducing LLMs' complexity while maintaining performance by
introducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an
innovative data-dependent tempered selection (DDTS) mechanism within a linear
attention-based, purely recurrent framework, achieving significant accuracy
while drastically reducing the memory usage typically associated with recurrent
models. This method exemplifies semantic compression by maintaining essential
input information with fixed-size hidden states. Building on this, Rodimus$+$
combines Rodimus with the innovative Sliding Window Shared-Key Attention
(SW-SKA) in a hybrid approach, effectively leveraging the complementary
semantic, token, and head compression techniques. Our experiments demonstrate
that Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior
downstream performance against models trained on more tokens, including
Qwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the
accuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints
will be available soon."
On The Relationship between Visual Anomaly-free and Anomalous Representations,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Anomaly Detection is an important problem within computer vision, having
variety of real-life applications. Yet, the current set of solutions to this
problem entail known, systematic shortcomings. Specifically, contemporary
surface Anomaly Detection task assumes the presence of multiple specific
anomaly classes e.g. cracks, rusting etc., unlike one-class classification
model of past. However, building a deep learning model in such setup remains a
challenge because anomalies arise rarely, and hence anomaly samples are quite
scarce. Transfer learning has been a preferred paradigm in such situations. But
the typical source domains with large dataset sizes e.g. ImageNet, JFT-300M,
LAION-2B do not correlate well with the domain of surfaces and materials, an
important premise of transfer learning. In this paper, we make an important
hypothesis and show, by exhaustive experimentation, that the space of
anomaly-free visual patterns of the normal samples correlates well with each of
the various spaces of anomalous patterns of the class-specific anomaly samples.
The first results of using this hypothesis in transfer learning have indeed
been quite encouraging. We expect that finding such a simple closeby domain
that readily entails large number of samples, and which also oftentimes shows
interclass separability though with narrow margins, will be a useful discovery.
Especially, it is expected to improve domain adaptation for anomaly detection,
and few-shot learning for anomaly detection, making in-the-wild anomaly
detection realistically possible in future."
Disturbance Observer-based Control Barrier Functions with Residual Model Learning for Safe Reinforcement Learning,cs.RO,Robotics,2024-10-09,"Reinforcement learning (RL) agents need to explore their environment to learn
optimal behaviors and achieve maximum rewards. However, exploration can be
risky when training RL directly on real systems, while simulation-based
training introduces the tricky issue of the sim-to-real gap. Recent approaches
have leveraged safety filters, such as control barrier functions (CBFs), to
penalize unsafe actions during RL training. However, the strong safety
guarantees of CBFs rely on a precise dynamic model. In practice, uncertainties
always exist, including internal disturbances from the errors of dynamics and
external disturbances such as wind. In this work, we propose a new safe RL
framework based on disturbance rejection-guarded learning, which allows for an
almost model-free RL with an assumed but not necessarily precise nominal
dynamic model. We demonstrate our results on the Safety-gym benchmark for Point
and Car robots on all tasks where we can outperform state-of-the-art approaches
that use only residual model learning or a disturbance observer (DOB). We
further validate the efficacy of our framework using a physical F1/10 racing
car. Videos: https://sites.google.com/view/res-dob-cbf-rl"
Convex Distillation: Efficient Compression of Deep Networks via Convex Optimization,cs.LG,Machine Learning,2024-10-09,"Deploying large and complex deep neural networks on resource-constrained edge
devices poses significant challenges due to their computational demands and the
complexities of non-convex optimization. Traditional compression methods such
as distillation and pruning often retain non-convexity that complicates
fine-tuning in real-time on such devices. Moreover, these methods often
necessitate extensive end-to-end network fine-tuning after compression to
preserve model performance, which is not only time-consuming but also requires
fully annotated datasets, thus potentially negating the benefits of efficient
network compression. In this paper, we introduce a novel distillation technique
that efficiently compresses the model via convex optimization -- eliminating
intermediate non-convex activation functions and using only intermediate
activations from the original model. Our approach enables distillation in a
label-free data setting and achieves performance comparable to the original
model without requiring any post-compression fine-tuning. We demonstrate the
effectiveness of our method for image classification models on multiple
standard datasets, and further show that in the data limited regime, our method
can outperform standard non-convex distillation approaches. Our method promises
significant advantages for deploying high-efficiency, low-footprint models on
edge devices, making it a practical choice for real-world applications. We show
that convex neural networks, when provided with rich feature representations
from a large pre-trained non-convex model, can achieve performance comparable
to their non-convex counterparts, opening up avenues for future research at the
intersection of convex optimization and deep learning."
Detecting Bias and Enhancing Diagnostic Accuracy in Large Language Models for Healthcare,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Biased AI-generated medical advice and misdiagnoses can jeopardize patient
safety, making the integrity of AI in healthcare more critical than ever. As
Large Language Models (LLMs) take on a growing role in medical decision-making,
addressing their biases and enhancing their accuracy is key to delivering safe,
reliable care. This study addresses these challenges head-on by introducing new
resources designed to promote ethical and precise AI in healthcare. We present
two datasets: BiasMD, featuring 6,007 question-answer pairs crafted to evaluate
and mitigate biases in health-related LLM outputs, and DiseaseMatcher, with
32,000 clinical question-answer pairs spanning 700 diseases, aimed at assessing
symptom-based diagnostic accuracy. Using these datasets, we developed the
EthiClinician, a fine-tuned model built on the ChatDoctor framework, which
outperforms GPT-4 in both ethical reasoning and clinical judgment. By exposing
and correcting hidden biases in existing models for healthcare, our work sets a
new benchmark for safer, more reliable patient outcomes."
Agile Mobility with Rapid Online Adaptation via Meta-learning and Uncertainty-aware MPPI,cs.RO,Robotics,2024-10-09,"Modern non-linear model-based controllers require an accurate physics model
and model parameters to be able to control mobile robots at their limits. Also,
due to surface slipping at high speeds, the friction parameters may continually
change (like tire degradation in autonomous racing), and the controller may
need to adapt rapidly. Many works derive a task-specific robot model with a
parameter adaptation scheme that works well for the task but requires a lot of
effort and tuning for each platform and task. In this work, we design a full
model-learning-based controller based on meta pre-training that can very
quickly adapt using few-shot dynamics data to any wheel-based robot with any
model parameters, while also reasoning about model uncertainty. We demonstrate
our results in small-scale numeric simulation, the large-scale Unity simulator,
and on a medium-scale hardware platform with a wide range of settings. We show
that our results are comparable to domain-specific well-engineered controllers,
and have excellent generalization performance across all scenarios."
Efficient and Robust Knowledge Distillation from A Stronger Teacher Based on Correlation Matching,cs.LG,Machine Learning,2024-10-09,"Knowledge Distillation (KD) has emerged as a pivotal technique for neural
network compression and performance enhancement. Most KD methods aim to
transfer dark knowledge from a cumbersome teacher model to a lightweight
student model based on Kullback-Leibler (KL) divergence loss. However, the
student performance improvements achieved through KD exhibit diminishing
marginal returns, where a stronger teacher model does not necessarily lead to a
proportionally stronger student model. To address this issue, we empirically
find that the KL-based KD method may implicitly change the inter-class
relationships learned by the student model, resulting in a more complex and
ambiguous decision boundary, which in turn reduces the model's accuracy and
generalization ability. Therefore, this study argues that the student model
should learn not only the probability values from the teacher's output but also
the relative ranking of classes, and proposes a novel Correlation Matching
Knowledge Distillation (CMKD) method that combines the Pearson and Spearman
correlation coefficients-based KD loss to achieve more efficient and robust
distillation from a stronger teacher model. Moreover, considering that samples
vary in difficulty, CMKD dynamically adjusts the weights of the Pearson-based
loss and Spearman-based loss. CMKD is simple yet practical, and extensive
experiments demonstrate that it can consistently achieve state-of-the-art
performance on CIRAR-100 and ImageNet, and adapts well to various teacher
architectures, sizes, and other KD methods."
Mitigating Time Discretization Challenges with WeatherODE: A Sandwich Physics-Driven Neural ODE for Weather Forecasting,cs.LG,Machine Learning,2024-10-09,"In the field of weather forecasting, traditional models often grapple with
discretization errors and time-dependent source discrepancies, which limit
their predictive performance. In this paper, we present WeatherODE, a novel
one-stage, physics-driven ordinary differential equation (ODE) model designed
to enhance weather forecasting accuracy. By leveraging wave equation theory and
integrating a time-dependent source model, WeatherODE effectively addresses the
challenges associated with time-discretization error and dynamic atmospheric
processes. Moreover, we design a CNN-ViT-CNN sandwich structure, facilitating
efficient learning dynamics tailored for distinct yet interrelated tasks with
varying optimization biases in advection equation estimation. Through rigorous
experiments, WeatherODE demonstrates superior performance in both global and
regional weather forecasting tasks, outperforming recent state-of-the-art
approaches by significant margins of over 40.0\% and 31.8\% in root mean square
error (RMSE), respectively. The source code is available at
\url{https://github.com/DAMO-DI-ML/WeatherODE}."
Deep Correlated Prompting for Visual Recognition with Missing Modalities,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Large-scale multimodal models have shown excellent performance over a series
of tasks powered by the large corpus of paired multimodal training data.
Generally, they are always assumed to receive modality-complete inputs.
However, this simple assumption may not always hold in the real world due to
privacy constraints or collection difficulty, where models pretrained on
modality-complete data easily demonstrate degraded performance on
missing-modality cases. To handle this issue, we refer to prompt learning to
adapt large pretrained multimodal models to handle missing-modality scenarios
by regarding different missing cases as different types of input. Instead of
only prepending independent prompts to the intermediate layers, we present to
leverage the correlations between prompts and input features and excavate the
relationships between different layers of prompts to carefully design the
instructions. We also incorporate the complementary semantics of different
modalities to guide the prompting design for each modality. Extensive
experiments on three commonly-used datasets consistently demonstrate the
superiority of our method compared to the previous approaches upon different
missing scenarios. Plentiful ablations are further given to show the
generalizability and reliability of our method upon different modality-missing
ratios and types."
ING-VP: MLLMs cannot Play Easy Vision-based Games Yet,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"As multimodal large language models (MLLMs) continue to demonstrate
increasingly competitive performance across a broad spectrum of tasks, more
intricate and comprehensive benchmarks have been developed to assess these
cutting-edge models. These benchmarks introduce new challenges to core
capabilities such as perception, reasoning, and planning. However, existing
multimodal benchmarks fall short in providing a focused evaluation of
multi-step planning based on spatial relationships in images. To bridge this
gap, we present ING-VP, the first INteractive Game-based Vision Planning
benchmark, specifically designed to evaluate the spatial imagination and
multi-step reasoning abilities of MLLMs. ING-VP features 6 distinct games,
encompassing 300 levels, each with 6 unique configurations. A single model
engages in over 60,000 rounds of interaction. The benchmark framework allows
for multiple comparison settings, including image-text vs. text-only inputs,
single-step vs. multi-step reasoning, and with-history vs. without-history
conditions, offering valuable insights into the model's capabilities. We
evaluated numerous state-of-the-art MLLMs, with the highest-performing model,
Claude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the
anticipated standard. This work aims to provide a specialized evaluation
framework to drive advancements in MLLMs' capacity for complex spatial
reasoning and planning. The code is publicly available at
https://github.com/Thisisus7/ING-VP.git."
The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Reinforcement Learning from Human Feedback significantly enhances Natural
Language Processing by aligning language models with human expectations. A
critical factor in this alignment is the strength of reward models used during
training. This study explores whether stronger reward models invariably lead to
better language models. In this paper, through experiments on relevance,
factuality, and completeness tasks using the QA-FEEDBACK dataset and reward
models based on Longformer, we uncover a surprising paradox: language models
trained with moderately accurate reward models outperform those guided by
highly accurate ones. This challenges the widely held belief that stronger
reward models always lead to better language models, and opens up new avenues
for future research into the key factors driving model performance and how to
choose the most suitable reward models. Code and additional details are
available at
[https://github.com/EIT-NLP/AccuracyParadox-RLHF](https://github.com/EIT-NLP/AccuracyParadox-RLHF)."
DCP: Learning Accelerator Dataflow for Neural Network via Propagation,cs.LG,Machine Learning,2024-10-09,"Deep neural network (DNN) hardware (HW) accelerators have achieved great
success in improving DNNs' performance and efficiency. One key reason is
dataflow in executing a DNN layer, including on-chip data partitioning,
computation parallelism, and scheduling policy, which have large impacts on
latency and energy consumption. Unlike prior works that required considerable
efforts from HW engineers to design suitable dataflows for different DNNs, this
work proposes an efficient data-centric approach, named Dataflow Code
Propagation (DCP), to automatically find the optimal dataflow for DNN layers in
seconds without human effort. It has several attractive benefits that prior
arts do not have. (i) We translate the HW dataflow configuration into a code
representation in a unified dataflow coding space, which can be optimized by
backpropagating gradients given a DNN layer or network. (ii) DCP learns a
neural predictor to efficiently update the dataflow codes towards the desired
gradient directions to minimize various optimization objectives e.g., latency
and energy. (iii) It can be easily generalized to unseen HW configurations in a
zero-shot or few-shot learning manner. For example, without using additional
training data, DCP surpasses the GAMMA method that performs a full search using
thousands of samples. Extensive experiments on several representative models
such as MobileNet, ResNet, and ViT show that DCP outperforms its counterparts
in various settings."
InstantIR: Blind Image Restoration with Instant Generative Reference,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Handling test-time unknown degradation is the major challenge in Blind Image
Restoration (BIR), necessitating high model generalization. An effective
strategy is to incorporate prior knowledge, either from human input or
generative model. In this paper, we introduce Instant-reference Image
Restoration (InstantIR), a novel diffusion-based BIR method which dynamically
adjusts generation condition during inference. We first extract a compact
representation of the input via a pre-trained vision encoder. At each
generation step, this representation is used to decode current diffusion latent
and instantiate it in the generative prior. The degraded image is then encoded
with this reference, providing robust generation condition. We observe the
variance of generative references fluctuate with degradation intensity, which
we further leverage as an indicator for developing a sampling algorithm
adaptive to input quality. Extensive experiments demonstrate InstantIR achieves
state-of-the-art performance and offering outstanding visual quality. Through
modulating generative references with textual description, InstantIR can
restore extreme degradation and additionally feature creative restoration."
Investigating Cost-Efficiency of LLM-Generated Training Data for Conversational Semantic Frame Analysis,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Recent studies have demonstrated that few-shot learning allows LLMs to
generate training data for supervised models at a low cost. However, the
quality of LLM-generated data may not entirely match that of human-labeled
data. This raises a crucial question: how should one balance the trade-off
between the higher quality but more expensive human data and the lower quality
yet substantially cheaper LLM-generated data? In this paper, we synthesized
training data for conversational semantic frame analysis using GPT-4 and
examined how to allocate budgets optimally to achieve the best performance. Our
experiments, conducted across various budget levels, reveal that optimal
cost-efficiency is achieved by combining both human and LLM-generated data
across a wide range of budget levels. Notably, as the budget decreases, a
higher proportion of LLM-generated data becomes more preferable."
DiffGAD: A Diffusion-based Unsupervised Graph Anomaly Detector,cs.LG,Machine Learning,2024-10-09,"Graph Anomaly Detection (GAD) is crucial for identifying abnormal entities
within networks, garnering significant attention across various fields.
Traditional unsupervised methods, which decode encoded latent representations
of unlabeled data with a reconstruction focus, often fail to capture critical
discriminative content, leading to suboptimal anomaly detection. To address
these challenges, we present a Diffusion-based Graph Anomaly Detector
(DiffGAD). At the heart of DiffGAD is a novel latent space learning paradigm,
meticulously designed to enhance its proficiency by guiding it with
discriminative content. This innovative approach leverages diffusion sampling
to infuse the latent space with discriminative content and introduces a
content-preservation mechanism that retains valuable information across
different scales, significantly improving its adeptness at identifying
anomalies with limited time and space complexity. Our comprehensive evaluation
of DiffGAD, conducted on six real-world and large-scale datasets with various
metrics, demonstrated its exceptional performance."
TuringQ: Benchmarking AI Comprehension in Theory of Computation,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"We present TuringQ, the first benchmark designed to evaluate the reasoning
capabilities of large language models (LLMs) in the theory of computation.
TuringQ consists of 4,006 undergraduate and graduate-level question-answer
pairs, categorized into four difficulty levels and covering seven core
theoretical areas. We evaluate several open-source LLMs, as well as GPT-4,
using Chain of Thought prompting and expert human assessment. Additionally, we
propose an automated LLM-based evaluation system that demonstrates competitive
accuracy when compared to human evaluation. Fine-tuning a Llama3-8B model on
TuringQ shows measurable improvements in reasoning ability and out-of-domain
tasks such as algebra. TuringQ serves as both a benchmark and a resource for
enhancing LLM performance in complex computational reasoning tasks. Our
analysis offers insights into LLM capabilities and advances in AI comprehension
of theoretical computer science."
Signal Watermark on Large Language Models,cs.CR,Cryptography and Security,2024-10-09,"As Large Language Models (LLMs) become increasingly sophisticated, they raise
significant security concerns, including the creation of fake news and academic
misuse. Most detectors for identifying model-generated text are limited by
their reliance on variance in perplexity and burstiness, and they require
substantial computational resources. In this paper, we proposed a watermarking
method embedding a specific watermark into the text during its generation by
LLMs, based on a pre-defined signal pattern. This technique not only ensures
the watermark's invisibility to humans but also maintains the quality and
grammatical integrity of model-generated text. We utilize LLMs and Fast Fourier
Transform (FFT) for token probability computation and detection of the signal
watermark. The unique application of signal processing principles within the
realm of text generation by LLMs allows for subtle yet effective embedding of
watermarks, which do not compromise the quality or coherence of the generated
text. Our method has been empirically validated across multiple LLMs,
consistently maintaining high detection accuracy, even with variations in
temperature settings during text generation. In the experiment of
distinguishing between human-written and watermarked text, our method achieved
an AUROC score of 0.97, significantly outperforming existing methods like
GPTZero, which scored 0.64. The watermark's resilience to various attacking
scenarios further confirms its robustness, addressing significant challenges in
model-generated text authentication."
Gumbel Rao Monte Carlo based Bi-Modal Neural Architecture Search for Audio-Visual Deepfake Detection,cs.CR,Cryptography and Security,2024-10-09,"Deepfakes pose a critical threat to biometric authentication systems by
generating highly realistic synthetic media. Existing multimodal deepfake
detectors often struggle to adapt to diverse data and rely on simple fusion
methods. To address these challenges, we propose Gumbel-Rao Monte Carlo
Bi-modal Neural Architecture Search (GRMC-BMNAS), a novel architecture search
framework that employs Gumbel-Rao Monte Carlo sampling to optimize multimodal
fusion. It refines the Straight through Gumbel Softmax (STGS) method by
reducing variance with Rao-Blackwellization, stabilizing network training.
Using a two-level search approach, the framework optimizes the network
architecture, parameters, and performance. Crucial features are efficiently
identified from backbone networks, while within the cell structure, a weighted
fusion operation integrates information from various sources. By varying
parameters such as temperature and number of Monte carlo samples yields an
architecture that maximizes classification performance and better
generalisation capability. Experimental results on the FakeAVCeleb and SWAN-DF
datasets demonstrate an impressive AUC percentage of 95.4\%, achieved with
minimal model parameters."
Chip-Tuning: Classify Before Language Models Say,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"The rapid development in the performance of large language models (LLMs) is
accompanied by the escalation of model size, leading to the increasing cost of
model training and inference. Previous research has discovered that certain
layers in LLMs exhibit redundancy, and removing these layers brings only
marginal loss in model performance. In this paper, we adopt the probing
technique to explain the layer redundancy in LLMs and demonstrate that language
models can be effectively pruned with probing classifiers. We propose
chip-tuning, a simple and effective structured pruning framework specialized
for classification problems. Chip-tuning attaches tiny probing classifiers
named chips to different layers of LLMs, and trains chips with the backbone
model frozen. After selecting a chip for classification, all layers subsequent
to the attached layer could be removed with marginal performance loss.
Experimental results on various LLMs and datasets demonstrate that chip-tuning
significantly outperforms previous state-of-the-art baselines in both accuracy
and pruning ratio, achieving a pruning ratio of up to 50%. We also find that
chip-tuning could be applied on multimodal models, and could be combined with
model finetuning, proving its excellent compatibility."
Happy: A Debiased Learning Framework for Continual Generalized Category Discovery,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Constantly discovering novel concepts is crucial in evolving environments.
This paper explores the underexplored task of Continual Generalized Category
Discovery (C-GCD), which aims to incrementally discover new classes from
unlabeled data while maintaining the ability to recognize previously learned
classes. Although several settings are proposed to study the C-GCD task, they
have limitations that do not reflect real-world scenarios. We thus study a more
practical C-GCD setting, which includes more new classes to be discovered over
a longer period, without storing samples of past classes. In C-GCD, the model
is initially trained on labeled data of known classes, followed by multiple
incremental stages where the model is fed with unlabeled data containing both
old and new classes. The core challenge involves two conflicting objectives:
discover new classes and prevent forgetting old ones. We delve into the
conflicts and identify that models are susceptible to prediction bias and
hardness bias. To address these issues, we introduce a debiased learning
framework namely Happy. For the prediction bias, we first introduce
clustering-guided initialization to provide robust features. In addition, we
propose soft entropy regularization to assign appropriate probabilities to new
classes, which can significantly enhance the clustering performance of new
classes. For the harness bias, we present the hardness-aware prototype
sampling, which can effectively reduce the forgetting issue for previously seen
classes, especially for difficult classes. Experimental results demonstrate our
method proficiently manages the conflicts of C-GCD and achieves remarkable
performance across various datasets, e.g., 7.5% overall gains on ImageNet-100.
Our code is publicly available at https://github.com/mashijie1028/Happy-CGCD."
TopoTune : A Framework for Generalized Combinatorial Complex Neural Networks,cs.LG,Machine Learning,2024-10-09,"Graph Neural Networks (GNNs) excel in learning from relational datasets,
processing node and edge features in a way that preserves the symmetries of the
graph domain. However, many complex systems--such as biological or social
networks--involve multiway complex interactions that are more naturally
represented by higher-order topological spaces. The emerging field of
Topological Deep Learning (TDL) aims to accommodate and leverage these
higher-order structures. Combinatorial Complex Neural Networks (CCNNs), fairly
general TDL models, have been shown to be more expressive and better performing
than GNNs. However, differently from the graph deep learning ecosystem, TDL
lacks a principled and standardized framework for easily defining new
architectures, restricting its accessibility and applicability. To address this
issue, we introduce Generalized CCNNs (GCCNs), a novel simple yet powerful
family of TDL models that can be used to systematically transform any (graph)
neural network into its TDL counterpart. We prove that GCCNs generalize and
subsume CCNNs, while extensive experiments on a diverse class of GCCNs show
that these architectures consistently match or outperform CCNNs, often with
less model complexity. In an effort to accelerate and democratize TDL, we
introduce TopoTune, a lightweight software that allows practitioners to define,
build, and train GCCNs with unprecedented flexibility and ease."
The Sampling-Gaussian for stereo matching,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"The soft-argmax operation is widely adopted in neural network-based stereo
matching methods to enable differentiable regression of disparity. However,
network trained with soft-argmax is prone to being multimodal due to absence of
explicit constraint to the shape of the probability distribution. Previous
methods leverages Laplacian distribution and cross-entropy for training but
failed to effectively improve the accuracy and even compromises the efficiency
of the network. In this paper, we conduct a detailed analysis of the previous
distribution-based methods and propose a novel supervision method for stereo
matching, Sampling-Gaussian. We sample from the Gaussian distribution for
supervision. Moreover, we interpret the training as minimizing the distance in
vector space and propose a combined loss of L1 loss and cosine similarity loss.
Additionally, we leveraged bilinear interpolation to upsample the cost volume.
Our method can be directly applied to any soft-argmax-based stereo matching
method without a reduction in efficiency. We have conducted comprehensive
experiments to demonstrate the superior performance of our Sampling-Gaussian.
The experimental results prove that we have achieved better accuracy on five
baseline methods and two datasets. Our method is easy to implement, and the
code is available online."
Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Recent advancements of large language models (LLMs) have led to claims of AI
surpassing humans in natural language processing (NLP) tasks such as textual
understanding and reasoning. This work investigates these assertions by
introducing CAIMIRA, a novel framework rooted in item response theory (IRT)
that enables quantitative assessment and comparison of problem-solving
abilities of question-answering (QA) agents: humans and AI systems. Through
analysis of over 300,000 responses from ~70 AI systems and 155 humans across
thousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in
knowledge domains and reasoning skills. Humans outperform AI systems in
knowledge-grounded abductive and conceptual reasoning, while state-of-the-art
LLMs like GPT-4 and LLaMA show superior performance on targeted information
retrieval and fact-based reasoning, particularly when information gaps are
well-defined and addressable through pattern matching or data retrieval. These
findings highlight the need for future QA tasks to focus on questions that
challenge not only higher-order reasoning and scientific thinking, but also
demand nuanced linguistic interpretation and cross-contextual knowledge
application, helping advance AI developments that better emulate or complement
human cognitive abilities in real-world problem-solving."
On the Security of Bitstream-level JPEG Encryption with Restart Markers,cs.CR,Cryptography and Security,2024-10-09,"This paper aims to evaluate the security of a bitstream-level JPEG encryption
method using restart (RST) markers, where encrypted image can keep the JPEG
file format with the same file size as non-encrypted image. Data encrypted
using this method can be decoded without altering header information by
employing a standard JPEG decoder. Moreover, the use of RST markers enables the
definition of extended blocks divided by the markers, so spatially partial
encryption and block-permutation-based encryption can be carried out. However,
the security of the method was evaluated only with respect to the key space
analysis for brute-force attacks and other limited attacks. Accordingly, in
this paper, we evaluated the security of the method with respect to robustness
against ciphertext-only attacks including state-of-the-art attacks. In
experiments, the method is compared with conventional encryption methods, and
it is confirmed to be robust against ciphertext-only attacks if parameters used
for image encryption are carefully chosen."
Real-to-Sim Grasp: Rethinking the Gap between Simulation and Real World in Grasp Detection,cs.RO,Robotics,2024-10-09,"For 6-DoF grasp detection, simulated data is expandable to train more
powerful model, but it faces the challenge of the large gap between simulation
and real world. Previous works bridge this gap with a sim-to-real way. However,
this way explicitly or implicitly forces the simulated data to adapt to the
noisy real data when training grasp detectors, where the positional drift and
structural distortion within the camera noise will harm the grasp learning. In
this work, we propose a Real-to-Sim framework for 6-DoF Grasp detection, named
R2SGrasp, with the key insight of bridging this gap in a real-to-sim way, which
directly bypasses the camera noise in grasp detector training through an
inference-time real-to-sim adaption. To achieve this real-to-sim adaptation,
our R2SGrasp designs the Real-to-Sim Data Repairer (R2SRepairer) to mitigate
the camera noise of real depth maps in data-level, and the Real-to-Sim Feature
Enhancer (R2SEnhancer) to enhance real features with precise simulated
geometric primitives in feature-level. To endow our framework with the
generalization ability, we construct a large-scale simulated dataset
cost-efficiently to train our grasp detector, which includes 64,000 RGB-D
images with 14.4 million grasp annotations. Sufficient experiments show that
R2SGrasp is powerful and our real-to-sim perspective is effective. The
real-world experiments further show great generalization ability of R2SGrasp.
Project page is available on https://isee-laboratory.github.io/R2SGrasp."
A Novel LLM-based Two-stage Summarization Approach for Long Dialogues,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Long document summarization poses a significant challenge in natural language
processing due to input lengths that exceed the capacity of most
state-of-the-art pre-trained language models. This study proposes a
hierarchical framework that segments and condenses information from long
documents, subsequently fine-tuning the processed text with an abstractive
summarization model. Unsupervised topic segmentation methods identify
semantically appropriate breakpoints. The condensation stage utilizes an
unsupervised generation model to generate condensed data, and our current
experiments employ ChatGPT(v3.5). The summarization stage fine-tunes the
abstractive summarization model on the condensed data to generate the final
results. This framework enables long documents to be processed on models even
when the document length exceeds the model's maximum input size. The exclusion
of the entire document from the summarization model reduces the time and
computational resources required for training, making the framework suitable
for contexts with constrained local computational resources."
SEGMENT+: Long Text Processing with Short-Context Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"There is a growing interest in expanding the input capacity of language
models (LMs) across various domains. However, simply increasing the context
window does not guarantee robust performance across diverse long-input
processing tasks, such as understanding extensive documents and extracting
detailed information from lengthy and noisy data. In response, we introduce
SEGMENT+, a general framework that enables LMs to handle extended inputs within
limited context windows efficiently. SEGMENT+ utilizes structured notes and a
filtering module to manage information flow, resulting in a system that is both
controllable and interpretable. Our extensive experiments across various model
sizes, focusing on long-document question-answering and Needle-in-a-Haystack
tasks, demonstrate the effectiveness of SEGMENT+ in improving performance."
QuadBEV: An Efficient Quadruple-Task Perception Framework via Bird's-Eye-View Representation,cs.RO,Robotics,2024-10-09,"Bird's-Eye-View (BEV) perception has become a vital component of autonomous
driving systems due to its ability to integrate multiple sensor inputs into a
unified representation, enhancing performance in various downstream tasks.
However, the computational demands of BEV models pose challenges for real-world
deployment in vehicles with limited resources. To address these limitations, we
propose QuadBEV, an efficient multitask perception framework that leverages the
shared spatial and contextual information across four key tasks: 3D object
detection, lane detection, map segmentation, and occupancy prediction. QuadBEV
not only streamlines the integration of these tasks using a shared backbone and
task-specific heads but also addresses common multitask learning challenges
such as learning rate sensitivity and conflicting task objectives. Our
framework reduces redundant computations, thereby enhancing system efficiency,
making it particularly suited for embedded systems. We present comprehensive
experiments that validate the effectiveness and robustness of QuadBEV,
demonstrating its suitability for real-world applications."
MORSE: An Efficient Homomorphic Secret Sharing Scheme Enabling Non-Linear Operation,cs.CR,Cryptography and Security,2024-10-09,"Homomorphic secret sharing (HSS) enables two servers to locally perform
functions on encrypted data directly and obtain the results in the form of
shares. A Paillier-based HSS solution seamlessly achieves multiplicative
homomorphism and consumes less communication costs. Unfortunately, existing
Paillier-based HSS schemes suffer from a large private key size, potential
calculation error, expensive computation and storage overhead, and only valid
on linear operations (e.g., addition and multiplication). To this end, inspired
by the Paillier cryptosystem with fast encryption and decryption, we propose
MORSE, an efficient homomorphic secret sharing scheme enabling non-linear
operation, which enjoys a small key size, no calculation error and low
overhead. In terms of functions, MORSE supports addition, subtraction,
multiplication, scalar-multiplication, and comparison. Particularly, we
carefully design two conversion protocols achieving the mutual conversion
between one Paillier ciphertext and two secret shares, which allows MORSE to
continuously perform the above operations. Rigorous analyses demonstrate that
MORSE securely outputs correct results. Experimental results show that MORSE
makes a runtime improvement of up to 9.3 times in terms of secure
multiplication, and a communication costs reduction of up to 16.6% in secure
comparison, compared to the state-of-the-art."
MotionRL: Align Text-to-Motion Generation to Human Preferences with Multi-Reward Reinforcement Learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"We introduce MotionRL, the first approach to utilize Multi-Reward
Reinforcement Learning (RL) for optimizing text-to-motion generation tasks and
aligning them with human preferences. Previous works focused on improving
numerical performance metrics on the given datasets, often neglecting the
variability and subjectivity of human feedback. In contrast, our novel approach
uses reinforcement learning to fine-tune the motion generator based on human
preferences prior knowledge of the human perception model, allowing it to
generate motions that better align human preferences. In addition, MotionRL
introduces a novel multi-objective optimization strategy to approximate Pareto
optimality between text adherence, motion quality, and human preferences.
Extensive experiments and user studies demonstrate that MotionRL not only
allows control over the generated results across different objectives but also
significantly enhances performance across these metrics compared to other
algorithms."
TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"The development of large language models (LLMs) has been instrumental in
advancing state-of-the-art natural language processing applications. Training
LLMs with billions of parameters and trillions of tokens require sophisticated
distributed systems that enable composing and comparing several
state-of-the-art techniques in order to efficiently scale across thousands of
accelerators. However, existing solutions are complex, scattered across
multiple libraries/repositories, lack interoperability, and are cumbersome to
maintain. Thus, curating and empirically comparing training recipes require
non-trivial engineering effort.
  This paper introduces TorchTitan, an open-source, PyTorch-native distributed
training system that unifies state-of-the-art techniques, streamlining
integration and reducing overhead. TorchTitan enables 3D parallelism in a
modular manner with elastic scaling, providing comprehensive logging,
checkpointing, and debugging tools for production-ready training. It also
incorporates hardware-software co-designed solutions, leveraging features like
Float8 training and SymmetricMemory. As a flexible test bed, TorchTitan
facilitates custom recipe curation and comparison, allowing us to develop
optimized training recipes for Llama 3.1 and provide guidance on selecting
techniques for maximum efficiency based on our experiences.
  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8
billion to 405 billion parameters, and showcase its exceptional performance,
modular composability, and elastic scalability. By stacking training
optimizations, we demonstrate accelerations of 65.08% with 1D parallelism at
the 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at
the 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at
the 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized
baselines."
PFAttack: Stealthy Attack Bypassing Group Fairness in Federated Learning,cs.LG,Machine Learning,2024-10-09,"Federated learning (FL), integrating group fairness mechanisms, allows
multiple clients to collaboratively train a global model that makes unbiased
decisions for different populations grouped by sensitive attributes (e.g.,
gender and race). Due to its distributed nature, previous studies have
demonstrated that FL systems are vulnerable to model poisoning attacks.
However, these studies primarily focus on perturbing accuracy, leaving a
critical question unexplored: Can an attacker bypass the group fairness
mechanisms in FL and manipulate the global model to be biased? The motivations
for such an attack vary; an attacker might seek higher accuracy, yet fairness
considerations typically limit the accuracy of the global model or aim to cause
ethical disruption. To address this question, we design a novel form of attack
in FL, termed Profit-driven Fairness Attack (PFATTACK), which aims not to
degrade global model accuracy but to bypass fairness mechanisms. Our
fundamental insight is that group fairness seeks to weaken the dependence of
outputs on input attributes related to sensitive information. In the proposed
PFATTACK, an attacker can recover this dependence through local fine-tuning
across various sensitive groups, thereby creating a biased yet
accuracy-preserving malicious model and injecting it into FL through model
replacement. Compared to attacks targeting accuracy, PFATTACK is more stealthy.
The malicious model in PFATTACK exhibits subtle parameter variations relative
to the original global model, making it robust against detection and filtering
by Byzantine-resilient aggregations. Extensive experiments on benchmark
datasets are conducted for four fair FL frameworks and three
Byzantine-resilient aggregations against model poisoning, demonstrating the
effectiveness and stealth of PFATTACK in bypassing group fairness mechanisms in
FL."
Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning,cs.LG,Machine Learning,2024-10-09,"Monte Carlo Tree Search (MCTS) has recently emerged as a powerful technique
for enhancing the reasoning capabilities of LLMs. Techniques such as SFT or DPO
have enabled LLMs to distill high-quality behaviors from MCTS, improving their
reasoning performance. However, existing distillation methods underutilize the
rich trajectory information generated by MCTS, limiting the potential for
improvements in LLM reasoning. In this paper, we propose AlphaLLM-CPL, a novel
pairwise training framework that enables LLMs to self-improve through MCTS
behavior distillation. AlphaLLM-CPL efficiently leverages MCTS trajectories via
two key innovations: (1) AlphaLLM-CPL constructs stepwise trajectory pairs from
child nodes sharing the same parent in the search tree, providing step-level
information for more effective MCTS behavior distillation. (2) AlphaLLM-CPL
introduces curriculum preference learning, dynamically adjusting the training
sequence of trajectory pairs in each offline training epoch to prioritize
critical learning steps and mitigate overfitting. Experimental results on
mathematical reasoning tasks demonstrate that AlphaLLM-CPL significantly
outperforms previous MCTS behavior distillation methods, substantially boosting
the reasoning capabilities of LLMs."
Chemistry-Inspired Diffusion with Non-Differentiable Guidance,cs.LG,Machine Learning,2024-10-09,"Recent advances in diffusion models have shown remarkable potential in the
conditional generation of novel molecules. These models can be guided in two
ways: (i) explicitly, through additional features representing the condition,
or (ii) implicitly, using a property predictor. However, training property
predictors or conditional diffusion models requires an abundance of labeled
data and is inherently challenging in real-world applications. We propose a
novel approach that attenuates the limitations of acquiring large labeled
datasets by leveraging domain knowledge from quantum chemistry as a
non-differentiable oracle to guide an unconditional diffusion model. Instead of
relying on neural networks, the oracle provides accurate guidance in the form
of estimated gradients, allowing the diffusion process to sample from a
conditional distribution specified by quantum chemistry. We show that this
results in more precise conditional generation of novel and stable molecular
structures. Our experiments demonstrate that our method: (1) significantly
reduces atomic forces, enhancing the validity of generated molecules when used
for stability optimization; (2) is compatible with both explicit and implicit
guidance in diffusion models, enabling joint optimization of molecular
properties and stability; and (3) generalizes effectively to molecular
optimization tasks beyond stability optimization."
On the Similarity of Circuits across Languages: a Case Study on the Subject-verb Agreement Task,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Several algorithms implemented by language models have recently been
successfully reversed-engineered. However, these findings have been
concentrated on specific tasks and models, leaving it unclear how universal
circuits are across different settings. In this paper, we study the circuits
implemented by Gemma 2B for solving the subject-verb agreement task across two
different languages, English and Spanish. We discover that both circuits are
highly consistent, being mainly driven by a particular attention head writing a
`subject number' signal to the last residual stream, which is read by a small
set of neurons in the final MLPs. Notably, this subject number signal is
represented as a direction in the residual stream space, and is
language-independent. We demonstrate that this direction has a causal effect on
the model predictions, effectively flipping the Spanish predicted verb number
by intervening with the direction found in English. Finally, we present
evidence of similar behavior in other models within the Gemma 1 and Gemma 2
families."
Conformal Prediction: A Data Perspective,cs.LG,Machine Learning,2024-10-09,"Conformal prediction (CP), a distribution-free uncertainty quantification
(UQ) framework, reliably provides valid predictive inference for black-box
models. CP constructs prediction sets that contain the true output with a
specified probability. However, modern data science diverse modalities, along
with increasing data and model complexity, challenge traditional CP methods.
These developments have spurred novel approaches to address evolving scenarios.
This survey reviews the foundational concepts of CP and recent advancements
from a data-centric perspective, including applications to structured,
unstructured, and dynamic data. We also discuss the challenges and
opportunities CP faces in large-scale data and models."
"BiC-MPPI: Goal-Pursuing, Sampling-Based Bidirectional Rollout Clustering Path Integral for Trajectory Optimization",cs.RO,Robotics,2024-10-09,"This paper introduces the Bidirectional Clustered MPPI (BiC-MPPI) algorithm,
a novel trajectory optimization method aimed at enhancing goal-directed
guidance within the Model Predictive Path Integral (MPPI) framework. BiC-MPPI
incorporates bidirectional dynamics approximations and a new guide cost
mechanism, improving both trajectory planning and goal-reaching performance. By
leveraging forward and backward rollouts, the bidirectional approach ensures
effective trajectory connections between initial and terminal states, while the
guide cost helps discover dynamically feasible paths. Experimental results
demonstrate that BiC-MPPI outperforms existing MPPI variants in both 2D and 3D
environments, achieving higher success rates and competitive computation times
across 900 simulations on a modified BARN dataset for autonomous navigation.
  GitHub: https://github.com/i-ASL/BiC-MPPI"
Overcoming Autoware-Ubuntu Incompatibility in Autonomous Driving Systems-Equipped Vehicles: Lessons Learned,cs.RO,Robotics,2024-10-09,"Autonomous vehicles have been rapidly developed as demand that provides
safety and efficiency in transportation systems. As autonomous vehicles are
designed based on open-source operating and computing systems, there are
numerous resources aimed at building an operating platform composed of Ubuntu,
Autoware, and Robot Operating System (ROS). However, no explicit guidelines
exist to help scholars perform trouble-shooting due to incompatibility between
the Autoware platform and Ubuntu operating systems installed in autonomous
driving systems-equipped vehicles (i.e., Chrysler Pacifica). The paper presents
an overview of integrating the Autoware platform into the autonomous vehicle's
interface based on lessons learned from trouble-shooting processes for
resolving incompatible issues. The trouble-shooting processes are presented
based on resolving the incompatibility and integration issues of Ubuntu 20.04,
Autoware.AI, and ROS Noetic software installed in an autonomous driving
systems-equipped vehicle. Specifically, the paper focused on common
incompatibility issues and code-solving protocols involving Python
compatibility, Compute Unified Device Architecture (CUDA) installation,
Autoware installation, and simulation in Autoware.AI. The objective of the
paper is to provide an explicit and detail-oriented presentation to showcase
how to address incompatibility issues among an autonomous vehicle's operating
interference. The lessons and experience presented in the paper will be useful
for researchers who encountered similar issues and could follow up by
performing trouble-shooting activities and implementing ADS-related projects in
the Ubuntu, Autoware, and ROS operating systems."
Honesty to Subterfuge: In-Context Reinforcement Learning Can Make Honest Models Reward Hack,cs.AI,Artificial Intelligence,2024-10-09,"Previous work has shown that training ""helpful-only"" LLMs with reinforcement
learning on a curriculum of gameable environments can lead models to generalize
to egregious specification gaming, such as editing their own reward function or
modifying task checklists to appear more successful. We show that gpt-4o,
gpt-4o-mini, o1-preview, and o1-mini - frontier models trained to be helpful,
harmless, and honest - can engage in specification gaming without training on a
curriculum of tasks, purely from in-context iterative reflection (which we call
in-context reinforcement learning, ""ICRL""). We also show that using ICRL to
generate highly-rewarded outputs for expert iteration (compared to the standard
expert iteration reinforcement learning algorithm) may increase gpt-4o-mini's
propensity to learn specification-gaming policies, generalizing (in very rare
cases) to the most egregious strategy where gpt-4o-mini edits its own reward
function. Our results point toward the strong ability of in-context reflection
to discover rare specification-gaming strategies that models might not exhibit
zero-shot or with normal training, highlighting the need for caution when
relying on alignment of LLMs in zero-shot settings."
FedL2G: Learning to Guide Local Training in Heterogeneous Federated Learning,cs.LG,Machine Learning,2024-10-09,"Data and model heterogeneity are two core issues in Heterogeneous Federated
Learning (HtFL). In scenarios with heterogeneous model architectures,
aggregating model parameters becomes infeasible, leading to the use of
prototypes (i.e., class representative feature vectors) for aggregation and
guidance. However, they still experience a mismatch between the extra guiding
objective and the client's original local objective when aligned with global
prototypes. Thus, we propose a Federated Learning-to-Guide (FedL2G) method that
adaptively learns to guide local training in a federated manner and ensures the
extra guidance is beneficial to clients' original tasks. With theoretical
guarantees, FedL2G efficiently implements the learning-to-guide process using
only first-order derivatives w.r.t. model parameters and achieves a non-convex
convergence rate of O(1/T). We conduct extensive experiments on two data
heterogeneity and six model heterogeneity settings using 14 heterogeneous model
architectures (e.g., CNNs and ViTs) to demonstrate FedL2G's superior
performance compared to six counterparts."
"HFH-Font: Few-shot Chinese Font Synthesis with Higher Quality, Faster Speed, and Higher Resolution",cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"The challenge of automatically synthesizing high-quality vector fonts,
particularly for writing systems (e.g., Chinese) consisting of huge amounts of
complex glyphs, remains unsolved. Existing font synthesis techniques fall into
two categories: 1) methods that directly generate vector glyphs, and 2) methods
that initially synthesize glyph images and then vectorize them. However, the
first category often fails to construct complete and correct shapes for complex
glyphs, while the latter struggles to efficiently synthesize high-resolution
(i.e., 1024 $\times$ 1024 or higher) glyph images while preserving local
details. In this paper, we introduce HFH-Font, a few-shot font synthesis method
capable of efficiently generating high-resolution glyph images that can be
converted into high-quality vector glyphs. More specifically, our method
employs a diffusion model-based generative framework with component-aware
conditioning to learn different levels of style information adaptable to
varying input reference sizes. We also design a distillation module based on
Score Distillation Sampling for 1-step fast inference, and a style-guided
super-resolution module to refine and upscale low-resolution synthesis results.
Extensive experiments, including a user study with professional font designers,
have been conducted to demonstrate that our method significantly outperforms
existing font synthesis approaches. Experimental results show that our method
produces high-fidelity, high-resolution raster images which can be vectorized
into high-quality vector fonts. Using our method, for the first time,
large-scale Chinese vector fonts of a quality comparable to those manually
created by professional font designers can be automatically generated."
OledFL: Unleashing the Potential of Decentralized Federated Learning via Opposite Lookahead Enhancement,cs.LG,Machine Learning,2024-10-09,"Decentralized Federated Learning (DFL) surpasses Centralized Federated
Learning (CFL) in terms of faster training, privacy preservation, and light
communication, making it a promising alternative in the field of federated
learning. However, DFL still exhibits significant disparities with CFL in terms
of generalization ability such as rarely theoretical understanding and degraded
empirical performance due to severe inconsistency. In this paper, we enhance
the consistency of DFL by developing an opposite lookahead enhancement
technique (Ole), yielding OledFL to optimize the initialization of each client
in each communication round, thus significantly improving both the
generalization and convergence speed. Moreover, we rigorously establish its
convergence rate in non-convex setting and characterize its generalization
bound through uniform stability, which provides concrete reasons why OledFL can
achieve both the fast convergence speed and high generalization ability.
Extensive experiments conducted on the CIFAR10 and CIFAR100 datasets with
Dirichlet and Pathological distributions illustrate that our OledFL can achieve
up to 5\% performance improvement and 8$\times$ speedup, compared to the most
popular DFedAvg optimizer in DFL."
TCGU: Data-centric Graph Unlearning based on Transferable Condensation,cs.LG,Machine Learning,2024-10-09,"With growing demands for data privacy and model robustness, graph unlearning
(GU), which erases the influence of specific data on trained GNN models, has
gained significant attention. However, existing exact unlearning methods suffer
from either low efficiency or poor model performance. While being more
utility-preserving and efficient, current approximate unlearning methods are
not applicable in the zero-glance privacy setting, where the deleted samples
cannot be accessed during unlearning due to immediate deletion requested by
regulations. Besides, these approximate methods, which try to directly perturb
model parameters still involve high privacy concerns in practice. To fill the
gap, we propose Transferable Condensation Graph Unlearning (TCGU), a
data-centric solution to zero-glance graph unlearning. Specifically, we first
design a two-level alignment strategy to pre-condense the original graph into a
small yet utility-preserving dataset. Upon receiving an unlearning request, we
fine-tune the pre-condensed data with a low-rank plugin, to directly align its
distribution with the remaining graph, thus efficiently revoking the
information of deleted data without accessing them. A novel similarity
distribution matching approach and a discrimination regularizer are proposed to
effectively transfer condensed data and preserve its utility in GNN training,
respectively. Finally, we retrain the GNN on the transferred condensed data.
Extensive experiments on 6 benchmark datasets demonstrate that TCGU can achieve
superior performance in terms of model utility, unlearning efficiency, and
unlearning efficacy than existing GU methods."
LLM Compression with Neural Architecture Search,cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Large language models (LLMs) exhibit remarkable reasoning abilities, allowing
them to generalize across a wide range of downstream tasks, such as commonsense
reasoning or instruction following. However, as LLMs scale, inference costs
become increasingly prohibitive, accumulating significantly over their life
cycle. This poses the question: Can we compress pre-trained LLMs to meet
diverse size and latency requirements? We leverage Neural Architecture Search
(NAS) to compress LLMs by pruning structural components, such as attention
heads, neurons, and layers, aiming to achieve a Pareto-optimal balance between
performance and efficiency. While NAS already achieved promising results on
small language models in previous work, in this paper we propose various
extensions that allow us to scale to LLMs. Compared to structural pruning
baselines, we show that NAS improves performance up to 3.4% on MMLU with an
on-device latency speedup."
3D Representation Methods: A Survey,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"The field of 3D representation has experienced significant advancements,
driven by the increasing demand for high-fidelity 3D models in various
applications such as computer graphics, virtual reality, and autonomous
systems. This review examines the development and current state of 3D
representation methods, highlighting their research trajectories, innovations,
strength and weakness. Key techniques such as Voxel Grid, Point Cloud, Mesh,
Signed Distance Function (SDF), Neural Radiance Field (NeRF), 3D Gaussian
Splatting, Tri-Plane, and Deep Marching Tetrahedra (DMTet) are reviewed. The
review also introduces essential datasets that have been pivotal in advancing
the field, highlighting their characteristics and impact on research progress.
Finally, we explore potential research directions that hold promise for further
expanding the capabilities and applications of 3D representation methods."
Flipping-based Policy for Chance-Constrained Markov Decision Processes,cs.LG,Machine Learning,2024-10-09,"Safe reinforcement learning (RL) is a promising approach for many real-world
decision-making problems where ensuring safety is a critical necessity. In safe
RL research, while expected cumulative safety constraints (ECSCs) are typically
the first choices, chance constraints are often more pragmatic for
incorporating safety under uncertainties. This paper proposes a
\textit{flipping-based policy} for Chance-Constrained Markov Decision Processes
(CCMDPs). The flipping-based policy selects the next action by tossing a
potentially distorted coin between two action candidates. The probability of
the flip and the two action candidates vary depending on the state. We
establish a Bellman equation for CCMDPs and further prove the existence of a
flipping-based policy within the optimal solution sets. Since solving the
problem with joint chance constraints is challenging in practice, we then prove
that joint chance constraints can be approximated into Expected Cumulative
Safety Constraints (ECSCs) and that there exists a flipping-based policy in the
optimal solution sets for constrained MDPs with ECSCs. As a specific instance
of practical implementations, we present a framework for adapting constrained
policy optimization to train a flipping-based policy. This framework can be
applied to other safe RL algorithms. We demonstrate that the flipping-based
policy can improve the performance of the existing safe RL algorithms under the
same limits of safety constraints on Safety Gym benchmarks."
Grounding Robot Policies with Visuomotor Language Guidance,cs.RO,Robotics,2024-10-09,"Recent advances in the fields of natural language processing and computer
vision have shown great potential in understanding the underlying dynamics of
the world from large-scale internet data. However, translating this knowledge
into robotic systems remains an open challenge, given the scarcity of
human-robot interactions and the lack of large-scale datasets of real-world
robotic data. Previous robot learning approaches such as behavior cloning and
reinforcement learning have shown great capabilities in learning robotic skills
from human demonstrations or from scratch in specific environments. However,
these approaches often require task-specific demonstrations or designing
complex simulation environments, which limits the development of generalizable
and robust policies for new settings. Aiming to address these limitations, we
propose an agent-based framework for grounding robot policies to the current
context, considering the constraints of a current robot and its environment
using visuomotor-grounded language guidance. The proposed framework is composed
of a set of conversational agents designed for specific roles -- namely,
high-level advisor, visual grounding, monitoring, and robotic agents. Given a
base policy, the agents collectively generate guidance at run time to shift the
action distribution of the base policy towards more desirable future states. We
demonstrate that our approach can effectively guide manipulation policies to
achieve significantly higher success rates both in simulation and in real-world
experiments without the need for additional human demonstrations or extensive
exploration. Project videos at https://sites.google.com/view/motorcortex/home."
Enabling Novel Mission Operations and Interactions with ROSA: The Robot Operating System Agent,cs.RO,Robotics,2024-10-09,"The advancement of robotic systems has revolutionized numerous industries,
yet their operation often demands specialized technical knowledge, limiting
accessibility for non-expert users. This paper introduces ROSA (Robot Operating
System Agent), an AI-powered agent that bridges the gap between the Robot
Operating System (ROS) and natural language interfaces. By leveraging
state-of-the-art language models and integrating open-source frameworks, ROSA
enables operators to interact with robots using natural language, translating
commands into actions and interfacing with ROS through well-defined tools.
ROSA's design is modular and extensible, offering seamless integration with
both ROS1 and ROS2, along with safety mechanisms like parameter validation and
constraint enforcement to ensure secure, reliable operations. While ROSA is
originally designed for ROS, it can be extended to work with other robotics
middle-wares to maximize compatibility across missions. ROSA enhances
human-robot interaction by democratizing access to complex robotic systems,
empowering users of all expertise levels with multi-modal capabilities such as
speech integration and visual perception. Ethical considerations are thoroughly
addressed, guided by foundational principles like Asimov's Three Laws of
Robotics, ensuring that AI integration promotes safety, transparency, privacy,
and accountability. By making robotic technology more user-friendly and
accessible, ROSA not only improves operational efficiency but also sets a new
standard for responsible AI use in robotics and potentially future mission
operations. This paper introduces ROSA's architecture and showcases initial
mock-up operations in JPL's Mars Yard, a laboratory, and a simulation using
three different robots. The core ROSA library is available as open-source."
Does Spatial Cognition Emerge in Frontier Models?,cs.AI,Artificial Intelligence,2024-10-09,"Not yet. We present SPACE, a benchmark that systematically evaluates spatial
cognition in frontier models. Our benchmark builds on decades of research in
cognitive science. It evaluates large-scale mapping abilities that are brought
to bear when an organism traverses physical environments, smaller-scale
reasoning about object shapes and layouts, and cognitive infrastructure such as
spatial attention and memory. For many tasks, we instantiate parallel
presentations via text and images, allowing us to benchmark both large language
models and large multimodal models. Results suggest that contemporary frontier
models fall short of the spatial intelligence of animals, performing near
chance level on a number of classic tests of animal cognition."
WAPITI: A Watermark for Finetuned Open-Source LLMs,cs.CR,Cryptography and Security,2024-10-09,"Watermarking of large language models (LLMs) generation embeds an
imperceptible statistical pattern within texts, making it algorithmically
detectable. Watermarking is a promising method for addressing potential harm
and biases from LLMs, as it enables traceability, accountability, and detection
of manipulated content, helping to mitigate unintended consequences. However,
for open-source models, watermarking faces two major challenges: (i)
incompatibility with fine-tuned models, and (ii) vulnerability to fine-tuning
attacks. In this work, we propose WAPITI, a new method that transfers
watermarking from base models to fine-tuned models through parameter
integration. To the best of our knowledge, we propose the first watermark for
fine-tuned open-source LLMs that preserves their fine-tuned capabilities.
Furthermore, our approach offers an effective defense against fine-tuning
attacks. We test our method on various model architectures and watermarking
strategies. Results demonstrate that our method can successfully inject
watermarks and is highly compatible with fine-tuned models. Additionally, we
offer an in-depth analysis of how parameter editing influences the watermark
strength and overall capabilities of the resulting models."
Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders,cs.CR,Cryptography and Security,2024-10-09,"The research builds and evaluates the adversarial potential to introduce
copied code or hallucinated AI recommendations for malicious code in popular
code repositories. While foundational large language models (LLMs) from OpenAI,
Google, and Anthropic guard against both harmful behaviors and toxic strings,
previous work on math solutions that embed harmful prompts demonstrate that the
guardrails may differ between expert contexts. These loopholes would appear in
mixture of expert's models when the context of the question changes and may
offer fewer malicious training examples to filter toxic comments or recommended
offensive actions. The present work demonstrates that foundational models may
refuse to propose destructive actions correctly when prompted overtly but may
unfortunately drop their guard when presented with a sudden change of context,
like solving a computer programming challenge. We show empirical examples with
trojan-hosting repositories like GitHub, NPM, NuGet, and popular content
delivery networks (CDN) like jsDelivr which amplify the attack surface. In the
LLM's directives to be helpful, example recommendations propose application
programming interface (API) endpoints which a determined domain-squatter could
acquire and setup attack mobile infrastructure that triggers from the naively
copied code. We compare this attack to previous work on context-shifting and
contrast the attack surface as a novel version of ""living off the land"" attacks
in the malware literature. In the latter case, foundational language models can
hijack otherwise innocent user prompts to recommend actions that violate their
owners' safety policies when posed directly without the accompanying coding
support request."
A Benchmark on Directed Graph Representation Learning in Hardware Designs,cs.LG,Machine Learning,2024-10-09,"To keep pace with the rapid advancements in design complexity within modern
computing systems, directed graph representation learning (DGRL) has become
crucial, particularly for encoding circuit netlists, computational graphs, and
developing surrogate models for hardware performance prediction. However, DGRL
remains relatively unexplored, especially in the hardware domain, mainly due to
the lack of comprehensive and user-friendly benchmarks. This study presents a
novel benchmark comprising five hardware design datasets and 13 prediction
tasks spanning various levels of circuit abstraction. We evaluate 21 DGRL
models, employing diverse graph neural networks and graph transformers (GTs) as
backbones, enhanced by positional encodings (PEs) tailored for directed graphs.
Our results highlight that bidirected (BI) message passing neural networks
(MPNNs) and robust PEs significantly enhance model performance. Notably, the
top-performing models include PE-enhanced GTs interleaved with BI-MPNN layers
and BI-Graph Isomorphism Network, both surpassing baselines across the 13
tasks. Additionally, our investigation into out-of-distribution (OOD)
performance emphasizes the urgent need to improve OOD generalization in DGRL
models. This benchmark, implemented with a modular codebase, streamlines the
evaluation of DGRL models for both hardware and ML practitioners"
"LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",cs.CL,Computation and Language (Natural Language Processing),2024-10-09,"Instruction following is a key capability for LLMs. However, recent studies
have shown that LLMs often struggle with instructions containing multiple
constraints (e.g. a request to create a social media post ""in a funny tone""
with ""no hashtag""). Despite this, most evaluations focus solely on synthetic
data. To address this, we introduce RealInstruct, the first benchmark designed
to evaluate LLMs' ability to follow real-world multi-constrained instructions
by leveraging queries real users asked AI assistants. We also investigate
model-based evaluation as a cost-effective alternative to human annotation for
this task. Our findings reveal that even the proprietary GPT-4 model fails to
meet at least one constraint on over 21% of instructions, highlighting the
limitations of state-of-the-art models. To address the performance gap between
open-source and proprietary models, we propose the Decompose, Critique and
Refine (DeCRIM) self-correction pipeline, which enhances LLMs' ability to
follow constraints. DeCRIM works by decomposing the original instruction into a
list of constraints and using a Critic model to decide when and where the LLM's
response needs refinement. Our results show that DeCRIM improves Mistral's
performance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback.
Moreover, we demonstrate that with strong feedback, open-source LLMs with
DeCRIM can outperform GPT-4 on both benchmarks."
From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning,cs.CV,Computer Vision and Pattern Recognition,2024-10-09,"Large vision language models (VLMs) combine large language models with vision
encoders, demonstrating promise across various tasks. However, they often
underperform in task-specific applications due to domain gaps between
pre-training and fine-tuning. We introduce VITask, a novel framework that
enhances task-specific adaptability of VLMs by integrating task-specific models
(TSMs). VITask employs three key strategies: exemplar prompting (EP), response
distribution alignment (RDA), and contrastive response tuning (CRT) to improve
the task-specific performance of VLMs by adjusting their response
distributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to
adapt without TSMs during inference by learning from exemplar-prompted models.
CRT further optimizes the ranking of correct image-response pairs, thereby
reducing the risk of generating undesired responses. Experiments on 12 medical
diagnosis datasets across 9 imaging modalities show that VITask outperforms
both vanilla instruction-tuned VLMs and TSMs, showcasing its ability to
integrate complementary features from both models effectively. Additionally,
VITask offers practical advantages such as flexible TSM integration and
robustness to incomplete instructions, making it a versatile and efficient
solution for task-specific VLM tuning. Our code are available at
https://github.com/baiyang4/VITask."
Modeling chaotic Lorenz ODE System using Scientific Machine Learning,cs.LG,Machine Learning,2024-10-09,"In climate science, models for global warming and weather prediction face
significant challenges due to the limited availability of high-quality data and
the difficulty in obtaining it, making data efficiency crucial. In the past few
years, Scientific Machine Learning (SciML) models have gained tremendous
traction as they can be trained in a data-efficient manner, making them highly
suitable for real-world climate applications. Despite this, very little
attention has been paid to chaotic climate system modeling utilizing SciML
methods. In this paper, we have integrated SciML methods into foundational
weather models, where we have enhanced large-scale climate predictions with a
physics-informed approach that achieves high accuracy with reduced data. We
successfully demonstrate that by combining the interpretability of physical
climate models with the computational power of neural networks, SciML models
can prove to be a reliable tool for modeling climate. This indicates a shift
from the traditional black box-based machine learning modeling of climate
systems to physics-informed decision-making, leading to effective climate
policy implementation."
Machine Unlearning in Forgettability Sequence,cs.LG,Machine Learning,2024-10-09,"Machine unlearning (MU) is becoming a promising paradigm to achieve the
""right to be forgotten"", where the training trace of any chosen data points
could be eliminated, while maintaining the model utility on general testing
samples after unlearning. With the advancement of forgetting research, many
fundamental open questions remain unanswered: do different samples exhibit
varying levels of difficulty in being forgotten? Further, does the sequence in
which samples are forgotten, determined by their respective difficulty levels,
influence the performance of forgetting algorithms? In this paper, we identify
key factor affecting unlearning difficulty and the performance of unlearning
algorithms. We find that samples with higher privacy risks are more likely to
be unlearning, indicating that the unlearning difficulty varies among different
samples which motives a more precise unlearning mode. Built upon this insight,
we propose a general unlearning framework, dubbed RSU, which consists of
Ranking module and SeqUnlearn module."
Multi-label Classification for Android Malware Based on Active Learning,cs.CR,Cryptography and Security,2024-10-09,"The existing malware classification approaches (i.e., binary and family
classification) can barely benefit subsequent analysis with their outputs. Even
the family classification approaches suffer from lacking a formal naming
standard and an incomplete definition of malicious behaviors. More importantly,
the existing approaches are powerless for one malware with multiple malicious
behaviors, while this is a very common phenomenon for Android malware in the
wild. So, neither of them can provide researchers with a direct and
comprehensive enough understanding of malware. In this paper, we propose
MLCDroid, an ML-based multi-label classification approach that can directly
indicate the existence of pre-defined malicious behaviors. With an in-depth
analysis, we summarize six basic malicious behaviors from real-world malware
with security reports and construct a labeled dataset. We compare the results
of 70 algorithm combinations to evaluate the effectiveness (best at 73.3%).
Faced with the challenge of the expensive cost of data annotation, we further
propose an active learning approach based on data augmentation, which can
improve the overall accuracy to 86.7% with a data augmentation of 5,000+
high-quality samples from an unlabeled malware dataset. This is the first
multi-label Android malware classification approach intending to provide more
information on fine-grained malicious behaviors."
MaD-Scientist: AI-based Scientist solving Convection-Diffusion-Reaction Equations Using Massive PINN-Based Prior Data,cs.LG,Machine Learning,2024-10-09,"Large language models (LLMs), like ChatGPT, have shown that even trained with
noisy prior data, they can generalize effectively to new tasks through
in-context learning (ICL) and pre-training techniques. Motivated by this, we
explore whether a similar approach can be applied to scientific foundation
models (SFMs). Our methodology is structured as follows: (i) we collect
low-cost physics-informed neural network (PINN)-based approximated prior data
in the form of solutions to partial differential equations (PDEs) constructed
through an arbitrary linear combination of mathematical dictionaries; (ii) we
utilize Transformer architectures with self and cross-attention mechanisms to
predict PDE solutions without knowledge of the governing equations in a
zero-shot setting; (iii) we provide experimental evidence on the
one-dimensional convection-diffusion-reaction equation, which demonstrate that
pre-training remains robust even with approximated prior data, with only
marginal impacts on test accuracy. Notably, this finding opens the path to
pre-training SFMs with realistic, low-cost data instead of (or in conjunction
with) numerical high-cost data. These results support the conjecture that SFMs
can improve in a manner similar to LLMs, where fully cleaning the vast set of
sentences crawled from the Internet is nearly impossible."
Addax: Utilizing Zeroth-Order Gradients to Improve Memory Efficiency and Performance of SGD for Fine-Tuning Language Models,cs.LG,Machine Learning,2024-10-09,"Fine-tuning language models (LMs) with the Adam optimizer often demands
excessive memory, limiting accessibility. The ""in-place"" version of Stochastic
Gradient Descent (IP-SGD) and Memory-Efficient Zeroth-order Optimizer (MeZO)
have been proposed to address this. However, IP-SGD still requires substantial
memory, and MeZO suffers from slow convergence and degraded final performance
due to its zeroth-order nature. This paper introduces Addax, a novel method
that improves both memory efficiency and performance of IP-SGD by integrating
it with MeZO. Specifically, Addax computes zeroth- or first-order gradients of
data points in the minibatch based on their memory consumption, combining these
gradient estimates to update directions. By computing zeroth-order gradients
for data points that require more memory and first-order gradients for others,
Addax overcomes the slow convergence of MeZO and the excessive memory
requirement of IP-SGD. Additionally, the zeroth-order gradient acts as a
regularizer for the first-order gradient, further enhancing the model's final
performance. Theoretically, we establish the convergence of Addax under mild
assumptions, demonstrating faster convergence and less restrictive
hyper-parameter choices than MeZO. Our experiments with diverse LMs and tasks
show that Addax consistently outperforms MeZO regarding accuracy and
convergence speed while having a comparable memory footprint. When fine-tuning
OPT-13B with one A100 GPU, on average, Addax outperforms MeZO in accuracy/F1
score by 14% and runs 15x faster while using memory similar to MeZO. In our
experiments on the larger OPT-30B model, on average, Addax outperforms MeZO in
terms of accuracy/F1 score by >16 and runs 30x faster on a single H100 GPU.
Moreover, Addax surpasses the performance of standard fine-tuning approaches,
such as IP-SGD and Adam, in most tasks with significantly less memory
requirement."
LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality,cs.RO,Robotics,2024-10-09,"Understanding human locomotion is crucial for AI agents such as robots,
particularly in complex indoor home environments. Modeling human trajectories
in these spaces requires insight into how individuals maneuver around physical
obstacles and manage social navigation dynamics. These dynamics include subtle
behaviors influenced by proxemics - the social use of space, such as stepping
aside to allow others to pass or choosing longer routes to avoid collisions.
Previous research has developed datasets of human motion in indoor scenes, but
these are often limited in scale and lack the nuanced social navigation
dynamics common in home environments. To address this, we present LocoVR, a
dataset of 7000+ two-person trajectories captured in virtual reality from over
130 different indoor home environments. LocoVR provides full body pose data and
precise spatial information, along with rich examples of socially-motivated
movement behaviors. For example, the dataset captures instances of individuals
navigating around each other in narrow spaces, adjusting paths to respect
personal boundaries in living areas, and coordinating movements in high-traffic
zones like entryways and kitchens. Our evaluation shows that LocoVR
significantly enhances model performance in three practical indoor tasks
utilizing human trajectories, and demonstrates predicting socially-aware
navigation patterns in home environments."
Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs,cs.LG,Machine Learning,2024-10-09,"From common-sense reasoning to domain-specific tasks, parameter-efficient
fine tuning (PEFT) methods for large language models (LLMs) have showcased
significant performance improvements on downstream tasks. However, fine-tuned
LLMs often struggle with overconfidence in uncertain predictions, particularly
due to sparse training data. This overconfidence reflects poor epistemic
uncertainty calibration, which arises from limitations in the model's ability
to generalize with limited data. Existing PEFT uncertainty quantification
methods for LLMs focus on the post fine-tuning stage and thus have limited
capability in calibrating epistemic uncertainty. To address these limitations,
we propose Functional-Level Uncertainty Quantification for Calibrated
Fine-Tuning (UQ4CT), which captures and calibrates functional-level epistemic
uncertainty during the fine-tuning stage via a mixture-of-expert framework. We
show that UQ4CT reduces Expected Calibration Error (ECE) by more than $25\%$
while maintaining high accuracy across $5$ benchmarks. Furthermore, UQ4CT
maintains superior ECE performance with high accuracy under distribution shift,
showcasing improved generalizability."
Stress Detection on Code-Mixed Texts in Dravidian Languages using Machine Learning,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Stress is a common feeling in daily life, but it can affect mental well-being
in some situations, the development of robust detection models is imperative.
This study introduces a methodical approach to the stress identification in
code-mixed texts for Dravidian languages. The challenge encompassed two
datasets, targeting Tamil and Telugu languages respectively. This proposal
underscores the importance of using uncleaned text as a benchmark to refine
future classification methodologies, incorporating diverse preprocessing
techniques. Random Forest algorithm was used, featuring three textual
representations: TF-IDF, Uni-grams of words, and a composite of (1+2+3)-Grams
of characters. The approach achieved a good performance for both linguistic
categories, achieving a Macro F1-score of 0.734 in Tamil and 0.727 in Telugu,
overpassing results achieved with different complex techniques such as FastText
and Transformer models. The results underscore the value of uncleaned data for
mental state detection and the challenges classifying code-mixed texts for
stress, indicating the potential for improved performance through cleaning
data, other preprocessing techniques, or more complex models."
NLP Case Study on Predicting the Before and After of the Ukraine-Russia and Hamas-Israel Conflicts,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"We propose a method to predict toxicity and other textual attributes through
the use of natural language processing (NLP) techniques for two recent events:
the Ukraine-Russia and Hamas-Israel conflicts. This article provides a basis
for exploration in future conflicts with hopes to mitigate risk through the
analysis of social media before and after a conflict begins. Our work compiles
several datasets from Twitter and Reddit for both conflicts in a before and
after separation with an aim of predicting a future state of social media for
avoidance. More specifically, we show that: (1) there is a noticeable
difference in social media discussion leading up to and following a conflict
and (2) social media discourse on platforms like Twitter and Reddit is useful
in identifying future conflicts before they arise. Our results show that
through the use of advanced NLP techniques (both supervised and unsupervised)
toxicity and other attributes about language before and after a conflict is
predictable with a low error of nearly 1.2 percent for both conflicts."
Restructuring Vector Quantization with the Rotation Trick,cs.LG,Machine Learning,2024-10-08,"Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress
a continuous input to a discrete latent space and reconstruct it with minimal
distortion. They operate by maintaining a set of vectors -- often referred to
as the codebook -- and quantizing each encoder output to the nearest vector in
the codebook. However, as vector quantization is non-differentiable, the
gradient to the encoder flows around the vector quantization layer rather than
through it in a straight-through approximation. This approximation may be
undesirable as all information from the vector quantization operation is lost.
In this work, we propose a way to propagate gradients through the vector
quantization layer of VQ-VAEs. We smoothly transform each encoder output into
its corresponding codebook vector via a rotation and rescaling linear
transformation that is treated as a constant during backpropagation. As a
result, the relative magnitude and angle between encoder output and codebook
vector becomes encoded into the gradient as it propagates through the vector
quantization layer and back to the encoder. Across 11 different VQ-VAE training
paradigms, we find this restructuring improves reconstruction metrics, codebook
utilization, and quantization error. Our code is available at
https://github.com/cfifty/rotation_trick."
FAIREDU: A Multiple Regression-Based Method for Enhancing Fairness in Machine Learning Models for Educational Applications,cs.LG,Machine Learning,2024-10-08,"Fairness in artificial intelligence and machine learning (AI/ML) models is
becoming critically important, especially as decisions made by these systems
impact diverse groups. In education, a vital sector for all countries, the
widespread application of AI/ML systems raises specific concerns regarding
fairness. Current research predominantly focuses on fairness for individual
sensitive features, which limits the comprehensiveness of fairness assessments.
This paper introduces FAIREDU, a novel and effective method designed to improve
fairness across multiple sensitive features. Through extensive experiments, we
evaluate FAIREDU effectiveness in enhancing fairness without compromising model
performance. The results demonstrate that FAIREDU addresses intersectionality
across features such as gender, race, age, and other sensitive features,
outperforming state-of-the-art methods with minimal effect on model accuracy.
The paper also explores potential future research directions to enhance further
the method robustness and applicability to various machine-learning models and
datasets."
Predicting Battery Capacity Fade Using Probabilistic Machine Learning Models With and Without Pre-Trained Priors,cs.LG,Machine Learning,2024-10-08,"Lithium-ion batteries are a key energy storage technology driving revolutions
in mobile electronics, electric vehicles and renewable energy storage. Capacity
retention is a vital performance measure that is frequently utilized to assess
whether these batteries have approached their end-of-life. Machine learning
(ML) offers a powerful tool for predicting capacity degradation based on past
data, and, potentially, prior physical knowledge, but the degree to which an ML
prediction can be trusted is of significant practical importance in situations
where consequential decisions must be made based on battery state of health.
This study explores the efficacy of fully Bayesian machine learning in
forecasting battery health with the quantification of uncertainty in its
predictions. Specifically, we implemented three probabilistic ML approaches and
evaluated the accuracy of their predictions and uncertainty estimates: a
standard Gaussian process (GP), a structured Gaussian process (sGP), and a
fully Bayesian neural network (BNN). In typical applications of GP and sGP,
their hyperparameters are learned from a single sample while, in contrast, BNNs
are typically pre-trained on an existing dataset to learn the weight
distributions before being used for inference. This difference in methodology
gives the BNN an advantage in learning global trends in a dataset and makes
BNNs a good choice when training data is available. However, we show that
pre-training can also be leveraged for GP and sGP approaches to learn the prior
distributions of the hyperparameters and that in the case of the pre-trained
sGP, similar accuracy and improved uncertainty estimation compared to the BNN
can be achieved. This approach offers a framework for a broad range of
probabilistic machine learning scenarios where past data is available and can
be used to learn priors for (hyper)parameters of probabilistic ML models."
ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"The global shortage of healthcare workers has demanded the development of
smart healthcare assistants, which can help monitor and alert healthcare
workers when necessary. We examine the healthcare knowledge of existing Large
Vision Language Models (LVLMs) via the Visual Question Answering (VQA) task in
hospital settings through expert annotated open-ended questions. We introduce
the Emergency Room Visual Question Answering (ERVQA) dataset, consisting of
<image, question, answer> triplets covering diverse emergency room scenarios, a
seminal benchmark for LVLMs. By developing a detailed error taxonomy and
analyzing answer trends, we reveal the nuanced nature of the task. We benchmark
state-of-the-art open-source and closed LVLMs using traditional and adapted VQA
metrics: Entailment Score and CLIPScore Confidence. Analyzing errors across
models, we infer trends based on properties like decoder type, model size, and
in-context examples. Our findings suggest the ERVQA dataset presents a highly
complex task, highlighting the need for specialized, domain-specific solutions."
MIRACLE 3D: Memory-efficient Integrated Robust Approach for Continual Learning on Point Clouds via Shape Model construction,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"In this paper, we introduce a novel framework for memory-efficient and
privacy-preserving continual learning in 3D object classification. Unlike
conventional memory-based approaches in continual learning that require storing
numerous exemplars, our method constructs a compact shape model for each class,
retaining only the mean shape along with a few key modes of variation. This
strategy not only enables the generation of diverse training samples while
drastically reducing memory usage but also enhances privacy by eliminating the
need to store original data. To further improve model robustness against input
variations, an issue common in 3D domains due to the absence of strong
backbones and limited training data, we incorporate Gradient Mode
Regularization. This technique enhances model stability and broadens
classification margins, resulting in accuracy improvements. We validate our
approach through extensive experiments on the ModelNet40, ShapeNet, and ScanNet
datasets, where we achieve state-of-the-art performance. Notably, our method
consumes only 15% of the memory required by competing methods on the ModelNet40
and ShapeNet, while achieving comparable performance on the challenging ScanNet
dataset with just 8.5% of the memory. These results underscore the scalability,
effectiveness, and privacy-preserving strengths of our framework for 3D object
classification."
Stochastic Sparse Sampling: A Framework for Variable-Length Medical Time Series Classification,cs.LG,Machine Learning,2024-10-08,"While the majority of time series classification research has focused on
modeling fixed-length sequences, variable-length time series classification
(VTSC) remains critical in healthcare, where sequence length may vary among
patients and events. To address this challenge, we propose
$\textbf{S}$tochastic $\textbf{S}$parse $\textbf{S}$ampling (SSS), a novel VTSC
framework developed for medical time series. SSS manages variable-length
sequences by sparsely sampling fixed windows to compute local predictions,
which are then aggregated and calibrated to form a global prediction. We apply
SSS to the task of seizure onset zone (SOZ) localization, a critical VTSC
problem requiring identification of seizure-inducing brain regions from
variable-length electrophysiological time series. We evaluate our method on the
Epilepsy iEEG Multicenter Dataset, a heterogeneous collection of intracranial
electroencephalography (iEEG) recordings obtained from four independent medical
centers. SSS demonstrates superior performance compared to state-of-the-art
(SOTA) baselines across most medical centers, and superior performance on all
out-of-distribution (OOD) unseen medical centers. Additionally, SSS naturally
provides post-hoc insights into local signal characteristics related to the
SOZ, by visualizing temporally averaged local predictions throughout the
signal."
BEVLoc: Cross-View Localization and Matching via Birds-Eye-View Synthesis,cs.RO,Robotics,2024-10-08,"Ground to aerial matching is a crucial and challenging task in outdoor
robotics, particularly when GPS is absent or unreliable. Structures like
buildings or large dense forests create interference, requiring GNSS
replacements for global positioning estimates. The true difficulty lies in
reconciling the perspective difference between the ground and air images for
acceptable localization. Taking inspiration from the autonomous driving
community, we propose a novel framework for synthesizing a birds-eye-view (BEV)
scene representation to match and localize against an aerial map in off-road
environments. We leverage contrastive learning with domain specific hard
negative mining to train a network to learn similar representations between the
synthesized BEV and the aerial map. During inference, BEVLoc guides the
identification of the most probable locations within the aerial map through a
coarse-to-fine matching strategy. Our results demonstrate promising initial
outcomes in extremely difficult forest environments with limited semantic
diversity. We analyze our model's performance for coarse and fine matching,
assessing both the raw matching capability of our model and its performance as
a GNSS replacement. Our work delves into off-road map localization while
establishing a foundational baseline for future developments in localization.
Our code is available at: https://github.com/rpl-cmu/bevloc"
Automating Data Science Pipelines with Tensor Completion,cs.LG,Machine Learning,2024-10-08,"Hyperparameter optimization is an essential component in many data science
pipelines and typically entails exhaustive time and resource-consuming
computations in order to explore the combinatorial search space. Similar to
this problem, other key operations in data science pipelines exhibit the exact
same properties. Important examples are: neural architecture search, where the
goal is to identify the best design choices for a neural network, and query
cardinality estimation, where given different predicate values for a SQL query
the goal is to estimate the size of the output. In this paper, we abstract away
those essential components of data science pipelines and we model them as
instances of tensor completion, where each variable of the search space
corresponds to one mode of the tensor, and the goal is to identify all missing
entries of the tensor, corresponding to all combinations of variable values,
starting from a very small sample of observed entries. In order to do so, we
first conduct a thorough experimental evaluation of existing state-of-the-art
tensor completion techniques and introduce domain-inspired adaptations (such as
smoothness across the discretized variable space) and an ensemble technique
which is able to achieve state-of-the-art performance. We extensively evaluate
existing and proposed methods in a number of datasets generated corresponding
to (a) hyperparameter optimization for non-neural network models, (b) neural
architecture search, and (c) variants of query cardinality estimation,
demonstrating the effectiveness of tensor completion as a tool for automating
data science pipelines. Furthermore, we release our generated datasets and code
in order to provide benchmarks for future work on this topic."
A Skewness-Based Criterion for Addressing Heteroscedastic Noise in Causal Discovery,cs.LG,Machine Learning,2024-10-08,"Real-world data often violates the equal-variance assumption
(homoscedasticity), making it essential to account for heteroscedastic noise in
causal discovery. In this work, we explore heteroscedastic symmetric noise
models (HSNMs), where the effect $Y$ is modeled as $Y = f(X) + \sigma(X)N$,
with $X$ as the cause and $N$ as independent noise following a symmetric
distribution. We introduce a novel criterion for identifying HSNMs based on the
skewness of the score (i.e., the gradient of the log density) of the data
distribution. This criterion establishes a computationally tractable
measurement that is zero in the causal direction but nonzero in the anticausal
direction, enabling the causal direction discovery. We extend this
skewness-based criterion to the multivariate setting and propose SkewScore, an
algorithm that handles heteroscedastic noise without requiring the extraction
of exogenous noise. We also conduct a case study on the robustness of SkewScore
in a bivariate model with a latent confounder, providing theoretical insights
into its performance. Empirical studies further validate the effectiveness of
the proposed method."
Topology-Agnostic Graph U-Nets for Scalar Field Prediction on Unstructured Meshes,cs.LG,Machine Learning,2024-10-08,"Machine-learned surrogate models to accelerate lengthy computer simulations
are becoming increasingly important as engineers look to streamline the product
design cycle. In many cases, these approaches offer the ability to predict
relevant quantities throughout a geometry, but place constraints on the form of
the input data. In a world of diverse data types, a preferred approach would
not restrict the input to a particular structure. In this paper, we propose
Topology-Agnostic Graph U-Net (TAG U-Net), a graph convolutional network that
can be trained to input any mesh or graph structure and output a prediction of
a target scalar field at each node. The model constructs coarsened versions of
each input graph and performs a set of convolution and pooling operations to
predict the node-wise outputs on the original graph. By training on a diverse
set of shapes, the model can make strong predictions, even for shapes unlike
those seen during training. A 3-D additive manufacturing dataset is presented,
containing Laser Powder Bed Fusion simulation results for thousands of parts.
The model is demonstrated on this dataset, and it performs well, predicting
both 2-D and 3-D scalar fields with a median R-squared > 0.85 on test
geometries. Code and datasets are available online."
"Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the Importance of 2D Representation, Positions, and Objects",cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"The Abstraction and Reasoning Corpus (ARC) is a popular benchmark focused on
visual reasoning in the evaluation of Artificial Intelligence systems. In its
original framing, an ARC task requires solving a program synthesis problem over
small 2D images using a few input-output training pairs. In this work, we adopt
the recently popular data-driven approach to the ARC and ask whether a Vision
Transformer (ViT) can learn the implicit mapping, from input image to output
image, that underlies the task. We show that a ViT -- otherwise a
state-of-the-art model for images -- fails dramatically on most ARC tasks even
when trained on one million examples per task. This points to an inherent
representational deficiency of the ViT architecture that makes it incapable of
uncovering the simple structured mappings underlying the ARC tasks. Building on
these insights, we propose ViTARC, a ViT-style architecture that unlocks some
of the visual reasoning capabilities required by the ARC. Specifically, we use
a pixel-level input representation, design a spatially-aware tokenization
scheme, and introduce a novel object-based positional encoding that leverages
automatic segmentation, among other enhancements. Our task-specific ViTARC
models achieve a test solve rate close to 100% on more than half of the 400
public ARC tasks strictly through supervised learning from input-output grids.
This calls attention to the importance of imbuing the powerful (Vision)
Transformer with the correct inductive biases for abstract visual reasoning
that are critical even when the training data is plentiful and the mapping is
noise-free. Hence, ViTARC provides a strong foundation for future research in
visual reasoning using transformer-based architectures."
Trajectory Improvement and Reward Learning from Comparative Language Feedback,cs.RO,Robotics,2024-10-08,"Learning from human feedback has gained traction in fields like robotics and
natural language processing in recent years. While prior works mostly rely on
human feedback in the form of comparisons, language is a preferable modality
that provides more informative insights into user preferences. In this work, we
aim to incorporate comparative language feedback to iteratively improve robot
trajectories and to learn reward functions that encode human preferences. To
achieve this goal, we learn a shared latent space that integrates trajectory
data and language feedback, and subsequently leverage the learned latent space
to improve trajectories and learn human preferences. To the best of our
knowledge, we are the first to incorporate comparative language feedback into
reward learning. Our simulation experiments demonstrate the effectiveness of
the learned latent space and the success of our learning algorithms. We also
conduct human subject studies that show our reward learning algorithm achieves
a 23.9% higher subjective score on average and is 11.3% more time-efficient
compared to preference-based reward learning, underscoring the superior
performance of our method. Our website is at
https://liralab.usc.edu/comparative-language-feedback/"
Adaptive Random Fourier Features Training Stabilized By Resampling With Applications in Image Regression,cs.LG,Machine Learning,2024-10-08,"This paper presents an enhanced adaptive random Fourier features (ARFF)
training algorithm for shallow neural networks, building upon the work
introduced in ""Adaptive Random Fourier Features with Metropolis Sampling"",
Kammonen et al., Foundations of Data Science, 2(3):309--332, 2020. This
improved method uses a particle filter type resampling technique to stabilize
the training process and reduce sensitivity to parameter choices. With
resampling, the Metropolis test may also be omitted, reducing the number of
hyperparameters and reducing the computational cost per iteration, compared to
ARFF. We present comprehensive numerical experiments demonstrating the efficacy
of our proposed algorithm in function regression tasks, both as a standalone
method and as a pre-training step before gradient-based optimization, here
Adam. Furthermore, we apply our algorithm to a simple image regression problem,
showcasing its utility in sampling frequencies for the random Fourier features
(RFF) layer of coordinate-based multilayer perceptrons (MLPs). In this context,
we use the proposed algorithm to sample the parameters of the RFF layer in an
automated manner."
Provable Accuracy Bounds for Hybrid Dynamical Optimization and Sampling,cs.LG,Machine Learning,2024-10-08,"Analog dynamical accelerators (DXs) are a growing sub-field in computer
architecture research, offering order-of-magnitude gains in power efficiency
and latency over traditional digital methods in several machine learning,
optimization, and sampling tasks. However, limited-capacity accelerators
require hybrid analog/digital algorithms to solve real-world problems, commonly
using large-neighborhood local search (LNLS) frameworks. Unlike fully digital
algorithms, hybrid LNLS has no non-asymptotic convergence guarantees and no
principled hyperparameter selection schemes, particularly limiting cross-device
training and inference.
  In this work, we provide non-asymptotic convergence guarantees for hybrid
LNLS by reducing to block Langevin Diffusion (BLD) algorithms. Adapting tools
from classical sampling theory, we prove exponential KL-divergence convergence
for randomized and cyclic block selection strategies using ideal DXs. With
finite device variation, we provide explicit bounds on the 2-Wasserstein bias
in terms of step duration, noise strength, and function parameters. Our BLD
model provides a key link between established theory and novel computing
platforms, and our theoretical results provide a closed-form expression linking
device variation, algorithm hyperparameters, and performance."
MLissard: Multilingual Long and Simple Sequential Reasoning Benchmarks,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Language models are now capable of solving tasks that require dealing with
long sequences consisting of hundreds of thousands of tokens. However, they
often fail on tasks that require repetitive use of simple rules, even on
sequences that are much shorter than those seen during training. For example,
state-of-the-art LLMs can find common items in two lists with up to 20 items
but fail when lists have 80 items. In this paper, we introduce MLissard, a
multilingual benchmark designed to evaluate models' abilities to process and
generate texts of varied lengths and offers a mechanism for controlling
sequence complexity.
  Our evaluation of open-source and proprietary models show a consistent
decline in performance across all models and languages as the complexity of the
sequence increases. Surprisingly, the use of in-context examples in languages
other than English helps increase extrapolation performance significantly. The
datasets and code are available at https://github.com/unicamp-dl/Lissard"
Multimodal Representation Learning using Adaptive Graph Construction,cs.LG,Machine Learning,2024-10-08,"Multimodal contrastive learning train neural networks by levergaing data from
heterogeneous sources such as images and text. Yet, many current multimodal
learning architectures cannot generalize to an arbitrary number of modalities
and need to be hand-constructed. We propose AutoBIND, a novel contrastive
learning framework that can learn representations from an arbitrary number of
modalites through graph optimization. We evaluate AutoBIND on Alzhiemer's
disease detection because it has real-world medical applicability and it
contains a broad range of data modalities. We show that AutoBIND outperforms
previous methods on this task, highlighting the generalizablility of the
approach."
Counterfactual Causal Inference in Natural Language with Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Causal structure discovery methods are commonly applied to structured data
where the causal variables are known and where statistical testing can be used
to assess the causal relationships. By contrast, recovering a causal structure
from unstructured natural language data such as news articles contains numerous
challenges due to the absence of known variables or counterfactual data to
estimate the causal links. Large Language Models (LLMs) have shown promising
results in this direction but also exhibit limitations. This work investigates
LLM's abilities to build causal graphs from text documents and perform
counterfactual causal inference. We propose an end-to-end causal structure
discovery and causal inference method from natural language: we first use an
LLM to extract the instantiated causal variables from text data and build a
causal graph. We merge causal graphs from multiple data sources to represent
the most exhaustive set of causes possible. We then conduct counterfactual
inference on the estimated graph. The causal graph conditioning allows
reduction of LLM biases and better represents the causal estimands. We use our
method to show that the limitations of LLMs in counterfactual causal reasoning
come from prediction errors and propose directions to mitigate them. We
demonstrate the applicability of our method on real-world news articles."
Validation of the Scientific Literature via Chemputation Augmented by Large Language Models,cs.AI,Artificial Intelligence,2024-10-08,"Chemputation is the process of programming chemical robots to do experiments
using a universal symbolic language, but the literature can be error prone and
hard to read due to ambiguities. Large Language Models (LLMs) have demonstrated
remarkable capabilities in various domains, including natural language
processing, robotic control, and more recently, chemistry. Despite significant
advancements in standardizing the reporting and collection of synthetic
chemistry data, the automatic reproduction of reported syntheses remains a
labour-intensive task. In this work, we introduce an LLM-based chemical
research agent workflow designed for the automatic validation of synthetic
literature procedures. Our workflow can autonomously extract synthetic
procedures and analytical data from extensive documents, translate these
procedures into universal XDL code, simulate the execution of the procedure in
a hardware-specific setup, and ultimately execute the procedure on an
XDL-controlled robotic system for synthetic chemistry. This demonstrates the
potential of LLM-based workflows for autonomous chemical synthesis with
Chemputers. Due to the abstraction of XDL this approach is safe, secure, and
scalable since hallucinations will not be chemputable and the XDL can be both
verified and encrypted. Unlike previous efforts, which either addressed only a
limited portion of the workflow, relied on inflexible hard-coded rules, or
lacked validation in physical systems, our approach provides four realistic
examples of syntheses directly executed from synthetic literature. We
anticipate that our workflow will significantly enhance automation in
robotically driven synthetic chemistry research, streamline data extraction,
improve the reproducibility, scalability, and safety of synthetic and
experimental chemistry."
Adver-City: Open-Source Multi-Modal Dataset for Collaborative Perception Under Adverse Weather Conditions,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Adverse weather conditions pose a significant challenge to the widespread
adoption of Autonomous Vehicles (AVs) by impacting sensors like LiDARs and
cameras. Even though Collaborative Perception (CP) improves AV perception in
difficult conditions, existing CP datasets lack adverse weather conditions. To
address this, we introduce Adver-City, the first open-source synthetic CP
dataset focused on adverse weather conditions. Simulated in CARLA with OpenCDA,
it contains over 24 thousand frames, over 890 thousand annotations, and 110
unique scenarios across six different weather conditions: clear weather, soft
rain, heavy rain, fog, foggy heavy rain and, for the first time in a synthetic
CP dataset, glare. It has six object categories including pedestrians and
cyclists, and uses data from vehicles and roadside units featuring LiDARs, RGB
and semantic segmentation cameras, GNSS, and IMUs. Its scenarios, based on real
crash reports, depict the most relevant road configurations for adverse weather
and poor visibility conditions, varying in object density, with both dense and
sparse scenes, allowing for novel testing conditions of CP models. Benchmarks
run on the dataset show that weather conditions created challenging conditions
for perception models, reducing multi-modal object detection performance by up
to 19%, while object density affected LiDAR-based detection by up to 29%. The
dataset, code and documentation are available at
https://labs.cs.queensu.ca/quarrg/datasets/adver-city/."
Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"This paper delves into the interplay between vision backbones and optimizers,
unvealing an inter-dependent phenomenon termed
\textit{\textbf{b}ackbone-\textbf{o}ptimizer \textbf{c}oupling \textbf{b}ias}
(BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a
marked co-dependency with SGD families, while recent architectures like ViTs
and ConvNeXt share a tight coupling with the adaptive learning rate ones. We
further show that BOCB can be introduced by both optimizers and certain
backbone designs and may significantly impact the pre-training and downstream
fine-tuning of vision models. Through in-depth empirical analysis, we summarize
takeaways on recommended optimizers and insights into robust vision backbone
architectures. We hope this work can inspire the community to question
long-held assumptions on backbones and optimizers, stimulate further
explorations, and thereby contribute to more robust vision systems. The source
code and models are publicly available at https://bocb-ai.github.io/."
Cooperative and Asynchronous Transformer-based Mission Planning for Heterogeneous Teams of Mobile Robots,cs.RO,Robotics,2024-10-08,"Coordinating heterogeneous teams of mobile robots for tasks such as search
and rescue is highly challenging. This is due to the complexities of
perception, decision making and planning in such environments, with agents'
non-synchronous operation, constrained communication, and limited computational
resources. This paper presents the Cooperative and Asynchronous
Transformer-based Mission Planning (CATMiP) framework, which leverages
multi-agent reinforcement learning (MARL) to effectively coordinate agents with
heterogeneous sensing, motion, and actuation capabilities. The framework
introduces a Class-based Macro-Action Decentralized Partially Observable Markov
Decision Process (CMD-POMDP) model to handle asynchronous decision-making among
different agent classes via macro-actions. It also extends the Multi-Agent
Transformer (MAT) architecture to facilitate distributed, ad hoc communication
among the agents. CATMiP easily adapts to mission complexities and
communication constraints, and scales to varying environment sizes and team
compositions. Simulations demonstrate its scalability and ability to achieve
cooperative mission objectives with two classes of explorer and rescuer agents,
even under severe communication constraints. The code is available at
https://github.com/mylad13/CATMiP."
HumVI: A Multilingual Dataset for Detecting Violent Incidents Impacting Humanitarian Aid,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Humanitarian organizations can enhance their effectiveness by analyzing data
to discover trends, gather aggregated insights, manage their security risks,
support decision-making, and inform advocacy and funding proposals. However,
data about violent incidents with direct impact and relevance for humanitarian
aid operations is not readily available. An automatic data collection and
NLP-backed classification framework aligned with humanitarian perspectives can
help bridge this gap. In this paper, we present HumVI - a dataset comprising
news articles in three languages (English, French, Arabic) containing instances
of different types of violent incidents categorized by the humanitarian sector
they impact, e.g., aid security, education, food security, health, and
protection. Reliable labels were obtained for the dataset by partnering with a
data-backed humanitarian organization, Insecurity Insight. We provide multiple
benchmarks for the dataset, employing various deep learning architectures and
techniques, including data augmentation and mask loss, to address different
task-related challenges, e.g., domain expansion. The dataset is publicly
available at https://github.com/dataminr-ai/humvi-dataset."
Communication-Efficient Federated Group Distributionally Robust Optimization,cs.LG,Machine Learning,2024-10-08,"Federated learning faces challenges due to the heterogeneity in data volumes
and distributions at different clients, which can compromise model
generalization ability to various distributions. Existing approaches to address
this issue based on group distributionally robust optimization (GDRO) often
lead to high communication and sample complexity. To this end, this work
introduces algorithms tailored for communication-efficient Federated Group
Distributionally Robust Optimization (FGDRO). Our contributions are threefold:
Firstly, we introduce the FGDRO-CVaR algorithm, which optimizes the average
top-K losses while reducing communication complexity to $O(1/\epsilon^4)$,
where $\epsilon$ denotes the desired precision level. Secondly, our FGDRO-KL
algorithm is crafted to optimize KL regularized FGDRO, cutting communication
complexity to $O(1/\epsilon^3)$. Lastly, we propose FGDRO-KL-Adam to utilize
Adam-type local updates in FGDRO-KL, which not only maintains a communication
cost of $O(1/\epsilon^3)$ but also shows potential to surpass SGD-type local
steps in practical applications. The effectiveness of our algorithms has been
demonstrated on a variety of real-world tasks, including natural language
processing and computer vision."
Physics-Informed Regularization for Domain-Agnostic Dynamical System Modeling,cs.LG,Machine Learning,2024-10-08,"Learning complex physical dynamics purely from data is challenging due to the
intrinsic properties of systems to be satisfied. Incorporating physics-informed
priors, such as in Hamiltonian Neural Networks (HNNs), achieves high-precision
modeling for energy-conservative systems. However, real-world systems often
deviate from strict energy conservation and follow different physical priors.
To address this, we present a framework that achieves high-precision modeling
for a wide range of dynamical systems from the numerical aspect, by enforcing
Time-Reversal Symmetry (TRS) via a novel regularization term. It helps preserve
energies for conservative systems while serving as a strong inductive bias for
non-conservative, reversible systems. While TRS is a domain-specific physical
prior, we present the first theoretical proof that TRS loss can universally
improve modeling accuracy by minimizing higher-order Taylor terms in ODE
integration, which is numerically beneficial to various systems regardless of
their properties, even for irreversible systems. By integrating the TRS loss
within neural ordinary differential equation models, the proposed model TREAT
demonstrates superior performance on diverse physical systems. It achieves a
significant 11.5% MSE improvement in a challenging chaotic triple-pendulum
scenario, underscoring TREAT's broad applicability and effectiveness."
SpaLLM: Unified Compressive Adaptation of Large Language Models with Sketching,cs.LG,Machine Learning,2024-10-08,"Compressive adaptation approaches, such as QLoRA, are widely popular
alternatives for reducing memory requirements during fine-tuning of large
language models (LLMs) while producing models capable of handling various
downstream tasks. The key idea is to employ a ""two-tower"" architecture:
compressing pre-trained LLM parameters into compact representations and
fine-tuning the additive full-precision adapter, which typically has few
tunable parameters in low-rank format. However, the strict algebraic
assumptions, such as low-rank assumption, and the complexity of composing
two-tower architectures are some of the known shortcomings, resulting in a poor
accuracy-efficiency trade-off. In response to these known limitations, we
propose SpaLLM (Sketched Parameter Adaptation of LLMs), a novel compressive
adaptation approach for LLMs. This method is also the first to illustrate
parameter-sharing compression methods for LLM fine-tuning, which, unlike QLoRA,
are free from strict low-rank algebraic assumptions on adapters. Furthermore,
our proposal unifies model compression and adaptation into a single,
streamlined process, eliminating the need for two-tower architectures. SpaLLM
sketches pre-trained LLM weights into lookup tables and directly fine-tunes the
values in these tables. This approach simplifies LLMs' compressive adaptation
workflow, potentially improves multi-user serving efficiency, and delivers
significantly better accuracy for both natural language understanding and
generation tasks. Moreover, by avoiding the ""two-tower"" architecture, our
framework only requires one compressed matrix multiplication per layer during
inference, demonstrating superior inference efficiency compared to previous
methods."
Context-Aware Command Understanding for Tabletop Scenarios,cs.RO,Robotics,2024-10-08,"This paper presents a novel hybrid algorithm designed to interpret natural
human commands in tabletop scenarios. By integrating multiple sources of
information, including speech, gestures, and scene context, the system extracts
actionable instructions for a robot, identifying relevant objects and actions.
The system operates in a zero-shot fashion, without reliance on predefined
object models, enabling flexible and adaptive use in various environments. We
assess the integration of multiple deep learning models, evaluating their
suitability for deployment in real-world robotic setups. Our algorithm performs
robustly across different tasks, combining language processing with visual
grounding. In addition, we release a small dataset of video recordings used to
evaluate the system. This dataset captures real-world interactions in which a
human provides instructions in natural language to a robot, a contribution to
future research on human-robot interaction. We discuss the strengths and
limitations of the system, with particular focus on how it handles multimodal
command interpretation, and its ability to be integrated into symbolic robotic
frameworks for safe and explainable decision-making."
Language-Assisted Human Part Motion Learning for Skeleton-Based Temporal Action Segmentation,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Skeleton-based Temporal Action Segmentation involves the dense action
classification of variable-length skeleton sequences. Current approaches
primarily apply graph-based networks to extract framewise, whole-body-level
motion representations, and use one-hot encoded labels for model optimization.
However, whole-body motion representations do not capture fine-grained
part-level motion representations and the one-hot encoded labels neglect the
intrinsic semantic relationships within the language-based action definitions.
To address these limitations, we propose a novel method named Language-assisted
Human Part Motion Representation Learning (LPL), which contains a Disentangled
Part Motion Encoder (DPE) to extract dual-level (i.e., part and whole-body)
motion representations and a Language-assisted Distribution Alignment (LDA)
strategy for optimizing spatial relations within representations. Specifically,
after part-aware skeleton encoding via DPE, LDA generates dual-level action
descriptions to construct a textual embedding space with the help of a
large-scale language model. Then, LDA motivates the alignment of the embedding
space between text descriptions and motions. This alignment allows LDA not only
to enhance intra-class compactness but also to transfer the language-encoded
semantic correlations among actions to skeleton-based motion learning.
Moreover, we propose a simple yet efficient Semantic Offset Adapter to smooth
the cross-domain misalignment. Our experiments indicate that LPL achieves
state-of-the-art performance across various datasets (e.g., +4.4\% Accuracy,
+5.6\% F1 on the PKU-MMD dataset). Moreover, LDA is compatible with existing
methods and improves their performance (e.g., +4.8\% Accuracy, +4.3\% F1 on the
LARa dataset) without additional inference costs."
Tree-Based Leakage Inspection and Control in Concept Bottleneck Models,cs.LG,Machine Learning,2024-10-08,"As AI models grow larger, the demand for accountability and interpretability
has become increasingly critical for understanding their decision-making
processes. Concept Bottleneck Models (CBMs) have gained attention for enhancing
interpretability by mapping inputs to intermediate concepts before making final
predictions. However, CBMs often suffer from information leakage, where
additional input data, not captured by the concepts, is used to improve task
performance, complicating the interpretation of downstream predictions. In this
paper, we introduce a novel approach for training both joint and sequential
CBMs that allows us to identify and control leakage using decision trees. Our
method quantifies leakage by comparing the decision paths of hard CBMs with
their soft, leaky counterparts. Specifically, we show that soft leaky CBMs
extend the decision paths of hard CBMs, particularly in cases where concept
information is incomplete. Using this insight, we develop a technique to better
inspect and manage leakage, isolating the subsets of data most affected by
this. Through synthetic and real-world experiments, we demonstrate that
controlling leakage in this way not only improves task accuracy but also yields
more informative and transparent explanations."
Robust Domain Generalisation with Causal Invariant Bayesian Neural Networks,cs.LG,Machine Learning,2024-10-08,"Deep neural networks can obtain impressive performance on various tasks under
the assumption that their training domain is identical to their target domain.
Performance can drop dramatically when this assumption does not hold. One
explanation for this discrepancy is the presence of spurious domain-specific
correlations in the training data that the network exploits. Causal mechanisms,
in the other hand, can be made invariant under distribution changes as they
allow disentangling the factors of distribution underlying the data generation.
Yet, learning causal mechanisms to improve out-of-distribution generalisation
remains an under-explored area. We propose a Bayesian neural architecture that
disentangles the learning of the the data distribution from the inference
process mechanisms. We show theoretically and experimentally that our model
approximates reasoning under causal interventions. We demonstrate the
performance of our method, outperforming point estimate-counterparts, on
out-of-distribution image recognition tasks where the data distribution acts as
strong adversarial confounders."
Harnessing the Power of Noise: A Survey of Techniques and Applications,cs.LG,Machine Learning,2024-10-08,"Noise, traditionally considered a nuisance in computational systems, is
reconsidered for its unexpected and counter-intuitive benefits across a wide
spectrum of domains, including nonlinear information processing, signal
processing, image processing, machine learning, network science, and natural
language processing. Through a comprehensive review of both historical and
contemporary research, this survey presents a dual perspective on noise,
acknowledging its potential to both disrupt and enhance performance.
Particularly, we highlight how noise-enhanced training strategies can lead to
models that better generalize from noisy data, positioning noise not just as a
challenge to overcome but as a strategic tool for improvement. This work calls
for a shift in how we perceive noise, proposing that it can be a spark for
innovation and advancement in the information era."
Solving Multi-Goal Robotic Tasks with Decision Transformer,cs.RO,Robotics,2024-10-08,"Artificial intelligence plays a crucial role in robotics, with reinforcement
learning (RL) emerging as one of the most promising approaches for robot
control. However, several key challenges hinder its broader application. First,
many RL methods rely on online learning, which requires either real-world
hardware or advanced simulation environments--both of which can be costly,
time-consuming, and impractical. Offline reinforcement learning offers a
solution, enabling models to be trained without ongoing access to physical
robots or simulations.
  A second challenge is learning multi-goal tasks, where robots must achieve
multiple objectives simultaneously. This adds complexity to the training
process, as the model must generalize across different goals. At the same time,
transformer architectures have gained significant popularity across various
domains, including reinforcement learning. Yet, no existing methods effectively
combine offline training, multi-goal learning, and transformer-based
architectures.
  In this paper, we address these challenges by introducing a novel adaptation
of the decision transformer architecture for offline multi-goal reinforcement
learning in robotics. Our approach integrates goal-specific information into
the decision transformer, allowing it to handle complex tasks in an offline
setting. To validate our method, we developed a new offline reinforcement
learning dataset using the Panda robotic platform in simulation. Our extensive
experiments demonstrate that the decision transformer can outperform
state-of-the-art online reinforcement learning methods."
FedGraph: A Research Library and Benchmark for Federated Graph Learning,cs.LG,Machine Learning,2024-10-08,"Federated graph learning is an emerging field with significant practical
challenges. While many algorithms have been proposed to enhance model accuracy,
their system performance, crucial for real-world deployment, is often
overlooked. To address this gap, we present FedGraph, a research library
designed for practical distributed deployment and benchmarking in federated
graph learning. FedGraph supports a range of state-of-the-art methods and
includes profiling tools for system performance evaluation, focusing on
communication and computation costs during training. FedGraph can then
facilitate the development of practical applications and guide the design of
future algorithms."
Filtered Randomized Smoothing: A New Defense for Robust Modulation Classification,cs.LG,Machine Learning,2024-10-08,"Deep Neural Network (DNN) based classifiers have recently been used for the
modulation classification of RF signals. These classifiers have shown
impressive performance gains relative to conventional methods, however, they
are vulnerable to imperceptible (low-power) adversarial attacks. Some of the
prominent defense approaches include adversarial training (AT) and randomized
smoothing (RS). While AT increases robustness in general, it fails to provide
resilience against previously unseen adaptive attacks. Other approaches, such
as Randomized Smoothing (RS), which injects noise into the input, address this
shortcoming by providing provable certified guarantees against arbitrary
attacks, however, they tend to sacrifice accuracy.
  In this paper, we study the problem of designing robust DNN-based modulation
classifiers that can provide provable defense against arbitrary attacks without
significantly sacrificing accuracy. To this end, we first analyze the spectral
content of commonly studied attacks on modulation classifiers for the benchmark
RadioML dataset. We observe that spectral signatures of un-perturbed RF signals
are highly localized, whereas attack signals tend to be spread out in
frequency. To exploit this spectral heterogeneity, we propose Filtered
Randomized Smoothing (FRS), a novel defense which combines spectral filtering
together with randomized smoothing. FRS can be viewed as a strengthening of RS
by leveraging the specificity (spectral Heterogeneity) inherent to the
modulation classification problem. In addition to providing an approach to
compute the certified accuracy of FRS, we also provide a comprehensive set of
simulations on the RadioML dataset to show the effectiveness of FRS and show
that it significantly outperforms existing defenses including AT and RS in
terms of accuracy on both attacked and benign signals."
Are Large Language Models State-of-the-art Quality Estimators for Machine Translation of User-generated Content?,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"This paper investigates whether large language models (LLMs) are
state-of-the-art quality estimators for machine translation of user-generated
content (UGC) that contains emotional expressions, without the use of reference
translations. To achieve this, we employ an existing emotion-related dataset
with human-annotated errors and calculate quality evaluation scores based on
the Multi-dimensional Quality Metrics. We compare the accuracy of several LLMs
with that of our fine-tuned baseline models, under in-context learning and
parameter-efficient fine-tuning (PEFT) scenarios. We find that PEFT of LLMs
leads to better performance in score prediction with human interpretable
explanations than fine-tuned models. However, a manual analysis of LLM outputs
reveals that they still have problems such as refusal to reply to a prompt and
unstable output while evaluating machine translation of UGC."
Batched Bayesian optimization with correlated candidate uncertainties,cs.LG,Machine Learning,2024-10-08,"Batched Bayesian optimization (BO) can accelerate molecular design by
efficiently identifying top-performing compounds from a large chemical library.
Existing acquisition strategies for batch design in BO aim to balance
exploration and exploitation. This often involves optimizing non-additive batch
acquisition functions, necessitating approximation via myopic construction
and/or diversity heuristics. In this work, we propose an acquisition strategy
for discrete optimization that is motivated by pure exploitation, qPO
(multipoint Probability of Optimality). qPO maximizes the probability that the
batch includes the true optimum, which is expressible as the sum over
individual acquisition scores and thereby circumvents the combinatorial
challenge of optimizing a batch acquisition function. We differentiate the
proposed strategy from parallel Thompson sampling and discuss how it implicitly
captures diversity. Finally, we apply our method to the model-guided
exploration of large chemical libraries and provide empirical evidence that it
performs better than or on par with state-of-the-art methods in batched
Bayesian optimization."
Boolean Nearest Neighbor Language in the Knowledge Compilation Map,cs.AI,Artificial Intelligence,2024-10-08,"The Boolean Nearest Neighbor (BNN) representation of Boolean functions was
recently introduced by Hajnal, Liu and Turan. A BNN representation of $f$ is a
pair $(P,N)$ of sets of Boolean vectors (called positive and negative
prototypes) where $f(x)=1$ for every positive prototype $x \in P$, $f(x)=0$ for
all every negative prototype $x \in N$, and the value $f(x)$ for $x \not\in P
\cup N$ is determined by the type of the closest prototype. The main aim of
this paper is to determine the position of the BNN language in the Knowledge
Compilation Map (KCM). To this end, we derive results which compare the
succinctness of the BNN language to several standard languages from KCM, and
determine the complexity status of most standard queries and transformations
for BNN inputs."
Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"The locate-then-edit paradigm has shown significant promise for knowledge
editing (KE) in Large Language Models (LLMs). While previous methods perform
well on single-hop fact recall tasks, they consistently struggle with multi-hop
factual recall tasks involving newly edited knowledge. In this paper,
leveraging tools in mechanistic interpretability, we first identify that in
multi-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper
MLP layers, unlike single-hop tasks, which rely on earlier layers. This
distinction explains the poor performance of current methods in multi-hop
queries, as they primarily focus on editing shallow layers, leaving deeper
layers unchanged. To address this, we propose IFMET, a novel locate-then-edit
KE approach designed to edit both shallow and deep MLP layers. IFMET employs
multi-hop editing prompts and supplementary sets to locate and modify knowledge
across different reasoning stages. Experimental results demonstrate that IFMET
significantly improves performance on multi-hop factual recall tasks,
effectively overcoming the limitations of previous locate-then-edit methods."
Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Recent advancements in prompt engineering strategies, such as
Chain-of-Thought (CoT) and Self-Discover, have demonstrated significant
potential in improving the reasoning abilities of Large Language Models (LLMs).
However, these state-of-the-art (SOTA) prompting strategies rely on single or
fixed set of static seed reasoning modules like \emph{""think step by step""} or
\emph{""break down this problem""} intended to simulate human approach to
problem-solving. This constraint limits the flexibility of models in tackling
diverse problems effectively. In this paper, we introduce Auto-Evolve, a novel
framework that enables LLMs to self-create dynamic reasoning modules and
downstream action plan, resulting in significant improvements over current SOTA
methods. We evaluate Auto-Evolve on the challenging BigBench-Hard (BBH) dataset
with Claude 2.0, Claude 3 Sonnet, Mistral Large, and GPT 4, where it
consistently outperforms the SOTA prompt strategies. Auto-Evolve outperforms
CoT by up to 10.4\% and on an average by 7\% across these four models. Our
framework introduces two innovations: a) Auto-Evolve dynamically generates
reasoning modules for each task while aligning with human reasoning paradigm,
thus eliminating the need for predefined templates. b) We introduce an
iterative refinement component, that incrementally refines instruction guidance
for LLMs and helps boost performance by average 2.8\% compared to doing it in a
single step."
Meta-Learning Augmented MPC for Disturbance-Aware Motion Planning and Control of Quadrotors,cs.RO,Robotics,2024-10-08,"A major challenge in autonomous flights is unknown disturbances, which can
jeopardize safety and lead to collisions, especially in obstacle-rich
environments. This paper presents a disturbance-aware motion planning and
control framework designed for autonomous aerial flights. The framework is
composed of two key components: a disturbance-aware motion planner and a
tracking controller. The disturbance-aware motion planner consists of a
predictive control scheme and a learned model of disturbances that is adapted
online. The tracking controller is designed using contraction control methods
to provide safety bounds on the quadrotor behaviour in the vicinity of the
obstacles with respect to the disturbance-aware motion plan. Finally, the
algorithm is tested in simulation scenarios with a quadrotor facing strong
crosswind and ground-induced disturbances."
Differentiation Through Black-Box Quadratic Programming Solvers,cs.LG,Machine Learning,2024-10-08,"In recent years, many deep learning approaches have incorporated layers that
solve optimization problems (e.g., linear, quadratic, and semidefinite
programs). Integrating these optimization problems as differentiable layers
requires computing the derivatives of the optimization problem's solution with
respect to its objective and constraints. This has so far prevented the use of
state-of-the-art black-box numerical solvers within neural networks, as they
lack a differentiable interface. To address this issue for one of the most
common convex optimization problems -- quadratic programming (QP) -- we
introduce dQP, a modular framework that enables plug-and-play differentiation
for any QP solver, allowing seamless integration into neural networks and
bi-level optimization tasks. Our solution is based on the core theoretical
insight that knowledge of the active constraint set at the QP optimum allows
for explicit differentiation. This insight reveals a unique relationship
between the computation of the solution and its derivative, enabling efficient
differentiation of any solver, that only requires the primal solution. Our
implementation, which will be made publicly available, interfaces with an
existing framework that supports over 15 state-of-the-art QP solvers, providing
each with a fully differentiable backbone for immediate use as a differentiable
layer in learning setups. To demonstrate the scalability and effectiveness of
dQP, we evaluate it on a large benchmark dataset of QPs with varying
structures. We compare dQP with existing differentiable QP methods,
demonstrating its advantages across a range of problems, from challenging small
and dense problems to large-scale sparse ones, including a novel bi-level
geometry optimization problem."
Learning in complex action spaces without policy gradients,cs.LG,Machine Learning,2024-10-08,"Conventional wisdom suggests that policy gradient methods are better suited
to complex action spaces than action-value methods. However, foundational
studies have shown equivalences between these paradigms in small and finite
action spaces (O'Donoghue et al., 2017; Schulman et al., 2017a). This raises
the question of why their computational applicability and performance diverge
as the complexity of the action space increases. We hypothesize that the
apparent superiority of policy gradients in such settings stems not from
intrinsic qualities of the paradigm, but from universal principles that can
also be applied to action-value methods to serve similar functionality. We
identify three such principles and provide a framework for incorporating them
into action-value methods. To support our hypothesis, we instantiate this
framework in what we term QMLE, for Q-learning with maximum likelihood
estimation. Our results show that QMLE can be applied to complex action spaces
with a controllable computational cost that is comparable to that of policy
gradient methods, all without using policy gradients. Furthermore, QMLE
demonstrates strong performance on the DeepMind Control Suite, even when
compared to the state-of-the-art methods such as DMPO and D4PG."
Incremental Learning for Robot Shared Autonomy,cs.RO,Robotics,2024-10-08,"Shared autonomy holds promise for improving the usability and accessibility
of assistive robotic arms, but current methods often rely on costly expert
demonstrations and lack the ability to adapt post-deployment. This paper
introduces ILSA, an Incrementally Learned Shared Autonomy framework that
continually improves its assistive control policy through repeated user
interactions. ILSA leverages synthetic kinematic trajectories for initial
pretraining, reducing the need for expert demonstrations, and then
incrementally finetunes its policy after each manipulation interaction, with
mechanisms to balance new knowledge acquisition with existing knowledge
retention during incremental learning. We validate ILSA for complex
long-horizon tasks through a comprehensive ablation study and a user study with
20 participants, demonstrating its effectiveness and robustness in both
quantitative performance and user-reported qualitative metrics. Code and videos
are available at https://ilsa-robo.github.io/."
Temporal Image Caption Retrieval Competition -- Description and Results,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Multimodal models, which combine visual and textual information, have
recently gained significant recognition. This paper addresses the multimodal
challenge of Text-Image retrieval and introduces a novel task that extends the
modalities to include temporal data. The Temporal Image Caption Retrieval
Competition (TICRC) presented in this paper is based on the Chronicling America
and Challenging America projects, which offer access to an extensive collection
of digitized historic American newspapers spanning 274 years. In addition to
the competition results, we provide an analysis of the delivered dataset and
the process of its creation."
Benchmarking of a new data splitting method on volcanic eruption data,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"In this paper, a novel method for data splitting is presented: an iterative
procedure divides the input dataset of volcanic eruption, chosen as the
proposed use case, into two parts using a dissimilarity index calculated on the
cumulative histograms of these two parts. The Cumulative Histogram
Dissimilarity (CHD) index is introduced as part of the design. Based on the
obtained results the proposed model in this case, compared to both Random
splitting and K-means implemented over different configurations, achieves the
best performance, with a slightly higher number of epochs. However, this
demonstrates that the model can learn more deeply from the input dataset, which
is attributable to the quality of the splitting. In fact, each model was
trained with early stopping, suitable in case of overfitting, and the higher
number of epochs in the proposed method demonstrates that early stopping did
not detect overfitting, and consequently, the learning was optimal."
Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Hallucinations in large language models (LLMs) pose significant challenges in
tasks requiring complex multi-step reasoning, such as mathematical
problem-solving. Existing approaches primarily detect the presence of
hallucinations but lack a nuanced understanding of their types and
manifestations. In this paper, we first introduce a comprehensive taxonomy that
categorizes the common hallucinations in mathematical reasoning task into six
types: fabrication, factual inconsistency, context inconsistency, instruction
inconsistency, logical inconsistency, and logical error. We then propose FG-PRM
(Fine-Grained Process Reward Model), an augmented model designed to detect and
mitigate hallucinations in a fine-grained, step-level manner. To address the
limitations of manually labeling training data, we propose an automated method
for generating fine-grained hallucination data using LLMs. By injecting
hallucinations into reasoning steps of correct solutions, we create a diverse
and balanced synthetic dataset for training FG-PRM, which consists of six
specialized Process Reward Models (PRMs), each tailored to detect a specific
hallucination type. Our FG-PRM demonstrates superior performance across two key
tasks: 1) Fine-grained hallucination detection: classifying hallucination types
for each reasoning step; and 2) Verification: ranking multiple LLM-generated
outputs to select the most accurate solution, mitigating reasoning
hallucinations. Our experiments show that FG-PRM outperforms ChatGPT-3.5 and
Claude-3 on fine-grained hallucination detection and substantially boosts the
performance of LLMs on GSM8K and MATH benchmarks."
Compositional Risk Minimization,cs.LG,Machine Learning,2024-10-08,"In this work, we tackle a challenging and extreme form of subpopulation
shift, which is termed compositional shift. Under compositional shifts, some
combinations of attributes are totally absent from the training distribution
but present in the test distribution. We model the data with flexible additive
energy distributions, where each energy term represents an attribute, and
derive a simple alternative to empirical risk minimization termed compositional
risk minimization (CRM). We first train an additive energy classifier to
predict the multiple attributes and then adjust this classifier to tackle
compositional shifts. We provide an extensive theoretical analysis of CRM,
where we show that our proposal extrapolates to special affine hulls of seen
attribute combinations. Empirical evaluations on benchmark datasets confirms
the improved robustness of CRM compared to other methods from the literature
designed to tackle various forms of subpopulation shifts."
Amortized SHAP values via sparse Fourier function approximation,cs.LG,Machine Learning,2024-10-08,"SHAP values are a popular local feature-attribution method widely used in
interpretable and explainable AI. We tackle the problem of efficiently
computing these values. We cover both the model-agnostic (black-box) setting,
where one only has query access to the model and also the case of (ensembles
of) trees where one has access to the structure of the tree. For both the
black-box and the tree setting we propose a two-stage approach for estimating
SHAP values. Our algorithm's first step harnesses recent results showing that
many real-world predictors have a spectral bias that allows us to either
exactly represent (in the case of ensembles of decision trees), or efficiently
approximate them (in the case of neural networks) using a compact Fourier
representation. In the second step of the algorithm, we use the Fourier
representation to exactly compute SHAP values. The second step is
computationally very cheap because firstly, the representation is compact and
secondly, we prove that there exists a closed-form expression for SHAP values
for the Fourier basis functions. Furthermore, the expression we derive
effectively linearizes the computation into a simple summation and is amenable
to parallelization on multiple cores or a GPU. Since the function approximation
(first step) is only done once, it allows us to produce Shapley values in an
amortized way. We show speedups compared to relevant baseline methods equal
levels of accuracy for both the tree and black-box settings. Moreover, this
approach introduces a reliable and fine-grained continuous trade-off between
computation and accuracy through the sparsity of the Fourier approximation, a
feature previously unavailable in all black-box methods."
A Taxonomy of Collectible Card Games from a Game-Playing AI Perspective,cs.AI,Artificial Intelligence,2024-10-08,"Collectible card games are challenging, widely played games that have
received increasing attention from the AI research community in recent years.
Despite important breakthroughs, the field still poses many unresolved
challenges. This work aims to help further research on the genre by proposing a
taxonomy of collectible card games by analyzing their rules, mechanics, and
game modes from the perspective of game-playing AI research. To achieve this,
we studied a set of popular games and provided a thorough discussion about
their characteristics."
Conformal Structured Prediction,cs.LG,Machine Learning,2024-10-08,"Conformal prediction has recently emerged as a promising strategy for
quantifying the uncertainty of a predictive model; these algorithms modify the
model to output sets of labels that are guaranteed to contain the true label
with high probability. However, existing conformal prediction algorithms have
largely targeted classification and regression settings, where the structure of
the prediction set has a simple form as a level set of the scoring function.
However, for complex structured outputs such as text generation, these
prediction sets might include a large number of labels and therefore be hard
for users to interpret. In this paper, we propose a general framework for
conformal prediction in the structured prediction setting, that modifies
existing conformal prediction algorithms to output structured prediction sets
that implicitly represent sets of labels. In addition, we demonstrate how our
approach can be applied in domains where the prediction sets can be represented
as a set of nodes in a directed acyclic graph; for instance, for hierarchical
labels such as image classification, a prediction set might be a small subset
of coarse labels implicitly representing the prediction set of all their more
fine-descendants. We demonstrate how our algorithm can be used to construct
prediction sets that satisfy a desired coverage guarantee in several domains."
A General Formulation for Path Constrained Time-Optimized Trajectory Planning with Environmental and Object Contacts,cs.RO,Robotics,2024-10-08,"A typical manipulation task consists of a manipulator equipped with a gripper
to grasp and move an object with constraints on the motion of the hand-held
object, which may be due to the nature of the task itself or from
object-environment contacts. In this paper, we study the problem of computing
joint torques and grasping forces for time-optimal motion of an object, while
ensuring that the grasp is not lost and any constraints on the motion of the
object, either due to dynamics, environment contact, or no-slip requirements,
are also satisfied. We present a second-order cone program (SOCP) formulation
of the time-optimal trajectory planning problem that considers nonlinear
friction cone constraints at the hand-object and object-environment contacts.
Since SOCPs are convex optimization problems that can be solved optimally in
polynomial time using interior point methods, we can solve the trajectory
optimization problem efficiently. We present simulation results on three
examples, including a non-prehensile manipulation task, which shows the
generality and effectiveness of our approach."
Accelerated Preference Optimization for Large Language Model Alignment,cs.LG,Machine Learning,2024-10-08,"Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal
tool for aligning large language models (LLMs) with human preferences. Direct
Preference Optimization (DPO), one of the most popular approaches, formulates
RLHF as a policy optimization problem without explicitly estimating the reward
function. It overcomes the stability and efficiency issues of two-step
approaches, which typically involve first estimating the reward function and
then optimizing the policy via proximal policy optimization (PPO). Since RLHF
is essentially an optimization problem, and it is well-known that momentum
techniques can accelerate optimization both theoretically and empirically, a
natural question arises: Can RLHF be accelerated by momentum? This paper
answers this question in the affirmative. In detail, we first show that the
iterative preference optimization method can be viewed as a proximal point
method. Based on this observation, we propose a general Accelerated Preference
Optimization (APO) framework, which unifies many existing preference
optimization algorithms and employs Nesterov's momentum technique to speed up
the alignment of LLMs. Theoretically, we demonstrate that APO can achieve a
faster convergence rate than the standard iterative preference optimization
methods, including DPO and Self-Play Preference Optimization (SPPO).
Empirically, we show the superiority of APO over DPO, iterative DPO, and other
strong baselines for RLHF on the AlpacaEval 2.0 benchmark."
Non-Halting Queries: Exploiting Fixed Points in LLMs,cs.LG,Machine Learning,2024-10-08,"We introduce a new vulnerability that exploits fixed points in autoregressive
models and use it to craft queries that never halt, i.e. an LLM output that
does not terminate. More precisely, for what we call non-halting queries, the
LLM never samples the end-of-string token (<eos>). We rigorously analyze the
conditions under which the non-halting anomaly presents itself. In particular,
at temperature zero, we prove that if a repeating (cyclic) sequence of tokens
is observed at the output beyond the context size, then the LLM does not halt.
  We demonstrate the non-halting anomaly in a number of experiments performed
in base (unaligned) models where repeating tokens immediately lead to a
non-halting cyclic behavior as predicted by the analysis. Further, we develop a
simple recipe that takes the same fixed points observed in the base model and
creates a prompt structure to target aligned models. We study the recipe
behavior in bypassing alignment in a number of LLMs including GPT-4o,
llama-3-8b-instruct, and gemma-2-9b-it where all models are forced into a
non-halting state. Further, we demonstrate the recipe's success in sending most
major models released over the past year into a non-halting state with the same
simple prompt even at higher temperatures. Further, we study direct inversion
based techniques to craft new short prompts to induce the non-halting state.
Our experiments with the gradient search based inversion technique ARCA show
that non-halting is prevalent across models and may be easily induced with a
few input tokens.
  While its impact on the reliability of hosted systems can be mitigated by
configuring a hard maximum token limit in the sampler, the non-halting anomaly
still manages to break alignment. This underlines the need for further studies
and stronger forms of alignment against non-halting anomalies."
Monocular Visual Place Recognition in LiDAR Maps via Cross-Modal State Space Model and Multi-View Matching,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Achieving monocular camera localization within pre-built LiDAR maps can
bypass the simultaneous mapping process of visual SLAM systems, potentially
reducing the computational overhead of autonomous localization. To this end,
one of the key challenges is cross-modal place recognition, which involves
retrieving 3D scenes (point clouds) from a LiDAR map according to online RGB
images. In this paper, we introduce an efficient framework to learn descriptors
for both RGB images and point clouds. It takes visual state space model
(VMamba) as the backbone and employs a pixel-view-scene joint training strategy
for cross-modal contrastive learning. To address the field-of-view differences,
independent descriptors are generated from multiple evenly distributed
viewpoints for point clouds. A visible 3D points overlap strategy is then
designed to quantify the similarity between point cloud views and RGB images
for multi-view supervision. Additionally, when generating descriptors from
pixel-level features using NetVLAD, we compensate for the loss of geometric
information, and introduce an efficient scheme for multi-view generation.
Experimental results on the KITTI and KITTI-360 datasets demonstrate the
effectiveness and generalization of our method. The code will be released upon
acceptance."
Is Pontryagin's Maximum Principle all you need? Solving optimal control problems with PMP-inspired neural networks,cs.LG,Machine Learning,2024-10-08,"Calculus of Variations is the mathematics of functional optimization, i.e.,
when the solutions are functions over a time interval. This is particularly
important when the time interval is unknown like in minimum-time control
problems, so that forward in time solutions are not possible. Calculus of
Variations offers a robust framework for learning optimal control and
inference. How can this framework be leveraged to design neural networks to
solve challenges in control and inference? We propose the Pontryagin's Maximum
Principle Neural Network (PMP-net) that is tailored to estimate control and
inference solutions, in accordance with the necessary conditions outlined by
Pontryagin's Maximum Principle. We assess PMP-net on two classic optimal
control and inference problems: optimal linear filtering and minimum-time
control. Our findings indicate that PMP-net can be effectively trained in an
unsupervised manner to solve these problems without the need for ground-truth
data, successfully deriving the classical ""Kalman filter"" and ""bang-bang""
control solution. This establishes a new approach for addressing general,
possibly yet unsolved, optimal control problems."
PREDICT: Preference Reasoning by Evaluating Decomposed preferences Inferred from Candidate Trajectories,cs.AI,Artificial Intelligence,2024-10-08,"Accommodating human preferences is essential for creating AI agents that
deliver personalized and effective interactions. Recent work has shown the
potential for LLMs to infer preferences from user interactions, but they often
produce broad and generic preferences, failing to capture the unique and
individualized nature of human preferences. This paper introduces PREDICT, a
method designed to enhance the precision and adaptability of inferring
preferences. PREDICT incorporates three key elements: (1) iterative refinement
of inferred preferences, (2) decomposition of preferences into constituent
components, and (3) validation of preferences across multiple trajectories. We
evaluate PREDICT on two distinct environments: a gridworld setting and a new
text-domain environment (PLUME). PREDICT more accurately infers nuanced human
preferences improving over existing baselines by 66.2\% (gridworld environment)
and 41.0\% (PLUME)."
The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"While LLMs have emerged as performant architectures for reasoning tasks,
their compositional generalization capabilities have been questioned. In this
work, we introduce a Compositional Generalization Challenge for Graph-based
Commonsense Reasoning (CGGC) that goes beyond previous evaluations that are
based on sequences or tree structures - and instead involves a reasoning graph:
It requires models to generate a natural sentence based on given concepts and a
corresponding reasoning graph, where the presented graph involves a previously
unseen combination of relation types. To master this challenge, models need to
learn how to reason over relation tupels within the graph, and how to compose
them when conceptualizing a verbalization. We evaluate seven well-known LLMs
using in-context learning and find that performant LLMs still struggle in
compositional generalization. We investigate potential causes of this gap by
analyzing the structures of reasoning graphs, and find that different
structures present varying levels of difficulty for compositional
generalization. Arranging the order of demonstrations according to the
structures' difficulty shows that organizing samples in an easy-to-hard schema
enhances the compositional generalization ability of LLMs."
Probing the Robustness of Theory of Mind in Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"With the success of ChatGPT and other similarly sized SotA LLMs, claims of
emergent human like social reasoning capabilities, especially Theory of Mind
(ToM), in these models have appeared in the scientific literature. On the one
hand those ToM-capabilities have been successfully tested using tasks styled
similar to those used in psychology (Kosinski, 2023). On the other hand, follow
up studies showed that those capabilities vanished when the tasks were slightly
altered (Ullman, 2023). In this work we introduce a novel dataset of 68 tasks
for probing ToM in LLMs, including potentially challenging variations which are
assigned to 10 complexity classes. This way it is providing novel insights into
the challenges LLMs face with those task variations. We evaluate the ToM
performance of four SotA open source LLMs on our dataset and the dataset
introduced by (Kosinski, 2023). The overall low goal accuracy across all
evaluated models indicates only a limited degree of ToM capabilities. The LLMs'
performance on simple complexity class tasks from both datasets are similar.
Whereas we find a consistent tendency in all tested LLMs to perform poorly on
tasks that require the realization that an agent has knowledge of automatic
state changes in its environment, even when those are spelled out to the model.
For task complications that change the relationship between objects by
replacing prepositions, we notice a performance drop in all models, with the
strongest impact on the mixture-of-experts model. With our dataset of tasks
grouped by complexity we offer directions for further research on how to
stabilize and advance ToM capabilities in LLM."
MC-MoE: Mixture Compressor for Mixture-of-Experts LLMs Gains More,cs.LG,Machine Learning,2024-10-08,"Mixture-of-Experts large language models (MoE-LLMs) marks a significant step
forward of language models, however, they encounter two critical challenges in
practice: 1) expert parameters lead to considerable memory consumption and
loading latency; and 2) the current activated experts are redundant, as many
tokens may only require a single expert. Motivated by these issues, we
investigate the MoE-LLMs and make two key observations: a) different experts
exhibit varying behaviors on activation reconstruction error, routing scores,
and activated frequencies, highlighting their differing importance, and b) not
all tokens are equally important -- only a small subset is critical. Building
on these insights, we propose MC-MoE, a training-free Mixture-Compressor for
MoE-LLMs, which leverages the significance of both experts and tokens to
achieve an extreme compression. First, to mitigate storage and loading
overheads, we introduce Pre-Loading Mixed-Precision Quantization, which
formulates the adaptive bit-width allocation as a Linear Programming problem,
where the objective function balances multi-factors reflecting the importance
of each expert. Additionally, we develop Online Dynamic Pruning, which
identifies important tokens to retain and dynamically select activated experts
for other tokens during inference to optimize efficiency while maintaining
performance. Our MC-MoE integrates static quantization and dynamic pruning to
collaboratively achieve extreme compression for MoE-LLMs with less accuracy
loss, ensuring an optimal trade-off between performance and efficiency.
Extensive experiments confirm the effectiveness of our approach. For instance,
at 2.54 bits, MC-MoE compresses 76.6% of the model, with only a 3.8% average
accuracy loss. During dynamic inference, we further reduce activated parameters
by 15%, with a performance drop of less than 0.6%."
Near Exact Privacy Amplification for Matrix Mechanisms,cs.CR,Cryptography and Security,2024-10-08,"We study the problem of computing the privacy parameters for DP machine
learning when using privacy amplification via random batching and noise
correlated across rounds via a correlation matrix $\textbf{C}$ (i.e., the
matrix mechanism). Past work on this problem either only applied to banded
$\textbf{C}$, or gave loose privacy parameters. In this work, we give a
framework for computing near-exact privacy parameters for any lower-triangular,
non-negative $\textbf{C}$. Our framework allows us to optimize the correlation
matrix $\textbf{C}$ while accounting for amplification, whereas past work could
not. Empirically, we show this lets us achieve smaller RMSE on prefix sums than
the previous state-of-the-art (SOTA). We also show that we can improve on the
SOTA performance on deep learning tasks. Our two main technical tools are (i)
using Monte Carlo accounting to bypass composition, which was the main
technical challenge for past work, and (ii) a ""balls-in-bins"" batching scheme
that enables easy privacy analysis and is closer to practical random batching
than Poisson sampling."
SHADE: Deep Density-based Clustering,cs.LG,Machine Learning,2024-10-08,"Detecting arbitrarily shaped clusters in high-dimensional noisy data is
challenging for current clustering methods. We introduce SHADE
(Structure-preserving High-dimensional Analysis with Density-based
Exploration), the first deep clustering algorithm that incorporates
density-connectivity into its loss function. Similar to existing deep
clustering algorithms, SHADE supports high-dimensional and large data sets with
the expressive power of a deep autoencoder. In contrast to most existing deep
clustering methods that rely on a centroid-based clustering objective, SHADE
incorporates a novel loss function that captures density-connectivity. SHADE
thereby learns a representation that enhances the separation of
density-connected clusters. SHADE detects a stable clustering and noise points
fully automatically without any user input. It outperforms existing methods in
clustering quality, especially on data that contain non-Gaussian clusters, such
as video data. Moreover, the embedded space of SHADE is suitable for
visualization and interpretation of the clustering results as the individual
shapes of the clusters are preserved."
Think While You Generate: Discrete Diffusion with Planned Denoising,cs.LG,Machine Learning,2024-10-08,"Discrete diffusion has achieved state-of-the-art performance, outperforming
or approaching autoregressive models on standard benchmarks. In this work, we
introduce Discrete Diffusion with Planned Denoising (DDPD), a novel framework
that separates the generation process into two models: a planner and a
denoiser. At inference time, the planner selects which positions to denoise
next by identifying the most corrupted positions in need of denoising,
including both initially corrupted and those requiring additional refinement.
This plan-and-denoise approach enables more efficient reconstruction during
generation by iteratively identifying and denoising corruptions in the optimal
order. DDPD outperforms traditional denoiser-only mask diffusion methods,
achieving superior results on language modeling benchmarks such as text8,
OpenWebText, and token-based generation on ImageNet $256 \times 256$. Notably,
in language modeling, DDPD significantly reduces the performance gap between
diffusion-based and autoregressive methods in terms of generative perplexity.
Code is available at https://github.com/liusulin/DDPD."
BoxMap: Efficient Structural Mapping and Navigation,cs.RO,Robotics,2024-10-08,"While humans can successfully navigate using abstractions, ignoring details
that are irrelevant to the task at hand, most existing robotic applications
require the maintenance of a detailed environment representation which consumes
a significant amount of sensing, computing, and storage. These issues are
particularly important in a resource-constrained setting with limited power
budget. Deep learning methods can learn from prior experience to abstract
knowledge of unknown environments, and use it to execute tasks (e.g., frontier
exploration, object search, or scene understanding) more efficiently. We
propose BoxMap, a Detection-Transformer-based architecture that takes advantage
of the structure of the sensed partial environment to update a topological
graph of the environment as a set of semantic entities (e.g. rooms and doors)
and their relations (e.g. connectivity). These predictions from low-level
measurements can then be leveraged to achieve high-level goals with lower
computational costs than methods based on detailed representations. As an
example application, we consider a robot equipped with a 2-D laser scanner
tasked with exploring a residential building. Our BoxMap representation scales
quadratically with the number of rooms (with a small constant), resulting in
significant savings over a full geometric map. Moreover, our high-level
topological representation results in 30.9% shorter trajectories in the
exploration task with respect to a standard method."
SymDiff: Equivariant Diffusion via Stochastic Symmetrisation,cs.LG,Machine Learning,2024-10-08,"We propose SymDiff, a novel method for constructing equivariant diffusion
models using the recently introduced framework of stochastic symmetrisation.
SymDiff resembles a learned data augmentation that is deployed at sampling
time, and is lightweight, computationally efficient, and easy to implement on
top of arbitrary off-the-shelf models. Notably, in contrast to previous work,
SymDiff typically does not require any neural network components that are
intrinsically equivariant, avoiding the need for complex parameterizations and
the use of higher-order geometric features. Instead, our method can leverage
highly scalable modern architectures as drop-in replacements for these more
constrained alternatives. We show that this additional flexibility yields
significant empirical benefit on $\mathrm{E}(3)$-equivariant molecular
generation. To the best of our knowledge, this is the first application of
symmetrisation to generative modelling, suggesting its potential in this domain
more generally."
HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Reconstructing 3D scenes from multiple viewpoints is a fundamental task in
stereo vision. Recently, advances in generalizable 3D Gaussian Splatting have
enabled high-quality novel view synthesis for unseen scenes from sparse input
views by feed-forward predicting per-pixel Gaussian parameters without extra
optimization. However, existing methods typically generate single-scale 3D
Gaussians, which lack representation of both large-scale structure and texture
details, resulting in mislocation and artefacts. In this paper, we propose a
novel framework, HiSplat, which introduces a hierarchical manner in
generalizable 3D Gaussian Splatting to construct hierarchical 3D Gaussians via
a coarse-to-fine strategy. Specifically, HiSplat generates large coarse-grained
Gaussians to capture large-scale structures, followed by fine-grained Gaussians
to enhance delicate texture details. To promote inter-scale interactions, we
propose an Error Aware Module for Gaussian compensation and a Modulating Fusion
Module for Gaussian repair. Our method achieves joint optimization of
hierarchical representations, allowing for novel view synthesis using only
two-view reference images. Comprehensive experiments on various datasets
demonstrate that HiSplat significantly enhances reconstruction quality and
cross-dataset generalization compared to prior single-scale methods. The
corresponding ablation study and analysis of different-scale 3D Gaussians
reveal the mechanism behind the effectiveness. Project website:
https://open3dvlab.github.io/HiSplat/"
Story-Adapter: A Training-free Iterative Framework for Long Story Visualization,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Story visualization, the task of generating coherent images based on a
narrative, has seen significant advancements with the emergence of
text-to-image models, particularly diffusion models. However, maintaining
semantic consistency, generating high-quality fine-grained interactions, and
ensuring computational feasibility remain challenging, especially in long story
visualization (i.e., up to 100 frames). In this work, we propose a
training-free and computationally efficient framework, termed Story-Adapter, to
enhance the generative capability of long stories. Specifically, we propose an
iterative paradigm to refine each generated image, leveraging both the text
prompt and all generated images from the previous iteration. Central to our
framework is a training-free global reference cross-attention module, which
aggregates all generated images from the previous iteration to preserve
semantic consistency across the entire story, while minimizing computational
costs with global embeddings. This iterative process progressively optimizes
image generation by repeatedly incorporating text constraints, resulting in
more precise and fine-grained interactions. Extensive experiments validate the
superiority of Story-Adapter in improving both semantic consistency and
generative capability for fine-grained interactions, particularly in long story
scenarios. The project page and associated code can be accessed via
https://jwmao1.github.io/storyadapter ."
Unsupervised Model Diagnosis,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Ensuring model explainability and robustness is essential for reliable
deployment of deep vision systems. Current methods for evaluating robustness
rely on collecting and annotating extensive test sets. While this is common
practice, the process is labor-intensive and expensive with no guarantee of
sufficient coverage across attributes of interest. Recently, model diagnosis
frameworks have emerged leveraging user inputs (e.g., text) to assess the
vulnerability of the model. However, such dependence on human can introduce
bias and limitation given the domain knowledge of particular users. This paper
proposes Unsupervised Model Diagnosis (UMO), that leverages generative models
to produce semantic counterfactual explanations without any user guidance.
Given a differentiable computer vision model (i.e., the target model), UMO
optimizes for the most counterfactual directions in a generative latent space.
Our approach identifies and visualizes changes in semantics, and then matches
these changes to attributes from wide-ranging text sources, such as
dictionaries or language models. We validate the framework on multiple vision
tasks (e.g., classification, segmentation, keypoint detection). Extensive
experiments show that our unsupervised discovery of semantic directions can
correctly highlight spurious correlations and visualize the failure mode of
target models without any human intervention."
BroadWay: Boost Your Text-to-Video Generation Model in a Training-free Way,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"The text-to-video (T2V) generation models, offering convenient visual
creation, have recently garnered increasing attention. Despite their
substantial potential, the generated videos may present artifacts, including
structural implausibility, temporal inconsistency, and a lack of motion, often
resulting in near-static video. In this work, we have identified a correlation
between the disparity of temporal attention maps across different blocks and
the occurrence of temporal inconsistencies. Additionally, we have observed that
the energy contained within the temporal attention maps is directly related to
the magnitude of motion amplitude in the generated videos. Based on these
observations, we present BroadWay, a training-free method to improve the
quality of text-to-video generation without introducing additional parameters,
augmenting memory or sampling time. Specifically, BroadWay is composed of two
principal components: 1) Temporal Self-Guidance improves the structural
plausibility and temporal consistency of generated videos by reducing the
disparity between the temporal attention maps across various decoder blocks. 2)
Fourier-based Motion Enhancement enhances the magnitude and richness of motion
by amplifying the energy of the map. Extensive experiments demonstrate that
BroadWay significantly improves the quality of text-to-video generation with
negligible additional cost."
OrionNav: Online Planning for Robot Autonomy with Context-Aware LLM and Open-Vocabulary Semantic Scene Graphs,cs.RO,Robotics,2024-10-08,"Enabling robots to autonomously navigate unknown, complex, dynamic
environments and perform diverse tasks remains a fundamental challenge in
developing robust autonomous physical agents. They must effectively perceive
their surroundings while leveraging world knowledge for decision-making. While
recent approaches utilize vision-language and large language models for scene
understanding and planning, they often rely on offline processing, external
computing, or restrictive environmental assumptions. We present a novel
framework for efficient and scalable real-time, onboard autonomous navigation
that integrates multi-level abstraction in both perception and planning in
unknown large-scale environments that change over time. Our system fuses data
from multiple onboard sensors for localization and mapping and integrates it
with open-vocabulary semantics to generate hierarchical scene graphs. An
LLM-based planner leverages these graphs to generate high-level task execution
strategies, which guide low-level controllers in safely accomplishing goals.
Our framework's real-time operation enables continuous updates to scene graphs
and plans, allowing swift responses to environmental changes and on-the-fly
error correction. This is a key advantage over static or rule-based planning
systems. We demonstrate our system's efficacy on a quadruped robot navigating
large-scale, dynamic environments, showcasing its adaptability and robustness
in diverse scenarios."
EVOLvE: Evaluating and Optimizing LLMs For Exploration,cs.LG,Machine Learning,2024-10-08,"Despite their success in many domains, large language models (LLMs) remain
under-studied in scenarios requiring optimal decision-making under uncertainty.
This is crucial as many real-world applications, ranging from personalized
recommendations to healthcare interventions, demand that LLMs not only predict
but also actively learn to make optimal decisions through exploration. In this
work, we measure LLMs' (in)ability to make optimal decisions in bandits, a
state-less reinforcement learning setting relevant to many applications. We
develop a comprehensive suite of environments, including both context-free and
contextual bandits with varying task difficulties, to benchmark LLMs'
performance. Motivated by the existence of optimal exploration algorithms, we
propose efficient ways to integrate this algorithmic knowledge into LLMs: by
providing explicit algorithm-guided support during inference; and through
algorithm distillation via in-context demonstrations and fine-tuning, using
synthetic data generated from these algorithms. Impressively, these techniques
allow us to achieve superior exploration performance with smaller models,
surpassing larger models on various tasks. We conducted an extensive ablation
study to shed light on various factors, such as task difficulty and data
representation, that influence the efficiency of LLM exploration. Additionally,
we conduct a rigorous analysis of the LLM's exploration efficiency using the
concept of regret, linking its ability to explore to the model size and
underlying algorithm."
BUMBLE: Unifying Reasoning and Acting with Vision-Language Models for Building-wide Mobile Manipulation,cs.RO,Robotics,2024-10-08,"To operate at a building scale, service robots must perform very long-horizon
mobile manipulation tasks by navigating to different rooms, accessing different
floors, and interacting with a wide and unseen range of everyday objects. We
refer to these tasks as Building-wide Mobile Manipulation. To tackle these
inherently long-horizon tasks, we introduce BUMBLE, a unified Vision-Language
Model (VLM)-based framework integrating open-world RGBD perception, a wide
spectrum of gross-to-fine motor skills, and dual-layered memory. Our extensive
evaluation (90+ hours) indicates that BUMBLE outperforms multiple baselines in
long-horizon building-wide tasks that require sequencing up to 12 ground truth
skills spanning 15 minutes per trial. BUMBLE achieves 47.1% success rate
averaged over 70 trials in different buildings, tasks, and scene layouts from
different starting rooms and floors. Our user study demonstrates 22% higher
satisfaction with our method than state-of-the-art mobile manipulation methods.
Finally, we demonstrate the potential of using increasingly-capable foundation
models to push performance further. For more information, see
https://robin-lab.cs.utexas.edu/BUMBLE/"
SD-$$XL: Generating Low-Resolution Quantized Imagery via Score Distillation,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Low-resolution quantized imagery, such as pixel art, is seeing a revival in
modern applications ranging from video game graphics to digital design and
fabrication, where creativity is often bound by a limited palette of elemental
units. Despite their growing popularity, the automated generation of quantized
images from raw inputs remains a significant challenge, often necessitating
intensive manual input. We introduce SD-$\pi$XL, an approach for producing
quantized images that employs score distillation sampling in conjunction with a
differentiable image generator. Our method enables users to input a prompt and
optionally an image for spatial conditioning, set any desired output size $H
\times W$, and choose a palette of $n$ colors or elements. Each color
corresponds to a distinct class for our generator, which operates on an $H
\times W \times n$ tensor. We adopt a softmax approach, computing a convex sum
of elements, thus rendering the process differentiable and amenable to
backpropagation. We show that employing Gumbel-softmax reparameterization
allows for crisp pixel art effects. Unique to our method is the ability to
transform input images into low-resolution, quantized versions while retaining
their key semantic features. Our experiments validate SD-$\pi$XL's performance
in creating visually pleasing and faithful representations, consistently
outperforming the current state-of-the-art. Furthermore, we showcase
SD-$\pi$XL's practical utility in fabrication through its applications in
interlocking brick mosaic, beading and embroidery design."
Parameter Choice and Neuro-Symbolic Approaches for Deep Domain-Invariant Learning,cs.LG,Machine Learning,2024-10-08,"As artificial intelligence (AI) systems advance, we move towards broad AI:
systems capable of performing well on diverse tasks, understanding context, and
adapting rapidly to new scenarios. A central challenge for broad AI systems is
to generalize over tasks in related domains and being robust to distribution
shifts. Neuro-symbolic (NeSy) AI bridges the gap between symbolic and
sub-symbolic paradigms to address these challenges, enabling adaptable,
generalizable, and more interpretable systems. The development of broad AI
requires advancements in domain adaptation (DA), enabling models trained on
source domains to effectively generalize to unseen target domains. Traditional
approaches often rely on parameter optimization and fine-tuning, which can be
impractical due to high costs and risks of catastrophic forgetting. NeSy AI
systems use multiple models and methods to generalize to unseen domains and
maintain performance across varying conditions. We analyze common DA and NeSy
approaches with a focus on deep domain-invariant learning, extending to
real-world challenges such as adapting to continuously changing domains and
handling large domain gaps. We showcase state-of-the-art model-selection
methods for scenarios with limited samples and introduce domain-specific
adaptations without gradient-based updates for cases where model tuning is
infeasible. This work establishes a framework for scalable and generalizable
broad AI systems applicable across various problem settings, demonstrating how
symbolic reasoning and large language models can build universal computational
graphs that generalize across domains and problems, contributing to more
adaptable AI approaches for real-world applications."
TEOChat: A Large Vision-Language Assistant for Temporal Earth Observation Data,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Large vision and language assistants have enabled new capabilities for
interpreting natural images. These approaches have recently been adapted to
earth observation data, but they are only able to handle single image inputs,
limiting their use for many real-world tasks. In this work, we develop a new
vision and language assistant called TEOChat that can engage in conversations
about temporal sequences of earth observation data. To train TEOChat, we curate
an instruction-following dataset composed of many single image and temporal
tasks including building change and damage assessment, semantic change
detection, and temporal scene classification. We show that TEOChat can perform
a wide variety of spatial and temporal reasoning tasks, substantially
outperforming previous vision and language assistants, and even achieving
comparable or better performance than specialist models trained to perform
these specific tasks. Furthermore, TEOChat achieves impressive zero-shot
performance on a change detection and change question answering dataset,
outperforms GPT-4o and Gemini 1.5 Pro on multiple temporal tasks, and exhibits
stronger single image capabilities than a comparable single EO image
instruction-following model. We publicly release our data, models, and code at
https://github.com/ermongroup/TEOChat ."
RelitLRM: Generative Relightable Radiance for Large Reconstruction Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"We propose RelitLRM, a Large Reconstruction Model (LRM) for generating
high-quality Gaussian splatting representations of 3D objects under novel
illuminations from sparse (4-8) posed images captured under unknown static
lighting. Unlike prior inverse rendering methods requiring dense captures and
slow optimization, often causing artifacts like incorrect highlights or shadow
baking, RelitLRM adopts a feed-forward transformer-based model with a novel
combination of a geometry reconstructor and a relightable appearance generator
based on diffusion. The model is trained end-to-end on synthetic multi-view
renderings of objects under varying known illuminations. This architecture
design enables to effectively decompose geometry and appearance, resolve the
ambiguity between material and lighting, and capture the multi-modal
distribution of shadows and specularity in the relit appearance. We show our
sparse-view feed-forward RelitLRM offers competitive relighting results to
state-of-the-art dense-view optimization-based baselines while being
significantly faster. Our project page is available at:
https://relitlrm.github.io/."
A Timeline and Analysis for Representation Plasticity in Large Language Models,cs.LG,Machine Learning,2024-10-08,"The ability to steer AI behavior is crucial to preventing its long term
dangerous and catastrophic potential. Representation Engineering (RepE) has
emerged as a novel, powerful method to steer internal model behaviors, such as
""honesty"", at a top-down level. Understanding the steering of representations
should thus be placed at the forefront of alignment initiatives. Unfortunately,
current efforts to understand plasticity at this level are highly neglected.
This paper aims to bridge the knowledge gap and understand how LLM
representation stability, specifically for the concept of ""honesty"", and model
plasticity evolve by applying steering vectors extracted at different
fine-tuning stages, revealing differing magnitudes of shifts in model behavior.
The findings are pivotal, showing that while early steering exhibits high
plasticity, later stages have a surprisingly responsive critical window. This
pattern is observed across different model architectures, signaling that there
is a general pattern of model plasticity that can be used for effective
intervention. These insights greatly contribute to the field of AI
transparency, addressing a pressing lack of efficiency limiting our ability to
effectively steer model behavior."
DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"The process of creating training data to teach models is currently driven by
humans, who manually analyze model weaknesses and plan how to create data that
improves a student model. Recent approaches using LLMs as annotators reduce
human effort, but still require humans to interpret feedback from evaluations
and control the LLM to produce data the student needs. Automating this
labor-intensive process by creating autonomous data generation agents - or
teachers - is desirable, but requires environments that can simulate the
feedback-driven, iterative, closed loop of data creation. To enable rapid and
scalable testing for such agents and their modules, we introduce DataEnvGym, a
testbed of teacher environments for data generation agents. DataEnvGym frames
data generation as a sequential decision-making task, involving an agent
consisting of a data generation policy (which generates a plan for creating
training data) and a data generation engine (which transforms the plan into
data), inside an environment that provides student feedback. The agent's goal
is to improve student performance. Students are iteratively trained and
evaluated on generated data, with their feedback (in the form of errors or weak
skills) being reported to the agent after each iteration. DataEnvGym includes
multiple teacher environment instantiations across 3 levels of structure in the
state representation and action space. More structured environments are based
on inferred skills and offer more interpretability and curriculum control. We
support 3 diverse tasks (math, code, and VQA) and test multiple students and
teachers. Example agents in our teaching environments can iteratively improve
students across tasks and settings. Moreover, we show that environments teach
different skill levels and test variants of key modules, pointing to future
work in improving data generation agents, engines, and feedback mechanisms."
Fair-OBNC: Correcting Label Noise for Fairer Datasets,cs.LG,Machine Learning,2024-10-08,"Data used by automated decision-making systems, such as Machine Learning
models, often reflects discriminatory behavior that occurred in the past. These
biases in the training data are sometimes related to label noise, such as in
COMPAS, where more African-American offenders are wrongly labeled as having a
higher risk of recidivism when compared to their White counterparts. Models
trained on such biased data may perpetuate or even aggravate the biases with
respect to sensitive information, such as gender, race, or age. However, while
multiple label noise correction approaches are available in the literature,
these focus on model performance exclusively. In this work, we propose
Fair-OBNC, a label noise correction method with fairness considerations, to
produce training datasets with measurable demographic parity. The presented
method adapts Ordering-Based Noise Correction, with an adjusted criterion of
ordering, based both on the margin of error of an ensemble, and the potential
increase in the observed demographic parity of the dataset. We evaluate
Fair-OBNC against other different pre-processing techniques, under different
scenarios of controlled label noise. Our results show that the proposed method
is the overall better alternative within the pool of label correction methods,
being capable of attaining better reconstructions of the original labels.
Models trained in the corrected data have an increase, on average, of 150% in
demographic parity, when compared to models trained in data with noisy labels,
across the considered levels of label noise."
"RL, but don't do anything I wouldn't do",cs.LG,Machine Learning,2024-10-08,"In reinforcement learning, if the agent's reward differs from the designers'
true utility, even only rarely, the state distribution resulting from the
agent's policy can be very bad, in theory and in practice. When RL policies
would devolve into undesired behavior, a common countermeasure is KL
regularization to a trusted policy (""Don't do anything I wouldn't do""). All
current cutting-edge language models are RL agents that are KL-regularized to a
""base policy"" that is purely predictive. Unfortunately, we demonstrate that
when this base policy is a Bayesian predictive model of a trusted policy, the
KL constraint is no longer reliable for controlling the behavior of an advanced
RL agent. We demonstrate this theoretically using algorithmic information
theory, and while systems today are too weak to exhibit this theorized failure
precisely, we RL-finetune a language model and find evidence that our formal
results are plausibly relevant in practice. We also propose a theoretical
alternative that avoids this problem by replacing the ""Don't do anything I
wouldn't do"" principle with ""Don't do anything I mightn't do""."
Solving robust MDPs as a sequence of static RL problems,cs.LG,Machine Learning,2024-10-08,"Designing control policies whose performance level is guaranteed to remain
above a given threshold in a span of environments is a critical feature for the
adoption of reinforcement learning (RL) in real-world applications. The search
for such robust policies is a notoriously difficult problem, related to the
so-called dynamic model of transition function uncertainty, where the
environment dynamics are allowed to change at each time step. But in practical
cases, one is rather interested in robustness to a span of static transition
models throughout interaction episodes. The static model is known to be harder
to solve than the dynamic one, and seminal algorithms, such as robust value
iteration, as well as most recent works on deep robust RL, build upon the
dynamic model. In this work, we propose to revisit the static model. We suggest
an analysis of why solving the static model under some mild hypotheses is a
reasonable endeavor, based on an equivalence with the dynamic model, and
formalize the general intuition that robust MDPs can be solved by tackling a
series of static problems. We introduce a generic meta-algorithm called IWOCS,
which incrementally identifies worst-case transition models so as to guide the
search for a robust policy. Discussion on IWOCS sheds light on new ways to
decouple policy optimization and adversarial transition functions and opens new
perspectives for analysis. We derive a deep RL version of IWOCS and demonstrate
it is competitive with state-of-the-art algorithms on classical benchmarks."
LeanAgent: Lifelong Learning for Formal Theorem Proving,cs.LG,Machine Learning,2024-10-08,"Large Language Models (LLMs) have been successful in mathematical reasoning
tasks such as formal theorem proving when integrated with interactive proof
assistants like Lean. Existing approaches involve training or fine-tuning an
LLM on a specific dataset to perform well on particular domains, such as
undergraduate-level mathematics. These methods struggle with generalizability
to advanced mathematics. A fundamental limitation is that these approaches
operate on static domains, failing to capture how mathematicians often work
across multiple domains and projects simultaneously or cyclically. We present
LeanAgent, a novel lifelong learning framework for theorem proving that
continuously generalizes to and improves on ever-expanding mathematical
knowledge without forgetting previously learned knowledge. LeanAgent introduces
several key innovations, including a curriculum learning strategy that
optimizes the learning trajectory in terms of mathematical difficulty, a
dynamic database for efficient management of evolving mathematical knowledge,
and progressive training to balance stability and plasticity. LeanAgent
successfully proves 162 theorems previously unproved by humans across 23
diverse Lean repositories, many from advanced mathematics. It performs up to
11$\times$ better than the static LLM baseline, proving challenging theorems in
domains like abstract algebra and algebraic topology while showcasing a clear
progression of learning from basic concepts to advanced topics. In addition, we
analyze LeanAgent's superior performance on key lifelong learning metrics.
LeanAgent achieves exceptional scores in stability and backward transfer, where
learning new tasks improves performance on previously learned tasks. This
emphasizes LeanAgent's continuous generalizability and improvement, explaining
its superior theorem proving performance."
Round and Round We Go! What makes Rotary Positional Encodings useful?,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Positional Encodings (PEs) are a critical component of Transformer-based
Large Language Models (LLMs), providing the attention mechanism with important
sequence-position information. One of the most popular types of encoding used
today in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries
and keys based on their relative distance. A common belief is that RoPE is
useful because it helps to decay token dependency as relative distance
increases. In this work, we argue that this is unlikely to be the core reason.
We study the internals of a trained Gemma 7B model to understand how RoPE is
being used at a mechanical level. We find that Gemma learns to use RoPE to
construct robust ""positional"" attention patterns by exploiting the highest
frequencies. We also find that, in general, Gemma greatly prefers to use the
lowest frequencies of RoPE, which we suspect are used to carry semantic
information. We mathematically prove interesting behaviours of RoPE and conduct
experiments to verify our findings, proposing a modification of RoPE that fixes
some highlighted issues and improves performance. We believe that this work
represents an interesting step in better understanding PEs in LLMs, which we
believe holds crucial value for scaling LLMs to large sizes and context
lengths."
Integrating Planning into Single-Turn Long-Form Text Generation,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Generating high-quality, in-depth textual documents, such as academic papers,
news articles, Wikipedia entries, and books, remains a significant challenge
for Large Language Models (LLMs). In this paper, we propose to use planning to
generate long form content. To achieve our goal, we generate intermediate steps
via an auxiliary task that teaches the LLM to plan, reason and structure before
generating the final text. Our main novelty lies in a single auxiliary task
that does not require multiple rounds of prompting or planning. To overcome the
scarcity of training data for these intermediate steps, we leverage LLMs to
generate synthetic intermediate writing data such as outlines, key information
and summaries from existing full articles. Our experiments demonstrate on two
datasets from different domains, namely the scientific news dataset SciNews and
Wikipedia datasets in KILT-Wiki and FreshWiki, that LLMs fine-tuned with the
auxiliary task generate higher quality documents. We observed +2.5% improvement
in ROUGE-Lsum, and a strong 3.60 overall win/loss ratio via human SxS
evaluation, with clear wins in organization, relevance, and verifiability."
Entering Real Social World! Benchmarking the Theory of Mind and Socialization Capabilities of LLMs from a First-person Perspective,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"In the social world, humans possess the capability to infer and reason about
others mental states (such as emotions, beliefs, and intentions), known as the
Theory of Mind (ToM). Simultaneously, humans own mental states evolve in
response to social situations, a capability we refer to as socialization.
Together, these capabilities form the foundation of human social interaction.
In the era of artificial intelligence (AI), especially with the development of
large language models (LLMs), we raise an intriguing question: How do LLMs
perform in terms of ToM and socialization capabilities? And more broadly, can
these AI models truly enter and navigate the real social world? Existing
research evaluating LLMs ToM and socialization capabilities by positioning LLMs
as passive observers from a third person perspective, rather than as active
participants. However, compared to the third-person perspective, observing and
understanding the world from an egocentric first person perspective is a
natural approach for both humans and AI agents. The ToM and socialization
capabilities of LLMs from a first person perspective, a crucial attribute for
advancing embodied AI agents, remain unexplored. To answer the aforementioned
questions and bridge the research gap, we introduce EgoSocialArena, a novel
framework designed to evaluate and investigate the ToM and socialization
capabilities of LLMs from a first person perspective. It encompasses two
evaluation environments: static environment and interactive environment, with
seven scenarios: Daily Life, Counterfactual, New World, Blackjack, Number
Guessing, and Limit Texas Hold em, totaling 2,195 data entries. With
EgoSocialArena, we have conducted a comprehensive evaluation of nine advanced
LLMs and observed some key insights regarding the future development of LLMs as
well as the capabilities levels of the most advanced LLMs currently available."
Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"The Direct Segment Anything Model (DirectSAM) excels in class-agnostic
contour extraction. In this paper, we explore its use by applying it to optical
remote sensing imagery, where semantic contour extraction-such as identifying
buildings, road networks, and coastlines-holds significant practical value.
Those applications are currently handled via training specialized small models
separately on small datasets in each domain. We introduce a foundation model
derived from DirectSAM, termed DirectSAM-RS, which not only inherits the strong
segmentation capability acquired from natural images, but also benefits from a
large-scale dataset we created for remote sensing semantic contour extraction.
This dataset comprises over 34k image-text-contour triplets, making it at least
30 times larger than individual dataset. DirectSAM-RS integrates a prompter
module: a text encoder and cross-attention layers attached to the DirectSAM
architecture, which allows flexible conditioning on target class labels or
referring expressions. We evaluate the DirectSAM-RS in both zero-shot and
fine-tuning setting, and demonstrate that it achieves state-of-the-art
performance across several downstream benchmarks."
Hibikino-Musashi@Home 2024 Team Description Paper,cs.RO,Robotics,2024-10-08,"This paper provides an overview of the techniques employed by
Hibikino-Musashi@Home, which intends to participate in the domestic standard
platform league. The team has developed a dataset generator for training a
robot vision system and an open-source development environment running on a
Human Support Robot simulator.
  The large language model powered task planner selects appropriate primitive
skills to perform the task requested by users. The team aims to design a home
service robot that can assist humans in their homes and continuously attends
competitions to evaluate and improve the developed system."
Benign Overfitting for Regression with Trained Two-Layer ReLU Networks,cs.LG,Machine Learning,2024-10-08,"We study the least-square regression problem with a two-layer fully-connected
neural network, with ReLU activation function, trained by gradient flow. Our
first result is a generalization result, that requires no assumptions on the
underlying regression function or the noise other than that they are bounded.
We operate in the neural tangent kernel regime, and our generalization result
is developed via a decomposition of the excess risk into estimation and
approximation errors, viewing gradient flow as an implicit regularizer. This
decomposition in the context of neural networks is a novel perspective of
gradient descent, and helps us avoid uniform convergence traps. In this work,
we also establish that under the same setting, the trained network overfits to
the data. Together, these results, establishes the first result on benign
overfitting for finite-width ReLU networks for arbitrary regression functions."
Neural-Bayesian Program Learning for Few-shot Dialogue Intent Parsing,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"With the growing importance of customer service in contemporary business,
recognizing the intents behind service dialogues has become essential for the
strategic success of enterprises. However, the nature of dialogue data varies
significantly across different scenarios, and implementing an intent parser for
a specific domain often involves tedious feature engineering and a heavy
workload of data labeling. In this paper, we propose a novel Neural-Bayesian
Program Learning model named Dialogue-Intent Parser (DI-Parser), which
specializes in intent parsing under data-hungry settings and offers promising
performance improvements. DI-Parser effectively utilizes data from multiple
sources in a ""Learning to Learn"" manner and harnesses the ""wisdom of the crowd""
through few-shot learning capabilities on human-annotated datasets.
Experimental results demonstrate that DI-Parser outperforms state-of-the-art
deep learning models and offers practical advantages for industrial-scale
applications."
The Last Iterate Advantage: Empirical Auditing and Principled Heuristic Analysis of Differentially Private SGD,cs.CR,Cryptography and Security,2024-10-08,"We propose a simple heuristic privacy analysis of noisy clipped stochastic
gradient descent (DP-SGD) in the setting where only the last iterate is
released and the intermediate iterates remain hidden. Namely, our heuristic
assumes a linear structure for the model.
  We show experimentally that our heuristic is predictive of the outcome of
privacy auditing applied to various training procedures. Thus it can be used
prior to training as a rough estimate of the final privacy leakage. We also
probe the limitations of our heuristic by providing some artificial
counterexamples where it underestimates the privacy leakage.
  The standard composition-based privacy analysis of DP-SGD effectively assumes
that the adversary has access to all intermediate iterates, which is often
unrealistic. However, this analysis remains the state of the art in practice.
While our heuristic does not replace a rigorous privacy analysis, it
illustrates the large gap between the best theoretical upper bounds and the
privacy auditing lower bounds and sets a target for further work to improve the
theoretical privacy analyses. We also empirically support our heuristic and
show existing privacy auditing attacks are bounded by our heuristic analysis in
both vision and language tasks."
SC-Bench: A Large-Scale Dataset for Smart Contract Auditing,cs.CR,Cryptography and Security,2024-10-08,"There is a huge demand to ensure the compliance of smart contracts listed on
blockchain platforms to safety and economic standards. Today, manual efforts in
the form of auditing are commonly used to achieve this goal. ML-based automated
techniques have the promise to alleviate human efforts and the resulting
monetary costs. However, unlike other domains where ML techniques have had huge
successes, no systematic ML techniques have been proposed or applied to smart
contract auditing. We present SC-Bench, the first dataset for automated
smart-contract auditing research. SC-Bench consists of 5,377 real-world smart
contracts running on Ethereum, a widely used blockchain platform, and 15,975
violations of standards on Ehereum called ERCs. Out of these violations, 139
are real violations programmers made. The remaining are errors we
systematically injected to reflect the violations of different ERC rules. We
evaluate SC-Bench using GPT-4 by prompting it with both the contracts and ERC
rules. In addition, we manually identify each violated rule and the
corresponding code site (i.e., oracle) and prompt GPT-4 with the information
asking for a True-or-False question. Our results show that without the oracle,
GPT-4 can only detect 0.9% violations, and with the oracle, it detects 22.9%
violations. These results show the potential room for improvement in ML-based
techniques for smart-contract auditing."
Manual Verbalizer Enrichment for Few-Shot Text Classification,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"With the continuous development of pre-trained language models, prompt-based
training becomes a well-adopted paradigm that drastically improves the
exploitation of models for many natural language processing tasks. Prompting
also shows great performance compared to traditional fine-tuning when adapted
to zero-shot or few-shot scenarios where the number of annotated data is
limited. In this framework, the role of verbalizers is essential, as an
interpretation from masked word distributions into output predictions. In this
work, we propose \acrshort{mave}, an approach for verbalizer construction by
enrichment of class labels using neighborhood relation in the embedding space
of words for the text classification task. In addition, we elaborate a
benchmarking procedure to evaluate typical baselines of verbalizers for
document classification in few-shot learning contexts. Our model achieves
state-of-the-art results while using significantly fewer resources. We show
that our approach is particularly effective in cases with extremely limited
supervision data."
Multimodal Situational Safety,cs.AI,Artificial Intelligence,2024-10-08,"Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating
impressive capabilities as multimodal assistants that interact with both humans
and their environments. However, this increased sophistication introduces
significant safety concerns. In this paper, we present the first evaluation and
analysis of a novel safety challenge termed Multimodal Situational Safety,
which explores how safety considerations vary based on the specific situation
in which the user or agent is engaged. We argue that for an MLLM to respond
safely, whether through language or action, it often needs to assess the safety
implications of a language query within its corresponding visual context. To
evaluate this capability, we develop the Multimodal Situational Safety
benchmark (MSSBench) to assess the situational safety performance of current
MLLMs. The dataset comprises 1,820 language query-image pairs, half of which
the image context is safe, and the other half is unsafe. We also develop an
evaluation framework that analyzes key safety aspects, including explicit
safety reasoning, visual understanding, and, crucially, situational safety
reasoning. Our findings reveal that current MLLMs struggle with this nuanced
safety problem in the instruction-following setting and struggle to tackle
these situational safety challenges all at once, highlighting a key area for
future research. Furthermore, we develop multi-agent pipelines to coordinately
solve safety challenges, which shows consistent improvement in safety over the
original MLLM response. Code and data: mssbench.github.io."
QGym: Scalable Simulation and Benchmarking of Queuing Network Controllers,cs.LG,Machine Learning,2024-10-08,"Queuing network control determines the allocation of scarce resources to
manage congestion, a fundamental problem in manufacturing, communications, and
healthcare. Compared to standard RL problems, queueing problems are
distinguished by unique challenges: i) a system operating in continuous time,
ii) high stochasticity, and iii) long horizons over which the system can become
unstable (exploding delays). To spur methodological progress tackling these
challenges, we present an open-sourced queueing simulation framework, QGym,
that benchmark queueing policies across realistic problem instances. Our
modular framework allows the researchers to build on our initial instances,
which provide a wide range of environments including parallel servers,
criss-cross, tandem, and re-entrant networks, as well as a realistically
calibrated hospital queuing system. QGym makes it easy to compare multiple
policies, including both model-free RL methods and classical queuing policies.
Our testbed complements the traditional focus on evaluating algorithms based on
mathematical guarantees in idealized settings, and significantly expands the
scope of empirical benchmarking in prior work. QGym code is open-sourced at
https://github.com/namkoong-lab/QGym."
Quadratic Is Not What You Need For Multimodal Large Language Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"In the past year, the capabilities of Multimodal Large Language Models
(MLLMs) have significantly improved across various aspects. However,
constrained by the quadratic growth of computation in LLMs as the number of
tokens increases, efficiency has become a bottleneck for further scaling MLLMs.
Although recent efforts have been made to prune visual tokens or use more
lightweight LLMs to reduce computation, the problem of quadratic growth in
computation with the increase of visual tokens still persists. To address this,
we propose a novel approach: instead of reducing the input visual tokens for
LLMs, we focus on pruning vision-related computations within the LLMs. After
pruning, the computation growth in the LLM is no longer quadratic with the
increase of visual tokens, but linear. Surprisingly, we found that after
applying such extensive pruning, the capabilities of MLLMs are comparable with
the original one and even superior on some benchmarks with only 25% of the
computation. This finding opens up the possibility for MLLMs to incorporate
much denser visual tokens. Additionally, based on this finding, we further
analyzed some architectural design deficiencies in existing MLLMs and proposed
promising improvements. To the best of our knowledge, this is the first study
to investigate the computational redundancy in the LLM's vision component of
MLLMs. Code and checkpoints will be released soon."
Temporal Reasoning Transfer from Text to Video,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Video Large Language Models (Video LLMs) have shown promising capabilities in
video comprehension, yet they struggle with tracking temporal changes and
reasoning about temporal relationships. While previous research attributed this
limitation to the ineffective temporal encoding of visual inputs, our
diagnostic study reveals that video representations contain sufficient
information for even small probing classifiers to achieve perfect accuracy.
Surprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning
capability stems from the underlying LLM's inherent difficulty with temporal
concepts, as evidenced by poor performance on textual temporal
question-answering tasks. Building on this discovery, we introduce the Textual
Temporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning
tasks in pure text format from existing image-text datasets, addressing the
scarcity of video samples with complex temporal scenarios. Remarkably, without
using any video data, T3 enhances LongVA-7B's temporal understanding, yielding
a 5.3 absolute accuracy improvement on the challenging TempCompass benchmark,
which enables our model to outperform ShareGPT4Video-8B trained on 28,000 video
samples. Additionally, the enhanced LongVA-7B model achieves competitive
performance on comprehensive video benchmarks. For example, it achieves a 49.7
accuracy on the Temporal Reasoning task of Video-MME, surpassing powerful
large-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further
analysis reveals a strong correlation between textual and video temporal task
performance, validating the efficacy of transferring temporal reasoning
abilities from text to video domains."
GSLoc: Visual Localization with 3D Gaussian Splatting,cs.RO,Robotics,2024-10-08,"We present GSLoc: a new visual localization method that performs dense camera
alignment using 3D Gaussian Splatting as a map representation of the scene.
GSLoc backpropagates pose gradients over the rendering pipeline to align the
rendered and target images, while it adopts a coarse-to-fine strategy by
utilizing blurring kernels to mitigate the non-convexity of the problem and
improve the convergence. The results show that our approach succeeds at visual
localization in challenging conditions of relatively small overlap between
initial and target frames inside textureless environments when state-of-the-art
neural sparse methods provide inferior results. Using the byproduct of
realistic rendering from the 3DGS map representation, we show how to enhance
localization results by mixing a set of observed and virtual reference
keyframes when solving the image retrieval problem. We evaluate our method both
on synthetic and real-world data, discussing its advantages and application
potential."
GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation,cs.RO,Robotics,2024-10-08,"We present GR-2, a state-of-the-art generalist robot agent for versatile and
generalizable robot manipulation. GR-2 is first pre-trained on a vast number of
Internet videos to capture the dynamics of the world. This large-scale
pre-training, involving 38 million video clips and over 50 billion tokens,
equips GR-2 with the ability to generalize across a wide range of robotic tasks
and environments during subsequent policy learning. Following this, GR-2 is
fine-tuned for both video generation and action prediction using robot
trajectories. It exhibits impressive multi-task learning capabilities,
achieving an average success rate of 97.7% across more than 100 tasks.
Moreover, GR-2 demonstrates exceptional generalization to new, previously
unseen scenarios, including novel backgrounds, environments, objects, and
tasks. Notably, GR-2 scales effectively with model size, underscoring its
potential for continued growth and application. Project page:
\url{https://gr2-manipulation.github.io}."
Detecting Android Malware by Visualizing App Behaviors from Multiple Complementary Views,cs.CR,Cryptography and Security,2024-10-08,"Deep learning has emerged as a promising technology for achieving Android
malware detection. To further unleash its detection potentials, software
visualization can be integrated for analyzing the details of app behaviors
clearly. However, facing increasingly sophisticated malware, existing
visualization-based methods, analyzing from one or randomly-selected few views,
can only detect limited attack types. We propose and implement LensDroid, a
novel technique that detects Android malware by visualizing app behaviors from
multiple complementary views. Our goal is to harness the power of combining
deep learning and software visualization to automatically capture and aggregate
high-level features that are not inherently linked, thereby revealing hidden
maliciousness of Android app behaviors. To thoroughly comprehend the details of
apps, we visualize app behaviors from three related but distinct views of
behavioral sensitivities, operational contexts and supported environments. We
then extract high-order semantics based on the views accordingly. To exploit
semantic complementarity of the views, we design a deep neural network based
model for fusing the visualized features from local to global based on their
contributions to downstream tasks. A comprehensive comparison with five
baseline techniques is performed on datasets of more than 51K apps in three
real-world typical scenarios, including overall threats, app evolution and
zero-day malware. The experimental results show that the overall performance of
LensDroid is better than the baseline techniques. We also validate the
complementarity of the views and demonstrate that the multi-view fusion in
LensDroid enhances Android malware detection."
GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"In this work, we propose a novel method (GLOV) enabling Large Language Models
(LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to
enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the
downstream task description, querying it for suitable VLM prompts (e.g., for
zero-shot classification with CLIP). These prompts are ranked according to a
purity measure obtained through a fitness function. In each respective
optimization step, the ranked prompts are fed as in-context examples (with
their accuracies) to equip the LLM with the knowledge of the type of text
prompts preferred by the downstream VLM. Furthermore, we also explicitly steer
the LLM generation process in each optimization step by specifically adding an
offset difference vector of the embeddings from the positive and negative
solutions found by the LLM, in previous optimization steps, to the intermediate
layer of the network for the next generation step. This offset vector steers
the LLM generation toward the type of language preferred by the downstream VLM,
resulting in enhanced performance on the downstream vision tasks. We
comprehensively evaluate our GLOV on 16 diverse datasets using two families of
VLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models
-- showing that the discovered solutions can enhance the recognition
performance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these
models."
AgentSquare: Automatic LLM Agent Search in Modular Design Space,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Recent advancements in Large Language Models (LLMs) have led to a rapid
growth of agentic systems capable of handling a wide range of complex tasks.
However, current research largely relies on manual, task-specific design,
limiting their adaptability to novel tasks. In this paper, we introduce a new
research problem: Modularized LLM Agent Search (MoLAS). We propose a modular
design space that abstracts existing LLM agent designs into four fundamental
modules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.
Building on this design space, we present a novel LLM agent search framework
called AgentSquare, which introduces two core mechanisms, i.e., module
evolution and recombination, to efficiently search for optimized LLM agents. To
further accelerate the process, we design a performance predictor that uses
in-context surrogate models to skip unpromising agent designs. Extensive
experiments across six benchmarks, covering the diverse scenarios of web,
embodied, tool use and game applications, show that AgentSquare substantially
outperforms hand-crafted agents, achieving an average performance gain of 17.2%
against best-known human designs. Moreover, AgentSquare can generate
interpretable design insights, enabling a deeper understanding of agentic
architecture and its impact on task performance. We believe that the modular
design space and AgentSquare search framework offer a platform for fully
exploiting the potential of prior successful designs and consolidating the
collective efforts of research community. Code repo is available at
https://github.com/tsinghua-fib-lab/AgentSquare."
Quality Diversity Imitation Learning,cs.LG,Machine Learning,2024-10-08,"Imitation learning (IL) has shown great potential in various applications,
such as robot control. However, traditional IL methods are usually designed to
learn only one specific type of behavior since demonstrations typically
correspond to a single expert. In this work, we introduce the first generic
framework for Quality Diversity Imitation Learning (QD-IL), which enables the
agent to learn a broad range of skills from limited demonstrations. Our
framework integrates the principles of quality diversity with adversarial
imitation learning (AIL) methods, and can potentially improve any inverse
reinforcement learning (IRL) method. Empirically, our framework significantly
improves the QD performance of GAIL and VAIL on the challenging continuous
control tasks derived from Mujoco environments. Moreover, our method even
achieves 2x expert performance in the most challenging Humanoid environment."
Toward Scalable Image Feature Compression: A Content-Adaptive and Diffusion-Based Approach,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Traditional image codecs emphasize signal fidelity and human perception,
often at the expense of machine vision tasks. Deep learning methods have
demonstrated promising coding performance by utilizing rich semantic embeddings
optimized for both human and machine vision. However, these compact embeddings
struggle to capture fine details such as contours and textures, resulting in
imperfect reconstructions. Furthermore, existing learning-based codecs lack
scalability. To address these limitations, this paper introduces a
content-adaptive diffusion model for scalable image compression. The proposed
method encodes fine textures through a diffusion process, enhancing perceptual
quality while preserving essential features for machine vision tasks. The
approach employs a Markov palette diffusion model combined with widely used
feature extractors and image generators, enabling efficient data compression.
By leveraging collaborative texture-semantic feature extraction and
pseudo-label generation, the method accurately captures texture information. A
content-adaptive Markov palette diffusion model is then applied to represent
both low-level textures and high-level semantic content in a scalable manner.
This framework offers flexible control over compression ratios by selecting
intermediate diffusion states, eliminating the need for retraining deep
learning models at different operating points. Extensive experiments
demonstrate the effectiveness of the proposed framework in both image
reconstruction and downstream machine vision tasks such as object detection,
segmentation, and facial landmark detection, achieving superior perceptual
quality compared to state-of-the-art methods."
blockLAW: Blockchain Technology for Legal Automation and Workflow -- Cyber Ethics and Cybersecurity Platforms,cs.CR,Cryptography and Security,2024-10-08,"In the current legal environment, it is essential to prioritize the
protection and reliability of data to promote trust and effectiveness. This
study examines how blockchain technology in the form of blockLAW can be
applicable to investigate its effects on legal automation, cybersecurity, and
ethical concerns. The decentralized ledger and unchangeable characteristics of
Blockchain provide opportunities to simplify legal procedures, automate
contract execution with smart contracts, and improve transparency in legal
transactions. Blockchain is seen as a crucial instrument for updating legal
processes while maintaining ethical standards, tackling issues like
scalability, regulatory adherence, and ethical dilemmas such as privacy and
fairness. The study examines recent developments and evaluates blockchain
impact on legal structures, offering perspectives on its potential to enhance
legal procedures and guarantee transparency in legal systems. It further
emphasizes blockchain ability to redefine how legal professionals handle and
protect sensitive information, leading to stronger, more effective, and
reliable legal procedures. We have also discussed the technological
considerations when it comes to blockchain integration into legal systems like
integration planning, implementation strategies, innovations, advancements,
trends with Blockchain Integration Framework for legal systems."
Estimating the Number of HTTP/3 Responses in QUIC Using Deep Learning,cs.LG,Machine Learning,2024-10-08,"QUIC, a new and increasingly used transport protocol, enhances TCP by
providing better security, performance, and features like stream multiplexing.
These features, however, also impose challenges for network middle-boxes that
need to monitor and analyze web traffic. This paper proposes a novel solution
for estimating the number of HTTP/3 responses in a given QUIC connection by an
observer. This estimation reveals server behavior, client-server interactions,
and data transmission efficiency, which is crucial for various applications
such as designing a load balancing solution and detecting HTTP/3 flood attacks.
The proposed scheme transforms QUIC connection traces into a sequence of images
and trains machine learning (ML) models to predict the number of responses.
Then, by aggregating images of a QUIC connection, an observer can estimate the
total number of responses. As the problem is formulated as a discrete
regression problem, we introduce a dedicated loss function. The proposed scheme
is evaluated on a dataset of over seven million images, generated from
$100,000$ traces collected from over $44,000$ websites over a four-month
period, from various vantage points. The scheme achieves up to 97\% cumulative
accuracy in both known and unknown web server settings and 92\% accuracy in
estimating the total number of responses in unseen QUIC traces."
Adaptive Label Smoothing for Out-of-Distribution Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Out-of-distribution (OOD) detection, which aims to distinguish unknown
classes from known classes, has received increasing attention recently. A main
challenge within is the unavailable of samples from the unknown classes in the
training process, and an effective strategy is to improve the performance for
known classes. Using beneficial strategies such as data augmentation and longer
training is thus a way to improve OOD detection. However, label smoothing, an
effective method for classifying known classes, degrades the performance of OOD
detection, and this phenomenon is under exploration. In this paper, we first
analyze that the limited and predefined learning target in label smoothing
results in the smaller maximal probability and logit, which further leads to
worse OOD detection performance. To mitigate this issue, we then propose a
novel regularization method, called adaptive label smoothing (ALS), and the
core is to push the non-true classes to have same probabilities whereas the
maximal probability is neither fixed nor limited. Extensive experimental
results in six datasets with two backbones suggest that ALS contributes to
classifying known samples and discerning unknown samples with clear margins.
Our code will be available to the public."
Towards Unsupervised Eye-Region Segmentation for Eye Tracking,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Finding the eye and parsing out the parts (e.g. pupil and iris) is a key
prerequisite for image-based eye tracking, which has become an indispensable
module in today's head-mounted VR/AR devices. However, a typical route for
training a segmenter requires tedious handlabeling. In this work, we explore an
unsupervised way. First, we utilize priors of human eye and extract signals
from the image to establish rough clues indicating the eye-region structure.
Upon these sparse and noisy clues, a segmentation network is trained to
gradually identify the precise area for each part. To achieve accurate parsing
of the eye-region, we first leverage the pretrained foundation model Segment
Anything (SAM) in an automatic way to refine the eye indications. Then, the
learning process is designed in an end-to-end manner following progressive and
prior-aware principle. Experiments show that our unsupervised approach can
easily achieve 90% (the pupil and iris) and 85% (the whole eye-region) of the
performances under supervised learning."
Zero-Shot Learning of Causal Models,cs.LG,Machine Learning,2024-10-08,"With the increasing acquisition of datasets over time, we now have access to
precise and varied descriptions of the world, capturing all sorts of phenomena.
These datasets can be seen as empirical observations of unknown causal
generative processes, which can commonly be described by Structural Causal
Models (SCMs). Recovering these causal generative processes from observations
poses formidable challenges, and often require to learn a specific generative
model for each dataset. In this work, we propose to learn a \emph{single} model
capable of inferring in a zero-shot manner the causal generative processes of
datasets. Rather than learning a specific SCM for each dataset, we enable the
Fixed-Point Approach (FiP) proposed in~\cite{scetbon2024fip}, to infer the
generative SCMs conditionally on their empirical representations. More
specifically, we propose to amortize the learning of a conditional version of
FiP to infer generative SCMs from observations and causal structures on
synthetically generated datasets. We show that our model is capable of
predicting in zero-shot the true generative SCMs, and as a by-product, of (i)
generating new dataset samples, and (ii) inferring intervened ones. Our
experiments demonstrate that our amortized procedure achieves performances on
par with SoTA methods trained specifically for each dataset on both in and
out-of-distribution problems. To the best of our knowledge, this is the first
time that SCMs are inferred in a zero-shot manner from observations, paving the
way for a paradigmatic shift towards the assimilation of causal knowledge
across datasets."
De-VertiFL: A Solution for Decentralized Vertical Federated Learning,cs.LG,Machine Learning,2024-10-08,"Federated Learning (FL), introduced in 2016, was designed to enhance data
privacy in collaborative model training environments. Among the FL paradigm,
horizontal FL, where clients share the same set of features but different data
samples, has been extensively studied in both centralized and decentralized
settings. In contrast, Vertical Federated Learning (VFL), which is crucial in
real-world decentralized scenarios where clients possess different, yet
sensitive, data about the same entity, remains underexplored. Thus, this work
introduces De-VertiFL, a novel solution for training models in a decentralized
VFL setting. De-VertiFL contributes by introducing a new network architecture
distribution, an innovative knowledge exchange scheme, and a distributed
federated training process. Specifically, De-VertiFL enables the sharing of
hidden layer outputs among federation clients, allowing participants to benefit
from intermediate computations, thereby improving learning efficiency.
De-VertiFL has been evaluated using a variety of well-known datasets, including
both image and tabular data, across binary and multiclass classification tasks.
The results demonstrate that De-VertiFL generally surpasses state-of-the-art
methods in F1-score performance, while maintaining a decentralized and
privacy-preserving framework."
$\textit{X}^2$-DFD: A framework for e${X}$plainable and e${X}$tendable Deepfake Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Detecting deepfakes has become an important task. Most existing detection
methods provide only real/fake predictions without offering
human-comprehensible explanations. Recent studies leveraging MLLMs for deepfake
detection have shown improvements in explainability. However, the performance
of pre-trained MLLMs (e.g., LLaVA) remains limited due to a lack of
understanding of their capabilities for this task and strategies to enhance
them. In this work, we empirically assess the strengths and weaknesses of MLLMs
specifically in deepfake detection via forgery features analysis. Building on
these assessments, we propose a novel framework called ${X}^2$-DFD, consisting
of three core modules. The first module, Model Feature Assessment (MFA),
measures the detection capabilities of forgery features intrinsic to MLLMs, and
gives a descending ranking of these features. The second module, Strong Feature
Strengthening (SFS), enhances the detection and explanation capabilities by
fine-tuning the MLLM on a dataset constructed based on the top-ranked features.
The third module, Weak Feature Supplementing (WFS), improves the fine-tuned
MLLM's capabilities on lower-ranked features by integrating external dedicated
deepfake detectors. To verify the effectiveness of this framework, we further
present a practical implementation, where an automated forgery features
generation, evaluation, and ranking procedure is designed for MFA module; an
automated generation procedure of the fine-tuning dataset containing real and
fake images with explanations based on top-ranked features is developed for SFS
model; an external conventional deepfake detector focusing on blending
artifact, which corresponds to a low detection capability in the pre-trained
MLLM, is integrated for WFS module. Experiments show that our approach enhances
both detection and explanation performance."
Learning AND-OR Templates for Professional Photograph Parsing and Guidance,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Since the development of photography art, many so-called ""templates"" have
been formed, namely visual styles summarized from a series of themed and
stylized photography works. In this paper, we propose to analysize and and
summarize these 'templates' in photography by learning composite templates of
photography images. We present a framework for learning a hierarchical
reconfigurable image template from photography images to learn and characterize
the ""templates"" used in these photography images. Using this method, we
measured the artistic quality of photography on the photos and conducted
photography guidance. In addition, we also utilized the ""templates"" for
guidance in several image generation tasks. Experimental results show that the
learned templates can well describe the photography techniques and styles,
whereas the proposed approach can assess the quality of photography images as
human being does."
Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Retrieval-Augmented Generation (RAG) is widely used to inject external
non-parametric knowledge into large language models (LLMs). Recent works
suggest that Knowledge Graphs (KGs) contain valuable external knowledge for
LLMs. Retrieving information from KGs differs from extracting it from document
sets. Most existing approaches seek to directly retrieve relevant subgraphs,
thereby eliminating the need for extensive SPARQL annotations, traditionally
required by semantic parsing methods. In this paper, we model the subgraph
retrieval task as a conditional generation task handled by small language
models. Specifically, we define a subgraph identifier as a sequence of
relations, each represented as a special token stored in the language models.
Our base generative subgraph retrieval model, consisting of only 220M
parameters, achieves competitive retrieval performance compared to
state-of-the-art models relying on 7B parameters, demonstrating that small
language models are capable of performing the subgraph retrieval task.
Furthermore, our largest 3B model, when plugged with an LLM reader, sets new
SOTA end-to-end performance on both the WebQSP and CWQ benchmarks. Our model
and data will be made available online: https://github.com/hwy9855/GSR."
Uncertainty estimation via ensembles of deep learning models and dropout layers for seismic traces,cs.LG,Machine Learning,2024-10-08,"Deep learning models have demonstrated remarkable success in various fields,
including seismology. However, one major challenge in deep learning is the
presence of mislabeled examples. Additionally, accurately estimating model
uncertainty is another challenge in machine learning. In this study, we develop
Convolutional Neural Networks (CNNs) to classify seismic waveforms based on
first-motion polarity. We trained multiple CNN models with different settings.
We also constructed ensembles of networks to estimate uncertainty. The results
showed that each training setting achieved satisfactory performances, with the
ensemble method outperforming individual networks in uncertainty estimation. We
observe that the uncertainty estimation ability of the ensembles of networks
can be enhanced using dropout layers. In addition, comparisons among different
training settings revealed that the use of dropout improved the robustness of
networks to mislabeled examples."
Optimizing the Training Schedule of Multilingual NMT using Reinforcement Learning,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Multilingual NMT is a viable solution for translating low-resource languages
(LRLs) when data from high-resource languages (HRLs) from the same language
family is available. However, the training schedule, i.e. the order of
presentation of languages, has an impact on the quality of such systems. Here,
in a many-to-one translation setting, we propose to apply two algorithms that
use reinforcement learning to optimize the training schedule of NMT: (1)
Teacher-Student Curriculum Learning and (2) Deep Q Network. The former uses an
exponentially smoothed estimate of the returns of each action based on the loss
on monolingual or multilingual development subsets, while the latter estimates
rewards using an additional neural network trained from the history of actions
selected in different states of the system, together with the rewards received.
On a 8-to-1 translation dataset with LRLs and HRLs, our second method improves
BLEU and COMET scores with respect to both random selection of monolingual
batches and shuffled multilingual batches, by adjusting the number of
presentations of LRL vs. HRL batches."
UnSeGArmaNet: Unsupervised Image Segmentation using Graph Neural Networks with Convolutional ARMA Filters,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"The data-hungry approach of supervised classification drives the interest of
the researchers toward unsupervised approaches, especially for problems such as
medical image segmentation, where labeled data are difficult to get. Motivated
by the recent success of Vision transformers (ViT) in various computer vision
tasks, we propose an unsupervised segmentation framework with a pre-trained
ViT. Moreover, by harnessing the graph structure inherent within the image, the
proposed method achieves a notable performance in segmentation, especially in
medical images. We further introduce a modularity-based loss function coupled
with an Auto-Regressive Moving Average (ARMA) filter to capture the inherent
graph topology within the image. Finally, we observe that employing Scaled
Exponential Linear Unit (SELU) and SILU (Swish) activation functions within the
proposed Graph Neural Network (GNN) architecture enhances the performance of
segmentation. The proposed method provides state-of-the-art performance (even
comparable to supervised methods) on benchmark image segmentation datasets such
as ECSSD, DUTS, and CUB, as well as challenging medical image segmentation
datasets such as KVASIR, CVC-ClinicDB, ISIC-2018. The github repository of the
code is available on \url{https://github.com/ksgr5566/UnSeGArmaNet}."
Continuous Contrastive Learning for Long-Tailed Semi-Supervised Recognition,cs.LG,Machine Learning,2024-10-08,"Long-tailed semi-supervised learning poses a significant challenge in
training models with limited labeled data exhibiting a long-tailed label
distribution. Current state-of-the-art LTSSL approaches heavily rely on
high-quality pseudo-labels for large-scale unlabeled data. However, these
methods often neglect the impact of representations learned by the neural
network and struggle with real-world unlabeled data, which typically follows a
different distribution than labeled data. This paper introduces a novel
probabilistic framework that unifies various recent proposals in long-tail
learning. Our framework derives the class-balanced contrastive loss through
Gaussian kernel density estimation. We introduce a continuous contrastive
learning method, CCL, extending our framework to unlabeled data using reliable
and smoothed pseudo-labels. By progressively estimating the underlying label
distribution and optimizing its alignment with model predictions, we tackle the
diverse distribution of unlabeled data in real-world scenarios. Extensive
experiments across multiple datasets with varying unlabeled data distributions
demonstrate that CCL consistently outperforms prior state-of-the-art methods,
achieving over 4% improvement on the ImageNet-127 dataset. Our source code is
available at https://github.com/zhouzihao11/CCL"
ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution,cs.AI,Artificial Intelligence,2024-10-08,"Robotic planning and execution in open-world environments is a complex
problem due to the vast state spaces and high variability of task embodiment.
Recent advances in perception algorithms, combined with Large Language Models
(LLMs) for planning, offer promising solutions to these challenges, as the
common sense reasoning capabilities of LLMs provide a strong heuristic for
efficiently searching the action space. However, prior work fails to address
the possibility of hallucinations from LLMs, which results in failures to
execute the planned actions largely due to logical fallacies at high- or
low-levels. To contend with automation failure due to such hallucinations, we
introduce ConceptAgent, a natural language-driven robotic platform designed for
task execution in unstructured environments. With a focus on scalability and
reliability of LLM-based planning in complex state and action spaces, we
present innovations designed to limit these shortcomings, including 1)
Predicate Grounding to prevent and recover from infeasible actions, and 2) an
embodied version of LLM-guided Monte Carlo Tree Search with self reflection. In
simulation experiments, ConceptAgent achieved a 19% task completion rate across
three room layouts and 30 easy level embodied tasks outperforming other
state-of-the-art LLM-driven reasoning baselines that scored 10.26% and 8.11% on
the same benchmark. Additionally, ablation studies on moderate to hard embodied
tasks revealed a 20% increase in task completion from the baseline agent to the
fully enhanced ConceptAgent, highlighting the individual and combined
contributions of Predicate Grounding and LLM-guided Tree Search to enable more
robust automation in complex state and action spaces."
RefineStyle: Dynamic Convolution Refinement for StyleGAN,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"In StyleGAN, convolution kernels are shaped by both static parameters shared
across images and dynamic modulation factors $w^+\in\mathcal{W}^+$ specific to
each image. Therefore, $\mathcal{W}^+$ space is often used for image inversion
and editing. However, pre-trained model struggles with synthesizing
out-of-domain images due to the limited capabilities of $\mathcal{W}^+$ and its
resultant kernels, necessitating full fine-tuning or adaptation through a
complex hypernetwork. This paper proposes an efficient refining strategy for
dynamic kernels. The key idea is to modify kernels by low-rank residuals,
learned from input image or domain guidance. These residuals are generated by
matrix multiplication between two sets of tokens with the same number, which
controls the complexity. We validate the refining scheme in image inversion and
domain adaptation. In the former task, we design grouped transformer blocks to
learn these token sets by one- or two-stage training. In the latter task, token
sets are directly optimized to support synthesis in the target domain while
preserving original content. Extensive experiments show that our method
achieves low distortions for image inversion and high quality for out-of-domain
editing."
Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning,cs.AI,Artificial Intelligence,2024-10-08,"Reinforcement learning (RL) has emerged as a pivotal technique for
fine-tuning large language models (LLMs) on specific tasks. However, prevailing
RL fine-tuning methods predominantly rely on PPO and its variants. Though these
algorithms are effective in general RL settings, they often exhibit suboptimal
performance and vulnerability to distribution collapse when applied to the
fine-tuning of LLMs. In this paper, we propose CORY, extending the RL
fine-tuning of LLMs to a sequential cooperative multi-agent reinforcement
learning framework, to leverage the inherent coevolution and emergent
capabilities of multi-agent systems. In CORY, the LLM to be fine-tuned is
initially duplicated into two autonomous agents: a pioneer and an observer. The
pioneer generates responses based on queries, while the observer generates
responses using both the queries and the pioneer's responses. The two agents
are trained together. During training, the agents exchange roles periodically,
fostering cooperation and coevolution between them. Experiments evaluate CORY's
performance by fine-tuning GPT-2 and Llama-2 under subjective and objective
reward functions on the IMDB Review and GSM8K datasets, respectively. Results
show that CORY outperforms PPO in terms of policy optimality, resistance to
distribution collapse, and training robustness, thereby underscoring its
potential as a superior methodology for refining LLMs in real-world
applications."
Decoding Decoded: Understanding Hyperparameter Effects in Open-Ended Text Generation,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Decoding strategies for large language models (LLMs) are a critical but often
underexplored aspect of text generation tasks. Since LLMs produce probability
distributions over the entire vocabulary, various decoding methods have been
developed to transform these probabilities into coherent and fluent text, each
with its own set of hyperparameters. In this study, we present a large-scale,
comprehensive analysis of how hyperparameter selection affects text quality in
open-ended text generation across multiple LLMs, datasets, and evaluation
metrics. Through an extensive sensitivity analysis, we provide practical
guidelines for hyperparameter tuning and demonstrate the substantial influence
of these choices on text quality. Using three established datasets, spanning
factual domains (e.g., news) and creative domains (e.g., fiction), we show that
hyperparameter tuning significantly impacts generation quality, though its
effects vary across models and tasks. We offer in-depth insights into these
effects, supported by both human evaluations and a synthesis of widely-used
automatic evaluation metrics."
Listen to the Patient: Enhancing Medical Dialogue Generation with Patient Hallucination Detection and Mitigation,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Medical dialogue systems aim to provide medical services through
patient-agent conversations. Previous methods typically regard patients as
ideal users, focusing mainly on common challenges in dialogue systems, while
neglecting the potential biases or misconceptions that might be introduced by
real patients, who are typically non-experts. This study investigates the
discrepancy between patients' expressions during medical consultations and
their actual health conditions, defined as patient hallucination. Such
phenomena often arise from patients' lack of knowledge and comprehension,
concerns, and anxieties, resulting in the transmission of inaccurate or wrong
information during consultations. To address this issue, we propose MedPH, a
Medical dialogue generation method for mitigating the problem of Patient
Hallucinations designed to detect and cope with hallucinations. MedPH
incorporates a detection method that utilizes one-dimensional structural
entropy over a temporal dialogue entity graph, and a mitigation strategy based
on hallucination-related information to guide patients in expressing their
actual conditions. Experimental results indicate the high effectiveness of
MedPH when compared to existing approaches in both medical entity prediction
and response generation tasks, while also demonstrating its effectiveness in
mitigating hallucinations within interactive scenarios."
TOWER: Tree Organized Weighting for Evaluating Complex Instructions,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Evaluating the ability of large language models (LLMs) to follow complex
human-written instructions is essential for their deployment in real-world
applications. While benchmarks like Chatbot Arena use human judges to assess
model performance, they are resource-intensive and time-consuming. Alternative
methods using LLMs as judges, such as AlpacaEval, MT Bench, WildBench, and
InFoBench offer improvements but still do not capture that certain complex
instruction aspects are more important than others to follow.
  To address this gap, we propose a novel evaluation metric, \textsc{TOWER},
that incorporates human-judged importance into the assessment of complex
instruction following. We show that human annotators agree with tree-based
representations of these complex instructions nearly as much as they agree with
other human annotators. We release tree-based annotations of the InFoBench
dataset and the corresponding evaluation code to facilitate future research."
The GDPR's Rules on Data Breaches: Analysing Their Rationales and Effects,cs.CR,Cryptography and Security,2024-10-08,"The General Data Protection Regulation (GDPR) requires an organisation that
suffers a data breach to notify the competent Data Protection Authority. The
organisation must also inform the relevant individuals, when a data breach
threatens their rights and freedoms. This paper focuses on the following
question: given the goals of the GDPR's data breach notification obligation,
and we assess the obligation in the light of those goals. We refer to insights
from information security and economics, and present them in a reader-friendly
way for lawyers. Our main conclusion is that the GDPR's data breach rules are
likely to contribute to the goals. For instance, the data breach notification
obligation can nudge organisations towards better security; such an obligation
enables regulators to perform their duties; and such an obligation improves
transparency and accountability. However, the paper also warns that we should
not have unrealistic expectations of the possibilities for people to protect
their interests after a data breach notice. Likewise, we should not have high
expectations of people switching to other service providers after receiving a
data breach notification. Lastly, the paper calls for Data Protection
Authorities to publish more information about reported data breaches. Such
information can help to analyse security threats."
Diversity-Rewarded CFG Distillation,cs.LG,Machine Learning,2024-10-08,"Generative models are transforming creative domains such as music generation,
with inference-time strategies like Classifier-Free Guidance (CFG) playing a
crucial role. However, CFG doubles inference cost while limiting originality
and diversity across generated contents. In this paper, we introduce
diversity-rewarded CFG distillation, a novel finetuning procedure that distills
the strengths of CFG while addressing its limitations. Our approach optimises
two training objectives: (1) a distillation objective, encouraging the model
alone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL
objective with a diversity reward, promoting the generation of diverse outputs
for a given prompt. By finetuning, we learn model weights with the ability to
generate high-quality and diverse outputs, without any inference overhead. This
also unlocks the potential of weight-based model merging strategies: by
interpolating between the weights of two models (the first focusing on quality,
the second on diversity), we can control the quality-diversity trade-off at
deployment time, and even further boost performance. We conduct extensive
experiments on the MusicLM (Agostinelli et al., 2023) text-to-music generative
model, where our approach surpasses CFG in terms of quality-diversity Pareto
optimality. According to human evaluators, our finetuned-then-merged model
generates samples with higher quality-diversity than the base model augmented
with CFG. Explore our generations at
https://google-research.github.io/seanet/musiclm/diverse_music/."
Scalable Mechanistic Neural Networks,cs.LG,Machine Learning,2024-10-08,"We propose Scalable Mechanistic Neural Network (S-MNN), an enhanced neural
network framework designed for scientific machine learning applications
involving long temporal sequences. By reformulating the original Mechanistic
Neural Network (MNN) (Pervez et al., 2024), we reduce the computational time
and space complexities from cubic and quadratic with respect to the sequence
length, respectively, to linear. This significant improvement enables efficient
modeling of long-term dynamics without sacrificing accuracy or
interpretability. Extensive experiments demonstrate that S-MNN matches the
original MNN in precision while substantially reducing computational resources.
Consequently, S-MNN can drop-in replace the original MNN in applications,
providing a practical and efficient tool for integrating mechanistic
bottlenecks into neural network models of complex dynamical systems."
Training-free LLM-generated Text Detection by Mining Token Probability Sequences,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Large language models (LLMs) have demonstrated remarkable capabilities in
generating high-quality texts across diverse domains. However, the potential
misuse of LLMs has raised significant concerns, underscoring the urgent need
for reliable detection of LLM-generated texts. Conventional training-based
detectors often struggle with generalization, particularly in cross-domain and
cross-model scenarios. In contrast, training-free methods, which focus on
inherent discrepancies through carefully designed statistical features, offer
improved generalization and interpretability. Despite this, existing
training-free detection methods typically rely on global text sequence
statistics, neglecting the modeling of local discriminative features, thereby
limiting their detection efficacy. In this work, we introduce a novel
training-free detector, termed \textbf{Lastde} that synergizes local and global
statistics for enhanced detection. For the first time, we introduce time series
analysis to LLM-generated text detection, capturing the temporal dynamics of
token probability sequences. By integrating these local statistics with global
ones, our detector reveals significant disparities between human and
LLM-generated texts. We also propose an efficient alternative,
\textbf{Lastde++} to enable real-time detection. Extensive experiments on six
datasets involving cross-domain, cross-model, and cross-lingual detection
scenarios, under both white-box and black-box settings, demonstrated that our
method consistently achieves state-of-the-art performance. Furthermore, our
approach exhibits greater robustness against paraphrasing attacks compared to
existing baseline methods."
Enforcing Interpretability in Time Series Transformers: A Concept Bottleneck Framework,cs.LG,Machine Learning,2024-10-08,"There has been a recent push of research on Transformer-based models for
long-term time series forecasting, even though they are inherently difficult to
interpret and explain. While there is a large body of work on interpretability
methods for various domains and architectures, the interpretability of
Transformer-based forecasting models remains largely unexplored. To address
this gap, we develop a framework based on Concept Bottleneck Models to enforce
interpretability of time series Transformers. We modify the training objective
to encourage a model to develop representations similar to predefined
interpretable concepts. In our experiments, we enforce similarity using
Centered Kernel Alignment, and the predefined concepts include time features
and an interpretable, autoregressive surrogate model (AR). We apply the
framework to the Autoformer model, and present an in-depth analysis for a
variety of benchmark tasks. We find that the model performance remains mostly
unaffected, while the model shows much improved interpretability. Additionally,
interpretable concepts become local, which makes the trained model easily
intervenable. As a proof of concept, we demonstrate a successful intervention
in the scenario of a time shift in the data, which eliminates the need to
retrain."
Provable Methods for Searching with an Imperfect Sensor,cs.RO,Robotics,2024-10-08,"Assume that a target is known to be present at an unknown point among a
finite set of locations in the plane. We search for it using a mobile robot
that has imperfect sensing capabilities. It takes time for the robot to move
between locations and search a location; we have a total time budget within
which to conduct the search. We study the problem of computing a search
path/strategy for the robot that maximizes the probability of detection of the
target. Considering non-uniform travel times between points (e.g., based on the
distance between them) is crucial for search and rescue applications; such
problems have been investigated to a limited extent due to their inherent
complexity. In this paper, we describe fast algorithms with performance
guarantees for this search problem and some variants, complement them with
complexity results, and perform experiments to observe their performance."
Contrastive Learning to Fine-Tune Feature Extraction Models for the Visual Cortex,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Predicting the neural response to natural images in the visual cortex
requires extracting relevant features from the images and relating those
feature to the observed responses. In this work, we optimize the feature
extraction in order to maximize the information shared between the image
features and the neural response across voxels in a given region of interest
(ROI) extracted from the BOLD signal measured by fMRI. We adapt contrastive
learning (CL) to fine-tune a convolutional neural network, which was pretrained
for image classification, such that a mapping of a given image's features are
more similar to the corresponding fMRI response than to the responses to other
images. We exploit the recently released Natural Scenes Dataset (Allen et al.,
2022) as organized for the Algonauts Project (Gifford et al., 2023), which
contains the high-resolution fMRI responses of eight subjects to tens of
thousands of naturalistic images. We show that CL fine-tuning creates feature
extraction models that enable higher encoding accuracy in early visual ROIs as
compared to both the pretrained network and a baseline approach that uses a
regression loss at the output of the network to tune it for fMRI response
encoding. We investigate inter-subject transfer of the CL fine-tuned models,
including subjects from another, lower-resolution dataset (Gong et al., 2023).
We also pool subjects for fine-tuning to further improve the encoding
performance. Finally, we examine the performance of the fine-tuned models on
common image classification tasks, explore the landscape of ROI-specific models
by applying dimensionality reduction on the Bhattacharya dissimilarity matrix
created using the predictions on those tasks (Mao et al., 2024), and
investigate lateralization of the processing for early visual ROIs using
salience maps of the classifiers built on the CL-tuned models."
Posets and Bounded Probabilities for Discovering Order-inducing Features in Event Knowledge Graphs,cs.LG,Machine Learning,2024-10-08,"Event knowledge graphs (EKG) extend the classical notion of a trace to
capture multiple, interacting views of a process execution. In this paper, we
tackle the open problem of automating EKG discovery from uncurated data through
a principled, probabilistic framing based on the outcome space resulting from
featured-derived partial orders on events. From this, we derive an EKG
discovery algorithm based upon statistical inference rather than an ad-hoc or
heuristic-based strategy, or relying on manual analysis from domain experts.
  This approach comes at the computational cost of exploring a large,
non-convex hypothesis space. In particular, solving the maximum likelihood term
involves counting the number of linear extensions of posets, which in general
is #P-complete. Fortunately, bound estimates suffice for model comparison, and
admit incorporation into a bespoke branch-and-bound algorithm. We show that the
posterior probability as defined is antitonic w.r.t. search depth for branching
rules that are monotonic w.r.t. model inclusion. This allows pruning of large
portions of the search space, which we show experimentally leads to rapid
convergence toward optimal solutions that are consistent with manually built
EKGs."
Hierarchical Matrix Completion for the Prediction of Properties of Binary Mixtures,cs.LG,Machine Learning,2024-10-08,"Predicting the thermodynamic properties of mixtures is crucial for process
design and optimization in chemical engineering. Machine learning (ML) methods
are gaining increasing attention in this field, but experimental data for
training are often scarce, which hampers their application. In this work, we
introduce a novel generic approach for improving data-driven models: inspired
by the ancient rule ""similia similibus solvuntur"", we lump components that
behave similarly into chemical classes and model them jointly in the first step
of a hierarchical approach. While the information on class affiliations can
stem in principle from any source, we demonstrate how classes can reproducibly
be defined based on mixture data alone by agglomerative clustering. The
information from this clustering step is then used as an informed prior for
fitting the individual data. We demonstrate the benefits of this approach by
applying it in connection with a matrix completion method (MCM) for predicting
isothermal activity coefficients at infinite dilution in binary mixtures. Using
clustering leads to significantly improved predictions compared to an MCM
without clustering. Furthermore, the chemical classes learned from the
clustering give exciting insights into what matters on the molecular level for
modeling given mixture properties."
AP-LDM: Attentive and Progressive Latent Diffusion Model for Training-Free High-Resolution Image Generation,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Latent diffusion models (LDMs), such as Stable Diffusion, often experience
significant structural distortions when directly generating high-resolution
(HR) images that exceed their original training resolutions. A straightforward
and cost-effective solution is to adapt pre-trained LDMs for HR image
generation; however, existing methods often suffer from poor image quality and
long inference time. In this paper, we propose an Attentive and Progressive LDM
(AP-LDM), a novel, training-free framework aimed at enhancing HR image quality
while accelerating the generation process. AP-LDM decomposes the denoising
process of LDMs into two stages: (i) attentive training-resolution denoising,
and (ii) progressive high-resolution denoising. The first stage generates a
latent representation of a higher-quality training-resolution image through the
proposed attentive guidance, which utilizes a novel parameter-free
self-attention mechanism to enhance the structural consistency. The second
stage progressively performs upsampling in pixel space, alleviating the severe
artifacts caused by latent space upsampling. Leveraging the effective
initialization from the first stage enables denoising at higher resolutions
with significantly fewer steps, enhancing overall efficiency. Extensive
experimental results demonstrate that AP-LDM significantly outperforms
state-of-the-art methods, delivering up to a 5x speedup in HR image generation,
thereby highlighting its substantial advantages for real-world applications.
Code is available at https://github.com/kmittle/AP-LDM."
Concurrent-Learning Based Relative Localization in Shape Formation of Robot Swarms,cs.RO,Robotics,2024-10-08,"In this paper, we address the shape formation problem for massive robot
swarms in environments where external localization systems are unavailable.
Achieving this task effectively with solely onboard measurements is still
scarcely explored and faces some practical challenges. To solve this
challenging problem, we propose the following novel results. Firstly, to
estimate the relative positions among neighboring robots, a concurrent-learning
based estimator is proposed. It relaxes the persistent excitation condition
required in the classical ones such as least-square estimator. Secondly, we
introduce a finite-time agreement protocol to determine the shape location.
This is achieved by estimating the relative position between each robot and a
randomly assigned seed robot. The initial position of the seed one marks the
shape location. Thirdly, based on the theoretical results of the relative
localization, a novel behavior-based control strategy is devised. This strategy
not only enables adaptive shape formation of large group of robots but also
enhances the observability of inter-robot relative localization. Numerical
simulation results are provided to verify the performance of our proposed
strategy compared to the state-of-the-art ones. Additionally, outdoor
experiments on real robots further demonstrate the practical effectiveness and
robustness of our methods."
Gaussian-Based and Outside-the-Box Runtime Monitoring Join Forces,cs.LG,Machine Learning,2024-10-08,"Since neural networks can make wrong predictions even with high confidence,
monitoring their behavior at runtime is important, especially in
safety-critical domains like autonomous driving. In this paper, we combine
ideas from previous monitoring approaches based on observing the activation
values of hidden neurons. In particular, we combine the Gaussian-based
approach, which observes whether the current value of each monitored neuron is
similar to typical values observed during training, and the Outside-the-Box
monitor, which creates clusters of the acceptable activation values, and, thus,
considers the correlations of the neurons' values. Our experiments evaluate the
achieved improvement."
Extracting Finite State Machines from Transformers,cs.LG,Machine Learning,2024-10-08,"Fueled by the popularity of the transformer architecture in deep learning,
several works have investigated what formal languages a transformer can learn.
Nonetheless, existing results remain hard to compare and a fine-grained
understanding of the trainability of transformers on regular languages is still
lacking. We investigate transformers trained on regular languages from a
mechanistic interpretability perspective. Using an extension of the $L^*$
algorithm, we extract Moore machines from transformers. We empirically find
tighter lower bounds on the trainability of transformers, when a finite number
of symbols determine the state. Additionally, our mechanistic insight allows us
to characterise the regular languages a one-layer transformer can learn with
good length generalisation. However, we also identify failure cases where the
determining symbols get misrecognised due to saturation of the attention
mechanism."
HyperDet: Generalizable Detection of Synthesized Images by Generating and Merging A Mixture of Hyper LoRAs,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"The emergence of diverse generative vision models has recently enabled the
synthesis of visually realistic images, underscoring the critical need for
effectively detecting these generated images from real photos. Despite advances
in this field, existing detection approaches often struggle to accurately
identify synthesized images generated by different generative models. In this
work, we introduce a novel and generalizable detection framework termed
HyperDet, which innovatively captures and integrates shared knowledge from a
collection of functionally distinct and lightweight expert detectors. HyperDet
leverages a large pretrained vision model to extract general detection features
while simultaneously capturing and enhancing task-specific features. To achieve
this, HyperDet first groups SRM filters into five distinct groups to
efficiently capture varying levels of pixel artifacts based on their different
functionality and complexity. Then, HyperDet utilizes a hypernetwork to
generate LoRA model weights with distinct embedding parameters. Finally, we
merge the LoRA networks to form an efficient model ensemble. Also, we propose a
novel objective function that balances the pixel and semantic artifacts
effectively. Extensive experiments on the UnivFD and Fake2M datasets
demonstrate the effectiveness of our approach, achieving state-of-the-art
performance. Moreover, our work paves a new way to establish generalizable
domain-specific fake image detectors based on pretrained large vision models."
Weighted Embeddings for Low-Dimensional Graph Representation,cs.LG,Machine Learning,2024-10-08,"Learning low-dimensional numerical representations from symbolic data, e.g.,
embedding the nodes of a graph into a geometric space, is an important concept
in machine learning. While embedding into Euclidean space is common, recent
observations indicate that hyperbolic geometry is better suited to represent
hierarchical information and heterogeneous data (e.g., graphs with a scale-free
degree distribution). Despite their potential for more accurate
representations, hyperbolic embeddings also have downsides like being more
difficult to compute and harder to use in downstream tasks.
  We propose embedding into a weighted space, which is closely related to
hyperbolic geometry but mathematically simpler. We provide the embedding
algorithm WEmbed and demonstrate, based on generated as well as over 2000
real-world graphs, that our weighted embeddings heavily outperform
state-of-the-art Euclidean embeddings for heterogeneous graphs while using
fewer dimensions. The running time of WEmbed and embedding quality for the
remaining instances is on par with state-of-the-art Euclidean embedders."
Block Induced Signature Generative Adversarial Network (BISGAN): Signature Spoofing Using GANs and Their Evaluation,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Deep learning is actively being used in biometrics to develop efficient
identification and verification systems. Handwritten signatures are a common
subset of biometric data for authentication purposes. Generative adversarial
networks (GANs) learn from original and forged signatures to generate forged
signatures. While most GAN techniques create a strong signature verifier, which
is the discriminator, there is a need to focus more on the quality of forgeries
generated by the generator model. This work focuses on creating a generator
that produces forged samples that achieve a benchmark in spoofing signature
verification systems. We use CycleGANs infused with Inception model-like blocks
with attention heads as the generator and a variation of the SigCNN model as
the base Discriminator. We train our model with a new technique that results in
80% to 100% success in signature spoofing. Additionally, we create a custom
evaluation technique to act as a goodness measure of the generated forgeries.
Our work advocates generator-focused GAN architectures for spoofing data
quality that aid in a better understanding of biometric data generation and
evaluation."
QERA: an Analytical Framework for Quantization Error Reconstruction,cs.LG,Machine Learning,2024-10-08,"he growing number of parameters and computational demands of large language
models (LLMs) present significant challenges for their efficient deployment.
Recently, there is an increasing interest in quantizing weights to extremely
low precision while offsetting the resulting error with low-rank,
high-precision error reconstruction terms. The combination of quantization and
low-rank approximation is now popular in both adapter-based,
parameter-efficient fine-tuning methods such as LoftQ and low-precision
inference techniques including ZeroQuant-V2. Usually, the low-rank terms are
calculated via the singular value decomposition (SVD) of the weight
quantization error, minimizing the Frobenius and spectral norms of the weight
approximation error. Recent methods like LQ-LoRA and LQER introduced
hand-crafted heuristics to minimize errors in layer outputs (activations)
rather than weights, resulting improved quantization results. However, these
heuristic methods lack an analytical solution to guide the design of
quantization error reconstruction terms. In this paper, we revisit this problem
and formulate an analytical framework, named Quantization Error Reconstruction
Analysis (QERA), and offer a closed-form solution to the problem. We show QERA
benefits both existing low-precision fine-tuning and inference methods -- QERA
achieves a fine-tuned accuracy gain of $\Delta_{\text{acc}}$ = 6.05% of 2-bit
RoBERTa-base on GLUE compared to LoftQ; and obtains $\Delta_{\text{acc}}$ =
2.97% higher post-training quantization accuracy of 4-bit Llama-3.1-70B on
average than ZeroQuant-V2 and $\Delta_{\text{ppl}}$ = - 0.28 lower perplexity
on WikiText2 than LQER."
Data Quality Issues in Vulnerability Detection Datasets,cs.CR,Cryptography and Security,2024-10-08,"Vulnerability detection is a crucial yet challenging task to identify
potential weaknesses in software for cyber security. Recently, deep learning
(DL) has made great progress in automating the detection process. Due to the
complex multi-layer structure and a large number of parameters, a DL model
requires massive labeled (vulnerable or secure) source code to gain knowledge
to effectively distinguish between vulnerable and secure code. In the
literature, many datasets have been created to train DL models for this
purpose. However, these datasets suffer from several issues that will lead to
low detection accuracy of DL models. In this paper, we define three critical
issues (i.e., data imbalance, low vulnerability coverage, biased vulnerability
distribution) that can significantly affect the model performance and three
secondary issues (i.e., errors in source code, mislabeling, noisy historical
data) that also affect the performance but can be addressed through a dedicated
pre-processing procedure. In addition, we conduct a study of 14 papers along
with 54 datasets for vulnerability detection to confirm these defined issues.
Furthermore, we discuss good practices to use existing datasets and to create
new ones."
Sparse Repellency for Shielded Generation in Text-to-image Diffusion Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"The increased adoption of diffusion models in text-to-image generation has
triggered concerns on their reliability. Such models are now closely
scrutinized under the lens of various metrics, notably calibration, fairness,
or compute efficiency. We focus in this work on two issues that arise when
deploying these models: a lack of diversity when prompting images, and a
tendency to recreate images from the training set. To solve both problems, we
propose a method that coaxes the sampled trajectories of pretrained diffusion
models to land on images that fall outside of a reference set. We achieve this
by adding repellency terms to the diffusion SDE throughout the generation
trajectory, which are triggered whenever the path is expected to land too
closely to an image in the shielded reference set. Our method is sparse in the
sense that these repellency terms are zero and inactive most of the time, and
even more so towards the end of the generation trajectory. Our method, named
SPELL for sparse repellency, can be used either with a static reference set
that contains protected images, or dynamically, by updating the set at each
timestep with the expected images concurrently generated within a batch. We
show that adding SPELL to popular diffusion models improves their diversity
while impacting their FID only marginally, and performs comparatively better
than other recent training-free diversity methods. We also demonstrate how
SPELL can ensure a shielded generation away from a very large set of protected
images by considering all 1.2M images from ImageNet as the protected set."
Jet Expansions of Residual Computation,cs.LG,Machine Learning,2024-10-08,"We introduce a framework for expanding residual computational graphs using
jets, operators that generalize truncated Taylor series. Our method provides a
systematic approach to disentangle contributions of different computational
paths to model predictions. In contrast to existing techniques such as
distillation, probing, or early decoding, our expansions rely solely on the
model itself and requires no data, training, or sampling from the model. We
demonstrate how our framework grounds and subsumes logit lens, reveals a
(super-)exponential path structure in the recursive residual depth and opens up
several applications. These include sketching a transformer large language
model with $n$-gram statistics extracted from its computations, and indexing
the models' levels of toxicity knowledge. Our approach enables data-free
analysis of residual computation for model interpretability, development, and
evaluation."
Can Language Models Induce Grammatical Knowledge from Indirect Evidence?,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"What kinds of and how much data is necessary for language models to induce
grammatical knowledge to judge sentence acceptability? Recent language models
still have much room for improvement in their data efficiency compared to
humans. This paper investigates whether language models efficiently use
indirect data (indirect evidence), from which they infer sentence
acceptability. In contrast, humans use indirect evidence efficiently, which is
considered one of the inductive biases contributing to efficient language
acquisition. To explore this question, we introduce the Wug InDirect Evidence
Test (WIDET), a dataset consisting of training instances inserted into the
pre-training data and evaluation instances. We inject synthetic instances with
newly coined wug words into pretraining data and explore the model's behavior
on evaluation data that assesses grammatical acceptability regarding those
words. We prepare the injected instances by varying their levels of
indirectness and quantity. Our experiments surprisingly show that language
models do not induce grammatical knowledge even after repeated exposure to
instances with the same structure but differing only in lexical items from
evaluation instances in certain language phenomena. Our findings suggest a
potential direction for future research: developing models that use latent
indirect evidence to induce grammatical knowledge."
QT-DoG: Quantization-aware Training for Domain Generalization,cs.LG,Machine Learning,2024-10-08,"Domain Generalization (DG) aims to train models that perform well not only on
the training (source) domains but also on novel, unseen target data
distributions. A key challenge in DG is preventing overfitting to source
domains, which can be mitigated by finding flatter minima in the loss
landscape. In this work, we propose Quantization-aware Training for Domain
Generalization (QT-DoG) and demonstrate that weight quantization effectively
leads to flatter minima in the loss landscape, thereby enhancing domain
generalization. Unlike traditional quantization methods focused on model
compression, QT-DoG exploits quantization as an implicit regularizer by
inducing noise in model weights, guiding the optimization process toward
flatter minima that are less sensitive to perturbations and overfitting. We
provide both theoretical insights and empirical evidence demonstrating that
quantization inherently encourages flatter minima, leading to better
generalization across domains. Moreover, with the benefit of reducing the model
size through quantization, we demonstrate that an ensemble of multiple
quantized models further yields superior accuracy than the state-of-the-art DG
approaches with no computational or memory overheads. Our extensive experiments
demonstrate that QT-DoG generalizes across various datasets, architectures, and
quantization algorithms, and can be combined with other DG methods,
establishing its versatility and robustness."
Unveiling Transformer Perception by Exploring Input Manifolds,cs.LG,Machine Learning,2024-10-08,"This paper introduces a general method for the exploration of equivalence
classes in the input space of Transformer models. The proposed approach is
based on sound mathematical theory which describes the internal layers of a
Transformer architecture as sequential deformations of the input manifold.
Using eigendecomposition of the pullback of the distance metric defined on the
output space through the Jacobian of the model, we are able to reconstruct
equivalence classes in the input space and navigate across them. We illustrate
how this method can be used as a powerful tool for investigating how a
Transformer sees the input space, facilitating local and task-agnostic
explainability in Computer Vision and Natural Language Processing tasks."
SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting,cs.RO,Robotics,2024-10-08,"Many recent developments for robots to represent environments have focused on
photorealistic reconstructions. This paper particularly focuses on generating
sequences of images from the photorealistic Gaussian Splatting models, that
match instructions that are given by user-inputted language. We contribute a
novel framework, SplaTraj, which formulates the generation of images within
photorealistic environment representations as a continuous-time trajectory
optimization problem. Costs are designed so that a camera following the
trajectory poses will smoothly traverse through the environment and render the
specified spatial information in a photogenic manner. This is achieved by
querying a photorealistic representation with language embedding to isolate
regions that correspond to the user-specified inputs. These regions are then
projected to the camera's view as it moves over time and a cost is constructed.
We can then apply gradient-based optimization and differentiate through the
rendering to optimize the trajectory for the defined cost. The resulting
trajectory moves to photogenically view each of the specified objects. We
empirically evaluate our approach on a suite of environments and instructions,
and demonstrate the quality of generated image sequences."
"Sitting, Standing and Walking Control of the Series-Parallel Hybrid Recupera-Reha Exoskeleton",cs.RO,Robotics,2024-10-08,"This paper presents advancements in the functionalities of the Recupera-Reha
lower extremity exoskeleton robot. The exoskeleton features a series-parallel
hybrid design characterized by multiple kinematic loops resulting in 148
degrees of freedom in its spanning tree and 102 independent loop closure
constraints, which poses significant challenges for modeling and control. To
address these challenges, we applied an optimal control approach to generate
feasible trajectories such as sitting, standing, and static walking, and tested
these trajectories on the exoskeleton robot. Our method efficiently solves the
optimal control problem using a serial abstraction of the model to generate
trajectories. It then utilizes the full series-parallel hybrid model, which
takes all the kinematic loop constraints into account to generate the final
actuator commands. The experimental results demonstrate the effectiveness of
our approach in generating the desired motions for the exoskeleton."
Motion Forecasting in Continuous Driving,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Motion forecasting for agents in autonomous driving is highly challenging due
to the numerous possibilities for each agent's next action and their complex
interactions in space and time. In real applications, motion forecasting takes
place repeatedly and continuously as the self-driving car moves. However,
existing forecasting methods typically process each driving scene within a
certain range independently, totally ignoring the situational and contextual
relationships between successive driving scenes. This significantly simplifies
the forecasting task, making the solutions suboptimal and inefficient to use in
practice. To address this fundamental limitation, we propose a novel motion
forecasting framework for continuous driving, named RealMotion. It comprises
two integral streams both at the scene level: (1) The scene context stream
progressively accumulates historical scene information until the present
moment, capturing temporal interactive relationships among scene elements. (2)
The agent trajectory stream optimizes current forecasting by sequentially
relaying past predictions. Besides, a data reorganization strategy is
introduced to narrow the gap between existing benchmarks and real-world
applications, consistent with our network. These approaches enable exploiting
more broadly the situational and progressive insights of dynamic motion across
space and time. Extensive experiments on Argoverse series with different
settings demonstrate that our RealMotion achieves state-of-the-art performance,
along with the advantage of efficient real-world inference. The source code
will be available at https://github.com/fudan-zvg/RealMotion."
Is the MMI Criterion Necessary for Interpretability? Degenerating Non-causal Features to Plain Noise for Self-Rationalization,cs.LG,Machine Learning,2024-10-08,"An important line of research in the field of explainability is to extract a
small subset of crucial rationales from the full input. The most widely used
criterion for rationale extraction is the maximum mutual information (MMI)
criterion. However, in certain datasets, there are spurious features
non-causally correlated with the label and also get high mutual information,
complicating the loss landscape of MMI. Although some penalty-based methods
have been developed to penalize the spurious features (e.g., invariance
penalty, intervention penalty, etc) to help MMI work better, these are merely
remedial measures. In the optimization objectives of these methods, spurious
features are still distinguished from plain noise, which hinders the discovery
of causal rationales. This paper aims to develop a new criterion that treats
spurious features as plain noise, allowing the model to work on datasets rich
in spurious features as if it were working on clean datasets, thereby making
rationale extraction easier. We theoretically observe that removing either
plain noise or spurious features from the input does not alter the conditional
distribution of the remaining components relative to the task label. However,
significant changes in the conditional distribution occur only when causal
features are eliminated. Based on this discovery, the paper proposes a
criterion for \textbf{M}aximizing the \textbf{R}emaining \textbf{D}iscrepancy
(MRD). Experiments on six widely used datasets show that our MRD criterion
improves rationale quality (measured by the overlap with human-annotated
rationales) by up to $10.4\%$ as compared to several recent competitive MMI
variants. Code: \url{https://github.com/jugechengzi/Rationalization-MRD}."
"AIVIO: Closed-loop, Object-relative Navigation of UAVs with AI-aided Visual Inertial Odometry",cs.RO,Robotics,2024-10-08,"Object-relative mobile robot navigation is essential for a variety of tasks,
e.g. autonomous critical infrastructure inspection, but requires the capability
to extract semantic information about the objects of interest from raw sensory
data. While deep learning-based (DL) methods excel at inferring semantic object
information from images, such as class and relative 6 degree of freedom (6-DoF)
pose, they are computationally demanding and thus often not suitable for
payload constrained mobile robots. In this letter we present a real-time
capable unmanned aerial vehicle (UAV) system for object-relative, closed-loop
navigation with a minimal sensor configuration consisting of an inertial
measurement unit (IMU) and RGB camera. Utilizing a DL-based object pose
estimator, solely trained on synthetic data and optimized for companion board
deployment, the object-relative pose measurements are fused with the IMU data
to perform object-relative localization. We conduct multiple real-world
experiments to validate the performance of our system for the challenging use
case of power pole inspection. An example closed-loop flight is presented in
the supplementary video."
Aria: An Open Multimodal Native Mixture-of-Experts Model,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Information comes in diverse modalities. Multimodal native AI models are
essential to integrate real-world information and deliver comprehensive
understanding. While proprietary multimodal native models exist, their lack of
openness imposes obstacles for adoptions, let alone adaptations. To fill this
gap, we introduce Aria, an open multimodal native model with best-in-class
performance across a wide range of multimodal, language, and coding tasks. Aria
is a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual
token and text token, respectively. It outperforms Pixtral-12B and
Llama3.2-11B, and is competitive against the best proprietary models on various
multimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline,
which progressively equips the model with strong capabilities in language
understanding, multimodal understanding, long context window, and instruction
following. We open-source the model weights along with a codebase that
facilitates easy adoptions and adaptations of Aria in real-world applications."
Vector Grimoire: Codebook-based Shape Generation under Raster Image Supervision,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Scalable Vector Graphics (SVG) is a popular format on the web and in the
design industry. However, despite the great strides made in generative
modeling, SVG has remained underexplored due to the discrete and complex nature
of such data. We introduce GRIMOIRE, a text-guided SVG generative model that is
comprised of two modules: A Visual Shape Quantizer (VSQ) learns to map raster
images onto a discrete codebook by reconstructing them as vector shapes, and an
Auto-Regressive Transformer (ART) models the joint probability distribution
over shape tokens, positions and textual descriptions, allowing us to generate
vector graphics from natural language. Unlike existing models that require
direct supervision from SVG data, GRIMOIRE learns shape image patches using
only raster image supervision which opens up vector generative modeling to
significantly more data. We demonstrate the effectiveness of our method by
fitting GRIMOIRE for closed filled shapes on the MNIST and for outline strokes
on icon and font data, surpassing previous image-supervised methods in
generative quality and vector-supervised approach in flexibility."
Utilizing Lyapunov Exponents in designing deep neural networks,cs.LG,Machine Learning,2024-10-08,"Training large deep neural networks is resource intensive. This study
investigates whether Lyapunov exponents can accelerate this process by aiding
in the selection of hyperparameters. To study this I formulate an optimization
problem using neural networks with different activation functions in the hidden
layers. By initializing model weights with different random seeds, I calculate
the Lyapunov exponent while performing traditional gradient descent on these
model weights. The findings demonstrate that variations in the learning rate
can induce chaotic changes in model weights. I also show that activation
functions with more negative Lyapunov exponents exhibit better convergence
properties. Additionally, the study also demonstrates that Lyapunov exponents
can be utilized to select effective initial model weights for deep neural
networks, potentially enhancing the optimization process."
Asynchronous Stochastic Gradient Descent with Decoupled Backpropagation and Layer-Wise Updates,cs.LG,Machine Learning,2024-10-08,"The increasing size of deep learning models has created the need for more
efficient alternatives to the standard error backpropagation algorithm, that
make better use of asynchronous, parallel and distributed computing. One major
shortcoming of backpropagation is the interlocking between the forward phase of
the algorithm, which computes a global loss, and the backward phase where the
loss is backpropagated through all layers to compute the gradients, which are
used to update the network parameters. To address this problem, we propose a
method that parallelises SGD updates across the layers of a model by
asynchronously updating them from multiple threads. Furthermore, since we
observe that the forward pass is often much faster than the backward pass, we
use separate threads for the forward and backward pass calculations, which
allows us to use a higher ratio of forward to backward threads than the usual
1:1 ratio, reducing the overall staleness of the parameters. Thus, our approach
performs asynchronous stochastic gradient descent using separate threads for
the loss (forward) and gradient (backward) computations and performs layer-wise
partial updates to parameters in a distributed way. We show that this approach
yields close to state-of-the-art results while running up to 2.97x faster than
Hogwild! scaled on multiple devices (Locally-Partitioned-Asynchronous-Parallel
SGD). We theoretically prove the convergence of the algorithm using a novel
theoretical framework based on stochastic differential equations and the drift
diffusion process, by modeling the asynchronous parameter updates as a
stochastic process."
Are Minimal Radial Distortion Solvers Necessary for Relative Pose Estimation?,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Estimating the relative pose between two cameras is a fundamental step in
many applications such as Structure-from-Motion. The common approach to
relative pose estimation is to apply a minimal solver inside a RANSAC loop.
Highly efficient solvers exist for pinhole cameras. Yet, (nearly) all cameras
exhibit radial distortion. Not modeling radial distortion leads to
(significantly) worse results. However, minimal radial distortion solvers are
significantly more complex than pinhole solvers, both in terms of run-time and
implementation efforts. This paper compares radial distortion solvers with a
simple-to-implement approach that combines an efficient pinhole solver with
sampled radial distortion parameters. Extensive experiments on multiple
datasets and RANSAC variants show that this simple approach performs similarly
or better than the most accurate minimal distortion solvers at faster run-times
while being significantly more accurate than faster non-minimal solvers. We
clearly show that complex radial distortion solvers are not necessary in
practice. Code and benchmark are available at https://github.com/kocurvik/rd."
Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Retrieval-augmented generation (RAG) empowers large language models (LLMs) to
utilize external knowledge sources. The increasing capacity of LLMs to process
longer input sequences opens up avenues for providing more retrieved
information, to potentially enhance the quality of generated outputs. It is
plausible to assume that a larger retrieval set would contain more relevant
information (higher recall), that might result in improved performance.
However, our empirical findings demonstrate that for many long-context LLMs,
the quality of generated output initially improves first, but then subsequently
declines as the number of retrieved passages increases. This paper investigates
this phenomenon, identifying the detrimental impact of retrieved ""hard
negatives"" as a key contributor. To mitigate this and enhance the robustness of
long-context LLM-based RAG, we propose both training-free and training-based
approaches. We first showcase the effectiveness of retrieval reordering as a
simple yet powerful training-free optimization. Furthermore, we explore
training-based methods, specifically RAG-specific implicit LLM fine-tuning and
RAG-oriented fine-tuning with intermediate reasoning, demonstrating their
capacity for substantial performance gains. Finally, we conduct a systematic
analysis of design choices for these training-based methods, including data
distribution, retriever selection, and training context length."
DeMo: Decoupling Motion Forecasting into Directional Intentions and Dynamic States,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Accurate motion forecasting for traffic agents is crucial for ensuring the
safety and efficiency of autonomous driving systems in dynamically changing
environments. Mainstream methods adopt a one-query-one-trajectory paradigm,
where each query corresponds to a unique trajectory for predicting multi-modal
trajectories. While straightforward and effective, the absence of detailed
representation of future trajectories may yield suboptimal outcomes, given that
the agent states dynamically evolve over time. To address this problem, we
introduce DeMo, a framework that decouples multi-modal trajectory queries into
two types: mode queries capturing distinct directional intentions and state
queries tracking the agent's dynamic states over time. By leveraging this
format, we separately optimize the multi-modality and dynamic evolutionary
properties of trajectories. Subsequently, the mode and state queries are
integrated to obtain a comprehensive and detailed representation of the
trajectories. To achieve these operations, we additionally introduce combined
Attention and Mamba techniques for global information aggregation and state
sequence modeling, leveraging their respective strengths. Extensive experiments
on both the Argoverse 2 and nuScenes benchmarks demonstrate that our DeMo
achieves state-of-the-art performance in motion forecasting."
"Generalizing to any diverse distribution: uniformity, gentle finetuning and rebalancing",cs.LG,Machine Learning,2024-10-08,"As training datasets grow larger, we aspire to develop models that generalize
well to any diverse test distribution, even if the latter deviates
significantly from the training data. Various approaches like domain
adaptation, domain generalization, and robust optimization attempt to address
the out-of-distribution challenge by posing assumptions about the relation
between training and test distribution. Differently, we adopt a more
conservative perspective by accounting for the worst-case error across all
sufficiently diverse test distributions within a known domain. Our first
finding is that training on a uniform distribution over this domain is optimal.
We also interrogate practical remedies when uniform samples are unavailable by
considering methods for mitigating non-uniformity through finetuning and
rebalancing. Our theory provides a mathematical grounding for previous
observations on the role of entropy and rebalancing for o.o.d. generalization
and foundation model training. We also provide new empirical evidence across
tasks involving o.o.d. shifts which illustrate the broad applicability of our
perspective."
ConML: A Universal Meta-Learning Framework with Task-Level Contrastive Learning,cs.LG,Machine Learning,2024-10-08,"Meta-learning enables learning systems to adapt quickly to new tasks, similar
to humans. To emulate this human-like rapid learning and enhance alignment and
discrimination abilities, we propose ConML, a universal meta-learning framework
that can be applied to various meta-learning algorithms without relying on
specific model architectures nor target models. The core of ConML is task-level
contrastive learning, which extends contrastive learning from the
representation space in unsupervised learning to the model space in
meta-learning. By leveraging task identity as an additional supervision signal
during meta-training, we contrast the outputs of the meta-learner in the model
space, minimizing inner-task distance (between models trained on different
subsets of the same task) and maximizing inter-task distance (between models
from different tasks). We demonstrate that ConML integrates seamlessly with
optimization-based, metric-based, and amortization-based meta-learning
algorithms, as well as in-context learning, resulting in performance
improvements across diverse few-shot learning tasks."
PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with End-to-End Sparse Sampling,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Document understanding is a challenging task to process and comprehend large
amounts of textual and visual information. Recent advances in Large Language
Models (LLMs) have significantly improved the performance of this task.
However, existing methods typically focus on either plain text or a limited
number of document images, struggling to handle long PDF documents with
interleaved text and images, especially in academic papers. In this paper, we
introduce PDF-WuKong, a multimodal large language model (MLLM) which is
designed to enhance multimodal question-answering (QA) for long PDF documents.
PDF-WuKong incorporates a sparse sampler that operates on both text and image
representations, significantly improving the efficiency and capability of the
MLLM. The sparse sampler is integrated with the MLLM's image encoder and
selects the paragraphs or diagrams most pertinent to user queries for
processing by the language model. To effectively train and evaluate our model,
we construct PaperPDF, a dataset consisting of a broad collection of academic
papers sourced from arXiv, multiple strategies are proposed to generate
automatically 1M QA pairs along with their corresponding evidence sources.
Experimental results demonstrate the superiority and high efficiency of our
approach over other models on the task of long multimodal PDF understanding,
surpassing proprietary products by an average of 8.6% on F1. Our code and
dataset will be released at https://github.com/yh-hust/PDF-Wukong."
Deep neural network-based detection of counterfeit products from smartphone images,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Counterfeit products such as drugs and vaccines as well as luxury items such
as high-fashion handbags, watches, jewelry, garments, and cosmetics, represent
significant direct losses of revenue to legitimate manufacturers and vendors,
as well as indirect costs to societies at large. We present the world's first
purely computer-vision-based system to combat such counterfeiting-one that does
not require special security tags or other alterations to the products or
modifications to supply chain tracking. Our deep neural network system shows
high accuracy on branded garments from our first manufacturer tested (99.71%
after 3.06% rejections) using images captured under natural, weakly controlled
conditions, such as in retail stores, customs checkpoints, warehouses, and
outdoors. Our system, suitably transfer trained on a small number of fake and
genuine articles, should find application in additional product categories as
well, for example fashion accessories, perfume boxes, medicines, and more."
FLOPS: Forward Learning with OPtimal Sampling,cs.LG,Machine Learning,2024-10-08,"Given the limitations of backpropagation, perturbation-based gradient
computation methods have recently gained focus for learning with only forward
passes, also referred to as queries. Conventional forward learning consumes
enormous queries on each data point for accurate gradient estimation through
Monte Carlo sampling, which hinders the scalability of those algorithms.
However, not all data points deserve equal queries for gradient estimation. In
this paper, we study the problem of improving the forward learning efficiency
from a novel perspective: how to reduce the gradient estimation variance with
minimum cost? For this, we propose to allocate the optimal number of queries
over each data in one batch during training to achieve a good balance between
estimation accuracy and computational efficiency. Specifically, with a
simplified proxy objective and a reparameterization technique, we derive a
novel plug-and-play query allocator with minimal parameters. Theoretical
results are carried out to verify its optimality. We conduct extensive
experiments for fine-tuning Vision Transformers on various datasets and further
deploy the allocator to two black-box applications: prompt tuning and
multimodal alignment for foundation models. All findings demonstrate that our
proposed allocator significantly enhances the scalability of forward-learning
algorithms, paving the way for real-world applications."
STNet: Deep Audio-Visual Fusion Network for Robust Speaker Tracking,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Audio-visual speaker tracking aims to determine the location of human targets
in a scene using signals captured by a multi-sensor platform, whose accuracy
and robustness can be improved by multi-modal fusion methods. Recently, several
fusion methods have been proposed to model the correlation in multiple
modalities. However, for the speaker tracking problem, the cross-modal
interaction between audio and visual signals hasn't been well exploited. To
this end, we present a novel Speaker Tracking Network (STNet) with a deep
audio-visual fusion model in this work. We design a visual-guided acoustic
measurement method to fuse heterogeneous cues in a unified localization space,
which employs visual observations via a camera model to construct the enhanced
acoustic map. For feature fusion, a cross-modal attention module is adopted to
jointly model multi-modal contexts and interactions. The correlated information
between audio and visual features is further interacted in the fusion model.
Moreover, the STNet-based tracker is applied to multi-speaker cases by a
quality-aware module, which evaluates the reliability of multi-modal
observations to achieve robust tracking in complex scenarios. Experiments on
the AV16.3 and CAV3D datasets show that the proposed STNet-based tracker
outperforms uni-modal methods and state-of-the-art audio-visual speaker
trackers."
Training-Free Open-Ended Object Detection and Segmentation via Attention as Prompts,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Existing perception models achieve great success by learning from large
amounts of labeled data, but they still struggle with open-world scenarios. To
alleviate this issue, researchers introduce open-set perception tasks to detect
or segment unseen objects in the training set. However, these models require
predefined object categories as inputs during inference, which are not
available in real-world scenarios. Recently, researchers pose a new and more
practical problem, \textit{i.e.}, open-ended object detection, which discovers
unseen objects without any object categories as inputs. In this paper, we
present VL-SAM, a training-free framework that combines the generalized object
recognition model (\textit{i.e.,} Vision-Language Model) with the generalized
object localization model (\textit{i.e.,} Segment-Anything Model), to address
the open-ended object detection and segmentation task. Without additional
training, we connect these two generalized models with attention maps as the
prompts. Specifically, we design an attention map generation module by
employing head aggregation and a regularized attention flow to aggregate and
propagate attention maps across all heads and layers in VLM, yielding
high-quality attention maps. Then, we iteratively sample positive and negative
points from the attention maps with a prompt generation module and send the
sampled points to SAM to segment corresponding objects. Experimental results on
the long-tail instance segmentation dataset (LVIS) show that our method
surpasses the previous open-ended method on the object detection task and can
provide additional instance segmentation masks. Besides, VL-SAM achieves
favorable performance on the corner case object detection dataset (CODA),
demonstrating the effectiveness of VL-SAM in real-world applications. Moreover,
VL-SAM exhibits good model generalization that can incorporate various VLMs and
SAMs."
Pyramidal Flow Matching for Efficient Video Generative Modeling,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Video generation requires modeling a vast spatiotemporal space, which demands
significant computational resources and data usage. To reduce the complexity,
the prevailing approaches employ a cascaded architecture to avoid direct
training with full resolution. Despite reducing computational demands, the
separate optimization of each sub-stage hinders knowledge sharing and
sacrifices flexibility. This work introduces a unified pyramidal flow matching
algorithm. It reinterprets the original denoising trajectory as a series of
pyramid stages, where only the final stage operates at the full resolution,
thereby enabling more efficient video generative modeling. Through our
sophisticated design, the flows of different pyramid stages can be interlinked
to maintain continuity. Moreover, we craft autoregressive video generation with
a temporal pyramid to compress the full-resolution history. The entire
framework can be optimized in an end-to-end manner and with a single unified
Diffusion Transformer (DiT). Extensive experiments demonstrate that our method
supports generating high-quality 5-second (up to 10-second) videos at 768p
resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models
will be open-sourced at https://pyramid-flow.github.io."
Active Evaluation Acquisition for Efficient LLM Benchmarking,cs.LG,Machine Learning,2024-10-08,"As large language models (LLMs) become increasingly versatile, numerous large
scale benchmarks have been developed to thoroughly assess their capabilities.
These benchmarks typically consist of diverse datasets and prompts to evaluate
different aspects of LLM performance. However, comprehensive evaluations on
hundreds or thousands of prompts incur tremendous costs in terms of
computation, money, and time. In this work, we investigate strategies to
improve evaluation efficiency by selecting a subset of examples from each
benchmark using a learned policy. Our approach models the dependencies across
test examples, allowing accurate prediction of the evaluation outcomes for the
remaining examples based on the outcomes of the selected ones. Consequently, we
only need to acquire the actual evaluation outcomes for the selected subset. We
rigorously explore various subset selection policies and introduce a novel
RL-based policy that leverages the captured dependencies. Empirical results
demonstrate that our approach significantly reduces the number of evaluation
prompts required while maintaining accurate performance estimates compared to
previous methods."
Hyper Adversarial Tuning for Boosting Adversarial Robustness of Pretrained Large Vision Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Large vision models have been found vulnerable to adversarial examples,
emphasizing the need for enhancing their adversarial robustness. While
adversarial training is an effective defense for deep convolutional models, it
often faces scalability issues with large vision models due to high
computational costs. Recent approaches propose robust fine-tuning methods, such
as adversarial tuning of low-rank adaptation (LoRA) in large vision models, but
they still struggle to match the accuracy of full parameter adversarial
fine-tuning. The integration of various defense mechanisms offers a promising
approach to enhancing the robustness of large vision models, yet this paradigm
remains underexplored. To address this, we propose hyper adversarial tuning
(HyperAT), which leverages shared defensive knowledge among different methods
to improve model robustness efficiently and effectively simultaneously.
Specifically, adversarial tuning of each defense method is formulated as a
learning task, and a hypernetwork generates LoRA specific to this defense.
Then, a random sampling and tuning strategy is proposed to extract and
facilitate the defensive knowledge transfer between different defenses.
Finally, diverse LoRAs are merged to enhance the adversarial robustness.
Experiments on various datasets and model architectures demonstrate that
HyperAT significantly enhances the adversarial robustness of pretrained large
vision models without excessive computational overhead, establishing a new
state-of-the-art benchmark."
Single Point-Based Distributed Zeroth-Order Optimization with a Non-Convex Stochastic Objective Function,cs.LG,Machine Learning,2024-10-08,"Zero-order (ZO) optimization is a powerful tool for dealing with realistic
constraints. On the other hand, the gradient-tracking (GT) technique proved to
be an efficient method for distributed optimization aiming to achieve
consensus. However, it is a first-order (FO) method that requires knowledge of
the gradient, which is not always possible in practice. In this work, we
introduce a zero-order distributed optimization method based on a one-point
estimate of the gradient tracking technique. We prove that this new technique
converges with a single noisy function query at a time in the non-convex
setting. We then establish a convergence rate of $O(\frac{1}{\sqrt[3]{K}})$
after a number of iterations K, which competes with that of
$O(\frac{1}{\sqrt[4]{K}})$ of its centralized counterparts. Finally, a
numerical example validates our theoretical results."
TouchInsight: Uncertainty-aware Rapid Touch and Text Input for Mixed Reality from Egocentric Vision,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"While passive surfaces offer numerous benefits for interaction in mixed
reality, reliably detecting touch input solely from head-mounted cameras has
been a long-standing challenge. Camera specifics, hand self-occlusion, and
rapid movements of both head and fingers introduce considerable uncertainty
about the exact location of touch events. Existing methods have thus not been
capable of achieving the performance needed for robust interaction. In this
paper, we present a real-time pipeline that detects touch input from all ten
fingers on any physical surface, purely based on egocentric hand tracking. Our
method TouchInsight comprises a neural network to predict the moment of a touch
event, the finger making contact, and the touch location. TouchInsight
represents locations through a bivariate Gaussian distribution to account for
uncertainties due to sensing inaccuracies, which we resolve through contextual
priors to accurately infer intended user input. We first evaluated our method
offline and found that it locates input events with a mean error of 6.3 mm, and
accurately detects touch events (F1=0.99) and identifies the finger used
(F1=0.96). In an online evaluation, we then demonstrate the effectiveness of
our approach for a core application of dexterous touch input: two-handed text
entry. In our study, participants typed 37.0 words per minute with an
uncorrected error rate of 2.9% on average."
EMMA: Empowering Multi-modal Mamba with Structural and Hierarchical Alignment,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Mamba-based architectures have shown to be a promising new direction for deep
learning models owing to their competitive performance and sub-quadratic
deployment speed. However, current Mamba multi-modal large language models
(MLLM) are insufficient in extracting visual features, leading to imbalanced
cross-modal alignment between visual and textural latents, negatively impacting
performance on multi-modal tasks. In this work, we propose Empowering
Multi-modal Mamba with Structural and Hierarchical Alignment (EMMA), which
enables the MLLM to extract fine-grained visual information. Specifically, we
propose a pixel-wise alignment module to autoregressively optimize the learning
and processing of spatial image-level features along with textual tokens,
enabling structural alignment at the image level. In addition, to prevent the
degradation of visual information during the cross-model alignment process, we
propose a multi-scale feature fusion (MFF) module to combine multi-scale visual
features from intermediate layers, enabling hierarchical alignment at the
feature level. Extensive experiments are conducted across a variety of
multi-modal benchmarks. Our model shows lower latency than other Mamba-based
MLLMs and is nearly four times faster than transformer-based MLLMs of similar
scale during inference. Due to better cross-modal alignment, our model exhibits
lower degrees of hallucination and enhanced sensitivity to visual details,
which manifests in superior performance across diverse multi-modal benchmarks.
Code will be provided."
Athanor: Local Search over Abstract Constraint Specifications,cs.AI,Artificial Intelligence,2024-10-08,"Local search is a common method for solving combinatorial optimisation
problems. We focus on general-purpose local search solvers that accept as input
a constraint model - a declarative description of a problem consisting of a set
of decision variables under a set of constraints. Existing approaches typically
take as input models written in solver-independent constraint modelling
languages like MiniZinc. The Athanor solver we describe herein differs in that
it begins from a specification of a problem in the abstract constraint
specification language Essence, which allows problems to be described without
commitment to low-level modelling decisions through its support for a rich set
of abstract types. The advantage of proceeding from Essence is that the
structure apparent in a concise, abstract specification of a problem can be
exploited to generate high quality neighbourhoods automatically, avoiding the
difficult task of identifying that structure in an equivalent constraint model.
Based on the twin benefits of neighbourhoods derived from high level types and
the scalability derived by searching directly over those types, our empirical
results demonstrate strong performance in practice relative to existing
solution methods."
Learning Gaussian Data Augmentation in Feature Space for One-shot Object Detection in Manga,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"We tackle one-shot object detection in Japanese Manga. The rising global
popularity of Japanese manga has made the object detection of character faces
increasingly important, with potential applications such as automatic
colorization. However, obtaining sufficient data for training conventional
object detectors is challenging due to copyright restrictions. Additionally,
new characters appear every time a new volume of manga is released, making it
impractical to re-train object detectors each time to detect these new
characters. Therefore, one-shot object detection, where only a single query
(reference) image is required to detect a new character, is an essential task
in the manga industry. One challenge with one-shot object detection in manga is
the large variation in the poses and facial expressions of characters in target
images, despite having only one query image as a reference. Another challenge
is that the frequency of character appearances follows a long-tail
distribution. To overcome these challenges, we propose a data augmentation
method in feature space to increase the variation of the query. The proposed
method augments the feature from the query by adding Gaussian noise, with the
noise variance at each channel learned during training. The experimental
results show that the proposed method improves the performance for both seen
and unseen classes, surpassing data augmentation methods in image space."
Chameleon: An Efficient FHE Scheme Switching Acceleration on GPUs,cs.CR,Cryptography and Security,2024-10-08,"Fully homomorphic encryption (FHE) enables direct computation on encrypted
data, making it a crucial technology for privacy protection. However, FHE
suffers from significant performance bottlenecks. In this context, GPU
acceleration offers a promising solution to bridge the performance gap.
Existing efforts primarily focus on single-class FHE schemes, which fail to
meet the diverse requirements of data types and functions, prompting the
development of hybrid multi-class FHE schemes. However, studies have yet to
thoroughly investigate specific GPU optimizations for hybrid FHE schemes.
  In this paper, we present an efficient GPU-based FHE scheme switching
acceleration named Chameleon. First, we propose a scalable NTT acceleration
design that adapts to larger CKKS polynomials and smaller TFHE polynomials.
Specifically, Chameleon tackles synchronization issues by fusing stages to
reduce synchronization, employing polynomial coefficient shuffling to minimize
synchronization scale, and utilizing an SM-aware combination strategy to
identify the optimal switching point. Second, Chameleon is the first to
comprehensively analyze and optimize critical switching operations. It
introduces CMux-level parallelization to accelerate LUT evaluation and a
homomorphic rotation-free matrix-vector multiplication to improve repacking
efficiency. Finally, Chameleon outperforms the state-of-the-art GPU
implementations by 1.23x in CKKS HMUL and 1.15x in bootstrapping. It also
achieves up to 4.87x and 1.51x speedups for TFHE gate bootstrapping compared to
CPU and GPU versions, respectively, and delivers a 67.3x average speedup for
scheme switching over CPU-based implementation."
CubiX: Portable Wire-Driven Parallel Robot Connecting to and Utilizing the Environment,cs.RO,Robotics,2024-10-08,"A wire-driven parallel robot is a type of robotic system where multiple wires
are used to control the movement of a end-effector. The wires are attached to
the end-effector and anchored to fixed points on external structures. This
configuration allows for the separation of actuators and end-effectors,
enabling lightweight and simplified movable parts in the robot. However, its
range of motion remains confined within the space formed by the wires, limiting
the wire-driven capability to only within the pre-designed operational range.
Here, in this study, we develop a wire-driven robot, CubiX, capable of
connecting to and utilizing the environment. CubiX connects itself to the
environment using up to 8 wires and drives itself by winding these wires. By
integrating actuators for winding the wires into CubiX, a portable wire-driven
parallel robot is realized without limitations on its workspace. Consequently,
the robot can form parallel wire-driven structures by connecting wires to the
environment at any operational location."
Construction of Musculoskeletal Simulation for Shoulder Complex with Ligaments and Its Validation via Model Predictive Control,cs.RO,Robotics,2024-10-08,"The complex ways in which humans utilize their bodies in sports and martial
arts are remarkable, and human motion analysis is one of the most effective
tools for robot body design and control. On the other hand, motion analysis is
not easy, and it is difficult to measure complex body motions in detail due to
the influence of numerous muscles and soft tissues, mainly ligaments. In
response, various musculoskeletal simulators have been developed and applied to
motion analysis and robotics. However, none of them reproduce the ligaments but
only the muscles, nor do they focus on the shoulder complex, including the
clavicle and scapula, which is one of the most complex parts of the body.
Therefore, in this study, a detailed simulation model of the shoulder complex
including ligaments is constructed. The model will mimic not only the skeletal
structure and muscle arrangement but also the ligament arrangement and maximum
muscle strength. Through model predictive control based on the constructed
simulation, we confirmed that the ligaments contribute to joint stabilization
in the first movement and that the proper distribution of maximum muscle force
contributes to the equalization of the load on each muscle, demonstrating the
effectiveness of this simulation."
Fortify Your Foundations: Practical Privacy and Security for Foundation Model Deployments In The Cloud,cs.CR,Cryptography and Security,2024-10-08,"Foundation Models (FMs) display exceptional performance in tasks such as
natural language processing and are being applied across a growing range of
disciplines. Although typically trained on large public datasets, FMs are often
fine-tuned or integrated into Retrieval-Augmented Generation (RAG) systems,
which rely on private data. This access, along with their size and costly
training, heightens the risk of intellectual property theft. Moreover,
multimodal FMs may expose sensitive information. In this work, we examine the
FM threat model and discuss the practicality and comprehensiveness of various
approaches for securing against them, such as ML-based methods and trusted
execution environments (TEEs). We demonstrate that TEEs offer an effective
balance between strong security properties, usability, and performance.
Specifically, we present a solution achieving less than 10\% overhead versus
bare metal for the full Llama2 7B and 13B inference pipelines running inside
\intel\ SGX and \intel\ TDX. We also share our configuration files and insights
from our implementation. To our knowledge, our work is the first to show the
practicality of TEEs for securing FMs."
Beyond Captioning: Task-Specific Prompting for Improved VLM Performance in Mathematical Reasoning,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Vision-Language Models (VLMs) have transformed tasks requiring visual and
reasoning abilities, such as image retrieval and Visual Question Answering
(VQA). Despite their success, VLMs face significant challenges with tasks
involving geometric reasoning, algebraic problem-solving, and counting. These
limitations stem from difficulties effectively integrating multiple modalities
and accurately interpreting geometry-related tasks. Various works claim that
introducing a captioning pipeline before VQA tasks enhances performance. We
incorporated this pipeline for tasks involving geometry, algebra, and counting.
We found that captioning results are not generalizable, specifically with
larger VLMs primarily trained on downstream QnA tasks showing random
performance on math-related challenges. However, we present a promising
alternative: task-based prompting, enriching the prompt with task-specific
guidance. This approach shows promise and proves more effective than direct
captioning methods for math-heavy problems."
TIMBA: Time series Imputation with Bi-directional Mamba Blocks and Diffusion models,cs.LG,Machine Learning,2024-10-08,"The problem of imputing multivariate time series spans a wide range of
fields, from clinical healthcare to multi-sensor systems. Initially, Recurrent
Neural Networks (RNNs) were employed for this task; however, their error
accumulation issues led to the adoption of Transformers, leveraging attention
mechanisms to mitigate these problems. Concurrently, the promising results of
diffusion models in capturing original distributions have positioned them at
the forefront of current research, often in conjunction with Transformers. In
this paper, we propose replacing time-oriented Transformers with State-Space
Models (SSM), which are better suited for temporal data modeling. Specifically,
we utilize the latest SSM variant, S6, which incorporates attention-like
mechanisms. By embedding S6 within Mamba blocks, we develop a model that
integrates SSM, Graph Neural Networks, and node-oriented Transformers to
achieve enhanced spatiotemporal representations. Implementing these
architectural modifications, previously unexplored in this field, we present
Time series Imputation with Bi-directional mamba blocks and diffusion models
(TIMBA). TIMBA achieves superior performance in almost all benchmark scenarios
and performs comparably in others across a diverse range of missing value
situations and three real-world datasets. We also evaluate how the performance
of our model varies with different amounts of missing values and analyse its
performance on downstream tasks. In addition, we provide the original code to
replicate the results."
Give me a hint: Can LLMs take a hint to solve math problems?,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"While many state-of-the-art LLMs have shown poor logical and basic
mathematical reasoning, recent works try to improve their problem-solving
abilities using prompting techniques. We propose giving ""hints"" to improve the
language model's performance on advanced mathematical problems, taking
inspiration from how humans approach math pedagogically. We also test the
model's adversarial robustness to wrong hints. We demonstrate the effectiveness
of our approach by evaluating various LLMs, presenting them with a diverse set
of problems of different difficulties and topics from the MATH dataset and
comparing against techniques such as one-shot, few-shot, and chain of thought
prompting."
Accelerating Error Correction Code Transformers,cs.LG,Machine Learning,2024-10-08,"Error correction codes (ECC) are crucial for ensuring reliable information
transmission in communication systems. Choukroun & Wolf (2022b) recently
introduced the Error Correction Code Transformer (ECCT), which has demonstrated
promising performance across various transmission channels and families of
codes. However, its high computational and memory demands limit its practical
applications compared to traditional decoding algorithms. Achieving effective
quantization of the ECCT presents significant challenges due to its inherently
small architecture, since existing, very low-precision quantization techniques
often lead to performance degradation in compact neural networks. In this
paper, we introduce a novel acceleration method for transformer-based decoders.
We first propose a ternary weight quantization method specifically designed for
the ECCT, inducing a decoder with multiplication-free linear layers. We present
an optimized self-attention mechanism to reduce computational complexity via
codeaware multi-heads processing. Finally, we provide positional encoding via
the Tanner graph eigendecomposition, enabling a richer representation of the
graph connectivity. The approach not only matches or surpasses ECCT's
performance but also significantly reduces energy consumption, memory
footprint, and computational complexity. Our method brings transformer-based
error correction closer to practical implementation in resource-constrained
environments, achieving a 90% compression ratio and reducing arithmetic
operation energy consumption by at least 224 times on modern hardware."
MedUniSeg: 2D and 3D Medical Image Segmentation via a Prompt-driven Universal Model,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Universal segmentation models offer significant potential in addressing a
wide range of tasks by effectively leveraging discrete annotations. As the
scope of tasks and modalities expands, it becomes increasingly important to
generate and strategically position task- and modal-specific priors within the
universal model. However, existing universal models often overlook the
correlations between different priors, and the optimal placement and frequency
of these priors remain underexplored. In this paper, we introduce MedUniSeg, a
prompt-driven universal segmentation model designed for 2D and 3D multi-task
segmentation across diverse modalities and domains. MedUniSeg employs multiple
modal-specific prompts alongside a universal task prompt to accurately
characterize the modalities and tasks. To generate the related priors, we
propose the modal map (MMap) and the fusion and selection (FUSE) modules, which
transform modal and task prompts into corresponding priors. These modal and
task priors are systematically introduced at the start and end of the encoding
process. We evaluate MedUniSeg on a comprehensive multi-modal upstream dataset
consisting of 17 sub-datasets. The results demonstrate that MedUniSeg achieves
superior multi-task segmentation performance, attaining a 1.2% improvement in
the mean Dice score across the 17 upstream tasks compared to nnUNet baselines,
while using less than 1/10 of the parameters. For tasks that underperform
during the initial multi-task joint training, we freeze MedUniSeg and introduce
new modules to re-learn these tasks. This approach yields an enhanced version,
MedUniSeg*, which consistently outperforms MedUniSeg across all tasks.
Moreover, MedUniSeg surpasses advanced self-supervised and supervised
pre-trained models on six downstream tasks, establishing itself as a
high-quality, highly generalizable pre-trained segmentation model."
Automatic Summarization of Long Documents,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"A vast amount of textual data is added to the internet daily, making
utilization and interpretation of such data difficult and cumbersome. As a
result, automatic text summarization is crucial for extracting relevant
information, saving precious reading time. Although many transformer-based
models excel in summarization, they are constrained by their input size,
preventing them from processing texts longer than their context size. This
study introduces three novel algorithms that allow any LLM to efficiently
overcome its input size limitation, effectively utilizing its full potential
without any architectural modifications. We test our algorithms on texts with
more than 70,000 words, and our experiments show a significant increase in
BERTScore with competitive ROUGE scores."
Mini-Batch Kernel $k$-means,cs.LG,Machine Learning,2024-10-08,"We present the first mini-batch kernel $k$-means algorithm, offering an order
of magnitude improvement in running time compared to the full batch algorithm.
A single iteration of our algorithm takes $\widetilde{O}(kb^2)$ time,
significantly faster than the $O(n^2)$ time required by the full batch kernel
$k$-means, where $n$ is the dataset size and $b$ is the batch size. Extensive
experiments demonstrate that our algorithm consistently achieves a 10-100x
speedup with minimal loss in quality, addressing the slow runtime that has
limited kernel $k$-means adoption in practice. We further complement these
results with a theoretical analysis under an early stopping condition, proving
that with a batch size of $\widetilde{\Omega}(\max \{\gamma^{4}, \gamma^{2}\}
\cdot \epsilon^{-2})$, the algorithm terminates in $O(\gamma^2/\epsilon)$
iterations with high probability, where $\gamma$ bounds the norm of points in
feature space and $\epsilon$ is a termination threshold. Our analysis holds for
any reasonable center initialization, and when using $k$-means++
initialization, the algorithm achieves an approximation ratio of $O(\log k)$ in
expectation. For normalized kernels, such as Gaussian or Laplacian it holds
that $\gamma=1$. Taking $\epsilon = O(1)$ and $b=\Theta(\log n)$, the algorithm
terminates in $O(1)$ iterations, with each iteration running in
$\widetilde{O}(k)$ time."
MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly Detection in Surveillance Videos,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Detection of anomaly events is relevant for public safety and requires a
combination of fine-grained motion information and contextual events at
variable time-scales. To this end, we propose a Multi-Timescale Feature
Learning (MTFL) method to enhance the representation of anomaly features.
Short, medium, and long temporal tubelets are employed to extract
spatio-temporal video features using a Video Swin Transformer. Experimental
results demonstrate that MTFL outperforms state-of-the-art methods on the
UCF-Crime dataset, achieving an anomaly detection performance 89.78% AUC.
Moreover, it performs complementary to SotA with 95.32% AUC on the ShanghaiTech
and 84.57% AP on the XD-Violence dataset. Furthermore, we generate an extended
dataset of the UCF-Crime for development and evaluation on a wider range of
anomalies, namely Video Anomaly Detection Dataset (VADD), involving 2,591
videos in 18 classes with extensive coverage of realistic anomalies."
Brain-inspired continual pre-trained learner via silent synaptic consolidation,cs.LG,Machine Learning,2024-10-08,"Pre-trained models have demonstrated impressive generalization capabilities,
yet they remain vulnerable to catastrophic forgetting when incrementally
trained on new tasks. Existing architecture-based strategies encounter two
primary challenges: 1) Integrating a pre-trained network with a trainable
sub-network complicates the delicate balance between learning plasticity and
memory stability across evolving tasks during learning. 2) The absence of
robust interconnections between pre-trained networks and various sub-networks
limits the effective retrieval of pertinent information during inference. In
this study, we introduce the Artsy, inspired by the activation mechanisms of
silent synapses via spike-timing-dependent plasticity observed in mature
brains, to enhance the continual learning capabilities of pre-trained models.
The Artsy integrates two key components: During training, the Artsy mimics
mature brain dynamics by maintaining memory stability for previously learned
knowledge within the pre-trained network while simultaneously promoting
learning plasticity in task-specific sub-networks. During inference, artificial
silent and functional synapses are utilized to establish precise connections
between the pre-synaptic neurons in the pre-trained network and the
post-synaptic neurons in the sub-networks, facilitated through synaptic
consolidation, thereby enabling effective extraction of relevant information
from test samples. Comprehensive experimental evaluations reveal that our model
significantly outperforms conventional methods on class-incremental learning
tasks, while also providing enhanced biological interpretability for
architecture-based approaches. Moreover, we propose that the Artsy offers a
promising avenue for simulating biological synaptic mechanisms, potentially
advancing our understanding of neural plasticity in both artificial and
biological systems."
DimOL: Dimensional Awareness as A New 'Dimension' in Operator Learning,cs.LG,Machine Learning,2024-10-08,"In the realm of computational physics, an enduring topic is the numerical
solutions to partial differential equations (PDEs). Recently, the attention of
researchers has shifted towards Neural Operator methods, renowned for their
capability to approximate ``operators'' -- mappings from functions to
functions. Despite the universal approximation theorem within neural operators,
ensuring error bounds often requires employing numerous Fourier layers.
However, what about lightweight models? In response to this question, we
introduce DimOL (Dimension-aware Operator Learning), drawing insights from
dimensional analysis. To implement DimOL, we propose the ProdLayer, which can
be seamlessly integrated into FNO-based and Transformer-based PDE solvers,
enhancing their ability to handle sum-of-products structures inherent in many
physical systems. Empirically, DimOL models achieve up to 48% performance gain
within the PDE datasets. Furthermore, by analyzing Fourier components' weights,
we can symbolically discern the physical significance of each term. This sheds
light on the opaque nature of neural networks, unveiling underlying physical
principles."
Towards an Autonomous Surface Vehicle Prototype for Artificial Intelligence Applications of Water Quality Monitoring,cs.RO,Robotics,2024-10-08,"The use of Autonomous Surface Vehicles, equipped with water quality sensors
and artificial vision systems, allows for a smart and adaptive deployment in
water resources environmental monitoring. This paper presents a real
implementation of a vehicle prototype that to address the use of Artificial
Intelligence algorithms and enhanced sensing techniques for water quality
monitoring. The vehicle is fully equipped with high-quality sensors to measure
water quality parameters and water depth. Furthermore, by means of a
stereo-camera, it also can detect and locate macro-plastics in real
environments by means of deep visual models, such as YOLOv5. In this paper,
experimental results, carried out in Lago Mayor (Sevilla), has been presented
as proof of the capabilities of the proposed architecture. The overall system,
and the early results obtained, are expected to provide a solid example of a
real platform useful for the water resource monitoring task, and to serve as a
real case scenario for deploying Artificial Intelligence algorithms, such as
path planning, artificial vision, etc."
Ordering-Based Causal Discovery for Linear and Nonlinear Relations,cs.LG,Machine Learning,2024-10-08,"Identifying causal relations from purely observational data typically
requires additional assumptions on relations and/or noise. Most current methods
restrict their analysis to datasets that are assumed to have pure linear or
nonlinear relations, which is often not reflective of real-world datasets that
contain a combination of both. This paper presents CaPS, an ordering-based
causal discovery algorithm that effectively handles linear and nonlinear
relations. CaPS introduces a novel identification criterion for topological
ordering and incorporates the concept of ""parent score"" during the
post-processing optimization stage. These scores quantify the strength of the
average causal effect, helping to accelerate the pruning process and correct
inaccurate predictions in the pruning step. Experimental results demonstrate
that our proposed solutions outperform state-of-the-art baselines on synthetic
data with varying ratios of linear and nonlinear relations. The results
obtained from real-world data also support the competitiveness of CaPS. Code
and datasets are available at https://github.com/E2real/CaPS."
Deep learning-based fault identification in condition monitoring,cs.LG,Machine Learning,2024-10-08,"Vibration-based condition monitoring techniques are commonly used to identify
faults in rolling element bearings. Accuracy and speed of fault detection
procedures are critical performance measures in condition monitoring. Delay is
especially important in remote condition monitoring and time-sensitive
industrial applications. While most existing methods focus on accuracy, little
attention has been given to the inference time in the fault identification
process. In this paper, we address this gap by presenting a Convolutional
Neural Network (CNN) based approach for real-time fault identification in
rolling element bearings. We encode raw vibration signals into two-dimensional
images using various encoding methods and use these with a CNN to classify
several categories of bearing fault types and sizes. We analyse the interplay
between fault identification accuracy and processing time. For training and
evaluation we use a bearing failure CWRU dataset."
A Robust Quadruped Robot with Twisting Waist for Flexible Motions,cs.RO,Robotics,2024-10-08,"The waist plays a crucial role in the agile movement of many animals in
nature. It provides the torso with additional degrees of freedom and
flexibility, inspiring researchers to incorporate this biological feature into
robotic structures to enhance robot locomotion. This paper presents a
cost-effective and low-complexity waist mechanism integrated into the structure
of the open-source robot solo8, adding a new degree of freedom (DOF) to its
torso. We refer to this novel robot as solo9. Additionally, we propose a
full-body control method for the waist-equipped quadruped robot based on
generative adversarial imitation learning (GAIL). During training, the
discriminator is used as input for iterative optimization of the policy and
dataset, enabling solo9 to achieve flexible steering maneuvers across various
gaits. Extensive tests of solo9's steering capabilities, terrain adaptability,
and robustness are conducted in both simulation and real-world scenarios, with
detailed comparisons to solo8 and solo12, demonstrating the effectiveness of
the control algorithm and the advantages of the waist mechanism."
Edit Distances and Their Applications to Downstream Tasks in Research and Commercial Contexts,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"The tutorial describes the concept of edit distances applied to research and
commercial contexts. We use Translation Edit Rate (TER), Levenshtein,
Damerau-Levenshtein, Longest Common Subsequence and $n$-gram distances to
demonstrate the frailty of statistical metrics when comparing text sequences.
Our discussion disassembles them into their essential components. We discuss
the centrality of four editing actions: insert, delete, replace and move words,
and show their implementations in openly available packages and toolkits. The
application of edit distances in downstream tasks often assumes that these
accurately represent work done by post-editors and real errors that need to be
corrected in MT output. We discuss how imperfect edit distances are in
capturing the details of this error correction work and the implications for
researchers and for commercial applications, of these uses of edit distances.
In terms of commercial applications, we discuss their integration in
computer-assisted translation tools and how the perception of the connection
between edit distances and post-editor effort affects the definition of
translator rates."
Improved Sample Complexity for Private Nonsmooth Nonconvex Optimization,cs.LG,Machine Learning,2024-10-08,"We study differentially private (DP) optimization algorithms for stochastic
and empirical objectives which are neither smooth nor convex, and propose
methods that return a Goldstein-stationary point with sample complexity bounds
that improve on existing works. We start by providing a single-pass
$(\epsilon,\delta)$-DP algorithm that returns an $(\alpha,\beta)$-stationary
point as long as the dataset is of size
$\widetilde{\Omega}\left(1/\alpha\beta^{3}+d/\epsilon\alpha\beta^{2}+d^{3/4}/\epsilon^{1/2}\alpha\beta^{5/2}\right)$,
which is $\Omega(\sqrt{d})$ times smaller than the algorithm of Zhang et al.
[2024] for this task, where $d$ is the dimension. We then provide a multi-pass
polynomial time algorithm which further improves the sample complexity to
$\widetilde{\Omega}\left(d/\beta^2+d^{3/4}/\epsilon\alpha^{1/2}\beta^{3/2}\right)$,
by designing a sample efficient ERM algorithm, and proving that
Goldstein-stationary points generalize from the empirical loss to the
population loss."
MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"English-centric large language models (LLMs) often show strong multilingual
capabilities. However, the multilingual performance of these models remains
unclear and is not thoroughly evaluated for many languages. Most benchmarks for
multilinguality focus on classic NLP tasks, or cover a minimal number of
languages. We introduce MEXA, a method for assessing the multilingual
capabilities of pre-trained English-centric LLMs using parallel sentences,
which are available for more languages than existing downstream tasks. MEXA
leverages the fact that English-centric LLMs use English as a kind of pivot
language in their intermediate layers. It computes the alignment between
English and non-English languages using parallel sentences to evaluate the
transfer of language understanding from English to other languages. This
alignment can be used to estimate model performance in other languages. We
conduct studies using various parallel datasets (FLORES-200 and Bible), models
(Llama family, Gemma family, Mistral, and OLMo), and established downstream
tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute
embeddings in decoder-only models. Our results show that MEXA, in its default
settings, achieves a statistically significant average Pearson correlation of
0.90 with three established downstream tasks across nine models and two
parallel datasets. This suggests that MEXA is a reliable method for estimating
the multilingual capabilities of English-centric LLMs, providing a clearer
understanding of their multilingual potential and the inner workings of LLMs.
Leaderboard: https://huggingface.co/spaces/cis-lmu/Mexa, Code:
https://github.com/cisnlp/Mexa."
A second-order-like optimizer with adaptive gradient scaling for deep learning,cs.LG,Machine Learning,2024-10-08,"In this empirical article, we introduce INNAprop, an optimization algorithm
that combines the INNA method with the RMSprop adaptive gradient scaling. It
leverages second-order information and rescaling while keeping the memory
requirements of standard DL methods as AdamW or SGD with momentum.After having
recalled our geometrical motivations, we provide quite extensive experiments.
On image classification (CIFAR-10, ImageNet) and language modeling (GPT-2),
INNAprop consistently matches or outperforms AdamW both in training speed and
accuracy, with minimal hyperparameter tuning in large-scale settings. Our code
is publicly available at \url{https://github.com/innaprop/innaprop}."
Heuristics for Partially Observable Stochastic Contingent Planning,cs.AI,Artificial Intelligence,2024-10-08,"Acting to complete tasks in stochastic partially observable domains is an
important problem in artificial intelligence, and is often formulated as a
goal-based POMDP. Goal-based POMDPs can be solved using the RTDP-BEL algorithm,
that operates by running forward trajectories from the initial belief to the
goal. These trajectories can be guided by a heuristic, and more accurate
heuristics can result in significantly faster convergence. In this paper, we
develop a heuristic function that leverages the structured representation of
domain models. We compute, in a relaxed space, a plan to achieve the goal,
while taking into account the value of information, as well as the stochastic
effects. We provide experiments showing that while our heuristic is slower to
compute, it requires an order of magnitude less trajectories before
convergence. Overall, it thus speeds up RTDP-BEL, particularly in problems
where significant information gathering is needed."
Unobserved Object Detection using Generative Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Can we detect an object that is not visible in an image? This study
introduces the novel task of 2D and 3D unobserved object detection for
predicting the location of objects that are occluded or lie outside the image
frame. We adapt several state-of-the-art pre-trained generative models to solve
this task, including 2D and 3D diffusion models and vision--language models,
and show that they can be used to infer the presence of objects that are not
directly observed. To benchmark this task, we propose a suite of metrics that
captures different aspects of performance. Our empirical evaluations on indoor
scenes from the RealEstate10k dataset with COCO object categories demonstrate
results that motivate the use of generative models for the unobserved object
detection task. The current work presents a promising step towards compelling
applications like visual search and probabilistic planning that can leverage
object detection beyond what can be directly observed."
From Tokens to Words: on the inner lexicon of LLMs,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Natural language is composed of words, but modern LLMs process sub-words as
input. A natural question raised by this discrepancy is whether LLMs encode
words internally, and if so how. We present evidence that LLMs engage in an
intrinsic detokenization process, where sub-word sequences are combined into
coherent word representations. Our experiments show that this process takes
place primarily within the early and middle layers of the model. They also show
that it is robust to non-morphemic splits, typos and perhaps importantly-to
out-of-vocabulary words: when feeding the inner representation of such words to
the model as input vectors, it can ""understand"" them despite never seeing them
during training. Our findings suggest that LLMs maintain a latent vocabulary
beyond the tokenizer's scope. These insights provide a practical,
finetuning-free application for expanding the vocabulary of pre-trained models.
By enabling the addition of new vocabulary words, we reduce input length and
inference iterations, which reduces both space and model latency, with little
to no loss in model accuracy."
MelissaDL x Breed: Towards Data-Efficient On-line Supervised Training of Multi-parametric Surrogates with Active Learning,cs.LG,Machine Learning,2024-10-08,"Artificial intelligence is transforming scientific computing with deep neural
network surrogates that approximate solutions to partial differential equations
(PDEs). Traditional off-line training methods face issues with storage and I/O
efficiency, as the training dataset has to be computed with numerical solvers
up-front. Our previous work, the Melissa framework, addresses these problems by
enabling data to be created ""on-the-fly"" and streamed directly into the
training process. In this paper we introduce a new active learning method to
enhance data-efficiency for on-line surrogate training. The surrogate is direct
and multi-parametric, i.e., it is trained to predict a given timestep directly
with different initial and boundary conditions parameters. Our approach uses
Adaptive Multiple Importance Sampling guided by training loss statistics, in
order to focus NN training on the difficult areas of the parameter space.
Preliminary results for 2D heat PDE demonstrate the potential of this method,
called Breed, to improve the generalization capabilities of surrogates while
reducing computational overhead."
Communicating with Speakers and Listeners of Different Pragmatic Levels,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"This paper explores the impact of variable pragmatic competence on
communicative success through simulating language learning and conversing
between speakers and listeners with different levels of reasoning abilities.
Through studying this interaction, we hypothesize that matching levels of
reasoning between communication partners would create a more beneficial
environment for communicative success and language learning. Our research
findings indicate that learning from more explicit, literal language is
advantageous, irrespective of the learner's level of pragmatic competence.
Furthermore, we find that integrating pragmatic reasoning during language
learning, not just during evaluation, significantly enhances overall
communication performance. This paper provides key insights into the importance
of aligning reasoning levels and incorporating pragmatic reasoning in
optimizing communicative interactions."
ModalPrompt:Dual-Modality Guided Prompt for Continual Learning of Large Multimodal Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Large Multimodal Models (LMMs) exhibit remarkable multi-tasking ability by
learning mixed datasets jointly. However, novel tasks would be encountered
sequentially in dynamic world, and continually fine-tuning LMMs often leads to
performance degrades. To handle the challenges of catastrophic forgetting,
existing methods leverage data replay or model expansion, both of which are not
specially developed for LMMs and have their inherent limitations. In this
paper, we propose a novel dual-modality guided prompt learning framework
(ModalPrompt) tailored for multimodal continual learning to effectively learn
new tasks while alleviating forgetting of previous knowledge. Concretely, we
learn prototype prompts for each task and exploit efficient prompt selection
for task identifiers and prompt fusion for knowledge transfer based on
image-text supervision. Extensive experiments demonstrate the superiority of
our approach, e.g., ModalPrompt achieves +20% performance gain on LMMs
continual learning benchmarks with $\times$ 1.42 inference speed refraining
from growing training cost in proportion to the number of tasks. The code will
be made publically available."
Bottom-up Anytime Discovery of Generalised Multimodal Graph Patterns for Knowledge Graphs,cs.AI,Artificial Intelligence,2024-10-08,"Vast amounts of heterogeneous knowledge are becoming publicly available in
the form of knowledge graphs, often linking multiple sources of data that have
never been together before, and thereby enabling scholars to answer many new
research questions. It is often not known beforehand, however, which questions
the data might have the answers to, potentially leaving many interesting and
novel insights to remain undiscovered. To support scholars during this
scientific workflow, we introduce an anytime algorithm for the bottom-up
discovery of generalized multimodal graph patterns in knowledge graphs. Each
pattern is a conjunction of binary statements with (data-) type variables,
constants, and/or value patterns. Upon discovery, the patterns are converted to
SPARQL queries and presented in an interactive facet browser together with
metadata and provenance information, enabling scholars to explore, analyse, and
share queries. We evaluate our method from a user perspective, with the help of
domain experts in the humanities."
Time Transfer: On Optimal Learning Rate and Batch Size In The Infinite Data Limit,cs.LG,Machine Learning,2024-10-08,"One of the main challenges in optimal scaling of large language models (LLMs)
is the prohibitive cost of hyperparameter tuning, particularly learning rate
$\eta$ and batch size $B$. While techniques like $\mu$P (Yang et al., 2022)
provide scaling rules for optimal $\eta$ transfer in the infinite model size
limit, the optimal scaling behavior in the infinite data size limit ($T \to
\infty$) remains unknown. We fill in this gap by observing for the first time
an interplay of three optimal $\eta$ scaling regimes: $\eta \propto \sqrt{T}$,
$\eta \propto 1$, and $\eta \propto 1/\sqrt{T}$ with transitions controlled by
$B$ and its relation to the time-evolving critical batch size $B_\mathrm{crit}
\propto T$. Furthermore, we show that the optimal batch size is positively
correlated with $B_\mathrm{crit}$: keeping it fixed becomes suboptimal over
time even if learning rate is scaled optimally. Surprisingly, our results
demonstrate that the observed optimal $\eta$ and $B$ dynamics are preserved
with $\mu$P model scaling, challenging the conventional view of
$B_\mathrm{crit}$ dependence solely on loss value. Complementing optimality, we
examine the sensitivity of loss to changes in learning rate, where we find the
sensitivity to decrease with $T \to \infty$ and to remain constant with $\mu$P
model scaling. We hope our results make the first step towards a unified
picture of the joint optimal data and model scaling."
A noise-corrected Langevin algorithm and sampling by half-denoising,cs.LG,Machine Learning,2024-10-08,"The Langevin algorithm is a classic method for sampling from a given pdf in a
real space. In its basic version, it only requires knowledge of the gradient of
the log-density, also called the score function. However, in deep learning, it
is often easier to learn the so-called ""noisy score function"", i.e. the
gradient of the log-density of noisy data, more precisely when Gaussian noise
is added to the data. Such an estimate is biased and complicates the use of the
Langevin method. Here, we propose a noise-corrected version of the Langevin
algorithm, where the bias due to noisy data is removed, at least regarding
first-order terms. Unlike diffusion models, our algorithm needs to know the
noisy score function for one single noise level only. We further propose a
simple special case which has an interesting intuitive interpretation of
iteratively adding noise the data and then attempting to remove half of that
noise."
A GPT-based Decision Transformer for Multi-Vehicle Coordination at Unsignalized Intersections,cs.RO,Robotics,2024-10-08,"In this paper, we explore the application of the Decision Transformer, a
decision-making algorithm based on the Generative Pre-trained Transformer (GPT)
architecture, to multi-vehicle coordination at unsignalized intersections. We
formulate the coordination problem so as to find the optimal trajectories for
multiple vehicles at intersections, modeling it as a sequence prediction task
to fully leverage the power of GPTs as a sequence model. Through extensive
experiments, we compare our approach to a reservation-based intersection
management system. Our results show that the Decision Transformer can
outperform the training data in terms of total travel time and can be
generalized effectively to various scenarios, including noise-induced velocity
variations, continuous interaction environments, and different vehicle numbers
and road configurations."
Effort Allocation for Deadline-Aware Task and Motion Planning: A Metareasoning Approach,cs.RO,Robotics,2024-10-08,"In robot planning, tasks can often be achieved through multiple options, each
consisting of several actions. This work specifically addresses deadline
constraints in task and motion planning, aiming to find a plan that can be
executed within the deadline despite uncertain planning and execution times. We
propose an effort allocation problem, formulated as a Markov decision process
(MDP), to find such a plan by leveraging metareasoning perspectives to allocate
computational resources among the given options. We formally prove the
NP-hardness of the problem by reducing it from the knapsack problem.
  Both a model-based approach, where transition models are learned from past
experience, and a model-free approach, which overcomes the unavailability of
prior data acquisition through reinforcement learning, are explored. For the
model-based approach, we investigate Monte Carlo tree search (MCTS) to
approximately solve the proposed MDP and further design heuristic schemes to
tackle NP-hardness, leading to the approximate yet efficient algorithm called
DP_Rerun. In experiments, DP_Rerun demonstrates promising performance
comparable to MCTS while requiring negligible computation time."
Multi-Session Client-Centered Treatment Outcome Evaluation in Psychotherapy,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"In psychotherapy, therapeutic outcome assessment, or treatment outcome
evaluation, is essential for enhancing mental health care by systematically
evaluating therapeutic processes and outcomes. Existing large language model
approaches often focus on therapist-centered, single-session evaluations,
neglecting the client's subjective experience and longitudinal progress across
multiple sessions. To address these limitations, we propose IPAEval, a
client-Informed Psychological Assessment-based Evaluation framework that
automates treatment outcome evaluations from the client's perspective using
clinical interviews. IPAEval integrates cross-session client-contextual
assessment and session-focused client-dynamics assessment to provide a
comprehensive understanding of therapeutic progress. Experiments on our newly
developed TheraPhase dataset demonstrate that IPAEval effectively tracks
symptom severity and treatment outcomes over multiple sessions, outperforming
previous single-session models and validating the benefits of items-aware
reasoning mechanisms."
A Zero-Shot approach to the Conversational Tree Search Task,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"In sensitive domains, such as legal or medial domains, the correctness of
information given to users is critical. To address this, the recently
introduced task Conversational Tree Search (CTS) provides a graph-based
framework for controllable task-oriented dialog in sensitive domains. However,
a big drawback of state-of-the-art CTS agents is their long training time,
which is especially problematic as a new agent must be trained every time the
associated domain graph is updated. The goal of this paper is to eliminate the
need for training CTS agents altogether. To achieve this, we implement a novel
LLM-based method for zero-shot, controllable CTS agents. We show that these
agents significantly outperform state-of-the-art CTS agents (p<0.0001; Barnard
Exact test) in simulation. This generalizes to all available CTS domains.
Finally, we perform user evaluation to test the agent performance in the wild,
showing that our policy significantly (p<0.05; Barnard Exact) improves
task-success compared to the state-of-the-art Reinforcement Learning-based CTS
agent."
IncSAR: A Dual Fusion Incremental Learning Framework for SAR Target Recognition,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Deep learning techniques have been successfully applied in Synthetic Aperture
Radar (SAR) target recognition in static scenarios relying on predefined
datasets. However, in real-world scenarios, models must incrementally learn new
information without forgetting previously learned knowledge. Models' tendency
to forget old knowledge when learning new tasks, known as catastrophic
forgetting, remains an open challenge. In this paper, an incremental learning
framework, called IncSAR, is proposed to mitigate catastrophic forgetting in
SAR target recognition. IncSAR comprises a Vision Transformer (ViT) and a
custom-designed Convolutional Neural Network (CNN) in individual branches
combined through a late-fusion strategy. A denoising module, utilizing the
properties of Robust Principal Component Analysis (RPCA), is introduced to
alleviate the speckle noise present in SAR images. Moreover, a random
projection layer is employed to enhance the linear separability of features,
and a Linear Discriminant Analysis (LDA) approach is proposed to decorrelate
the extracted class prototypes. Experimental results on the MSTAR and
OpenSARShip benchmark datasets demonstrate that IncSAR outperforms
state-of-the-art approaches, leading to an improvement from $98.05\%$ to
$99.63\%$ in average accuracy and from $3.05\%$ to $0.33\%$ in performance
dropping rate."
CAP: Detecting Unauthorized Data Usage in Generative Models via Prompt Generation,cs.LG,Machine Learning,2024-10-08,"To achieve accurate and unbiased predictions, Machine Learning (ML) models
rely on large, heterogeneous, and high-quality datasets. However, this could
raise ethical and legal concerns regarding copyright and authorization aspects,
especially when information is gathered from the Internet. With the rise of
generative models, being able to track data has become of particular
importance, especially since they may (un)intentionally replicate copyrighted
contents. Therefore, this work proposes Copyright Audit via Prompts generation
(CAP), a framework for automatically testing whether an ML model has been
trained with unauthorized data. Specifically, we devise an approach to generate
suitable keys inducing the model to reveal copyrighted contents. To prove its
effectiveness, we conducted an extensive evaluation campaign on measurements
collected in four IoT scenarios. The obtained results showcase the
effectiveness of CAP, when used against both realistic and synthetic datasets."
Probing Language Models on Their Knowledge Source,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Large Language Models (LLMs) often encounter conflicts between their learned,
internal (parametric knowledge, PK) and external knowledge provided during
inference (contextual knowledge, CK). Understanding how LLMs models prioritize
one knowledge source over the other remains a challenge. In this paper, we
propose a novel probing framework to explore the mechanisms governing the
selection between PK and CK in LLMs. Using controlled prompts designed to
contradict the model's PK, we demonstrate that specific model activations are
indicative of the knowledge source employed. We evaluate this framework on
various LLMs of different sizes and demonstrate that mid-layer activations,
particularly those related to relations in the input, are crucial in predicting
knowledge source selection, paving the way for more reliable models capable of
handling knowledge conflicts effectively."
CALoR: Towards Comprehensive Model Inversion Defense,cs.CR,Cryptography and Security,2024-10-08,"Model Inversion Attacks (MIAs) aim at recovering privacy-sensitive training
data from the knowledge encoded in the released machine learning models. Recent
advances in the MIA field have significantly enhanced the attack performance
under multiple scenarios, posing serious privacy risks of Deep Neural Networks
(DNNs). However, the development of defense strategies against MIAs is
relatively backward to resist the latest MIAs and existing defenses fail to
achieve further trade-off between model utility and model robustness. In this
paper, we provide an in-depth analysis from the perspective of intrinsic
vulnerabilities of MIAs, comprehensively uncovering the weaknesses inherent in
the basic pipeline, which are partially investigated in the previous defenses.
Building upon these new insights, we propose a robust defense mechanism,
integrating Confidence Adaptation and Low-Rank compression(CALoR). Our method
includes a novel robustness-enhanced classification loss specially-designed for
model inversion defenses and reveals the extraordinary effectiveness of
compressing the classification header. With CALoR, we can mislead the
optimization objective, reduce the leaked information and impede the
backpropagation of MIAs, thus mitigating the risk of privacy leakage. Extensive
experimental results demonstrate that our method achieves state-of-the-art
(SOTA) defense performance against MIAs and exhibits superior generalization to
existing defenses across various scenarios."
Single Actuator Undulation Soft-bodied Robots Using A Precompressed Variable Thickness Flexible Beam,cs.RO,Robotics,2024-10-08,"Soft robots - due to their intrinsic flexibility of the body - can adaptively
navigate unstructured environments. One of the most popular locomotion gaits
that has been implemented in soft robots is undulation. The undulation motion
in soft robots resembles the locomotion gait of stringy creatures such as
snakes, eels, and C. Elegans. Typically, the implementation of undulation
locomotion on a soft robot requires many actuators to control each segment of
the stringy body. The added weight of multiple actuators limits the navigating
performance of soft-bodied robots. In this paper, we propose a simple
tendon-driven flexible beam with only one actuator (a DC motor) that can
generate a mechanical traveling wave along the beam to support the undulation
locomotion of soft robots. The beam will be precompressed along its axis by
shortening the length of the two tendons to form an S-shape, thus pretensioning
the tendons. The motor will wind and unwind the tendons to deform the flexible
beam and generate traveling waves along the body of the robot. We experiment
with different pre-tension to characterize the relationship between tendon
pre-tension forces and the DC-motor winding/unwinding. Our proposal enables a
simple implementation of undulation motion to support the locomotion of
soft-bodied robots."
Vision Transformer based Random Walk for Group Re-Identification,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Group re-identification (re-ID) aims to match groups with the same people
under different cameras, mainly involves the challenges of group members and
layout changes well. Most existing methods usually use the k-nearest neighbor
algorithm to update node features to consider changes in group membership, but
these methods cannot solve the problem of group layout changes. To this end, we
propose a novel vision transformer based random walk framework for group re-ID.
Specifically, we design a vision transformer based on a monocular depth
estimation algorithm to construct a graph through the average depth value of
pedestrian features to fully consider the impact of camera distance on group
members relationships. In addition, we propose a random walk module to
reconstruct the graph by calculating affinity scores between target and gallery
images to remove pedestrians who do not belong to the current group.
Experimental results show that our framework is superior to most methods."
Extended convexity and smoothness and their applications in deep learning,cs.LG,Machine Learning,2024-10-08,"The underlying mechanism by which simple gradient-based iterative algorithms
can effectively handle the non-convex problem of deep model training remains
incompletely understood within the traditional convex and non-convex analysis
frameworks, which often require the Lipschitz smoothness of the gradient and
strong convexity. In this paper, we introduce $\mathcal{H}(\phi)$-convexity and
$\mathcal{H}(\Phi)$-smoothness, which broaden the existing concepts of
smoothness and convexity, and delineate their fundamental properties. Building
on these concepts, we introduce the high-order gradient descent and high-order
stochastic gradient descent methods, which serve as extensions to the
traditional gradient descent and stochastic gradient descent methods,
respectively. Furthermore, we establish descent lemmas for the
$\mathcal{H}(\phi)$-convex and $\mathcal{H}(\Phi)$-smooth objective functions
when utilizing these four methods. On the basis of these findings, we develop
the gradient structure control algorithm to address non-convex optimization
objectives, encompassing both the functions represented by machine learning
models and common loss functions in deep learning. The effectiveness of the
proposed methodology is empirically validated through experiments."
PostCast: Generalizable Postprocessing for Precipitation Nowcasting via Unsupervised Blurriness Modeling,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Precipitation nowcasting plays a pivotal role in socioeconomic sectors,
especially in severe convective weather warnings. Although notable progress has
been achieved by approaches mining the spatiotemporal correlations with deep
learning, these methods still suffer severe blurriness as the lead time
increases, which hampers accurate predictions for extreme precipitation. To
alleviate blurriness, researchers explore generative methods conditioned on
blurry predictions. However, the pairs of blurry predictions and corresponding
ground truth need to be generated in advance, making the training pipeline
cumbersome and limiting the generality of generative models within blur modes
that appear in training data. By rethinking the blurriness in precipitation
nowcasting as a blur kernel acting on predictions, we propose an unsupervised
postprocessing method to eliminate the blurriness without the requirement of
training with the pairs of blurry predictions and corresponding ground truth.
Specifically, we utilize blurry predictions to guide the generation process of
a pre-trained unconditional denoising diffusion probabilistic model (DDPM) to
obtain high-fidelity predictions with eliminated blurriness. A zero-shot blur
kernel estimation mechanism and an auto-scale denoise guidance strategy are
introduced to adapt the unconditional DDPM to any blurriness modes varying from
datasets and lead times in precipitation nowcasting. Extensive experiments are
conducted on 7 precipitation radar datasets, demonstrating the generality and
superiority of our method."
CASA: Class-Agnostic Shared Attributes in Vision-Language Models for Efficient Incremental Object Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Incremental object detection (IOD) is challenged by background shift, where
background categories in sequential data may include previously learned or
future classes. Inspired by the vision-language foundation models such as CLIP,
these models capture shared attributes from extensive image-text paired data
during pre-training. We propose a novel method utilizing attributes in
vision-language foundation models for incremental object detection. Our method
constructs a Class-Agnostic Shared Attribute base (CASA) to capture common
semantic information among incremental classes. Specifically, we utilize large
language models to generate candidate textual attributes and select the most
relevant ones based on current training data, recording their significance in
an attribute assignment matrix. For subsequent tasks, we freeze the retained
attributes and continue selecting from the remaining candidates while updating
the attribute assignment matrix accordingly. Furthermore, we employ OWL-ViT as
our baseline, preserving the original parameters of the pre-trained foundation
model. Our method adds only 0.7% to parameter storage through
parameter-efficient fine-tuning to significantly enhance the scalability and
adaptability of IOD. Extensive two-phase and multi-phase experiments on the
COCO dataset demonstrate the state-of-the-art performance of our proposed
method."
Gradual Learning: Optimizing Fine-Tuning with Partially Mastered Knowledge in Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"During the pretraining phase, large language models (LLMs) acquire vast
amounts of knowledge from extensive text corpora. Nevertheless, in later stages
such as fine-tuning and inference, the model may encounter knowledge not
covered in the initial training, which can lead to hallucinations and degraded
performance. This issue has a profound impact on the model's capabilities, as
it will inevitably face out-of-scope knowledge after pretraining. Furthermore,
fine-tuning is often required to adapt LLMs to domain-specific tasks. However,
this phenomenon limits the model's ability to learn and integrate new
information during fine-tuning. The effectiveness of fine-tuning largely
depends on the type of knowledge involved. Existing research suggests that
fine-tuning the model on partially mastered knowledge-for instance,
question-answer pairs where the model has a chance of providing correct
responses under non-greedy decoding-can enable the model to acquire new
knowledge while mitigating hallucination. Notably, this approach can still lead
to the forgetting of fully mastered knowledge, constraining the fine-tuning
dataset to a narrower range and limiting the model's overall potential for
improvement. Given the model's intrinsic reasoning abilities and the
interconnectedness of different knowledge areas, it is likely that as the
model's capacity to utilize existing knowledge improves during fine-tuning,
previously unmastered knowledge may become more understandable. To explore this
hypothesis, we conducted experiments and, based on the results, proposed a
two-stage fine-tuning strategy. This approach not only improves the model's
overall test accuracy and knowledge retention but also preserves its accuracy
on previously mastered content. When fine-tuning on the WikiQA dataset, our
method increases the amount of knowledge acquired by the model in this stage by
24%."
"Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation",cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Recent Retrieval Augmented Generation (RAG) aims to enhance Large Language
Models (LLMs) by incorporating extensive knowledge retrieved from external
sources. However, such approach encounters some challenges: Firstly, the
original queries may not be suitable for precise retrieval, resulting in
erroneous contextual knowledge; Secondly, the language model can easily
generate inconsistent answer with external references due to their knowledge
boundary limitation. To address these issues, we propose the
chain-of-verification (CoV-RAG) to enhance the external retrieval correctness
and internal generation consistency. Specifically, we integrate the
verification module into the RAG, engaging in scoring, judgment, and rewriting.
To correct external retrieval errors, CoV-RAG retrieves new knowledge using a
revised query. To correct internal generation errors, we unify QA and
verification tasks with a Chain-of-Thought (CoT) reasoning during training. Our
comprehensive experiments across various LLMs demonstrate the effectiveness and
adaptability compared with other strong baselines. Especially, our CoV-RAG can
significantly surpass the state-of-the-art baselines using different LLM
backbones."
Core Tokensets for Data-efficient Sequential Training of Transformers,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Deep networks are frequently tuned to novel tasks and continue learning from
ongoing data streams. Such sequential training requires consolidation of new
and past information, a challenge predominantly addressed by retaining the most
important data points - formally known as coresets. Traditionally, these
coresets consist of entire samples, such as images or sentences. However,
recent transformer architectures operate on tokens, leading to the famous
assertion that an image is worth 16x16 words. Intuitively, not all of these
tokens are equally informative or memorable. Going beyond coresets, we thus
propose to construct a deeper-level data summary on the level of tokens. Our
respectively named core tokensets both select the most informative data points
and leverage feature attribution to store only their most relevant features. We
demonstrate that core tokensets yield significant performance retention in
incremental image classification, open-ended visual question answering, and
continual image captioning with significantly reduced memory. In fact, we
empirically find that a core tokenset of 1\% of the data performs comparably to
at least a twice as large and up to 10 times larger coreset."
SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Diffusion-based Video Super-Resolution (VSR) is renowned for generating
perceptually realistic videos, yet it grapples with maintaining detail
consistency across frames due to stochastic fluctuations. The traditional
approach of pixel-level alignment is ineffective for diffusion-processed frames
because of iterative disruptions. To overcome this, we introduce SeeClear--a
novel VSR framework leveraging conditional video generation, orchestrated by
instance-centric and channel-wise semantic controls. This framework integrates
a Semantic Distiller and a Pixel Condenser, which synergize to extract and
upscale semantic details from low-resolution frames. The Instance-Centric
Alignment Module (InCAM) utilizes video-clip-wise tokens to dynamically relate
pixels within and across frames, enhancing coherency. Additionally, the
Channel-wise Texture Aggregation Memory (CaTeGory) infuses extrinsic knowledge,
capitalizing on long-standing semantic textures. Our method also innovates the
blurring diffusion process with the ResShift mechanism, finely balancing
between sharpness and diffusion effects. Comprehensive experiments confirm our
framework's advantage over state-of-the-art diffusion-based VSR techniques. The
code is available: https://github.com/Tang1705/SeeClear-NeurIPS24."
Integrating Online Learning and Connectivity Maintenance for Communication-Aware Multi-Robot Coordination,cs.RO,Robotics,2024-10-08,"This paper proposes a novel data-driven control strategy for maintaining
connectivity in networked multi-robot systems. Existing approaches often rely
on a pre-determined communication model specifying whether pairwise robots can
communicate given their relative distance to guide the connectivity-aware
control design, which may not capture real-world communication conditions. To
relax that assumption, we present the concept of Data-driven Connectivity
Barrier Certificates, which utilize Control Barrier Functions (CBF) and
Gaussian Processes (GP) to characterize the admissible control space for
pairwise robots based on communication performance observed online. This allows
robots to maintain a satisfying level of pairwise communication quality
(measured by the received signal strength) while in motion. Then we propose a
Data-driven Connectivity Maintenance (DCM) algorithm that combines (1) online
learning of the communication signal strength and (2) a bi-level
optimization-based control framework for the robot team to enforce global
connectivity of the realistic multi-robot communication graph and minimally
deviate from their task-related motions. We provide theoretical proofs to
justify the properties of our algorithm and demonstrate its effectiveness
through simulations with up to 20 robots."
CodeCipher: Learning to Obfuscate Source Code Against LLMs,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"While large code language models have made significant strides in AI-assisted
coding tasks, there are growing concerns about privacy challenges. The user
code is transparent to the cloud LLM service provider, inducing risks of
unauthorized training, reading, and execution of the user code. In this paper,
we propose CodeCipher, a novel method that perturbs privacy from code while
preserving the original response from LLMs. CodeCipher transforms the LLM's
embedding matrix so that each row corresponds to a different word in the
original matrix, forming a token-to-token confusion mapping for obfuscating
source code. The new embedding matrix is optimized by minimizing the
task-specific loss function. To tackle the challenge of the discrete and sparse
nature of word vector spaces, CodeCipher adopts a discrete optimization
strategy that aligns the updated vector to the nearest valid token in the
vocabulary before each gradient update. We demonstrate the effectiveness of our
approach on three AI-assisted coding tasks including code completion,
summarization, and translation. Results show that our model successfully
confuses the privacy in source code while preserving the original LLM's
performance."
Hybrid Gripper with Passive Pneumatic Soft Joints for Grasping Deformable Thin Objects,cs.RO,Robotics,2024-10-08,"Grasping a variety of objects remains a key challenge in the development of
versatile robotic systems. The human hand is remarkably dexterous, capable of
grasping and manipulating objects with diverse shapes, mechanical properties,
and textures. Inspired by how humans use two fingers to pick up thin and large
objects such as fabric or sheets of paper, we aim to develop a gripper
optimized for grasping such deformable objects. Observing how the soft and
flexible fingertip joints of the hand approach and grasp thin materials, a
hybrid gripper design that incorporates both soft and rigid components was
proposed. The gripper utilizes a soft pneumatic ring wrapped around a rigid
revolute joint to create a flexible two-fingered gripper. Experiments were
conducted to characterize and evaluate the gripper performance in handling
sheets of paper and other objects. Compared to rigid grippers, the proposed
design improves grasping efficiency and reduces the gripping distance by up to
eightfold."
Enhanced Feature Based Granular Ball Twin Support Vector Machine,cs.LG,Machine Learning,2024-10-08,"In this paper, we propose enhanced feature based granular ball twin support
vector machine (EF-GBTSVM). EF-GBTSVM employs the coarse granularity of
granular balls (GBs) as input rather than individual data samples. The GBs are
mapped to the feature space of the hidden layer using random projection
followed by the utilization of a non-linear activation function. The
concatenation of original and hidden features derived from the centers of GBs
gives rise to an enhanced feature space, commonly referred to as the random
vector functional link (RVFL) space. This space encapsulates nuanced feature
information to GBs. Further, we employ twin support vector machine (TSVM) in
the RVFL space for classification. TSVM generates the two non-parallel
hyperplanes in the enhanced feature space, which improves the generalization
performance of the proposed EF-GBTSVM model. Moreover, the coarser granularity
of the GBs enables the proposed EF-GBTSVM model to exhibit robustness to
resampling, showcasing reduced susceptibility to the impact of noise and
outliers. We undertake a thorough evaluation of the proposed EF-GBTSVM model on
benchmark UCI and KEEL datasets. This evaluation encompasses scenarios with and
without the inclusion of label noise. Moreover, experiments using NDC datasets
further emphasize the proposed model's ability to handle large datasets.
Experimental results, supported by thorough statistical analyses, demonstrate
that the proposed EF-GBTSVM model significantly outperforms the baseline models
in terms of generalization capabilities, scalability, and robustness. The
source code for the proposed EF-GBTSVM model, along with additional results and
further details, can be accessed at https://github.com/mtanveer1/EF-GBTSVM."
Contextual Bandits with Non-Stationary Correlated Rewards for User Association in MmWave Vehicular Networks,cs.LG,Machine Learning,2024-10-08,"Millimeter wave (mmWave) communication has emerged as a propelling technology
in vehicular communication. Usually, an appropriate decision on user
association requires timely channel information between vehicles and base
stations (BSs), which is challenging given a fast-fading mmWave vehicular
channel. In this paper, relying solely on learning transmission rate, we
propose a low-complexity semi-distributed contextual correlated upper
confidence bound (SD-CC-UCB) algorithm to establish an up-to-date user
association without explicit measurement of channel state information (CSI).
Under a contextual multi-arm bandits framework, SD-CC-UCB learns and predicts
the transmission rate given the location and velocity of the vehicle, which can
adequately capture the intricate channel condition for a prompt decision on
user association. Further, SD-CC-UCB efficiently identifies the set of
candidate BSs which probably support supreme transmission rate by leveraging
the correlated distributions of transmission rates on different locations. To
further refine the learning transmission rate over the link to candidate BSs,
each vehicle deploys the Thompson Sampling algorithm by taking the interference
among vehicles and handover overhead into consideration. Numerical results show
that our proposed algorithm achieves the network throughput within 100%-103% of
a benchmark algorithm which requires perfect instantaneous CSI, demonstrating
the effectiveness of SD-CC-UCB in vehicular communications."
Reinforcement Learning From Imperfect Corrective Actions And Proxy Rewards,cs.LG,Machine Learning,2024-10-08,"In practice, reinforcement learning (RL) agents are often trained with a
possibly imperfect proxy reward function, which may lead to a human-agent
alignment issue (i.e., the learned policy either converges to non-optimal
performance with low cumulative rewards, or achieves high cumulative rewards
but in undesired manner). To tackle this issue, we consider a framework where a
human labeler can provide additional feedback in the form of corrective
actions, which expresses the labeler's action preferences although this
feedback may possibly be imperfect as well. In this setting, to obtain a
better-aligned policy guided by both learning signals, we propose a novel
value-based deep RL algorithm called Iterative learning from Corrective actions
and Proxy rewards (ICoPro), which cycles through three phases: (1) Solicit
sparse corrective actions from a human labeler on the agent's demonstrated
trajectories; (2) Incorporate these corrective actions into the Q-function
using a margin loss to enforce adherence to labeler's preferences; (3) Train
the agent with standard RL losses regularized with a margin loss to learn from
proxy rewards and propagate the Q-values learned from human feedback. Moreover,
another novel design in our approach is to integrate pseudo-labels from the
target Q-network to reduce human labor and further stabilize training. We
experimentally validate our proposition on a variety of tasks (Atari games and
autonomous driving on highway). On the one hand, using proxy rewards with
different levels of imperfection, our method can better align with human
preferences and is more sample-efficient than baseline methods. On the other
hand, facing corrective actions with different types of imperfection, our
method can overcome the non-optimality of this feedback thanks to the guidance
from proxy reward."
Song Emotion Classification of Lyrics with Out-of-Domain Data under Label Scarcity,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Songs have been found to profoundly impact human emotions, with lyrics having
significant power to stimulate emotional changes in the audience. There is a
scarcity of large, high quality in-domain datasets for lyrics-based song
emotion classification (Edmonds and Sedoc, 2021; Zhou, 2022). It has been noted
that in-domain training datasets are often difficult to acquire (Zhang and
Miao, 2023) and that label acquisition is often limited by cost, time, and
other factors (Azad et al., 2018). We examine the novel usage of a large
out-of-domain dataset as a creative solution to the challenge of training data
scarcity in the emotional classification of song lyrics. We find that CNN
models trained on a large Reddit comments dataset achieve satisfactory
performance and generalizability to lyrical emotion classification, thus giving
insights into and a promising possibility in leveraging large, publicly
available out-of-domain datasets for domains whose in-domain data are lacking
or costly to acquire."
Viscoelasticity Estimation of Sports Prosthesis by Energy-minimizing Inverse Kinematics and Its Validation by Forward Dynamics,cs.RO,Robotics,2024-10-08,"In this study, we present a method for estimating the viscoelasticity of a
leaf-spring sports prosthesis using advanced energy minimizing inverse
kinematics based on the Piece-wise Constant Strain (PCS) model to reconstruct
the three-dimensional dynamic behavior. Dynamic motion analysis of the athlete
and prosthesis is important to clarify the effect of prosthesis characteristics
on foot function. However, three-dimensional deformation calculations of the
prosthesis and viscoelasticity have rarely been investigated. In this letter,
we apply the PCS model to a prosthesis deformation, which can calculate
flexible deformation with low computational cost and handle kinematics and
dynamics. In addition, we propose an inverse kinematics calculation method that
is consistent with the material properties of the prosthesis by considering the
minimization of elastic energy. Furthermore, we propose a method to estimate
the viscoelasticity by solving a quadratic programming based on the measured
motion capture data. The calculated strains are more reasonable than the
results obtained by conventional inverse kinematics calculation. From the
result of the viscoelasticity estimation, we simulate the prosthetic motion by
forward dynamics calculation and confirm that this result corresponds to the
measured motion. These results indicate that our approach adequately models the
dynamic phenomena, including the viscoelasticity of the prosthesis."
ActionAtlas: A VideoQA Benchmark for Domain-specialized Action Recognition,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Our world is full of varied actions and moves across specialized domains that
we, as humans, strive to identify and understand. Within any single domain,
actions can often appear quite similar, making it challenging for deep models
to distinguish them accurately. To evaluate the effectiveness of multimodal
foundation models in helping us recognize such actions, we present ActionAtlas
v1.0, a multiple-choice video question answering benchmark featuring short
videos across various sports. Each video in the dataset is paired with a
question and four or five choices. The question pinpoints specific individuals,
asking which choice ""best"" describes their action within a certain temporal
context. Overall, the dataset includes 934 videos showcasing 580 unique actions
across 56 sports, with a total of 1896 actions within choices. Unlike most
existing video question answering benchmarks that only cover simplistic
actions, often identifiable from a single frame, ActionAtlas focuses on
intricate movements and rigorously tests the model's capability to discern
subtle differences between moves that look similar within each domain. We
evaluate open and proprietary foundation models on this benchmark, finding that
the best model, GPT-4o, achieves a maximum accuracy of 45.52%. Meanwhile,
Non-expert crowd workers, provided with action description for each choice,
achieve 61.64% accuracy, where random chance is approximately 21%. Our findings
with state-of-the-art models indicate that having a high frame sampling rate is
important for accurately recognizing actions in ActionAtlas, a feature that
some leading proprietary video models, such as Gemini, do not include in their
default configuration."
GLRT-Based Metric Learning for Remote Sensing Object Retrieval,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"With the improvement in the quantity and quality of remote sensing images,
content-based remote sensing object retrieval (CBRSOR) has become an
increasingly important topic. However, existing CBRSOR methods neglect the
utilization of global statistical information during both training and test
stages, which leads to the overfitting of neural networks to simple sample
pairs of samples during training and suboptimal metric performance. Inspired by
the Neyman-Pearson theorem, we propose a generalized likelihood ratio
test-based metric learning (GLRTML) approach, which can estimate the relative
difficulty of sample pairs by incorporating global data distribution
information during training and test phases. This guides the network to focus
more on difficult samples during the training process, thereby encourages the
network to learn more discriminative feature embeddings. In addition, GLRT is a
more effective than traditional metric space due to the utilization of global
data distribution information. Accurately estimating the distribution of
embeddings is critical for GLRTML. However, in real-world applications, there
is often a distribution shift between the training and target domains, which
diminishes the effectiveness of directly using the distribution estimated on
training data. To address this issue, we propose the clustering
pseudo-labels-based fast parameter adaptation (CPLFPA) method. CPLFPA
efficiently estimates the distribution of embeddings in the target domain by
clustering target domain instances and re-estimating the distribution
parameters for GLRTML. We reorganize datasets for CBRSOR tasks based on
fine-grained ship remote sensing image slices (FGSRSI-23) and military aircraft
recognition (MAR20) datasets. Extensive experiments on these datasets
demonstrate the effectiveness of our proposed GLRTML and CPLFPA."
Comparative Analysis of Novel View Synthesis and Photogrammetry for 3D Forest Stand Reconstruction and extraction of individual tree parameters,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Accurate and efficient 3D reconstruction of trees is crucial for forest
resource assessments and management. Close-Range Photogrammetry (CRP) is
commonly used for reconstructing forest scenes but faces challenges like low
efficiency and poor quality. Recently, Novel View Synthesis (NVS) technologies,
including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have
shown promise for 3D plant reconstruction with limited images. However,
existing research mainly focuses on small plants in orchards or individual
trees, leaving uncertainty regarding their application in larger, complex
forest stands. In this study, we collected sequential images of forest plots
with varying complexity and performed dense reconstruction using NeRF and 3DGS.
The resulting point clouds were compared with those from photogrammetry and
laser scanning. Results indicate that NVS methods significantly enhance
reconstruction efficiency. Photogrammetry struggles with complex stands,
leading to point clouds with excessive canopy noise and incorrectly
reconstructed trees, such as duplicated trunks. NeRF, while better for canopy
regions, may produce errors in ground areas with limited views. The 3DGS method
generates sparser point clouds, particularly in trunk areas, affecting diameter
at breast height (DBH) accuracy. All three methods can extract tree height
information, with NeRF yielding the highest accuracy; however, photogrammetry
remains superior for DBH accuracy. These findings suggest that NVS methods have
significant potential for 3D reconstruction of forest stands, offering valuable
support for complex forest resource inventory and visualization tasks."
Cefdet: Cognitive Effectiveness Network Based on Fuzzy Inference for Action Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Action detection and understanding provide the foundation for the generation
and interaction of multimedia content. However, existing methods mainly focus
on constructing complex relational inference networks, overlooking the judgment
of detection effectiveness. Moreover, these methods frequently generate
detection results with cognitive abnormalities. To solve the above problems,
this study proposes a cognitive effectiveness network based on fuzzy inference
(Cefdet), which introduces the concept of ""cognition-based detection"" to
simulate human cognition. First, a fuzzy-driven cognitive effectiveness
evaluation module (FCM) is established to introduce fuzzy inference into action
detection. FCM is combined with human action features to simulate the
cognition-based detection process, which clearly locates the position of frames
with cognitive abnormalities. Then, a fuzzy cognitive update strategy (FCS) is
proposed based on the FCM, which utilizes fuzzy logic to re-detect the
cognition-based detection results and effectively update the results with
cognitive abnormalities. Experimental results demonstrate that Cefdet exhibits
superior performance against several mainstream algorithms on the public
datasets, validating its effectiveness and superiority."
Efficient Few-shot Learning for Multi-label Classification of Scientific Documents with Many Classes,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Scientific document classification is a critical task and often involves many
classes. However, collecting human-labeled data for many classes is expensive
and usually leads to label-scarce scenarios. Moreover, recent work has shown
that sentence embedding model fine-tuning for few-shot classification is
efficient, robust, and effective. In this work, we propose FusionSent
(Fusion-based Sentence Embedding Fine-tuning), an efficient and prompt-free
approach for few-shot classification of scientific documents with many classes.
FusionSent uses available training examples and their respective label texts to
contrastively fine-tune two different sentence embedding models. Afterward, the
parameters of both fine-tuned models are fused to combine the complementary
knowledge from the separate fine-tuning steps into a single model. Finally, the
resulting sentence embedding model is frozen to embed the training instances,
which are then used as input features to train a classification head. Our
experiments show that FusionSent significantly outperforms strong baselines by
an average of $6.0$ $F_{1}$ points across multiple scientific document
classification datasets. In addition, we introduce a new dataset for
multi-label classification of scientific documents, which contains 183,565
scientific articles and 130 classes from the arXiv category taxonomy. Code and
data are available at https://github.com/sebischair/FusionSent."
Grounding is All You Need? Dual Temporal Grounding for Video Dialog,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"In the realm of video dialog response generation, the understanding of video
content and the temporal nuances of conversation history are paramount. While a
segment of current research leans heavily on large-scale pretrained
visual-language models and often overlooks temporal dynamics, another delves
deep into spatial-temporal relationships within videos but demands intricate
object trajectory pre-extractions and sidelines dialog temporal dynamics. This
paper introduces the Dual Temporal Grounding-enhanced Video Dialog model
(DTGVD), strategically designed to merge the strengths of both dominant
approaches. It emphasizes dual temporal relationships by predicting dialog
turn-specific temporal regions, filtering video content accordingly, and
grounding responses in both video and dialog contexts. One standout feature of
DTGVD is its heightened attention to chronological interplay. By recognizing
and acting upon the dependencies between different dialog turns, it captures
more nuanced conversational dynamics. To further bolster the alignment between
video and dialog temporal dynamics, we've implemented a list-wise contrastive
learning strategy. Within this framework, accurately grounded turn-clip
pairings are designated as positive samples, while less precise pairings are
categorized as negative. This refined classification is then funneled into our
holistic end-to-end response generation mechanism. Evaluations using
AVSD@DSTC-7 and AVSD@DSTC-8 datasets underscore the superiority of our
methodology."
StagedVulBERT: Multi-Granular Vulnerability Detection with a Novel Pre-trained Code Model,cs.CR,Cryptography and Security,2024-10-08,"The emergence of pre-trained model-based vulnerability detection methods has
significantly advanced the field of automated vulnerability detection. However,
these methods still face several challenges, such as difficulty in learning
effective feature representations of statements for fine-grained predictions
and struggling to process overly long code sequences. To address these issues,
this study introduces StagedVulBERT, a novel vulnerability detection framework
that leverages a pre-trained code language model and employs a coarse-to-fine
strategy. The key innovation and contribution of our research lies in the
development of the CodeBERT-HLS component within our framework, specialized in
hierarchical, layered, and semantic encoding. This component is designed to
capture semantics at both the token and statement levels simultaneously, which
is crucial for achieving more accurate multi-granular vulnerability detection.
Additionally, CodeBERT-HLS efficiently processes longer code token sequences,
making it more suited to real-world vulnerability detection. Comprehensive
experiments demonstrate that our method enhances the performance of
vulnerability detection at both coarse- and fine-grained levels. Specifically,
in coarse-grained vulnerability detection, StagedVulBERT achieves an F1 score
of 92.26%, marking a 6.58% improvement over the best-performing methods. At the
fine-grained level, our method achieves a Top-5% accuracy of 65.69%, which
outperforms the state-of-the-art methods by up to 75.17%."
Guided Self-attention: Find the Generalized Necessarily Distinct Vectors for Grain Size Grading,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"With the development of steel materials, metallographic analysis has become
increasingly important. Unfortunately, grain size analysis is a manual process
that requires experts to evaluate metallographic photographs, which is
unreliable and time-consuming. To resolve this problem, we propose a novel
classifi-cation method based on deep learning, namely GSNets, a family of
hybrid models which can effectively introduce guided self-attention for
classifying grain size. Concretely, we build our models from three insights:(1)
Introducing our novel guided self-attention module can assist the model in
finding the generalized necessarily distinct vectors capable of retaining
intricate rela-tional connections and rich local feature information; (2) By
improving the pixel-wise linear independence of the feature map, the highly
condensed semantic representation will be captured by the model; (3) Our novel
triple-stream merging module can significantly improve the generalization
capability and efficiency of the model. Experiments show that our GSNet yields
a classifi-cation accuracy of 90.1%, surpassing the state-of-the-art Swin
Transformer V2 by 1.9% on the steel grain size dataset, which comprises 3,599
images with 14 grain size levels. Furthermore, we intuitively believe our
approach is applicable to broader ap-plications like object detection and
semantic segmentation."
Training-free Diffusion Model Alignment with Sampling Demons,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Aligning diffusion models with user preferences has been a key challenge.
Existing methods for aligning diffusion models either require retraining or are
limited to differentiable reward functions. To address these limitations, we
propose a stochastic optimization approach, dubbed Demon, to guide the
denoising process at inference time without backpropagation through reward
functions or model retraining. Our approach works by controlling noise
distribution in denoising steps to concentrate density on regions corresponding
to high rewards through stochastic optimization. We provide comprehensive
theoretical and empirical evidence to support and validate our approach,
including experiments that use non-differentiable sources of rewards such as
Visual-Language Model (VLM) APIs and human judgements. To the best of our
knowledge, the proposed approach is the first inference-time,
backpropagation-free preference alignment method for diffusion models. Our
method can be easily integrated with existing diffusion models without further
training. Our experiments show that the proposed approach significantly
improves the average aesthetics scores for text-to-image generation."
Learning the Generalizable Manipulation Skills on Soft-body Tasks via Guided Self-attention Behavior Cloning Policy,cs.RO,Robotics,2024-10-08,"Embodied AI represents a paradigm in AI research where artificial agents are
situated within and interact with physical or virtual environments. Despite the
recent progress in Embodied AI, it is still very challenging to learn the
generalizable manipulation skills that can handle large deformation and
topological changes on soft-body objects, such as clay, water, and soil. In
this work, we proposed an effective policy, namely GP2E behavior cloning
policy, which can guide the agent to learn the generalizable manipulation
skills from soft-body tasks, including pouring, filling, hanging, excavating,
pinching, and writing. Concretely, we build our policy from three insights:(1)
Extracting intricate semantic features from point cloud data and seamlessly
integrating them into the robot's end-effector frame; (2) Capturing
long-distance interactions in long-horizon tasks through the incorporation of
our guided self-attention module; (3) Mitigating overfitting concerns and
facilitating model convergence to higher accuracy levels via the introduction
of our two-stage fine-tuning strategy. Through extensive experiments, we
demonstrate the effectiveness of our approach by achieving the 1st prize in the
soft-body track of the ManiSkill2 Challenge at the CVPR 2023 4th Embodied AI
workshop. Our findings highlight the potential of our method to improve the
generalization abilities of Embodied AI models and pave the way for their
practical applications in real-world scenarios."
Exploring the Meaningfulness of Nearest Neighbor Search in High-Dimensional Space,cs.LG,Machine Learning,2024-10-08,"Dense high dimensional vectors are becoming increasingly vital in fields such
as computer vision, machine learning, and large language models (LLMs), serving
as standard representations for multimodal data. Now the dimensionality of
these vector can exceed several thousands easily. Despite the nearest neighbor
search (NNS) over these dense high dimensional vectors have been widely used
for retrieval augmented generation (RAG) and many other applications, the
effectiveness of NNS in such a high-dimensional space remains uncertain, given
the possible challenge caused by the ""curse of dimensionality."" To address
above question, in this paper, we conduct extensive NNS studies with different
distance functions, such as $L_1$ distance, $L_2$ distance and
angular-distance, across diverse embedding datasets, of varied types,
dimensionality and modality. Our aim is to investigate factors influencing the
meaningfulness of NNS. Our experiments reveal that high-dimensional text
embeddings exhibit increased resilience as dimensionality rises to higher
levels when compared to random vectors. This resilience suggests that text
embeddings are less affected to the ""curse of dimensionality,"" resulting in
more meaningful NNS outcomes for practical use. Additionally, the choice of
distance function has minimal impact on the relevance of NNS. Our study shows
the effectiveness of the embedding-based data representation method and can
offer opportunity for further optimization of dense vector-related
applications."
Polynomial Time Cryptanalytic Extraction of Deep Neural Networks in the Hard-Label Setting,cs.CR,Cryptography and Security,2024-10-08,"Deep neural networks (DNNs) are valuable assets, yet their public
accessibility raises security concerns about parameter extraction by malicious
actors. Recent work by Carlini et al. (crypto'20) and Canales-Mart\'inez et al.
(eurocrypt'24) has drawn parallels between this issue and block cipher key
extraction via chosen plaintext attacks. Leveraging differential cryptanalysis,
they demonstrated that all the weights and biases of black-box ReLU-based DNNs
could be inferred using a polynomial number of queries and computational time.
However, their attacks relied on the availability of the exact numeric value of
output logits, which allowed the calculation of their derivatives. To overcome
this limitation, Chen et al. (asiacrypt'24) tackled the more realistic
hard-label scenario, where only the final classification label (e.g., ""dog"" or
""car"") is accessible to the attacker. They proposed an extraction method
requiring a polynomial number of queries but an exponential execution time. In
addition, their approach was applicable only to a restricted set of
architectures, could deal only with binary classifiers, and was demonstrated
only on tiny neural networks with up to four neurons split among up to two
hidden layers. This paper introduces new techniques that, for the first time,
achieve cryptanalytic extraction of DNN parameters in the most challenging
hard-label setting, using both a polynomial number of queries and polynomial
time. We validate our approach by extracting nearly one million parameters from
a DNN trained on the CIFAR-10 dataset, comprising 832 neurons in four hidden
layers. Our results reveal the surprising fact that all the weights of a
ReLU-based DNN can be efficiently determined by analyzing only the geometric
shape of its decision boundaries."
Label Confidence Weighted Learning for Target-level Sentence Simplification,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Multi-level sentence simplification generates simplified sentences with
varying language proficiency levels. We propose Label Confidence Weighted
Learning (LCWL), a novel approach that incorporates a label confidence
weighting scheme in the training loss of the encoder-decoder model, setting it
apart from existing confidence-weighting methods primarily designed for
classification. Experimentation on English grade-level simplification dataset
shows that LCWL outperforms state-of-the-art unsupervised baselines.
Fine-tuning the LCWL model on in-domain data and combining with Symmetric Cross
Entropy (SCE) consistently delivers better simplifications compared to strong
supervised methods. Our results highlight the effectiveness of label confidence
weighting techniques for text simplification tasks with encoder-decoder
architectures."
Wolf2Pack: The AutoFusion Framework for Dynamic Parameter Fusion,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"In the rapidly evolving field of deep learning, specialized models have
driven significant advancements in tasks such as computer vision and natural
language processing. However, this specialization leads to a fragmented
ecosystem where models lack the adaptability for broader applications. To
overcome this, we introduce AutoFusion, an innovative framework that fuses
distinct model parameters(with the same architecture) for multi-task learning
without pre-trained checkpoints. Using an unsupervised, end-to-end approach,
AutoFusion dynamically permutes model parameters at each layer, optimizing the
combination through a loss-minimization process that does not require labeled
data. We validate AutoFusion's effectiveness through experiments on commonly
used benchmark datasets, demonstrating superior performance over established
methods like Weight Interpolation, Git Re-Basin, and ZipIt. Our framework
offers a scalable and flexible solution for model integration, positioning it
as a powerful tool for future research and practical applications."
Learning to Race in Extreme Turning Scene with Active Exploration and Gaussian Process Regression-based MPC,cs.RO,Robotics,2024-10-08,"Extreme cornering in racing often induces large side-slip angles, presenting
a formidable challenge in vehicle control. To tackle this issue, this paper
introduces an Active Exploration with Double GPR (AEDGPR) system. The system
initiates by planning a minimum-time trajectory with a Gaussian Process
Regression(GPR) compensated model. The planning results show that in the
cornering section, the yaw angular velocity and side-slip angle are in opposite
directions, indicating that the vehicle is drifting. In response, we develop a
drift controller based on Model Predictive Control (MPC) and incorporate
Gaussian Process Regression to correct discrepancies in the vehicle dynamics
model. Moreover, the covariance from the GPR is employed to actively explore
various cornering states, aiming to minimize trajectory tracking errors. The
proposed algorithm is validated through simulations on the Simulink-Carsim
platform and experiments using a 1/10 scale RC vehicle."
"Design, Localization, Perception, and Control for GPS-Denied Autonomous Aerial Grasping and Harvesting",cs.RO,Robotics,2024-10-08,"In this paper, we present a comprehensive UAV system design to perform the
highly complex task of off-centered aerial grasping. This task has several
interdisciplinary research challenges which need to be addressed at once. The
main design challenges are GPS-denied functionality, solely onboard computing,
and avoiding off-the-shelf costly positioning systems. While in terms of
algorithms, visual perception, localization, control, and grasping are the
leading research problems. Hence in this paper, we make interdisciplinary
contributions: (i) A detailed description of the fundamental challenges in
indoor aerial grasping, (ii) a novel lightweight gripper design, (iii) a
complete aerial platform design and in-lab fabrication, and (iv) localization,
perception, control, grasping systems, and an end-to-end flight autonomy
state-machine. Finally, we demonstrate the resulting aerial grasping system
Drone-Bee achieving a high grasping rate for a highly challenging agricultural
task of apple-like fruit harvesting, indoors in a vertical farming setting
(Fig. 1). To our knowledge, such a system has not been previously discussed in
the literature, and with its capabilities, this system pushes aerial
manipulation towards 4th generation."
Thrust Microstepping via Acceleration Feedback in Quadrotor Control for Aerial Grasping of Dynamic Payload,cs.RO,Robotics,2024-10-08,"In this work, we propose an end-to-end Thrust Microstepping and Decoupled
Control (TMDC) of quadrotors. TMDC focuses on precise off-centered aerial
grasping of payloads dynamically, which are attached rigidly to the UAV body
via a gripper contrary to the swinging payload. The dynamic payload grasping
quickly changes UAV's mass, inertia etc, causing instability while performing a
grasping operation in-air. We identify that to handle unknown payload grasping,
the role of thrust controller is crucial. Hence, we focus on thrust control
without involving system parameters such as mass etc. TMDC is based on our
novel Thrust Microstepping via Acceleration Feedback (TMAF) thrust controller
and Decoupled Motion Control (DMC). TMAF precisely estimates the desired thrust
even at smaller loop rates while DMC decouples the horizontal and vertical
motion to counteract disturbances in the case of dynamic payloads. We prove the
controller's efficacy via exhaustive experiments in practically interesting and
adverse real-world cases, such as fully onboard state estimation without any
positioning sensor, narrow and indoor flying workspaces with intense wind
turbulence, heavy payloads, non-uniform loop rates, etc. Our TMDC outperforms
recent direct acceleration feedback thrust controller (DA) and geometric
tracking control (GT) in flying stably for aerial grasping and achieves RMSE
below 0.04m in contrast to 0.15m of DA and 0.16m of GT."
CUBE360: Learning Cubic Field Representation for Monocular 360 Depth Estimation for Virtual Reality,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Panoramic images provide comprehensive scene information and are suitable for
VR applications. Obtaining corresponding depth maps is essential for achieving
immersive and interactive experiences. However, panoramic depth estimation
presents significant challenges due to the severe distortion caused by
equirectangular projection (ERP) and the limited availability of panoramic
RGB-D datasets. Inspired by the recent success of neural rendering, we propose
a novel method, named $\mathbf{CUBE360}$, that learns a cubic field composed of
multiple MPIs from a single panoramic image for $\mathbf{continuous}$ depth
estimation at any view direction. Our CUBE360 employs cubemap projection to
transform an ERP image into six faces and extract the MPIs for each, thereby
reducing the memory consumption required for MPI processing of high-resolution
data. Additionally, this approach avoids the computational complexity of
handling the uneven pixel distribution inherent to equirectangular projectio.
An attention-based blending module is then employed to learn correlations among
the MPIs of cubic faces, constructing a cubic field representation with color
and density information at various depth levels. Furthermore, a novel sampling
strategy is introduced for rendering novel views from the cubic field at both
cubic and planar scales. The entire pipeline is trained using photometric loss
calculated from rendered views within a self-supervised learning approach,
enabling training on 360 videos without depth annotations. Experiments on both
synthetic and real-world datasets demonstrate the superior performance of
CUBE360 compared to prior SSL methods. We also highlight its effectiveness in
downstream applications, such as VR roaming and visual effects, underscoring
CUBE360's potential to enhance immersive experiences."
Diminishing Exploration: A Minimalist Approach to Piecewise Stationary Multi-Armed Bandits,cs.LG,Machine Learning,2024-10-08,"The piecewise-stationary bandit problem is an important variant of the
multi-armed bandit problem that further considers abrupt changes in the reward
distributions. The main theme of the problem is the trade-off between
exploration for detecting environment changes and exploitation of traditional
bandit algorithms. While this problem has been extensively investigated,
existing works either assume knowledge about the number of change points $M$ or
require extremely high computational complexity. In this work, we revisit the
piecewise-stationary bandit problem from a minimalist perspective. We propose a
novel and generic exploration mechanism, called diminishing exploration, which
eliminates the need for knowledge about $M$ and can be used in conjunction with
an existing change detection-based algorithm to achieve near-optimal regret
scaling. Simulation results show that despite oblivious of $M$, equipping
existing algorithms with the proposed diminishing exploration generally
achieves better empirical regret than the traditional uniform exploration."
Private and Communication-Efficient Federated Learning based on Differentially Private Sketches,cs.LG,Machine Learning,2024-10-08,"Federated learning (FL) faces two primary challenges: the risk of privacy
leakage due to parameter sharing and communication inefficiencies. To address
these challenges, we propose DPSFL, a federated learning method that utilizes
differentially private sketches. DPSFL compresses the local gradients of each
client using a count sketch, thereby improving communication efficiency, while
adding noise to the sketches to ensure differential privacy (DP). We provide a
theoretical analysis of privacy and convergence for the proposed method.
Gradient clipping is essential in DP learning to limit sensitivity and
constrain the addition of noise. However, clipping introduces bias into the
gradients, negatively impacting FL performance. To mitigate the impact of
clipping, we propose an enhanced method, DPSFL-AC, which employs an adaptive
clipping strategy. Experimental comparisons with existing techniques
demonstrate the superiority of our methods concerning privacy preservation,
communication efficiency, and model accuracy."
Equi-GSPR: Equivariant SE(3) Graph Network Model for Sparse Point Cloud Registration,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Point cloud registration is a foundational task for 3D alignment and
reconstruction applications. While both traditional and learning-based
registration approaches have succeeded, leveraging the intrinsic symmetry of
point cloud data, including rotation equivariance, has received insufficient
attention. This prohibits the model from learning effectively, resulting in a
requirement for more training data and increased model complexity. To address
these challenges, we propose a graph neural network model embedded with a local
Spherical Euclidean 3D equivariance property through SE(3) message passing
based propagation. Our model is composed mainly of a descriptor module,
equivariant graph layers, match similarity, and the final regression layers.
Such modular design enables us to utilize sparsely sampled input points and
initialize the descriptor by self-trained or pre-trained geometric feature
descriptors easily. Experiments conducted on the 3DMatch and KITTI datasets
exhibit the compelling and robust performance of our model compared to
state-of-the-art approaches, while the model complexity remains relatively low
at the same time."
Reducing fuzzy relation equations via concept lattices,cs.AI,Artificial Intelligence,2024-10-08,"This paper has taken into advantage the relationship between Fuzzy Relation
Equations (FRE) and Concept Lattices in order to introduce a procedure to
reduce a FRE, without losing information. Specifically, attribute reduction
theory in property-oriented and object-oriented concept lattices has been
considered in order to present a mechanism for detecting redundant equations.
As a first consequence, the computation of the whole solution set of a solvable
FRE is reduced. Moreover, we will also introduce a novel method for computing
approximate solutions of unsolvable FRE related to a (real) dataset with
uncertainty/imprecision data."
Less is more: Embracing sparsity and interpolation with Esiformer for time series forecasting,cs.LG,Machine Learning,2024-10-08,"Time series forecasting has played a significant role in many practical
fields. But time series data generated from real-world applications always
exhibits high variance and lots of noise, which makes it difficult to capture
the inherent periodic patterns of the data, hurting the prediction accuracy
significantly. To address this issue, we propose the Esiformer, which apply
interpolation on the original data, decreasing the overall variance of the data
and alleviating the influence of noise. What's more, we enhanced the vanilla
transformer with a robust Sparse FFN. It can enhance the representation ability
of the model effectively, and maintain the excellent robustness, avoiding the
risk of overfitting compared with the vanilla implementation. Through
evaluations on challenging real-world datasets, our method outperforms leading
model PatchTST, reducing MSE by 6.5% and MAE by 5.8% in multivariate time
series forecasting. Code is available at:
https://github.com/yyg1282142265/Esiformer/tree/main."
KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server,cs.CR,Cryptography and Security,2024-10-08,"The success of large language models (LLMs) facilitate many parties to
fine-tune LLMs on their own private data. However, this practice raises privacy
concerns due to the memorization of LLMs. Existing solutions, such as utilizing
synthetic data for substitution, struggle to simultaneously improve performance
and preserve privacy. They either rely on a local model for generation,
resulting in a performance decline, or take advantage of APIs, directly
exposing the data to API servers. To address this issue, we propose
\textit{KnowledgeSG}, a novel client-server framework which enhances synthetic
data quality and improves model performance while ensuring privacy. We achieve
this by learning local knowledge from the private data with differential
privacy (DP) and distilling professional knowledge from the server.
Additionally, inspired by federated learning, we transmit models rather than
data between the client and server to prevent privacy leakage. Extensive
experiments in medical and financial domains demonstrate the effectiveness of
KnowledgeSG. Our code is now publicly available at
https://github.com/wwh0411/KnowledgeSG."
Mero Nagarikta: Advanced Nepali Citizenship Data Extractor with Deep Learning-Powered Text Detection and OCR,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Transforming text-based identity documents, such as Nepali citizenship cards,
into a structured digital format poses several challenges due to the distinct
characteristics of the Nepali script and minor variations in print alignment
and contrast across different cards. This work proposes a robust system using
YOLOv8 for accurate text object detection and an OCR algorithm based on
Optimized PyTesseract. The system, implemented within the context of a mobile
application, allows for the automated extraction of important textual
information from both the front and the back side of Nepali citizenship cards,
including names, citizenship numbers, and dates of birth. The final YOLOv8
model was accurate, with a mean average precision of 99.1% for text detection
on the front and 96.1% on the back. The tested PyTesseract optimized for Nepali
characters outperformed the standard OCR regarding flexibility and accuracy,
extracting text from images with clean and noisy backgrounds and various
contrasts. Using preprocessing steps such as converting the images into
grayscale, removing noise from the images, and detecting edges further improved
the system's OCR accuracy, even for low-quality photos. This work expands the
current body of research in multilingual OCR and document analysis, especially
for low-resource languages such as Nepali. It emphasizes the effectiveness of
combining the latest object detection framework with OCR models that have been
fine-tuned for practical applications."
Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"This research addresses the need for high-definition (HD) maps for autonomous
vehicles (AVs), focusing on road lane information derived from aerial imagery.
While Earth observation data offers valuable resources for map creation,
specialized models for road lane extraction are still underdeveloped in remote
sensing. In this study, we perform an extensive comparison of twelve
foundational deep learning-based semantic segmentation models for road lane
marking extraction from high-definition remote sensing images, assessing their
performance under transfer learning with partially labeled datasets. These
models were fine-tuned on the partially labeled Waterloo Urban Scene dataset,
and pre-trained on the SkyScapes dataset, simulating a likely scenario of
real-life model deployment under partial labeling. We observed and assessed the
fine-tuning performance and overall performance. Models showed significant
performance improvements after fine-tuning, with mean IoU scores ranging from
33.56% to 76.11%, and recall ranging from 66.0% to 98.96%. Transformer-based
models outperformed convolutional neural networks, emphasizing the importance
of model pre-training and fine-tuning in enhancing HD map development for AV
navigation."
Demonstration Based Explainable AI for Learning from Demonstration Methods,cs.RO,Robotics,2024-10-08,"Learning from Demonstration (LfD) is a powerful type of machine learning that
can allow novices to teach and program robots to complete various tasks.
However, the learning process for these systems may still be difficult for
novices to interpret and understand, making effective teaching challenging.
Explainable artificial intelligence (XAI) aims to address this challenge by
explaining a system to the user. In this work, we investigate XAI within LfD by
implementing an adaptive explanatory feedback system on an inverse
reinforcement learning (IRL) algorithm. The feedback is implemented by
demonstrating selected learnt trajectories to users. The system adapts to user
teaching by categorizing and then selectively sampling trajectories shown to a
user, to show a representative sample of both successful and unsuccessful
trajectories. The system was evaluated through a user study with 26
participants teaching a robot a navigation task. The results of the user study
demonstrated that the proposed explanatory feedback system can improve robot
performance, teaching efficiency and user understanding of the robot."
Enhancing Temporal Modeling of Video LLMs via Time Gating,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Video Large Language Models (Video LLMs) have achieved impressive performance
on video-and-language tasks, such as video question answering. However, most
existing Video LLMs neglect temporal information in video data, leading to
struggles with temporal-aware video understanding. To address this gap, we
propose a Time Gating Video LLM (TG-Vid) designed to enhance temporal modeling
through a novel Time Gating module (TG). The TG module employs a time gating
mechanism on its sub-modules, comprising gating spatial attention, gating
temporal attention, and gating MLP. This architecture enables our model to
achieve a robust understanding of temporal information within videos. Extensive
evaluation of temporal-sensitive video benchmarks (i.e., MVBench, TempCompass,
and NExT-QA) demonstrates that our TG-Vid model significantly outperforms the
existing Video LLMs. Further, comprehensive ablation studies validate that the
performance gains are attributed to the designs of our TG module. Our code is
available at https://github.com/LaVi-Lab/TG-Vid."
Diffusion Auto-regressive Transformer for Effective Self-supervised Time Series Forecasting,cs.LG,Machine Learning,2024-10-08,"Self-supervised learning has become a popular and effective approach for
enhancing time series forecasting, enabling models to learn universal
representations from unlabeled data. However, effectively capturing both the
global sequence dependence and local detail features within time series data
remains challenging. To address this, we propose a novel generative
self-supervised method called TimeDART, denoting Diffusion Auto-regressive
Transformer for Time series forecasting. In TimeDART, we treat time series
patches as basic modeling units. Specifically, we employ an self-attention
based Transformer encoder to model the dependencies of inter-patches.
Additionally, we introduce diffusion and denoising mechanisms to capture the
detail locality features of intra-patch. Notably, we design a
cross-attention-based denoising decoder that allows for adjustable optimization
difficulty in the self-supervised task, facilitating more effective
self-supervised pre-training. Furthermore, the entire model is optimized in an
auto-regressive manner to obtain transferable representations. Extensive
experiments demonstrate that TimeDART achieves state-of-the-art fine-tuning
performance compared to the most advanced competitive methods in forecasting
tasks. Our code is publicly available at
https://github.com/Melmaphother/TimeDART."
PixLens: A Novel Framework for Disentangled Evaluation in Diffusion-Based Image Editing with Object Detection + SAM,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Evaluating diffusion-based image-editing models is a crucial task in the
field of Generative AI. Specifically, it is imperative to assess their capacity
to execute diverse editing tasks while preserving the image content and
realism. While recent developments in generative models have opened up
previously unheard-of possibilities for image editing, conducting a thorough
evaluation of these models remains a challenging and open task. The absence of
a standardized evaluation benchmark, primarily due to the inherent need for a
post-edit reference image for evaluation, further complicates this issue.
Currently, evaluations often rely on established models such as CLIP or require
human intervention for a comprehensive understanding of the performance of
these image editing models. Our benchmark, PixLens, provides a comprehensive
evaluation of both edit quality and latent representation disentanglement,
contributing to the advancement and refinement of existing methodologies in the
field."
A First-Order Algorithm for Graph Learning from Smooth Signals Under Partial Observability,cs.LG,Machine Learning,2024-10-08,"Learning graph structures from smooth signals is a significant problem in
data science and engineering. A common challenge in real-world scenarios is the
availability of only partially observed nodes. While some studies have
considered hidden nodes and proposed various optimization frameworks, existing
methods often lack the practical efficiency needed for large-scale networks or
fail to provide theoretical convergence guarantees. In this paper, we address
the problem of inferring network topologies from smooth signals with partially
observed nodes. We propose a first-order algorithmic framework that includes
two variants: one based on column sparsity regularization and the other on a
low-rank constraint. We establish theoretical convergence guarantees and
demonstrate the linear convergence rate of our algorithms. Extensive
experiments on both synthetic and real-world data show that our results align
with theoretical predictions, exhibiting not only linear convergence but also
superior speed compared to existing methods. To the best of our knowledge, this
is the first work to propose a first-order algorithmic framework for inferring
network structures from smooth signals under partial observability, offering
both guaranteed linear convergence and practical effectiveness for large-scale
networks."
A Two-Step Approach for Data-Efficient French Pronunciation Learning,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Recent studies have addressed intricate phonological phenomena in French,
relying on either extensive linguistic knowledge or a significant amount of
sentence-level pronunciation data. However, creating such resources is
expensive and non-trivial. To this end, we propose a novel two-step approach
that encompasses two pronunciation tasks: grapheme-to-phoneme and post-lexical
processing. We then investigate the efficacy of the proposed approach with a
notably limited amount of sentence-level pronunciation data. Our findings
demonstrate that the proposed two-step approach effectively mitigates the lack
of extensive labeled data, and serves as a feasible solution for addressing
French phonological phenomena even under resource-constrained environments."
Diffusing to the Top: Boost Graph Neural Networks with Minimal Hyperparameter Tuning,cs.LG,Machine Learning,2024-10-08,"Graph Neural Networks (GNNs) are proficient in graph representation learning
and achieve promising performance on versatile tasks such as node
classification and link prediction. Usually, a comprehensive hyperparameter
tuning is essential for fully unlocking GNN's top performance, especially for
complicated tasks such as node classification on large graphs and long-range
graphs. This is usually associated with high computational and time costs and
careful design of appropriate search spaces. This work introduces a
graph-conditioned latent diffusion framework (GNN-Diff) to generate
high-performing GNNs based on the model checkpoints of sub-optimal
hyperparameters selected by a light-tuning coarse search. We validate our
method through 166 experiments across four graph tasks: node classification on
small, large, and long-range graphs, as well as link prediction. Our
experiments involve 10 classic and state-of-the-art target models and 20
publicly available datasets. The results consistently demonstrate that
GNN-Diff: (1) boosts the performance of GNNs with efficient hyperparameter
tuning; and (2) presents high stability and generalizability on unseen data
across multiple generation runs. The code is available at
https://github.com/lequanlin/GNN-Diff."
Unlocking the Boundaries of Thought: A Reasoning Granularity Framework to Quantify and Optimize Chain-of-Thought,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Chain-of-Thought (CoT) reasoning has emerged as a promising approach for
enhancing the performance of large language models (LLMs) on complex reasoning
tasks. Recently, a series of studies attempt to explain the mechanisms
underlying CoT, aiming to deepen the understanding of its efficacy.
Nevertheless, the existing research faces two major challenges: (1) a lack of
quantitative metrics to assess CoT capabilities and (2) a dearth of guidance on
optimizing CoT performance. Motivated by this, in this work, we introduce a
novel reasoning granularity framework (RGF) to address these challenges. To
solve the lack of quantification, we first define a reasoning granularity (RG)
to quantify the upper bound of CoT and establish a combination law for RG,
enabling a practical quantitative approach applicable to various real-world CoT
tasks. To address the lack of optimization, we propose three categories of RGs.
We further optimize these categories with combination laws focused on RG
promotion and reasoning path optimization for CoT improvement. Through
extensive experiments on 25 models and 4 tasks, the study validates the
existence and rationality of the proposed framework. Furthermore, it explains
the effectiveness of 10 CoT strategies and guides optimization from two
perspectives. We hope this work can provide a comprehensive understanding of
the boundaries and optimization strategies for reasoning in LLMs. Our code and
data are available at https://github.com/LightChen233/reasoning-granularity."
DiffusionGuard: A Robust Defense Against Malicious Diffusion-based Image Editing,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Recent advances in diffusion models have introduced a new era of text-guided
image manipulation, enabling users to create realistic edited images with
simple textual prompts. However, there is significant concern about the
potential misuse of these methods, especially in creating misleading or harmful
content. Although recent defense strategies, which introduce imperceptible
adversarial noise to induce model failure, have shown promise, they remain
ineffective against more sophisticated manipulations, such as editing with a
mask. In this work, we propose DiffusionGuard, a robust and effective defense
method against unauthorized edits by diffusion-based image editing models, even
in challenging setups. Through a detailed analysis of these models, we
introduce a novel objective that generates adversarial noise targeting the
early stage of the diffusion process. This approach significantly improves the
efficiency and effectiveness of adversarial noises. We also introduce a
mask-augmentation technique to enhance robustness against various masks during
test time. Finally, we introduce a comprehensive benchmark designed to evaluate
the effectiveness and robustness of methods in protecting against privacy
threats in realistic scenarios. Through extensive experiments, we show that our
method achieves stronger protection and improved mask robustness with lower
computational costs compared to the strongest baseline. Additionally, our
method exhibits superior transferability and better resilience to noise removal
techniques compared to all baseline methods. Our source code is publicly
available at https://github.com/choi403/DiffusionGuard."
Extreme Value Modelling of Feature Residuals for Anomaly Detection in Dynamic Graphs,cs.LG,Machine Learning,2024-10-08,"Detecting anomalies in a temporal sequence of graphs can be applied is areas
such as the detection of accidents in transport networks and cyber attacks in
computer networks. Existing methods for detecting abnormal graphs can suffer
from multiple limitations, such as high false positive rates as well as
difficulties with handling variable-sized graphs and non-trivial temporal
dynamics. To address this, we propose a technique where temporal dependencies
are explicitly modelled via time series analysis of a large set of pertinent
graph features, followed by using residuals to remove the dependencies. Extreme
Value Theory is then used to robustly model and classify any remaining
extremes, aiming to produce low false positives rates. Comparative evaluations
on a multitude of graph instances show that the proposed approach obtains
considerably better accuracy than TensorSplat and Laplacian Anomaly Detection."
Whole-Body Dynamic Throwing with Legged Manipulators,cs.RO,Robotics,2024-10-08,"Most robotic behaviours focus on either manipulation or locomotion, where
tasks that require the integration of both, such as full-body throwing, remain
under-explored. Throwing with a robot involves complex coordination between
object manipulation and legged locomotion, which is crucial for advanced
real-world interactions. This work investigates the challenge of full-body
throwing in robotic systems and highlights the advantages of utilising the
robot's entire body. We propose a deep reinforcement learning (RL) approach
that leverages the robot's body to enhance throwing performance through a
strategically designed curriculum to avoid local optima and sparse but
informative reward functions to improve policy flexibility. The robot's body
learns to generate additional momentum and fine-tune the projectile release
velocity. Our full-body method achieves on average 47% greater throwing
distance and 34% greater throwing accuracy than the arm alone, across two robot
morphologies - an armed quadruped and a humanoid. We also extend our method to
optimise robot stability during throws. The learned policy effectively
generalises throwing to targets at any 3D point in space within a specified
range, which has not previously been achieved and does so with human-level
throwing accuracy. We successfully transferred this approach from simulation to
a real robot using sim2real techniques, demonstrating its practical viability."
Convolutional neural networks applied to modification of images,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"The reader will learn how digital images are edited using linear algebra and
calculus. Starting from the concept of filter towards machine learning
techniques such as convolutional neural networks."
"T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design",cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"In this paper, we focus on enhancing a diffusion-based text-to-video (T2V)
model during the post-training phase by distilling a highly capable consistency
model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2,
introduces a significant advancement by integrating various supervision
signals, including high-quality training data, reward model feedback, and
conditional guidance, into the consistency distillation process. Through
comprehensive ablation studies, we highlight the crucial importance of
tailoring datasets to specific learning objectives and the effectiveness of
learning from diverse reward models for enhancing both the visual quality and
text-video alignment. Additionally, we highlight the vast design space of
conditional guidance strategies, which centers on designing an effective energy
function to augment the teacher ODE solver. We demonstrate the potential of
this approach by extracting motion guidance from the training datasets and
incorporating it into the ODE solver, showcasing its effectiveness in improving
the motion quality of the generated videos with the improved motion-related
metrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2
establishes a new state-of-the-art result on VBench, with a Total score of
85.13, surpassing proprietary systems such as Gen-3 and Kling."
Understanding with toy surrogate models in machine learning,cs.LG,Machine Learning,2024-10-08,"In the natural and social sciences, it is common to use toy models --
extremely simple and highly idealized representations -- to understand complex
phenomena. Some of the simple surrogate models used to understand opaque
machine learning (ML) models, such as rule lists and sparse decision trees,
bear some resemblance to scientific toy models. They allow non-experts to
understand how an opaque ML model works globally via a much simpler model that
highlights the most relevant features of the input space and their effect on
the output. The obvious difference is that the common target of a toy and a
full-scale model in the sciences is some phenomenon in the world, while the
target of a surrogate model is another model. This essential difference makes
toy surrogate models (TSMs) a new object of study for theories of
understanding, one that is not easily accommodated under current analyses. This
paper provides an account of what it means to understand an opaque ML model
globally with the aid of such simple models."
Improving Disease Comorbidity Prediction Based on Human Interactome with Biologically Supervised Graph Embedding,cs.LG,Machine Learning,2024-10-08,"Comorbidity carries significant implications for disease understanding and
management. The genetic causes for comorbidity often trace back to mutations
occurred either in the same gene associated with two diseases or in different
genes associated with different diseases respectively but coming into
connection via protein-protein interactions. Therefore, human interactome has
been used in more sophisticated study of disease comorbidity. Human
interactome, as a large incomplete graph, presents its own challenges to
extracting useful features for comorbidity prediction. In this work, we
introduce a novel approach named Biologically Supervised Graph Embedding (BSE)
to allow for selecting most relevant features to enhance the prediction
accuracy of comorbid disease pairs. Our investigation into BSE's impact on both
centered and uncentered embedding methods showcases its consistent superiority
over the state-of-the-art techniques and its adeptness in selecting dimensions
enriched with vital biological insights, thereby improving prediction
performance significantly, up to 50% when measured by ROC for some variations.
Further analysis indicates that BSE consistently and substantially improves the
ratio of disease associations to gene connectivity, affirming its potential in
uncovering latent biological factors affecting comorbidity. The statistically
significant enhancements across diverse metrics underscore BSE's potential to
introduce novel avenues for precise disease comorbidity predictions and other
potential applications. The GitHub repository containing the source code can be
accessed at the following link:
https://github.com/xihan-qin/Biologically-Supervised-Graph-Embedding."
"ACPBench: Reasoning about Action, Change, and Planning",cs.AI,Artificial Intelligence,2024-10-08,"There is an increasing body of work using Large Language Models (LLMs) as
agents for orchestrating workflows and making decisions in domains that require
planning and multi-step reasoning. As a result, it is imperative to evaluate
LLMs on core skills required for planning. In this work, we present ACPBench, a
benchmark for evaluating the reasoning tasks in the field of planning. The
benchmark consists of 7 reasoning tasks over 13 planning domains. The
collection is constructed from planning domains described in a formal language.
This allows us to synthesize problems with provably correct solutions across
many tasks and domains. Further, it allows us the luxury of scale without
additional human effort, i.e., many additional problems can be created
automatically. Our extensive evaluation of 22 open-sourced and frontier LLMs
highlight the significant gap in the reasoning capability of the LLMs. The
average accuracy of one of the best-performing frontier LLMs -- GPT-4o on these
tasks can fall as low as 52.50% ACPBench collection is available at
https://ibm.github.io/ACPBench."
Edge-Cloud Collaborative Satellite Image Analysis for Efficient Man-Made Structure Recognition,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"The increasing availability of high-resolution satellite imagery has created
immense opportunities for various applications. However, processing and
analyzing such vast amounts of data in a timely and accurate manner poses
significant challenges. The paper presents a new satellite image processing
architecture combining edge and cloud computing to better identify man-made
structures against natural landscapes. By employing lightweight models at the
edge, the system initially identifies potential man-made structures from
satellite imagery. These identified images are then transmitted to the cloud,
where a more complex model refines the classification, determining specific
types of structures. The primary focus is on the trade-off between latency and
accuracy, as efficient models often sacrifice accuracy. We compare this hybrid
edge-cloud approach against traditional ""bent-pipe"" method in virtual
environment experiments as well as introduce a practical model and compare its
performance with existing lightweight models for edge deployment, focusing on
accuracy and latency. The results demonstrate that the edge-cloud collaborative
model not only reduces overall latency due to minimized data transmission but
also maintains high accuracy, offering substantial improvements over
traditional approaches under this scenario."
Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"As text-to-image diffusion models become advanced enough for commercial
applications, there is also increasing concern about their potential for
malicious and harmful use. Model unlearning has been proposed to mitigate the
concerns by removing undesired and potentially harmful information from the
pre-trained model. So far, the success of unlearning is mainly measured by
whether the unlearned model can generate a target concept while maintaining
image quality. However, unlearning is typically tested under limited scenarios,
and the side effects of unlearning have barely been studied in the current
literature. In this work, we thoroughly analyze unlearning under various
scenarios with five key aspects. Our investigation reveals that every method
has side effects or limitations, especially in more complex and realistic
situations. By releasing our comprehensive evaluation framework with the source
codes and artifacts, we hope to inspire further research in this area, leading
to more reliable and effective unlearning methods."
Abstract Hardware Grounding towards the Automated Design of Automation Systems,cs.RO,Robotics,2024-10-08,"Crafting automation systems tailored for specific domains requires aligning
the space of human experts' semantics with the space of robot executable
actions, and scheduling the required resources and system layout accordingly.
Regrettably, there are three major gaps, fine-grained domain-specific knowledge
injection, heterogeneity between human knowledge and robot instructions, and
diversity of users' preferences, resulting automation system design a
case-by-case and labour-intensive effort, thus hindering the democratization of
automation. We refer to this challenging alignment as the abstract hardware
grounding problem, where we firstly regard the procedural operations in humans'
semantics space as the abstraction of hardware requirements, then we ground
such abstractions to instantiated hardware devices, subject to constraints and
preferences in the real world -- optimizing this problem is essentially
standardizing and automating the design of automation systems. On this basis,
we develop an automated design framework in a hybrid data-driven and
principle-derived fashion. Results on designing self-driving laboratories for
enhancing experiment-driven scientific discovery suggest our framework's
potential to produce compact systems that fully satisfy domain-specific and
user-customized requirements with no redundancy."
Federated Learning with Dynamic Client Arrival and Departure: Convergence and Rapid Adaptation via Initial Model Construction,cs.LG,Machine Learning,2024-10-08,"While most existing federated learning (FL) approaches assume a fixed set of
clients in the system, in practice, clients can dynamically leave or join the
system depending on their needs or interest in the specific task. This dynamic
FL setting introduces several key challenges: (1) the objective function
dynamically changes depending on the current set of clients, unlike traditional
FL approaches that maintain a static optimization goal; (2) the current global
model may not serve as the best initial point for the next FL rounds and could
potentially lead to slow adaptation, given the possibility of clients leaving
or joining the system. In this paper, we consider a dynamic optimization
objective in FL that seeks the optimal model tailored to the currently active
set of clients. Building on our probabilistic framework that provides direct
insights into how the arrival and departure of different types of clients
influence the shifts in optimal points, we establish an upper bound on the
optimality gap, accounting for factors such as stochastic gradient noise, local
training iterations, non-IIDness of data distribution, and deviations between
optimal points caused by dynamic client pattern. We also propose an adaptive
initial model construction strategy that employs weighted averaging guided by
gradient similarity, prioritizing models trained on clients whose data
characteristics align closely with the current one, thereby enhancing
adaptability to the current clients. The proposed approach is validated on
various datasets and FL algorithms, demonstrating robust performance across
diverse client arrival and departure patterns, underscoring its effectiveness
in dynamic FL environments."
Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models,cs.LG,Machine Learning,2024-10-08,"The scaling of large language models (LLMs) is a critical research area for
the efficiency and effectiveness of model training and deployment. Our work
investigates the transferability and discrepancies of scaling laws between
Dense Models and Mixture of Experts (MoE) models. Through a combination of
theoretical analysis and extensive experiments, including consistent loss
scaling, optimal batch size and learning rate scaling, and resource allocation
strategies scaling, our findings reveal that the power-law scaling framework
also applies to MoE Models, indicating that the fundamental principles
governing the scaling behavior of these models are preserved, even though the
architecture differs. Additionally, MoE Models demonstrate superior
generalization, resulting in lower testing losses with the same training
compute budget compared to Dense Models. These findings indicate the scaling
consistency and transfer generalization capabilities of MoE Models, providing
new insights for optimizing MoE Model training and deployment strategies."
Robust Transfer Learning for Active Level Set Estimation with Locally Adaptive Gaussian Process Prior,cs.LG,Machine Learning,2024-10-08,"The objective of active level set estimation for a black-box function is to
precisely identify regions where the function values exceed or fall below a
specified threshold by iteratively performing function evaluations to gather
more information about the function. This becomes particularly important when
function evaluations are costly, drastically limiting our ability to acquire
large datasets. A promising way to sample-efficiently model the black-box
function is by incorporating prior knowledge from a related function. However,
this approach risks slowing down the estimation task if the prior knowledge is
irrelevant or misleading. In this paper, we present a novel transfer learning
method for active level set estimation that safely integrates a given prior
knowledge while constantly adjusting it to guarantee a robust performance of a
level set estimation algorithm even when the prior knowledge is irrelevant. We
theoretically analyze this algorithm to show that it has a better level set
convergence compared to standard transfer learning approaches that do not make
any adjustment to the prior. Additionally, extensive experiments across
multiple datasets confirm the effectiveness of our method when applied to
various different level set estimation algorithms as well as different transfer
learning scenarios."
On the Modeling Capabilities of Large Language Models for Sequential Decision Making,cs.AI,Artificial Intelligence,2024-10-08,"Large pretrained models are showing increasingly better performance in
reasoning and planning tasks across different modalities, opening the
possibility to leverage them for complex sequential decision making problems.
In this paper, we investigate the capabilities of Large Language Models (LLMs)
for reinforcement learning (RL) across a diversity of interactive domains. We
evaluate their ability to produce decision-making policies, either directly, by
generating actions, or indirectly, by first generating reward models to train
an agent with RL. Our results show that, even without task-specific
fine-tuning, LLMs excel at reward modeling. In particular, crafting rewards
through artificial intelligence (AI) feedback yields the most generally
applicable approach and can enhance performance by improving credit assignment
and exploration. Finally, in environments with unfamiliar dynamics, we explore
how fine-tuning LLMs with synthetic data can significantly improve their reward
modeling capabilities while mitigating catastrophic forgetting, further
broadening their utility in sequential decision-making tasks."
Efficient Policy Evaluation with Safety Constraint for Reinforcement Learning,cs.LG,Machine Learning,2024-10-08,"In reinforcement learning, classic on-policy evaluation methods often suffer
from high variance and require massive online data to attain the desired
accuracy. Previous studies attempt to reduce evaluation variance by searching
for or designing proper behavior policies to collect data. However, these
approaches ignore the safety of such behavior policies -- the designed behavior
policies have no safety guarantee and may lead to severe damage during online
executions. In this paper, to address the challenge of reducing variance while
ensuring safety simultaneously, we propose an optimal variance-minimizing
behavior policy under safety constraints. Theoretically, while ensuring safety
constraints, our evaluation method is unbiased and has lower variance than
on-policy evaluation. Empirically, our method is the only existing method to
achieve both substantial variance reduction and safety constraint satisfaction.
Furthermore, we show our method is even superior to previous methods in both
variance reduction and execution safety."
A Blockchain-Enhanced Framework for Privacy and Data Integrity in Crowdsourced Drone Services,cs.CR,Cryptography and Security,2024-10-08,"We present an innovative framework that integrates consumer-grade drones into
bushfire management, addressing both service improvement and data privacy
concerns under Australia's Privacy Act 1988. This system establishes a
marketplace where bushfire management authorities, as data consumers, access
critical information from drone operators, who serve as data providers. The
framework employs local differential privacy to safeguard the privacy of data
providers from all system entities, ensuring compliance with privacy standards.
Additionally, a blockchain-based solution facilitates fair data and fee
exchanges while maintaining immutable records for enhanced accountability.
Validated through a proof-of-concept implementation, the framework's
scalability and adaptability make it well-suited for large-scale, real-world
applications in bushfire management."
ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V)
diffusion models has greatly enhanced video generation, especially in terms of
keyframe interpolation. However, current image-to-video diffusion models, while
powerful in generating videos from a single conditioning frame, need adaptation
for two-frame (start & end) conditioned generation, which is essential for
effective bounded interpolation. Unfortunately, existing approaches that fuse
temporally forward and backward paths in parallel often suffer from
off-manifold issues, leading to artifacts or requiring multiple iterative
re-noising steps. In this work, we introduce a novel, bidirectional sampling
strategy to address these off-manifold issues without requiring extensive
re-noising or fine-tuning. Our method employs sequential sampling along both
forward and backward paths, conditioned on the start and end frames,
respectively, ensuring more coherent and on-manifold generation of intermediate
frames. Additionally, we incorporate advanced guidance techniques, CFG++ and
DDS, to further enhance the interpolation process. By integrating these, our
method achieves state-of-the-art performance, efficiently generating
high-quality, smooth videos between keyframes. On a single 3090 GPU, our method
can interpolate 25 frames at 1024 x 576 resolution in just 195 seconds,
establishing it as a leading solution for keyframe interpolation."
SIA-OVD: Shape-Invariant Adapter for Bridging the Image-Region Gap in Open-Vocabulary Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Open-vocabulary detection (OVD) aims to detect novel objects without
instance-level annotations to achieve open-world object detection at a lower
cost. Existing OVD methods mainly rely on the powerful open-vocabulary
image-text alignment capability of Vision-Language Pretrained Models (VLM) such
as CLIP. However, CLIP is trained on image-text pairs and lacks the perceptual
ability for local regions within an image, resulting in the gap between image
and region representations. Directly using CLIP for OVD causes inaccurate
region classification. We find the image-region gap is primarily caused by the
deformation of region feature maps during region of interest (RoI) extraction.
To mitigate the inaccurate region classification in OVD, we propose a new
Shape-Invariant Adapter named SIA-OVD to bridge the image-region gap in the OVD
task. SIA-OVD learns a set of feature adapters for regions with different
shapes and designs a new adapter allocation mechanism to select the optimal
adapter for each region. The adapted region representations can align better
with text representations learned by CLIP. Extensive experiments demonstrate
that SIA-OVD effectively improves the classification accuracy for regions by
addressing the gap between images and regions caused by shape deformation.
SIA-OVD achieves substantial improvements over representative methods on the
COCO-OVD benchmark. The code is available at
https://github.com/PKU-ICST-MIPL/SIA-OVD_ACMMM2024."
Does RoBERTa Perform Better than BERT in Continual Learning: An Attention Sink Perspective,cs.LG,Machine Learning,2024-10-08,"Continual learning (CL) aims to train models that can sequentially learn new
tasks without forgetting previous tasks' knowledge. Although previous works
observed that pre-training can benefit CL, it remains unclear whether a
pre-trained model with higher downstream capacity also performs better in CL.
In this paper, we observe that pre-trained models may allocate high attention
scores to some 'sink' tokens, such as [SEP] tokens, which are ubiquitous across
various tasks. Such attention sinks may lead to models' over-smoothing in
single-task learning and interference in sequential tasks' learning, which may
compromise the models' CL performance despite their high pre-trained
capabilities. To reduce these effects, we propose a pre-scaling mechanism that
encourages attention diversity across all tokens. Specifically, it first scales
the task's attention to the non-sink tokens in a probing stage, and then
fine-tunes the model with scaling. Experiments show that pre-scaling yields
substantial improvements in CL without experience replay, or progressively
storing parameters from previous tasks."
Score-Based Variational Inference for Inverse Problems,cs.LG,Machine Learning,2024-10-08,"Existing diffusion-based methods for inverse problems sample from the
posterior using score functions and accept the generated random samples as
solutions. In applications that posterior mean is preferred, we have to
generate multiple samples from the posterior which is time-consuming. In this
work, by analyzing the probability density evolution of the conditional reverse
diffusion process, we prove that the posterior mean can be achieved by tracking
the mean of each reverse diffusion step. Based on that, we establish a
framework termed reverse mean propagation (RMP) that targets the posterior mean
directly. We show that RMP can be implemented by solving a variational
inference problem, which can be further decomposed as minimizing a reverse KL
divergence at each reverse step. We further develop an algorithm that optimizes
the reverse KL divergence with natural gradient descent using score functions
and propagates the mean at each reverse step. Experiments demonstrate the
validity of the theory of our framework and show that our algorithm outperforms
state-of-the-art algorithms on reconstruction performance with lower
computational complexity in various inverse problems."
TRACE: Temporal Grounding Video LLM via Causal Event Modeling,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Video Temporal Grounding (VTG) is a crucial capability for video
understanding models and plays a vital role in downstream tasks such as video
browsing and editing. To effectively handle various tasks simultaneously and
enable zero-shot prediction, there is a growing trend in employing video LLMs
for VTG tasks. However, current video LLM-based methods rely exclusively on
natural language generation, lacking the ability to model the clear structure
inherent in videos, which restricts their effectiveness in tackling VTG tasks.
To address this issue, this paper first formally introduces causal event
modeling framework, which represents videos as sequences of events, and predict
the current event using previous events, video inputs, and textural
instructions. Each event consists of three components: timestamps, salient
scores, and textual captions. We then propose a novel task-interleaved video
LLM called TRACE to effectively implement the causal event modeling framework
in practice. The TRACE processes visual frames, timestamps, salient scores, and
text as distinct tasks, employing various encoders and decoding heads for each.
Task tokens are arranged in an interleaved sequence according to the causal
event modeling framework's formulation. Extensive experiments on various VTG
tasks and datasets demonstrate the superior performance of TRACE compared to
state-of-the-art video LLMs. Our model and code are available at
\url{https://github.com/gyxxyg/TRACE}."
"DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models",cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"The performance of Large Language Models (LLMs) is substantially influenced
by the pretraining corpus, which consists of vast quantities of unsupervised
data processed by the models. Despite its critical role in model performance,
ensuring the quality of this data is challenging due to its sheer volume and
the absence of sample-level quality annotations and enhancements. In this
paper, we introduce DecorateLM, a data engineering method designed to refine
the pretraining corpus through data rating, tagging and editing. Specifically,
DecorateLM rates texts against quality criteria, tags texts with hierarchical
labels, and edits texts into a more formalized format. Due to the massive size
of the pretraining corpus, adopting an LLM for decorating the entire corpus is
less efficient. Therefore, to balance performance with efficiency, we curate a
meticulously annotated training corpus for DecorateLM using a large language
model and distill data engineering expertise into a compact 1.2 billion
parameter small language model (SLM). We then apply DecorateLM to enhance 100
billion tokens of the training corpus, selecting 45 billion tokens that
exemplify high quality and diversity for the further training of another 1.2
billion parameter LLM. Our results demonstrate that employing such high-quality
data can significantly boost model performance, showcasing a powerful approach
to enhance the quality of the pretraining corpus."
Time Series Classification of Supraglacial Lakes Evolution over Greenland Ice Sheet,cs.LG,Machine Learning,2024-10-08,"The Greenland Ice Sheet (GrIS) has emerged as a significant contributor to
global sea level rise, primarily due to increased meltwater runoff.
Supraglacial lakes, which form on the ice sheet surface during the summer
months, can impact ice sheet dynamics and mass loss; thus, better understanding
these lakes' seasonal evolution and dynamics is an important task. This study
presents a computationally efficient time series classification approach that
uses Gaussian Mixture Models (GMMs) of the Reconstructed Phase Spaces (RPSs) to
identify supraglacial lakes based on their seasonal evolution: 1) those that
refreeze at the end of the melt season, 2) those that drain during the melt
season, and 3) those that become buried, remaining liquid insulated a few
meters beneath the surface. Our approach uses time series data from the
Sentinel-1 and Sentinel-2 satellites, which utilize microwave and visible
radiation, respectively. Evaluated on a GrIS-wide dataset, the RPS-GMM model,
trained on a single representative sample per class, achieves 85.46% accuracy
with Sentinel-1 data alone and 89.70% with combined Sentinel-1 and Sentinel-2
data. This performance significantly surpasses existing machine learning and
deep learning models which require a large training data. The results
demonstrate the robustness of the RPS-GMM model in capturing the complex
temporal dynamics of supraglacial lakes with minimal training data."
Federated Neural Nonparametric Point Processes,cs.LG,Machine Learning,2024-10-08,"Temporal point processes (TPPs) are effective for modeling event occurrences
over time, but they struggle with sparse and uncertain events in federated
systems, where privacy is a major concern. To address this, we propose
\textit{FedPP}, a Federated neural nonparametric Point Process model. FedPP
integrates neural embeddings into Sigmoidal Gaussian Cox Processes (SGCPs) on
the client side, which is a flexible and expressive class of TPPs, allowing it
to generate highly flexible intensity functions that capture client-specific
event dynamics and uncertainties while efficiently summarizing historical
records. For global aggregation, FedPP introduces a divergence-based mechanism
that communicates the distributions of SGCPs' kernel hyperparameters between
the server and clients, while keeping client-specific parameters local to
ensure privacy and personalization. FedPP effectively captures event
uncertainty and sparsity, and extensive experiments demonstrate its superior
performance in federated settings, particularly with KL divergence and
Wasserstein distance-based global aggregation."
Vector-ICL: In-context Learning with Continuous Vector Representations,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Large language models (LLMs) have shown remarkable in-context learning (ICL)
capabilities on textual data. We explore whether these capabilities can be
extended to continuous vectors from diverse domains, obtained from black-box
pretrained encoders. By aligning input data with an LLM's embedding space
through lightweight projectors, we observe that LLMs can effectively process
and learn from these projected vectors, which we term Vector-ICL. In
particular, we find that pretraining projectors with general language modeling
objectives enables Vector-ICL, while task-specific finetuning further enhances
performance. In our experiments across various tasks and modalities, including
text reconstruction, numerical function regression, text classification,
summarization, molecule captioning, time-series classification, graph
classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL
and domain-specific model or tuning. We further conduct analyses and case
studies, indicating the potential of LLMs to process vector representations
beyond traditional token-based paradigms."
Versatile Motion Langauge Models for Multi-Turn Interactive Agents,cs.AI,Artificial Intelligence,2024-10-08,"Recent advancements in large language models (LLMs) have greatly enhanced
their ability to generate natural and contextually relevant text, making AI
interactions more human-like. However, generating and understanding interactive
human-like motion, where two individuals engage in coordinated movements,
remains a challenge due to the complexity of modeling these coordinated
interactions. Furthermore, a versatile model is required to handle diverse
interactive scenarios, such as chat systems that follow user instructions or
adapt to their assigned role while adjusting interaction dynamics. To tackle
this problem, we introduce VIM, short for the Versatile Interactive Motion
language model, which integrates both language and motion modalities to
effectively understand, generate, and control interactive motions in multi-turn
conversational contexts. To address the scarcity of multi-turn interactive
motion data, we introduce a synthetic dataset, INERT-MT2, where we utilize
pre-trained models to create diverse instructional datasets with interactive
motion. Our approach first trains a motion tokenizer that encodes interactive
motions into residual discrete tokens. In the pretraining stage, the model
learns to align motion and text representations with these discrete tokens.
During the instruction fine-tuning stage, VIM adapts to multi-turn
conversations using the INTER-MT2 dataset. We evaluate the versatility of our
method across motion-related tasks, motion to text, text to motion, reaction
generation, motion editing, and reasoning about motion sequences. The results
highlight the versatility and effectiveness of proposed method in handling
complex interactive motion synthesis."
CLOSER: Towards Better Representation Learning for Few-Shot Class-Incremental Learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Aiming to incrementally learn new classes with only few samples while
preserving the knowledge of base (old) classes, few-shot class-incremental
learning (FSCIL) faces several challenges, such as overfitting and catastrophic
forgetting. Such a challenging problem is often tackled by fixing a feature
extractor trained on base classes to reduce the adverse effects of overfitting
and forgetting. Under such formulation, our primary focus is representation
learning on base classes to tackle the unique challenge of FSCIL:
simultaneously achieving the transferability and the discriminability of the
learned representation. Building upon the recent efforts for enhancing
transferability, such as promoting the spread of features, we find that trying
to secure the spread of features within a more confined feature space enables
the learned representation to strike a better balance between transferability
and discriminability. Thus, in stark contrast to prior beliefs that the
inter-class distance should be maximized, we claim that the closer different
classes are, the better for FSCIL. The empirical results and analysis from the
perspective of information bottleneck theory justify our simple yet seemingly
counter-intuitive representation learning method, raising research questions
and suggesting alternative research directions. The code is available at
https://github.com/JungHunOh/CLOSER_ECCV2024."
Remote Sensing Image Segmentation Using Vision Mamba and Multi-Scale Multi-Frequency Feature Fusion,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"As remote sensing imaging technology continues to advance and evolve,
processing high-resolution and diversified satellite imagery to improve
segmentation accuracy and enhance interpretation efficiency emerg as a pivotal
area of investigation within the realm of remote sensing. Although segmentation
algorithms based on CNNs and Transformers achieve significant progress in
performance, balancing segmentation accuracy and computational complexity
remains challenging, limiting their wide application in practical tasks. To
address this, this paper introduces state space model (SSM) and proposes a
novel hybrid semantic segmentation network based on vision Mamba (CVMH-UNet).
This method designs a cross-scanning visual state space block (CVSSBlock) that
uses cross 2D scanning (CS2D) to fully capture global information from multiple
directions, while by incorporating convolutional neural network branches to
overcome the constraints of Vision Mamba (VMamba) in acquiring local
information, this approach facilitates a comprehensive analysis of both global
and local features. Furthermore, to address the issue of limited discriminative
power and the difficulty in achieving detailed fusion with direct skip
connections, a multi-frequency multi-scale feature fusion block (MFMSBlock) is
designed. This module introduces multi-frequency information through 2D
discrete cosine transform (2D DCT) to enhance information utilization and
provides additional scale local detail information through point-wise
convolution branches. Finally, it aggregates multi-scale information along the
channel dimension, achieving refined feature fusion. Findings from experiments
conducted on renowned datasets of remote sensing imagery demonstrate that
proposed CVMH-UNet achieves superior segmentation performance while maintaining
low computational complexity, outperforming surpassing current leading-edge
segmentation algorithms."
"Understanding Gradient Boosting Classifier: Training, Prediction, and the Role of $_j$",cs.LG,Machine Learning,2024-10-08,"The Gradient Boosting Classifier (GBC) is a widely used machine learning
algorithm for binary classification, which builds decision trees iteratively to
minimize prediction errors. This document explains the GBC's training and
prediction processes, focusing on the computation of terminal node values
$\gamma_j$, which are crucial to optimizing the logistic loss function. We
derive $\gamma_j$ through a Taylor series approximation and provide a
step-by-step pseudocode for the algorithm's implementation. The guide explains
the theory of GBC and its practical application, demonstrating its
effectiveness in binary classification tasks. We provide a step-by-step example
in the appendix to help readers understand."
Stereotype or Personalization? User Identity Biases Chatbot Recommendations,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"We demonstrate that when people use large language models (LLMs) to generate
recommendations, the LLMs produce responses that reflect both what the user
wants and who the user is. While personalized recommendations are often desired
by users, it can be difficult in practice to distinguish cases of bias from
cases of personalization: we find that models generate racially stereotypical
recommendations regardless of whether the user revealed their identity
intentionally through explicit indications or unintentionally through implicit
cues. We argue that chatbots ought to transparently indicate when
recommendations are influenced by a user's revealed identity characteristics,
but observe that they currently fail to do so. Our experiments show that even
though a user's revealed identity significantly influences model
recommendations (p < 0.001), model responses obfuscate this fact in response to
user queries. This bias and lack of transparency occurs consistently across
multiple popular consumer LLMs (gpt-4o-mini, gpt-4-turbo, llama-3-70B, and
claude-3.5) and for four American racial groups."
Leveraging free energy in pretraining model selection for improved fine-tuning,cs.LG,Machine Learning,2024-10-08,"Recent advances in artificial intelligence have been fueled by the
development of foundation models such as BERT, GPT, T5, and Vision
Transformers. These models are first pretrained on vast and diverse datasets
and then adapted to specific downstream tasks, often with significantly less
data. However, the mechanisms behind the success of this ubiquitous
pretrain-then-adapt paradigm remain underexplored, particularly the
characteristics of pretraining checkpoints that lend themselves to good
downstream adaptation. We introduce a Bayesian model selection criterion,
called the downstream free energy, which quantifies a checkpoint's adaptability
by measuring the concentration of nearby favorable parameters for the
downstream task. We demonstrate that this free energy criterion can be
effectively implemented without access to the downstream data or prior
knowledge of the downstream task. Furthermore, we provide empirical evidence
that the free energy criterion reliably correlates with improved fine-tuning
performance, offering a principled approach to predicting model adaptability."
Chain-of-Thoughts for Molecular Understanding,cs.LG,Machine Learning,2024-10-08,"The adaptation of large language models (LLMs) to chemistry has shown
promising performance in molecular understanding tasks, such as generating a
text description from a molecule. However, proper reasoning based on molecular
structural information remains a significant challenge, e.g., even advanced
LLMs such as GPT-4o struggle to identify functional groups which are crucial
for inferring the molecular property of interest. To address this limitation,
we propose StructCoT, a structure-aware chain-of-thought (CoT) that enhances
LLMs' understanding of molecular structures by explicitly injecting the key
structural features of molecules. Moreover, we introduce two fine-tuning
frameworks for adapting the existing LLMs to use our StructCoT. Our experiments
demonstrate that incorporating StructCoT with our fine-tuning frameworks leads
to consistent improvements in both molecular understanding tasks."
"Multimodal Large Language Models and Tunings: Vision, Language, Sensors, Audio, and Beyond",cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"This tutorial explores recent advancements in multimodal pretrained and large
models, capable of integrating and processing diverse data forms such as text,
images, audio, and video. Participants will gain an understanding of the
foundational concepts of multimodality, the evolution of multimodal research,
and the key technical challenges addressed by these models. We will cover the
latest multimodal datasets and pretrained models, including those beyond vision
and language. Additionally, the tutorial will delve into the intricacies of
multimodal large models and instruction tuning strategies to optimise
performance for specific tasks. Hands-on laboratories will offer practical
experience with state-of-the-art multimodal models, demonstrating real-world
applications like visual storytelling and visual question answering. This
tutorial aims to equip researchers, practitioners, and newcomers with the
knowledge and skills to leverage multimodal AI. ACM Multimedia 2024 is the
ideal venue for this tutorial, aligning perfectly with our goal of
understanding multimodal pretrained and large language models, and their tuning
mechanisms."
Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition,cs.LG,Machine Learning,2024-10-08,"Large Language Models (LLMs) have demonstrated remarkable in-context learning
(ICL) capabilities. In this study, we explore a surprising phenomenon related
to ICL: LLMs can perform multiple, computationally distinct ICL tasks
simultaneously, during a single inference call, a capability we term ""task
superposition"". We provide empirical evidence of this phenomenon across various
LLM families and scales and show that this phenomenon emerges even if we train
the model to in-context learn one task at a time. We offer theoretical
explanations that this capability is well within the expressive power of
transformers. We also explore how LLMs internally compose task vectors during
superposition. Furthermore, we show that larger models can solve more ICL tasks
in parallel, and better calibrate their output distribution. Our findings offer
insights into the latent capabilities of LLMs, further substantiate the
perspective of ""LLMs as superposition of simulators"", and raise questions about
the mechanisms enabling simultaneous task execution."
ReFIR: Grounding Large Restoration Models with Retrieval Augmentation,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Recent advances in diffusion-based Large Restoration Models (LRMs) have
significantly improved photo-realistic image restoration by leveraging the
internal knowledge embedded within model weights. However, existing LRMs often
suffer from the hallucination dilemma, i.e., producing incorrect contents or
textures when dealing with severe degradations, due to their heavy reliance on
limited internal knowledge. In this paper, we propose an orthogonal solution
called the Retrieval-augmented Framework for Image Restoration (ReFIR), which
incorporates retrieved images as external knowledge to extend the knowledge
boundary of existing LRMs in generating details faithful to the original scene.
Specifically, we first introduce the nearest neighbor lookup to retrieve
content-relevant high-quality images as reference, after which we propose the
cross-image injection to modify existing LRMs to utilize high-quality textures
from retrieved images. Thanks to the additional external knowledge, our ReFIR
can well handle the hallucination challenge and facilitate faithfully results.
Extensive experiments demonstrate that ReFIR can achieve not only high-fidelity
but also realistic restoration results. Importantly, our ReFIR requires no
training and is adaptable to various LRMs."
Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"The widespread presence of hate speech on the internet, including formats
such as text-based tweets and vision-language memes, poses a significant
challenge to digital platform safety. Recent research has developed detection
models tailored to specific modalities; however, there is a notable gap in
transferring detection capabilities across different formats. This study
conducts extensive experiments using few-shot in-context learning with large
language models to explore the transferability of hate speech detection between
modalities. Our findings demonstrate that text-based hate speech examples can
significantly enhance the classification accuracy of vision-language hate
speech. Moreover, text-based demonstrations outperform vision-language
demonstrations in few-shot learning settings. These results highlight the
effectiveness of cross-modality knowledge transfer and offer valuable insights
for improving hate speech detection systems."
When Graph Neural Networks Meet Dynamic Mode Decomposition,cs.LG,Machine Learning,2024-10-08,"Graph Neural Networks (GNNs) have emerged as fundamental tools for a wide
range of prediction tasks on graph-structured data. Recent studies have drawn
analogies between GNN feature propagation and diffusion processes, which can be
interpreted as dynamical systems. In this paper, we delve deeper into this
perspective by connecting the dynamics in GNNs to modern Koopman theory and its
numerical method, Dynamic Mode Decomposition (DMD). We illustrate how DMD can
estimate a low-rank, finite-dimensional linear operator based on multiple
states of the system, effectively approximating potential nonlinear
interactions between nodes in the graph. This approach allows us to capture
complex dynamics within the graph accurately and efficiently. We theoretically
establish a connection between the DMD-estimated operator and the original
dynamic operator between system states. Building upon this foundation, we
introduce a family of DMD-GNN models that effectively leverage the low-rank
eigenfunctions provided by the DMD algorithm. We further discuss the potential
of enhancing our approach by incorporating domain-specific constraints such as
symmetry into the DMD computation, allowing the corresponding GNN models to
respect known physical properties of the underlying system. Our work paves the
path for applying advanced dynamical system analysis tools via GNNs. We
validate our approach through extensive experiments on various learning tasks,
including directed graphs, large-scale graphs, long-range interactions, and
spatial-temporal graphs. We also empirically verify that our proposed models
can serve as powerful encoders for link prediction tasks. The results
demonstrate that our DMD-enhanced GNNs achieve state-of-the-art performance,
highlighting the effectiveness of integrating DMD into GNN frameworks."
TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Despite significant advancements in customizing text-to-image and video
generation models, generating images and videos that effectively integrate
multiple personalized concepts remains a challenging task. To address this, we
present TweedieMix, a novel method for composing customized diffusion models
during the inference phase. By analyzing the properties of reverse diffusion
sampling, our approach divides the sampling process into two stages. During the
initial steps, we apply a multiple object-aware sampling technique to ensure
the inclusion of the desired target objects. In the later steps, we blend the
appearances of the custom concepts in the de-noised image space using Tweedie's
formula. Our results demonstrate that TweedieMix can generate multiple
personalized concepts with higher fidelity than existing methods. Moreover, our
framework can be effortlessly extended to image-to-video diffusion models,
enabling the generation of videos that feature multiple personalized concepts.
Results and source code are in our anonymous project page."
ParallelSpec: Parallel Drafter for Efficient Speculative Decoding,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Speculative decoding has proven to be an efficient solution to large language
model (LLM) inference, where the small drafter predicts future tokens at a low
cost, and the target model is leveraged to verify them in parallel. However,
most existing works still draft tokens auto-regressively to maintain sequential
dependency in language modeling, which we consider a huge computational burden
in speculative decoding. We present ParallelSpec, an alternative to
auto-regressive drafting strategies in state-of-the-art speculative decoding
approaches. In contrast to auto-regressive drafting in the speculative stage,
we train a parallel drafter to serve as an efficient speculative model.
ParallelSpec learns to efficiently predict multiple future tokens in parallel
using a single model, and it can be integrated into any speculative decoding
framework that requires aligning the output distributions of the drafter and
the target model with minimal training cost. Experimental results show that
ParallelSpec accelerates baseline methods in latency up to 62% on text
generation benchmarks from different domains, and it achieves 2.84X overall
speedup on the Llama-2-13B model using third-party evaluation criteria."
TeaserGen: Generating Teasers for Long Documentaries,cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Teasers are an effective tool for promoting content in entertainment,
commercial and educational fields. However, creating an effective teaser for
long videos is challenging for it requires long-range multimodal modeling on
the input videos, while necessitating maintaining audiovisual alignments,
managing scene changes and preserving factual accuracy for the output teasers.
Due to the lack of a publicly-available dataset, progress along this research
direction has been hindered. In this work, we present DocumentaryNet, a
collection of 1,269 documentaries paired with their teasers, featuring
multimodal data streams of video, speech, music, sound effects and narrations.
With DocumentaryNet, we propose a new two-stage system for generating teasers
from long documentaries. The proposed TeaserGen system first generates the
teaser narration from the transcribed narration of the documentary using a
pretrained large language model, and then selects the most relevant visual
content to accompany the generated narration through language-vision models.
For narration-video matching, we explore two approaches: a pretraining-based
model using pretrained contrastive language-vision models and a deep sequential
model that learns the mapping between the narrations and visuals. Our
experimental results show that the pretraining-based approach is more effective
at identifying relevant visual content than directly trained deep
autoregressive models."
Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?,cs.LG,Machine Learning,2024-10-08,"Reward Models (RMs) are crucial for aligning language models with human
preferences. Currently, the evaluation of RMs depends on measuring accuracy
against a validation set of manually annotated preference data. Although this
method is straightforward and widely adopted, the relationship between RM
accuracy and downstream policy performance remains under-explored. In this
work, we conduct experiments in a synthetic setting to investigate how
differences in RM measured by accuracy translate into gaps in optimized policy
performance. Our findings reveal that while there is a weak positive
correlation between accuracy and downstream performance, policies optimized
towards RMs with similar accuracy can exhibit quite different performance.
Moreover, we discover that the way of measuring accuracy significantly impacts
its ability to predict the final policy performance. Through the lens of
Regressional Goodhart's effect, we identify the existence of exogenous
variables impacting the relationship between RM quality measured by accuracy
and policy model capability. This underscores the inadequacy of relying solely
on accuracy to reflect their impact on policy optimization."
NegMerge: Consensual Weight Negation for Strong Machine Unlearning,cs.LG,Machine Learning,2024-10-08,"Machine unlearning aims to selectively remove specific knowledge from a
model. Current methods, such as task arithmetic, rely on fine-tuning models on
the forget set, generating a task vector, and subtracting it from the original
model. However, we argue the effectiveness of this approach is highly sensitive
to hyperparameter selection, necessitating careful validation to identify the
best model among many fine-tuned candidates. In this paper, we propose a novel
method that leverages all given fine-tuned models rather than selecting a
single one. By constructing task vectors from models trained with varied
hyperparameters and merging only the components of the task vectors with
consistent signs, we perform unlearning by negating the merged task vector from
the original model. Given that existing methods also utilize multiple
fine-tuned models, our approach delivers more effective unlearning without
incurring additional computational costs. We demonstrate the effectiveness of
our method on both vision-language models and standard image classification
models, showing improved unlearning performance with minimal degradation on the
retain set, outperforming state-of-the-art techniques."
Gen-Drive: Enhancing Diffusion Generative Driving Policies with Reward Modeling and Reinforcement Learning Fine-tuning,cs.RO,Robotics,2024-10-08,"Autonomous driving necessitates the ability to reason about future
interactions between traffic agents and to make informed evaluations for
planning. This paper introduces the \textit{Gen-Drive} framework, which shifts
from the traditional prediction and deterministic planning framework to a
generation-then-evaluation planning paradigm. The framework employs a behavior
diffusion model as a scene generator to produce diverse possible future
scenarios, thereby enhancing the capability for joint interaction reasoning. To
facilitate decision-making, we propose a scene evaluator (reward) model,
trained with pairwise preference data collected through VLM assistance, thereby
reducing human workload and enhancing scalability. Furthermore, we utilize an
RL fine-tuning framework to improve the generation quality of the diffusion
model, rendering it more effective for planning tasks. We conduct training and
closed-loop planning tests on the nuPlan dataset, and the results demonstrate
that employing such a generation-then-evaluation strategy outperforms other
learning-based approaches. Additionally, the fine-tuned generative driving
policy shows significant enhancements in planning performance. We further
demonstrate that utilizing our learned reward model for evaluation or RL
fine-tuning leads to better planning performance compared to relying on
human-designed rewards. Project website: https://mczhi.github.io/GenDrive."
Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"In the last decade, the generalization and adaptation abilities of deep
learning models were typically evaluated on fixed training and test
distributions. Contrary to traditional deep learning, large language models
(LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text
corpora curated from the Internet with minimal human intervention, and (iii)
trained in an online fashion. These stark contrasts prevent researchers from
transferring lessons learned on model generalization and adaptation in deep
learning contexts to LLMs. To this end, our short paper introduces empirical
observations that aim to shed light on further training of already pretrained
language models. Specifically, we demonstrate that training a model on a text
domain could degrade its perplexity on the test portion of the same domain. We
observe with our subsequent analysis that the performance degradation is
positively correlated with the similarity between the additional and the
original pretraining dataset of the LLM. Our further token-level perplexity
observations reveals that the perplexity degradation is due to a handful of
tokens that are not informative about the domain. We hope these findings will
guide us in determining when to adapt a model vs when to rely on its
foundational capabilities."
Swift Sampler: Efficient Learning of Sampler by 10 Parameters,cs.LG,Machine Learning,2024-10-08,"Data selection is essential for training deep learning models. An effective
data sampler assigns proper sampling probability for training data and helps
the model converge to a good local minimum with high performance. Previous
studies in data sampling are mainly based on heuristic rules or learning
through a huge amount of time-consuming trials. In this paper, we propose an
automatic \textbf{swift sampler} search algorithm, \textbf{SS}, to explore
automatically learning effective samplers efficiently. In particular,
\textbf{SS} utilizes a novel formulation to map a sampler to a low dimension of
hyper-parameters and uses an approximated local minimum to quickly examine the
quality of a sampler. Benefiting from its low computational expense,
\textbf{SS} can be applied on large-scale data sets with high efficiency.
Comprehensive experiments on various tasks demonstrate that \textbf{SS} powered
sampling can achieve obvious improvements (e.g., 1.5\% on ImageNet) and
transfer among different neural networks. Project page:
https://github.com/Alexander-Yao/Swift-Sampler."
"Underwater Object Detection in the Era of Artificial Intelligence: Current, Challenge, and Future",cs.CV,Computer Vision and Pattern Recognition,2024-10-08,"Underwater object detection (UOD), aiming to identify and localise the
objects in underwater images or videos, presents significant challenges due to
the optical distortion, water turbidity, and changing illumination in
underwater scenes. In recent years, artificial intelligence (AI) based methods,
especially deep learning methods, have shown promising performance in UOD. To
further facilitate future advancements, we comprehensively study AI-based UOD.
In this survey, we first categorise existing algorithms into traditional
machine learning-based methods and deep learning-based methods, and summarise
them by considering learning strategy, experimental dataset, utilised features
or frameworks, and learning stage. Next, we discuss the potential challenges
and suggest possible solutions and new directions. We also perform both
quantitative and qualitative evaluations of mainstream algorithms across
multiple benchmark datasets by considering the diverse and biased experimental
setups. Finally, we introduce two off-the-shelf detection analysis tools,
Diagnosis and TIDE, which well-examine the effects of object characteristics
and various types of errors on detectors. These tools help identify the
strengths and weaknesses of detectors, providing insigts for further
improvement. The source codes, trained models, utilised datasets, detection
results, and detection analysis tools are public available at
\url{https://github.com/LongChenCV/UODReview}, and will be regularly updated."
Submodular Optimization for Keyframe Selection & Usage in SLAM,cs.RO,Robotics,2024-10-08,"Keyframes are LiDAR scans saved for future reference in Simultaneous
Localization And Mapping (SLAM), but despite their central importance most
algorithms leave choices of which scans to save and how to use them to wasteful
heuristics. This work proposes two novel keyframe selection strategies for
localization and map summarization, as well as a novel approach to submap
generation which selects keyframes that best constrain localization. Our
results show that online keyframe selection and submap generation reduce the
number of saved keyframes and improve per scan computation time without
compromising localization performance. We also present a map summarization
feature for quickly capturing environments under strict map size constraints."
ClaimBrush: A Novel Framework for Automated Patent Claim Refinement Based on Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-08,"Automatic refinement of patent claims in patent applications is crucial from
the perspective of intellectual property strategy. In this paper, we propose
ClaimBrush, a novel framework for automated patent claim refinement that
includes a dataset and a rewriting model. We constructed a dataset for training
and evaluating patent claim rewriting models by collecting a large number of
actual patent claim rewriting cases from the patent examination process. Using
the constructed dataset, we built an automatic patent claim rewriting model by
fine-tuning a large language model. Furthermore, we enhanced the performance of
the automatic patent claim rewriting model by applying preference optimization
based on a prediction model of patent examiners' Office Actions. The
experimental results showed that our proposed rewriting model outperformed
heuristic baselines and zero-shot learning in state-of-the-art large language
models. Moreover, preference optimization based on patent examiners'
preferences boosted the performance of patent claim refinement."
TaeBench: Improving Quality of Toxic Adversarial Examples,cs.CR,Cryptography and Security,2024-10-08,"Toxicity text detectors can be vulnerable to adversarial examples - small
perturbations to input text that fool the systems into wrong detection.
Existing attack algorithms are time-consuming and often produce invalid or
ambiguous adversarial examples, making them less useful for evaluating or
improving real-world toxicity content moderators. This paper proposes an
annotation pipeline for quality control of generated toxic adversarial examples
(TAE). We design model-based automated annotation and human-based quality
verification to assess the quality requirements of TAE. Successful TAE should
fool a target toxicity model into making benign predictions, be grammatically
reasonable, appear natural like human-generated text, and exhibit semantic
toxicity. When applying these requirements to more than 20 state-of-the-art
(SOTA) TAE attack recipes, we find many invalid samples from a total of 940k
raw TAE attack generations. We then utilize the proposed pipeline to filter and
curate a high-quality TAE dataset we call TaeBench (of size 264k). Empirically,
we demonstrate that TaeBench can effectively transfer-attack SOTA toxicity
content moderation models and services. Our experiments also show that TaeBench
with adversarial training achieve significant improvements of the robustness of
two toxicity detectors."
Improved deep learning of chaotic dynamical systems with multistep penalty losses,cs.LG,Machine Learning,2024-10-08,"Predicting the long-term behavior of chaotic systems remains a formidable
challenge due to their extreme sensitivity to initial conditions and the
inherent limitations of traditional data-driven modeling approaches. This paper
introduces a novel framework that addresses these challenges by leveraging the
recently proposed multi-step penalty (MP) optimization technique. Our approach
extends the applicability of MP optimization to a wide range of deep learning
architectures, including Fourier Neural Operators and UNETs. By introducing
penalized local discontinuities in the forecast trajectory, we effectively
handle the non-convexity of loss landscapes commonly encountered in training
neural networks for chaotic systems. We demonstrate the effectiveness of our
method through its application to two challenging use-cases: the prediction of
flow velocity evolution in two-dimensional turbulence and ocean dynamics using
reanalysis data. Our results highlight the potential of this approach for
accurate and stable long-term prediction of chaotic dynamics, paving the way
for new advancements in data-driven modeling of complex natural phenomena."
Chain and Causal Attention for Efficient Entity Tracking,cs.LG,Machine Learning,2024-10-07,"This paper investigates the limitations of transformers for entity-tracking
tasks in large language models. We identify a theoretical constraint, showing
that transformers require at least $\log_2 (n+1)$ layers to handle entity
tracking with $n$ state changes. To address this issue, we propose an efficient
and frugal enhancement to the standard attention mechanism, enabling it to
manage long-term dependencies more efficiently. By considering attention as an
adjacency matrix, our model can track entity states with a single layer.
Empirical results demonstrate significant improvements in entity tracking
datasets while keeping competitive performance on standard natural language
modeling. Our modified attention allows us to achieve the same performance with
drastically fewer layers. Additionally, our enhanced mechanism reveals
structured internal representations of attention. Extensive experiments on both
toy and complex datasets validate our approach. Our contributions include
theoretical insights, an improved attention mechanism, and empirical
validation."
Unsupervised Representation Learning from Sparse Transformation Analysis,cs.LG,Machine Learning,2024-10-07,"There is a vast literature on representation learning based on principles
such as coding efficiency, statistical independence, causality,
controllability, or symmetry. In this paper we propose to learn representations
from sequence data by factorizing the transformations of the latent variables
into sparse components. Input data are first encoded as distributions of latent
activations and subsequently transformed using a probability flow model, before
being decoded to predict a future input state. The flow model is decomposed
into a number of rotational (divergence-free) vector fields and a number of
potential flow (curl-free) fields. Our sparsity prior encourages only a small
number of these fields to be active at any instant and infers the speed with
which the probability flows along these fields. Training this model is
completely unsupervised using a standard variational objective and results in a
new form of disentangled representations where the input is not only
represented by a combination of independent factors, but also by a combination
of independent transformation primitives given by the learned flow fields. When
viewing the transformations as symmetries one may interpret this as learning
approximately equivariant representations. Empirically we demonstrate that this
model achieves state of the art in terms of both data likelihood and
unsupervised approximate equivariance errors on datasets composed of sequence
transformations."
Rational Metareasoning for Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Being prompted to engage in reasoning has emerged as a core technique for
using large language models (LLMs), deploying additional inference-time compute
to improve task performance. However, as LLMs increase in both size and
adoption, inference costs are correspondingly becoming increasingly burdensome.
How, then, might we optimize reasoning's cost-performance tradeoff? This work
introduces a novel approach based on computational models of metareasoning used
in cognitive science, training LLMs to selectively use intermediate reasoning
steps only when necessary. We first develop a reward function that incorporates
the Value of Computation by penalizing unnecessary reasoning, then use this
reward function with Expert Iteration to train the LLM. Compared to few-shot
chain-of-thought prompting and STaR, our method significantly reduces inference
costs (20-37\% fewer tokens generated across three models) while maintaining
task performance across diverse datasets."
FogROS2-PLR: Probabilistic Latency-Reliability For Cloud Robotics,cs.RO,Robotics,2024-10-07,"Cloud robotics enables robots to offload computationally intensive tasks to
cloud servers for performance, cost, and ease of management. However, the
network and cloud computing infrastructure are not designed for reliable timing
guarantees, due to fluctuating Quality-of-Service (QoS). In this work, we
formulate an impossibility triangle theorem for: Latency reliability, Singleton
server, and Commodity hardware. The LSC theorem suggests that providing
replicated servers with uncorrelated failures can exponentially reduce the
probability of missing a deadline. We present FogROS2-Probabilistic Latency
Reliability (PLR) that uses multiple independent network interfaces to send
requests to replicated cloud servers and uses the first response back. We
design routing mechanisms to discover, connect, and route through non-default
network interfaces on robots. FogROS2-PLR optimizes the selection of interfaces
to servers to minimize the probability of missing a deadline. We conduct a
cloud-connected driving experiment with two 5G service providers, demonstrating
FogROS2-PLR effectively provides smooth service quality even if one of the
service providers experiences low coverage and base station handover. We use 99
Percentile (P99) latency to evaluate anomalous long-tail latency behavior. In
one experiment, FogROS2-PLR improves P99 latency by up to 3.7x compared to
using one service provider. We deploy FogROS2-PLR on a physical Stretch 3 robot
performing an indoor human-tracking task. Even in a fully covered Wi-Fi and 5G
environment, FogROS2-PLR improves the responsiveness of the robot reducing mean
latency by 36% and P99 latency by 33%."
"Cyber Threats to Canadian Federal Election: Emerging Threats, Assessment, and Mitigation Strategies",cs.CR,Cryptography and Security,2024-10-07,"As Canada prepares for the 2025 federal election, ensuring the integrity and
security of the electoral process against cyber threats is crucial. Recent
foreign interference in elections globally highlight the increasing
sophistication of adversaries in exploiting technical and human
vulnerabilities. Such vulnerabilities also exist in Canada's electoral system
that relies on a complex network of IT systems, vendors, and personnel. To
mitigate these vulnerabilities, a threat assessment is crucial to identify
emerging threats, develop incident response capabilities, and build public
trust and resilience against cyber threats. Therefore, this paper presents a
comprehensive national cyber threat assessment, following the NIST Special
Publication 800-30 framework, focusing on identifying and mitigating
cybersecurity risks to the upcoming 2025 Canadian federal election. The
research identifies three major threats: misinformation, disinformation, and
malinformation (MDM) campaigns; attacks on critical infrastructure and election
support systems; and espionage by malicious actors. Through detailed analysis,
the assessment offers insights into the capabilities, intent, and potential
impact of these threats. The paper also discusses emerging technologies and
their influence on election security and proposes a multi-faceted approach to
risk mitigation ahead of the election."
Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"We propose a constraint learning schema for fine-tuning Large Language Models
(LLMs) with attribute control. Given a training corpus and control criteria
formulated as a sequence-level constraint on model outputs, our method
fine-tunes the LLM on the training corpus while enhancing constraint
satisfaction with minimal impact on its utility and generation quality.
Specifically, our approach regularizes the LLM training by penalizing the KL
divergence between the desired output distribution, which satisfies the
constraints, and the LLM's posterior. This regularization term can be
approximated by an auxiliary model trained to decompose the sequence-level
constraints into token-level guidance, allowing the term to be measured by a
closed-form formulation. To further improve efficiency, we design a parallel
scheme for concurrently updating both the LLM and the auxiliary model. We
evaluate the empirical performance of our approach by controlling the toxicity
when training an LLM. We show that our approach leads to an LLM that produces
fewer inappropriate responses while achieving competitive performance on
benchmarks and a toxicity detection task."
Narrative-of-Thought: Improving Temporal Reasoning of Large Language Models via Recounted Narratives,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Reasoning about time and temporal relations is an integral aspect of human
cognition, essential for perceiving the world and navigating our experiences.
Though large language models (LLMs) have demonstrated impressive performance in
many reasoning tasks, temporal reasoning remains challenging due to its
intrinsic complexity. In this work, we first study an essential task of
temporal reasoning -- temporal graph generation, to unveil LLMs' inherent,
global reasoning capabilities. We show that this task presents great challenges
even for the most powerful LLMs, such as GPT-3.5/4. We also notice a
significant performance gap by small models (<10B) that lag behind LLMs by 50%.
Next, we study how to close this gap with a budget constraint, e.g., not using
model finetuning. We propose a new prompting technique tailored for temporal
reasoning, Narrative-of-Thought (NoT), that first converts the events set to a
Python class, then prompts a small model to generate a temporally grounded
narrative, guiding the final generation of a temporal graph. Extensive
experiments showcase the efficacy of NoT in improving various metrics. Notably,
NoT attains the highest F1 on the Schema-11 evaluation set, while securing an
overall F1 on par with GPT-3.5. NoT also achieves the best structural
similarity across the board, even compared with GPT-3.5/4. Our code is
available at https://github.com/launchnlp/NoT."
Rethinking Weak-to-Strong Augmentation in Source-Free Domain Adaptive Object Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Source-Free domain adaptive Object Detection (SFOD) aims to transfer a
detector (pre-trained on source domain) to new unlabelled target domains.
Current SFOD methods typically follow the Mean Teacher framework, where
weak-to-strong augmentation provides diverse and sharp contrast for
self-supervised learning. However, this augmentation strategy suffers from an
inherent problem called crucial semantics loss: Due to random, strong
disturbance, strong augmentation is prone to losing typical visual components,
hindering cross-domain feature extraction. To address this thus-far ignored
limitation, this paper introduces a novel Weak-to-Strong Contrastive Learning
(WSCoL) approach. The core idea is to distill semantics lossless knowledge in
the weak features (from the weak/teacher branch) to guide the representation
learning upon the strong features (from the strong/student branch). To achieve
this, we project the original features into a shared space using a mapping
network, thereby reducing the bias between the weak and strong features.
Meanwhile, a weak features-guided contrastive learning is performed in a
weak-to-strong manner alternatively. Specifically, we first conduct an
adaptation-aware prototype-guided clustering on the weak features to generate
pseudo labels for corresponding strong features matched through proposals.
Sequentially, we identify positive-negative samples based on the pseudo labels
and perform cross-category contrastive learning on the strong features where an
uncertainty estimator encourages adaptive background contrast. Extensive
experiments demonstrate that WSCoL yields new state-of-the-art performance,
offering a built-in mechanism mitigating crucial semantics loss for traditional
Mean Teacher framework. The code and data will be released soon."
MultiNash-PF: A Particle Filtering Approach for Computing Multiple Local Generalized Nash Equilibria in Trajectory Games,cs.RO,Robotics,2024-10-07,"Modern-world robotics involves complex environments where multiple autonomous
agents must interact with each other and other humans. This necessitates
advanced interactive multi-agent motion planning techniques. Generalized Nash
equilibrium(GNE), a solution concept in constrained game theory, provides a
mathematical model to predict the outcome of interactive motion planning, where
each agent needs to account for other agents in the environment. However, in
practice, multiple local GNEs may exist. Finding a single GNE itself is complex
as it requires solving coupled constrained optimal control problems.
Furthermore, finding all such local GNEs requires exploring the solution space
of GNEs, which is a challenging task. This work proposes the MultiNash-PF
framework to efficiently compute multiple local GNEs in constrained trajectory
games. Potential games are a class of games for which a local GNE of a
trajectory game can be found by solving a single constrained optimal control
problem. We propose MultiNash-PF that integrates the potential game approach
with implicit particle filtering, a sample-efficient method for non-convex
trajectory optimization. We first formulate the underlying game as a
constrained potential game and then utilize the implicit particle filtering to
identify the coarse estimates of multiple local minimizers of the game's
potential function. MultiNash-PF then refines these estimates with optimization
solvers, obtaining different local GNEs. We show through numerical simulations
that MultiNash-PF reduces computation time by up to 50\% compared to a baseline
approach."
On Instruction-Finetuning Neural Machine Translation Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"In this work, we introduce instruction finetuning for Neural Machine
Translation (NMT) models, which distills instruction following capabilities
from Large Language Models (LLMs) into orders-of-magnitude smaller NMT models.
Our instruction-finetuning recipe for NMT models enables customization of
translations for a limited but disparate set of translation-specific tasks. We
show that NMT models are capable of following multiple instructions
simultaneously and demonstrate capabilities of zero-shot composition of
instructions. We also show that through instruction finetuning, traditionally
disparate tasks such as formality-controlled machine translation, multi-domain
adaptation as well as multi-modal translations can be tackled jointly by a
single instruction finetuned NMT model, at a performance level comparable to
LLMs such as GPT-3.5-Turbo. To the best of our knowledge, our work is among the
first to demonstrate the instruction-following capabilities of traditional NMT
models, which allows for faster, cheaper and more efficient serving of
customized translations."
Understanding and Imitating Human-Robot Motion with Restricted Visual Fields,cs.RO,Robotics,2024-10-07,"When working around humans, it is important to model their perception
limitations in order to predict their behavior more accurately. In this work,
we consider agents with a limited field of view, viewing range, and ability to
miss objects within viewing range (e.g., transparency). By considering the
observation model independently from the motion policy, we can better predict
the agent's behavior by considering these limitations and approximating them.
We perform a user study where human operators navigate a cluttered scene while
scanning the region for obstacles with a limited field of view and range. Using
imitation learning, we show that a robot can adopt a human's strategy for
observing an environment with limitations on observation and navigate with
minimal collision with dynamic and static obstacles. We also show that this
learned model helps it successfully navigate a physical hardware vehicle in
real time."
Aiding Global Convergence in Federated Learning via Local Perturbation and Mutual Similarity Information,cs.LG,Machine Learning,2024-10-07,"Federated learning has emerged in the last decade as a distributed
optimization paradigm due to the rapidly increasing number of portable devices
able to support the heavy computational needs related to the training of
machine learning models. Federated learning utilizes gradient-based
optimization to minimize a loss objective shared across participating agents.
To the best of our knowledge, the literature mostly lacks elegant solutions
that naturally harness the reciprocal statistical similarity between clients to
redesign the optimization procedure. To address this gap, by conceiving the
federated network as a similarity graph, we propose a novel modified framework
wherein each client locally performs a perturbed gradient step leveraging prior
information about other statistically affine clients. We theoretically prove
that our procedure, due to a suitably introduced adaptation in the update rule,
achieves a quantifiable speedup concerning the exponential contraction factor
in the strongly convex case compared with popular algorithms FedAvg and
FedProx, here analyzed as baselines. Lastly, we legitimize our conclusions
through experimental results on the CIFAR10 and FEMNIST datasets, where we show
that our algorithm speeds convergence up to a margin of 30 global rounds
compared with FedAvg while modestly improving generalization on unseen data in
heterogeneous settings."
On Feature Decorrelation in Cloth-Changing Person Re-identification,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Cloth-changing person re-identification (CC-ReID) poses a significant
challenge in computer vision. A prevailing approach is to prompt models to
concentrate on causal attributes, like facial features and hairstyles, rather
than confounding elements such as clothing appearance. Traditional methods to
achieve this involve integrating multi-modality data or employing manually
annotated clothing labels, which tend to complicate the model and require
extensive human effort. In our study, we demonstrate that simply reducing
feature correlations during training can significantly enhance the baseline
model's performance. We theoretically elucidate this effect and introduce a
novel regularization technique based on density ratio estimation. This
technique aims to minimize feature correlation in the training process of
cloth-changing ReID baselines. Our approach is model-independent, offering
broad enhancements without needing additional data or labels. We validate our
method through comprehensive experiments on prevalent CC-ReID datasets, showing
its effectiveness in improving baseline models' generalization capabilities."
Optimizing Tensor Computation Graphs with Equality Saturation and Monte Carlo Tree Search,cs.LG,Machine Learning,2024-10-07,"The real-world effectiveness of deep neural networks often depends on their
latency, thereby necessitating optimization techniques that can reduce a
model's inference time while preserving its performance. One popular approach
is to sequentially rewrite the input computation graph into an equivalent but
faster one by replacing individual subgraphs. This approach gives rise to the
so-called phase-ordering problem in which the application of one rewrite rule
can eliminate the possibility to apply an even better one later on. Recent work
has shown that equality saturation, a technique from compiler optimization, can
mitigate this issue by first building an intermediate representation (IR) that
efficiently stores multiple optimized versions of the input program before
extracting the best solution in a second step. In practice, however, memory
constraints prevent the IR from capturing all optimized versions and thus
reintroduce the phase-ordering problem in the construction phase. In this
paper, we present a tensor graph rewriting approach that uses Monte Carlo tree
search to build superior IRs by identifying the most promising rewrite rules.
We also introduce a novel extraction algorithm that can provide fast and
accurate runtime estimates of tensor programs represented in an IR. Our
approach improves the inference speedup of neural networks by up to 11%
compared to existing methods."
DOPL: Direct Online Preference Learning for Restless Bandits with Preference Feedback,cs.LG,Machine Learning,2024-10-07,"Restless multi-armed bandits (RMAB) has been widely used to model constrained
sequential decision making problems, where the state of each restless arm
evolves according to a Markov chain and each state transition generates a
scalar reward. However, the success of RMAB crucially relies on the
availability and quality of reward signals. Unfortunately, specifying an exact
reward function in practice can be challenging and even infeasible. In this
paper, we introduce Pref-RMAB, a new RMAB model in the presence of preference
signals, where the decision maker only observes pairwise preference feedback
rather than scalar reward from the activated arms at each decision epoch.
Preference feedback, however, arguably contains less information than the
scalar reward, which makes Pref-RMAB seemingly more difficult. To address this
challenge, we present a direct online preference learning (DOPL) algorithm for
Pref-RMAB to efficiently explore the unknown environments, adaptively collect
preference data in an online manner, and directly leverage the preference
feedback for decision-makings. We prove that DOPL yields a sublinear regret. To
our best knowledge, this is the first algorithm to ensure
$\tilde{\mathcal{O}}(\sqrt{T\ln T})$ regret for RMAB with preference feedback.
Experimental results further demonstrate the effectiveness of DOPL."
Generative Portrait Shadow Removal,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"We introduce a high-fidelity portrait shadow removal model that can
effectively enhance the image of a portrait by predicting its appearance under
disturbing shadows and highlights. Portrait shadow removal is a highly
ill-posed problem where multiple plausible solutions can be found based on a
single image. While existing works have solved this problem by predicting the
appearance residuals that can propagate local shadow distribution, such methods
are often incomplete and lead to unnatural predictions, especially for
portraits with hard shadows. We overcome the limitations of existing local
propagation methods by formulating the removal problem as a generation task
where a diffusion model learns to globally rebuild the human appearance from
scratch as a condition of an input portrait image. For robust and natural
shadow removal, we propose to train the diffusion model with a compositional
repurposing framework: a pre-trained text-guided image generation model is
first fine-tuned to harmonize the lighting and color of the foreground with a
background scene by using a background harmonization dataset; and then the
model is further fine-tuned to generate a shadow-free portrait image via a
shadow-paired dataset. To overcome the limitation of losing fine details in the
latent diffusion model, we propose a guided-upsampling network to restore the
original high-frequency details (wrinkles and dots) from the input image. To
enable our compositional training framework, we construct a high-fidelity and
large-scale dataset using a lightstage capturing system and synthetic graphics
simulation. Our generative framework effectively removes shadows caused by both
self and external occlusions while maintaining original lighting distribution
and high-frequency details. Our method also demonstrates robustness to diverse
subjects captured in real environments."
Scalar Field Prediction on Meshes Using Interpolated Multi-Resolution Convolutional Neural Networks,cs.LG,Machine Learning,2024-10-07,"Scalar fields, such as stress or temperature fields, are often calculated in
shape optimization and design problems in engineering. For complex problems
where shapes have varying topology and cannot be parametrized, data-driven
scalar field prediction can be faster than traditional finite element methods.
However, current data-driven techniques to predict scalar fields are limited to
a fixed grid domain, instead of arbitrary mesh structures. In this work, we
propose a method to predict scalar fields on arbitrary meshes. It uses a
convolutional neural network whose feature maps at multiple resolutions are
interpolated to node positions before being fed into a multilayer perceptron to
predict solutions to partial differential equations at mesh nodes. The model is
trained on finite element von Mises stress fields, and once trained it can
estimate stress values at each node on any input mesh. Two shape datasets are
investigated, and the model has strong performance on both, with a median
R-squared value of 0.91. We also demonstrate the model on a temperature field
in a heat conduction problem, where its predictions have a median R-squared
value of 0.99. Our method provides a potential flexible alternative to finite
element analysis in engineering design contexts. Code and datasets are
available online."
Toward General Object-level Mapping from Sparse Views with 3D Diffusion Priors,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Object-level mapping builds a 3D map of objects in a scene with detailed
shapes and poses from multi-view sensor observations. Conventional methods
struggle to build complete shapes and estimate accurate poses due to partial
occlusions and sensor noise. They require dense observations to cover all
objects, which is challenging to achieve in robotics trajectories. Recent work
introduces generative shape priors for object-level mapping from sparse views,
but is limited to single-category objects. In this work, we propose a General
Object-level Mapping system, GOM, which leverages a 3D diffusion model as shape
prior with multi-category support and outputs Neural Radiance Fields (NeRFs)
for both texture and geometry for all objects in a scene. GOM includes an
effective formulation to guide a pre-trained diffusion model with extra
nonlinear constraints from sensor measurements without finetuning. We also
develop a probabilistic optimization formulation to fuse multi-view sensor
observations and diffusion priors for joint 3D object pose and shape
estimation. Our GOM system demonstrates superior multi-category mapping
performance from sparse views, and achieves more accurate mapping results
compared to state-of-the-art methods on the real-world benchmarks. We will
release our code: https://github.com/TRAILab/GeneralObjectMapping."
Structural Constraints for Physics-augmented Learning,cs.LG,Machine Learning,2024-10-07,"When the physics is wrong, physics-informed machine learning becomes
physics-misinformed machine learning. A powerful black-box model should not be
able to conceal misconceived physics. We propose two criteria that can be used
to assert integrity that a hybrid (physics plus black-box) model: 0) the
black-box model should be unable to replicate the physical model, and 1) any
best-fit hybrid model has the same physical parameter as a best-fit standalone
physics model. We demonstrate them for a sample nonlinear mechanical system
approximated by its small-signal linearization."
Privacy Vulnerabilities in Marginals-based Synthetic Data,cs.CR,Cryptography and Security,2024-10-07,"When acting as a privacy-enhancing technology, synthetic data generation
(SDG) aims to maintain a resemblance to the real data while excluding
personally-identifiable information. Many SDG algorithms provide robust
differential privacy (DP) guarantees to this end. However, we show that the
strongest class of SDG algorithms--those that preserve \textit{marginal
probabilities}, or similar statistics, from the underlying data--leak
information about individuals that can be recovered more efficiently than
previously understood. We demonstrate this by presenting a novel membership
inference attack, MAMA-MIA, and evaluate it against three seminal DP SDG
algorithms: MST, PrivBayes, and Private-GSD. MAMA-MIA leverages knowledge of
which SDG algorithm was used, allowing it to learn information about the hidden
data more accurately, and orders-of-magnitude faster, than other leading
attacks. We use MAMA-MIA to lend insight into existing SDG vulnerabilities. Our
approach went on to win the first SNAKE (SaNitization Algorithm under attacK
... $\varepsilon$) competition."
Residual Kolmogorov-Arnold Network for Enhanced Deep Learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Despite the strong performance in many computer vision tasks, Convolutional
Neural Networks (CNNs) can sometimes struggle to efficiently capture
long-range, complex non-linear dependencies in deeper layers of the network. We
address this limitation by introducing Residual KAN, which incorporates the
Kolmogorov-Arnold Network (KAN) within the CNN framework as a residual
component. Our approach uses Chebyshev polynomials as the basis for KAN
convolutions that enables more expressive and adaptive feature representations
while maintaining computational efficiency. The proposed RKAN blocks, when
integrated into established architectures such as ResNet and DenseNet, offer
consistent improvements over the baseline models on various well-known
benchmarks. Our results demonstrate the potential of RKAN to enhance the
capabilities of deep CNNs in visual data."
Unitary convolutions for learning on graphs and groups,cs.LG,Machine Learning,2024-10-07,"Data with geometric structure is ubiquitous in machine learning often arising
from fundamental symmetries in a domain, such as permutation-invariance in
graphs and translation-invariance in images. Group-convolutional architectures,
which encode symmetries as inductive bias, have shown great success in
applications, but can suffer from instabilities as their depth increases and
often struggle to learn long range dependencies in data. For instance, graph
neural networks experience instability due to the convergence of node
representations (over-smoothing), which can occur after only a few iterations
of message-passing, reducing their effectiveness in downstream tasks. Here, we
propose and study unitary group convolutions, which allow for deeper networks
that are more stable during training. The main focus of the paper are graph
neural networks, where we show that unitary graph convolutions provably avoid
over-smoothing. Our experimental results confirm that unitary graph
convolutional networks achieve competitive performance on benchmark datasets
compared to state-of-the-art graph neural networks. We complement our analysis
of the graph domain with the study of general unitary convolutions and analyze
their role in enhancing stability in general group convolutional architectures."
EgoQR: Efficient QR Code Reading in Egocentric Settings,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"QR codes have become ubiquitous in daily life, enabling rapid information
exchange. With the increasing adoption of smart wearable devices, there is a
need for efficient, and friction-less QR code reading capabilities from
Egocentric point-of-views. However, adapting existing phone-based QR code
readers to egocentric images poses significant challenges. Code reading from
egocentric images bring unique challenges such as wide field-of-view, code
distortion and lack of visual feedback as compared to phones where users can
adjust the position and framing. Furthermore, wearable devices impose
constraints on resources like compute, power and memory. To address these
challenges, we present EgoQR, a novel system for reading QR codes from
egocentric images, and is well suited for deployment on wearable devices. Our
approach consists of two primary components: detection and decoding, designed
to operate on high-resolution images on the device with minimal power
consumption and added latency. The detection component efficiently locates
potential QR codes within the image, while our enhanced decoding component
extracts and interprets the encoded information. We incorporate innovative
techniques to handle the specific challenges of egocentric imagery, such as
varying perspectives, wider field of view, and motion blur. We evaluate our
approach on a dataset of egocentric images, demonstrating 34% improvement in
reading the code compared to an existing state of the art QR code readers."
Intuitions of Compromise: Utilitarianism vs. Contractualism,cs.AI,Artificial Intelligence,2024-10-07,"What is the best compromise in a situation where different people value
different things? The most commonly accepted method for answering this question
-- in fields across the behavioral and social sciences, decision theory,
philosophy, and artificial intelligence development -- is simply to add up
utilities associated with the different options and pick the solution with the
largest sum. This ``utilitarian'' approach seems like the obvious,
theory-neutral way of approaching the problem. But there is an important,
though often-ignored, alternative: a ``contractualist'' approach, which
advocates for an agreement-driven method of deciding. Remarkably, no research
has presented empirical evidence directly comparing the intuitive plausibility
of these two approaches. In this paper, we systematically explore the proposals
suggested by each algorithm (the ``Utilitarian Sum'' and the contractualist
''Nash Product''), using a paradigm that applies those algorithms to
aggregating preferences across groups in a social decision-making context.
While the dominant approach to value aggregation up to now has been
utilitarian, we find that people strongly prefer the aggregations recommended
by the contractualist algorithm. Finally, we compare the judgments of large
language models (LLMs) to that of our (human) participants, finding important
misalignment between model and human preferences."
Self-rationalization improves LLM as a fine-grained judge,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"LLM-as-a-judge models have been used for evaluating both human and AI
generated content, specifically by providing scores and rationales. Rationales,
in addition to increasing transparency, help models learn to calibrate its
judgments. Enhancing a model's rationale can therefore improve its calibration
abilities and ultimately the ability to score content. We introduce
Self-Rationalization, an iterative process of improving the rationales for the
judge models, which consequently improves the score for fine-grained
customizable scoring criteria (i.e., likert-scale scoring with arbitrary
evaluation criteria). Self-rationalization works by having the model generate
multiple judgments with rationales for the same input, curating a preference
pair dataset from its own judgements, and iteratively fine-tuning the judge via
DPO. Intuitively, this approach allows the judge model to self-improve by
learning from its own rationales, leading to better alignment and evaluation
accuracy. After just two iterations -- while only relying on examples in the
training set -- human evaluation shows that our judge model learns to produce
higher quality rationales, with a win rate of $62\%$ on average compared to
models just trained via SFT on rationale . This judge model also achieves high
scoring accuracy on BigGen Bench and Reward Bench, outperforming even bigger
sized models trained using SFT with rationale, self-consistency or best-of-$N$
sampling by $3\%$ to $9\%$."
Transformers learn variable-order Markov chains in-context,cs.LG,Machine Learning,2024-10-07,"Large language models have demonstrated impressive in-context learning (ICL)
capability. However, it is still unclear how the underlying transformers
accomplish it, especially in more complex scenarios. Toward this goal, several
recent works studied how transformers learn fixed-order Markov chains (FOMC) in
context, yet natural languages are more suitably modeled by variable-order
Markov chains (VOMC), i.e., context trees (CTs). In this work, we study the ICL
of VOMC by viewing language modeling as a form of data compression and focus on
small alphabets and low-order VOMCs. This perspective allows us to leverage
mature compression algorithms, such as context-tree weighting (CTW) and
prediction by partial matching (PPM) algorithms as baselines, the former of
which is Bayesian optimal for a class of CTW priors. We empirically observe a
few phenomena: 1) Transformers can indeed learn to compress VOMC in-context,
while PPM suffers significantly; 2) The performance of transformers is not very
sensitive to the number of layers, and even a two-layer transformer can learn
in-context quite well; and 3) Transformers trained and tested on non-CTW priors
can significantly outperform the CTW algorithm. To explain these phenomena, we
analyze the attention map of the transformers and extract two mechanisms, on
which we provide two transformer constructions: 1) A construction with $D+2$
layers that can mimic the CTW algorithm accurately for CTs of maximum order
$D$, 2) A 2-layer transformer that utilizes the feed-forward network for
probability blending. One distinction from the FOMC setting is that a counting
mechanism appears to play an important role. We implement these synthetic
transformer layers and show that such hybrid transformers can match the ICL
performance of transformers, and more interestingly, some of them can perform
even better despite the much-reduced parameter sets."
Pre-Ictal Seizure Prediction Using Personalized Deep Learning,cs.LG,Machine Learning,2024-10-07,"Introduction: Approximately 23 million or 30% of epilepsy patients worldwide
suffer from drug-resistant epilepsy (DRE). The unpredictability of seizure
occurrences, which causes safety issues as well as social concerns, restrict
the lifestyles of DRE patients. Surgical solutions and EEG-based solutions are
very expensive, unreliable, invasive or impractical. The goal of this research
was to employ improved technologies and methods to epilepsy patient
physiological data and predict seizures up to two hours before onset, enabling
non-invasive, affordable seizure prediction for DRE patients.
  Methods: This research used a 1D Convolutional Neural Network-Based
Bidirectional Long Short-Term Memory network that was trained on a diverse set
of epileptic patient physiological data to predict seizures. Transfer learning
was further utilized to personalize and optimize predictions for specific
patients. Clinical data was retrospectively obtained for nine epilepsy patients
via wearable devices over a period of about three to five days from a
prospectively maintained database. The physiological data included 54 seizure
occurrences and included heart rate, blood volume pulse, accelerometry, body
temperature, and electrodermal activity.
  Results and Conclusion: A general deep-learning model trained on the
physiological data with randomly sampled test data achieved an accuracy of
91.94%. However, such a generalized deep learning model had varied performances
on data from unseen patients. When the general model was personalized (further
trained) with patient-specific data, the personalized model achieved
significantly improved performance with accuracies as high as 97%. This
preliminary research shows that patient-specific personalization may be a
viable approach to achieve affordable, non-invasive seizure prediction that can
improve the quality of life for DRE patients."
Neural Networks Decoded: Targeted and Robust Analysis of Neural Network Decisions via Causal Explanations and Reasoning,cs.LG,Machine Learning,2024-10-07,"Despite their success and widespread adoption, the opaque nature of deep
neural networks (DNNs) continues to hinder trust, especially in critical
applications. Current interpretability solutions often yield inconsistent or
oversimplified explanations, or require model changes that compromise
performance. In this work, we introduce TRACER, a novel method grounded in
causal inference theory designed to estimate the causal dynamics underpinning
DNN decisions without altering their architecture or compromising their
performance. Our approach systematically intervenes on input features to
observe how specific changes propagate through the network, affecting internal
activations and final outputs. Based on this analysis, we determine the
importance of individual features, and construct a high-level causal map by
grouping functionally similar layers into cohesive causal nodes, providing a
structured and interpretable view of how different parts of the network
influence the decisions. TRACER further enhances explainability by generating
counterfactuals that reveal possible model biases and offer contrastive
explanations for misclassifications. Through comprehensive evaluations across
diverse datasets, we demonstrate TRACER's effectiveness over existing methods
and show its potential for creating highly compressed yet accurate models,
illustrating its dual versatility in both understanding and optimizing DNNs."
fPLSA: Learning Semantic Structures in Document Collections Using Foundation Models,cs.LG,Machine Learning,2024-10-07,"Humans have the ability to learn new tasks by inferring high-level concepts
from existing solution, then manipulating these concepts in lieu of the raw
data. Can we automate this process by deriving latent semantic structures in a
document collection using foundation models? We introduce fPLSA, a
foundation-model-based Probabilistic Latent Semantic Analysis (PLSA) method
that iteratively clusters and tags document segments based on document-level
contexts. These tags can be used to model the structure of given documents and
for hierarchical sampling of new texts. Our experiments on story writing, math,
and multi-step reasoning datasets demonstrate that fPLSA tags help reconstruct
the original texts better than existing tagging methods. Moreover, when used
for hierarchical sampling, fPLSA produces more diverse outputs with a higher
likelihood of hitting the correct answer than direct sampling and hierarchical
sampling with existing tagging methods."
Ensured: Explanations for Decreasing the Epistemic Uncertainty in Predictions,cs.AI,Artificial Intelligence,2024-10-07,"This paper addresses a significant gap in explainable AI: the necessity of
interpreting epistemic uncertainty in model explanations. Although current
methods mainly focus on explaining predictions, with some including
uncertainty, they fail to provide guidance on how to reduce the inherent
uncertainty in these predictions. To overcome this challenge, we introduce new
types of explanations that specifically target epistemic uncertainty. These
include ensured explanations, which highlight feature modifications that can
reduce uncertainty, and categorisation of uncertain explanations
counter-potential, semi-potential, and super-potential which explore
alternative scenarios. Our work emphasises that epistemic uncertainty adds a
crucial dimension to explanation quality, demanding evaluation based not only
on prediction probability but also on uncertainty reduction. We introduce a new
metric, ensured ranking, designed to help users identify the most reliable
explanations by balancing trade-offs between uncertainty, probability, and
competing alternative explanations. Furthermore, we extend the Calibrated
Explanations method, incorporating tools that visualise how changes in feature
values impact epistemic uncertainty. This enhancement provides deeper insights
into model behaviour, promoting increased interpretability and appropriate
trust in scenarios involving uncertain predictions."
R-Bench: Are your Large Multimodal Model Robust to Real-world Corruptions?,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"The outstanding performance of Large Multimodal Models (LMMs) has made them
widely applied in vision-related tasks. However, various corruptions in the
real world mean that images will not be as ideal as in simulations, presenting
significant challenges for the practical application of LMMs. To address this
issue, we introduce R-Bench, a benchmark focused on the **Real-world Robustness
of LMMs**. Specifically, we: (a) model the complete link from user capture to
LMMs reception, comprising 33 corruption dimensions, including 7 steps
according to the corruption sequence, and 7 groups based on low-level
attributes; (b) collect reference/distorted image dataset before/after
corruption, including 2,970 question-answer pairs with human labeling; (c)
propose comprehensive evaluation for absolute/relative robustness and benchmark
20 mainstream LMMs. Results show that while LMMs can correctly handle the
original reference images, their performance is not stable when faced with
distorted images, and there is a significant gap in robustness compared to the
human visual system. We hope that R-Bench will inspire improving the robustness
of LMMs, **extending them from experimental simulations to the real-world
application**. Check https://q-future.github.io/R-Bench for details."
"Neural machine translation system for Lezgian, Russian and Azerbaijani languages",cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"We release the first neural machine translation system for translation
between Russian, Azerbaijani and the endangered Lezgian languages, as well as
monolingual and parallel datasets collected and aligned for training and
evaluating the system. Multiple experiments are conducted to identify how
different sets of training language pairs and data domains can influence the
resulting translation quality. We achieve BLEU scores of 26.14 for
Lezgian-Azerbaijani, 22.89 for Azerbaijani-Lezgian, 29.48 for Lezgian-Russian
and 24.25 for Russian-Lezgian pairs. The quality of zero-shot translation is
assessed on a Large Language Model, showing its high level of fluency in
Lezgian. However, the model often refuses to translate, justifying itself with
its incompetence. We contribute our translation model along with the collected
parallel and monolingual corpora and sentence encoder for the Lezgian language."
Image Watermarks are Removable Using Controllable Regeneration from Clean Noise,cs.CR,Cryptography and Security,2024-10-07,"Image watermark techniques provide an effective way to assert ownership,
deter misuse, and trace content sources, which has become increasingly
essential in the era of large generative models. A critical attribute of
watermark techniques is their robustness against various manipulations. In this
paper, we introduce a watermark removal approach capable of effectively
nullifying the state of the art watermarking techniques. Our primary insight
involves regenerating the watermarked image starting from a clean Gaussian
noise via a controllable diffusion model, utilizing the extracted semantic and
spatial features from the watermarked image. The semantic control adapter and
the spatial control network are specifically trained to control the denoising
process towards ensuring image quality and enhancing consistency between the
cleaned image and the original watermarked image. To achieve a smooth trade-off
between watermark removal performance and image consistency, we further propose
an adjustable and controllable regeneration scheme. This scheme adds varying
numbers of noise steps to the latent representation of the watermarked image,
followed by a controlled denoising process starting from this noisy latent
representation. As the number of noise steps increases, the latent
representation progressively approaches clean Gaussian noise, facilitating the
desired trade-off. We apply our watermark removal methods across various
watermarking techniques, and the results demonstrate that our methods offer
superior visual consistency/quality and enhanced watermark removal performance
compared to existing regeneration approaches."
PH-Dropout: Prctical Epistemic Uncertainty Quantification for View Synthesis,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"View synthesis using Neural Radiance Fields (NeRF) and Gaussian Splatting
(GS) has demonstrated impressive fidelity in rendering real-world scenarios.
However, practical methods for accurate and efficient epistemic Uncertainty
Quantification (UQ) in view synthesis are lacking. Existing approaches for NeRF
either introduce significant computational overhead (e.g., ``10x increase in
training time"" or ``10x repeated training"") or are limited to specific
uncertainty conditions or models. Notably, GS models lack any systematic
approach for comprehensive epistemic UQ. This capability is crucial for
improving the robustness and scalability of neural view synthesis, enabling
active model updates, error estimation, and scalable ensemble modeling based on
uncertainty. In this paper, we revisit NeRF and GS-based methods from a
function approximation perspective, identifying key differences and connections
in 3D representation learning. Building on these insights, we introduce
PH-Dropout (Post hoc Dropout), the first real-time and accurate method for
epistemic uncertainty estimation that operates directly on pre-trained NeRF and
GS models. Extensive evaluations validate our theoretical findings and
demonstrate the effectiveness of PH-Dropout."
Herd Mentality in Augmentation -- Not a Good Idea! A Robust Multi-stage Approach towards Deepfake Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"The rapid increase in deepfake technology has raised significant concerns
about digital media integrity. Detecting deepfakes is crucial for safeguarding
digital media. However, most standard image classifiers fail to distinguish
between fake and real faces. Our analysis reveals that this failure is due to
the model's inability to explicitly focus on the artefacts typically in
deepfakes. We propose an enhanced architecture based on the GenConViT model,
which incorporates weighted loss and update augmentation techniques and
includes masked eye pretraining. This proposed model improves the F1 score by
1.71% and the accuracy by 4.34% on the Celeb-DF v2 dataset. The source code for
our model is available at
https://github.com/Monu-Khicher-1/multi-stage-learning"
On the Expressive Power of Tree-Structured Probabilistic Circuits,cs.AI,Artificial Intelligence,2024-10-07,"Probabilistic circuits (PCs) have emerged as a powerful framework to
compactly represent probability distributions for efficient and exact
probabilistic inference. It has been shown that PCs with a general directed
acyclic graph (DAG) structure can be understood as a mixture of exponentially
(in its height) many components, each of which is a product distribution over
univariate marginals. However, existing structure learning algorithms for PCs
often generate tree-structured circuits or use tree-structured circuits as
intermediate steps to compress them into DAG-structured circuits. This leads to
the intriguing question of whether there exists an exponential gap between DAGs
and trees for the PC structure. In this paper, we provide a negative answer to
this conjecture by proving that, for $n$ variables, there exists a
sub-exponential upper bound $n^{O(\log n)}$ on the size of an equivalent tree
computing the same probability distribution. On the other hand, we also show
that given a depth restriction on the tree, there is a super-polynomial
separation between tree and DAG-structured PCs. Our work takes an important
step towards understanding the expressive power of tree-structured PCs, and our
techniques may be of independent interest in the study of structure learning
algorithms for PCs."
Progressive distillation induces an implicit curriculum,cs.LG,Machine Learning,2024-10-07,"Knowledge distillation leverages a teacher model to improve the training of a
student model. A persistent challenge is that a better teacher does not always
yield a better student, to which a common mitigation is to use additional
supervision from several ``intermediate'' teachers. One empirically validated
variant of this principle is progressive distillation, where the student learns
from successive intermediate checkpoints of the teacher. Using sparse parity as
a sandbox, we identify an implicit curriculum as one mechanism through which
progressive distillation accelerates the student's learning. This curriculum is
available only through the intermediate checkpoints but not the final converged
one, and imparts both empirical acceleration and a provable sample complexity
benefit to the student. We then extend our investigation to Transformers
trained on probabilistic context-free grammars (PCFGs) and real-world
pre-training datasets (Wikipedia and Books). Through probing the teacher model,
we identify an analogous implicit curriculum where the model progressively
learns features that capture longer context. Our theoretical and empirical
findings on sparse parity, complemented by empirical observations on more
complex tasks, highlight the benefit of progressive distillation via implicit
curriculum across setups."
"LevAttention: Time, Space, and Streaming Efficient Algorithm for Heavy Attentions",cs.LG,Machine Learning,2024-10-07,"A central problem related to transformers can be stated as follows: given two
$n \times d$ matrices $Q$ and $K$, and a non-negative function $f$, define the
matrix $A$ as follows: (1) apply the function $f$ to each entry of the $n
\times n$ matrix $Q K^T$, and then (2) normalize each of the row sums of $A$ to
be equal to $1$. The matrix $A$ can be computed in $O(n^2 d)$ time assuming $f$
can be applied to a number in constant time, but the quadratic dependence on
$n$ is prohibitive in applications where it corresponds to long context
lengths. For a large class of functions $f$, we show how to find all the
``large attention scores"", i.e., entries of $A$ which are at least a positive
value $\varepsilon$, in time with linear dependence on $n$ (i.e., $n \cdot
\textrm{poly}(d/\varepsilon)$) for a positive parameter $\varepsilon > 0$. Our
class of functions include all functions $f$ of the form $f(x) = |x|^p$, as
explored recently in transformer models. Using recently developed tools from
randomized numerical linear algebra, we prove that for any $K$, there is a
``universal set"" $U \subset [n]$ of size independent of $n$, such that for any
$Q$ and any row $i$, the large attention scores $A_{i,j}$ in row $i$ of $A$ all
have $j \in U$. We also find $U$ in $n \cdot \textrm{poly}(d/\varepsilon)$
time. Notably, we (1) make no assumptions on the data, (2) our workspace does
not grow with $n$, and (3) our algorithms can be computed in streaming and
parallel settings. We call the attention mechanism that uses only the subset of
keys in the universal set as LevAttention since our algorithm to identify the
universal set $U$ is based on leverage scores. We empirically show the benefits
of our scheme for vision transformers, showing how to train new models that use
our universal set while training as well, showing that our model is able to
consistently select ``important keys'' during training."
From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency,cs.LG,Machine Learning,2024-10-07,"Chain-of-thought (CoT) significantly enhances the reasoning performance of
large language models (LLM). While current theoretical studies often attribute
this improvement to increased expressiveness and computational capacity, we
argue that expressiveness is not the primary limitation in the LLM regime, as
current large models will fail on simple tasks. Using a parity-learning setup,
we demonstrate that CoT can substantially improve sample efficiency even when
the representation power is sufficient. Specifically, with CoT, a transformer
can learn the function within polynomial samples, whereas without CoT, the
required sample size is exponential. Additionally, we show that CoT simplifies
the learning process by introducing sparse sequential dependencies among input
tokens, and leads to a sparse and interpretable attention. We validate our
theoretical analysis with both synthetic and real-world experiments, confirming
that sparsity in attention layers is a key factor of the improvement induced by
CoT."
Testing Credibility of Public and Private Surveys through the Lens of Regression,cs.LG,Machine Learning,2024-10-07,"Testing whether a sample survey is a credible representation of the
population is an important question to ensure the validity of any downstream
research. While this problem, in general, does not have an efficient solution,
one might take a task-based approach and aim to understand whether a certain
data analysis tool, like linear regression, would yield similar answers both on
the population and the sample survey. In this paper, we design an algorithm to
test the credibility of a sample survey in terms of linear regression. In other
words, we design an algorithm that can certify if a sample survey is good
enough to guarantee the correctness of data analysis done using linear
regression tools. Nowadays, one is naturally concerned about data privacy in
surveys. Thus, we further test the credibility of surveys published in a
differentially private manner. Specifically, we focus on Local Differential
Privacy (LDP), which is a standard technique to ensure privacy in surveys where
the survey participants might not trust the aggregator. We extend our algorithm
to work even when the data analysis has been done using surveys with LDP. In
the process, we also propose an algorithm that learns with high probability the
guarantees a linear regression model on a survey published with LDP. Our
algorithm also serves as a mechanism to learn linear regression models from
data corrupted with noise coming from any subexponential distribution. We prove
that it achieves the optimal estimation error bound for $\ell_1$ linear
regression, which might be of broader interest. We prove the theoretical
correctness of our algorithms while trying to reduce the sample complexity for
both public and private surveys. We also numerically demonstrate the
performance of our algorithms on real and synthetic datasets."
Dynamic HumTrans: Humming Transcription Using CNNs and Dynamic Programming,cs.LG,Machine Learning,2024-10-07,"We propose a novel approach for humming transcription that combines a
CNN-based architecture with a dynamic programming-based post-processing
algorithm, utilizing the recently introduced HumTrans dataset. We identify and
address inherent problems with the offset and onset ground truth provided by
the dataset, offering heuristics to improve these annotations, resulting in a
dataset with precise annotations that will aid future research. Additionally,
we compare the transcription accuracy of our method against several others,
demonstrating state-of-the-art (SOTA) results. All our code and corrected
dataset is available at
https://github.com/shubham-gupta-30/humming_transcription"
Automatic Identification and Visualization of Group Training Activities Using Wearable Data,cs.LG,Machine Learning,2024-10-07,"Human Activity Recognition (HAR) identifies daily activities from time-series
data collected by wearable devices like smartwatches. Recent advancements in
Internet of Things (IoT), cloud computing, and low-cost sensors have broadened
HAR applications across fields like healthcare, biometrics, sports, and
personal fitness. However, challenges remain in efficiently processing the vast
amounts of data generated by these devices and developing models that can
accurately recognize a wide range of activities from continuous recordings,
without relying on predefined activity training sessions. This paper presents a
comprehensive framework for imputing, analyzing, and identifying activities
from wearable data, specifically targeting group training scenarios without
explicit activity sessions. Our approach is based on data collected from 135
soldiers wearing Garmin 55 smartwatches over six months. The framework
integrates multiple data streams, handles missing data through cross-domain
statistical methods, and identifies activities with high accuracy using machine
learning (ML). Additionally, we utilized statistical analysis techniques to
evaluate the performance of each individual within the group, providing
valuable insights into their respective positions in the group in an
easy-to-understand visualization. These visualizations facilitate easy
understanding of performance metrics, enhancing group interactions and
informing individualized training programs. We evaluate our framework through
traditional train-test splits and out-of-sample scenarios, focusing on the
model's generalization capabilities. Additionally, we address sleep data
imputation without relying on ML, improving recovery analysis. Our findings
demonstrate the potential of wearable data for accurately identifying group
activities, paving the way for intelligent, data-driven training solutions."
Aligning LLMs to Be Robust Against Prompt Injection,cs.CR,Cryptography and Security,2024-10-07,"Large language models (LLMs) are becoming increasingly prevalent in modern
software systems, interfacing between the user and the internet to assist with
tasks that require advanced language understanding. To accomplish these tasks,
the LLM often uses external data sources such as user documents, web retrieval,
results from API calls, etc. This opens up new avenues for attackers to
manipulate the LLM via prompt injection. Adversarial prompts can be carefully
crafted and injected into external data sources to override the user's intended
instruction and instead execute a malicious instruction. Prompt injection
attacks constitute a major threat to LLM security, making the design and
implementation of practical countermeasures of paramount importance. To this
end, we show that alignment can be a powerful tool to make LLMs more robust
against prompt injection. Our method -- SecAlign -- first builds an alignment
dataset by simulating prompt injection attacks and constructing pairs of
desirable and undesirable responses. Then, we apply existing alignment
techniques to fine-tune the LLM to be robust against these simulated attacks.
Our experiments show that SecAlign robustifies the LLM substantially with a
negligible hurt on model utility. Moreover, SecAlign's protection generalizes
to strong attacks unseen in training. Specifically, the success rate of
state-of-the-art GCG-based prompt injections drops from 56% to 2% in Mistral-7B
after our alignment process. Our code is released at
https://github.com/facebookresearch/SecAlign"
AI-Driven Early Mental Health Screening with Limited Data: Analyzing Selfies of Pregnant Women,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Major Depressive Disorder and anxiety disorders affect millions globally,
contributing significantly to the burden of mental health issues. Early
screening is crucial for effective intervention, as timely identification of
mental health issues can significantly improve treatment outcomes. Artificial
intelligence (AI) can be valuable for improving the screening of mental
disorders, enabling early intervention and better treatment outcomes. AI-driven
screening can leverage the analysis of multiple data sources, including facial
features in digital images. However, existing methods often rely on controlled
environments or specialized equipment, limiting their broad applicability. This
study explores the potential of AI models for ubiquitous depression-anxiety
screening given face-centric selfies. The investigation focuses on high-risk
pregnant patients, a population that is particularly vulnerable to mental
health issues. To cope with limited training data resulting from our clinical
setup, pre-trained models were utilized in two different approaches:
fine-tuning convolutional neural networks (CNNs) originally designed for facial
expression recognition and employing vision-language models (VLMs) for
zero-shot analysis of facial expressions. Experimental results indicate that
the proposed VLM-based method significantly outperforms CNNs, achieving an
accuracy of 77.6% and an F1-score of 56.0%. Although there is significant room
for improvement, the results suggest that VLMs can be a promising approach for
mental health screening, especially in scenarios with limited data."
Task Diversity Shortens the ICL Plateau,cs.LG,Machine Learning,2024-10-07,"In-context learning (ICL) describes a language model's ability to generate
outputs based on a set of input demonstrations and a subsequent query. To
understand this remarkable capability, researchers have studied simplified,
stylized models. These studies have consistently observed long loss plateaus,
during which models exhibit minimal improvement, followed by a sudden, rapid
surge of learning. In this work, we reveal that training on multiple diverse
ICL tasks simultaneously shortens the loss plateaus, making each task easier to
learn. This finding is surprising as it contradicts the natural intuition that
the combined complexity of multiple ICL tasks would lengthen the learning
process, not shorten it. Our result suggests that the recent success in
large-scale training of language models may be attributed not only to the
richness of the data at scale but also to the easier optimization (training)
induced by the diversity of natural language training data."
"Propeller damage detection, classification and estimation in multirotor vehicles",cs.RO,Robotics,2024-10-07,"This manuscript details an architecture and training methodology for a
data-driven framework aimed at detecting, identifying, and quantifying damage
in the propeller blades of multirotor Unmanned Aerial Vehicles. By substituting
one propeller with a damaged counterpart-encompassing three distinct damage
types of varying severity-real flight data was collected. This data was then
used to train a composite model, comprising both classifiers and neural
networks, capable of accurately identifying the type of failure, estimating
damage severity, and pinpointing the affected rotor. The data employed for this
analysis was exclusively sourced from inertial measurements and control command
inputs, ensuring adaptability across diverse multirotor vehicle platforms."
Online scalable Gaussian processes with conformal prediction for guaranteed coverage,cs.LG,Machine Learning,2024-10-07,"The Gaussian process (GP) is a Bayesian nonparametric paradigm that is widely
adopted for uncertainty quantification (UQ) in a number of safety-critical
applications, including robotics, healthcare, as well as surveillance. The
consistency of the resulting uncertainty values however, hinges on the premise
that the learning function conforms to the properties specified by the GP
model, such as smoothness, periodicity and more, which may not be satisfied in
practice, especially with data arriving on the fly. To combat against such
model mis-specification, we propose to wed the GP with the prevailing conformal
prediction (CP), a distribution-free post-processing framework that produces it
prediction sets with a provably valid coverage under the sole assumption of
data exchangeability. However, this assumption is usually violated in the
online setting, where a prediction set is sought before revealing the true
label. To ensure long-term coverage guarantee, we will adaptively set the key
threshold parameter based on the feedback whether the true label falls inside
the prediction set. Numerical results demonstrate the merits of the online
GP-CP approach relative to existing alternatives in the long-term coverage
performance."
A Deep Learning-Based Approach for Mangrove Monitoring,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Mangroves are dynamic coastal ecosystems that are crucial to environmental
health, economic stability, and climate resilience. The monitoring and
preservation of mangroves are of global importance, with remote sensing
technologies playing a pivotal role in these efforts. The integration of
cutting-edge artificial intelligence with satellite data opens new avenues for
ecological monitoring, potentially revolutionizing conservation strategies at a
time when the protection of natural resources is more crucial than ever. The
objective of this work is to provide a comprehensive evaluation of recent
deep-learning models on the task of mangrove segmentation. We first introduce
and make available a novel open-source dataset, MagSet-2, incorporating
mangrove annotations from the Global Mangrove Watch and satellite images from
Sentinel-2, from mangrove positions all over the world. We then benchmark three
architectural groups, namely convolutional, transformer, and mamba models,
using the created dataset. The experimental outcomes further validate the deep
learning community's interest in the Mamba model, which surpasses other
architectures in all metrics."
Can LLMs Understand Time Series Anomalies?,cs.LG,Machine Learning,2024-10-07,"Large Language Models (LLMs) have gained popularity in time series
forecasting, but their potential for anomaly detection remains largely
unexplored. Our study investigates whether LLMs can understand and detect
anomalies in time series data, focusing on zero-shot and few-shot scenarios.
Inspired by conjectures about LLMs' behavior from time series forecasting
research, we formulate key hypotheses about LLMs' capabilities in time series
anomaly detection. We design and conduct principled experiments to test each of
these hypotheses.
  Our investigation reveals several surprising findings about LLMs for time
series:
  1. LLMs understand time series better as *images* rather than as text
  2. LLMs did not demonstrate enhanced performance when prompted to engage in
*explicit reasoning* about time series analysis
  3. Contrary to common beliefs, LLM's understanding of time series *do not*
stem from their repetition biases or arithmetic abilities
  4. LLMs' behaviors and performance in time series analysis *vary
significantly* across different model architectures
  This study provides the first comprehensive analysis of contemporary LLM
capabilities in time series anomaly detection. Our results suggest that while
LLMs can understand time series anomalies, many common conjectures based on
their reasoning capabilities do not hold. These insights pave the way for more
effective LLM-based approaches in time series analysis, bridging the gap
between forecasting and anomaly detection applications."
DAAL: Density-Aware Adaptive Line Margin Loss for Multi-Modal Deep Metric Learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Multi-modal deep metric learning is crucial for effectively capturing diverse
representations in tasks such as face verification, fine-grained object
recognition, and product search. Traditional approaches to metric learning,
whether based on distance or margin metrics, primarily emphasize class
separation, often overlooking the intra-class distribution essential for
multi-modal feature learning. In this context, we propose a novel loss function
called Density-Aware Adaptive Margin Loss(DAAL), which preserves the density
distribution of embeddings while encouraging the formation of adaptive
sub-clusters within each class. By employing an adaptive line strategy, DAAL
not only enhances intra-class variance but also ensures robust inter-class
separation, facilitating effective multi-modal representation. Comprehensive
experiments on benchmark fine-grained datasets demonstrate the superior
performance of DAAL, underscoring its potential in advancing retrieval
applications and multi-modal deep metric learning."
ESPACE: Dimensionality Reduction of Activations for Model Compression,cs.LG,Machine Learning,2024-10-07,"We propose ESPACE, an LLM compression technique based on dimensionality
reduction of activations. Unlike prior works on weight-centric tensor
decomposition, ESPACE projects activations onto a pre-calibrated set of
principal components. The activation-centrality of the approach enables
retraining LLMs with no loss of expressivity; while at inference, weight
decomposition is obtained as a byproduct of matrix multiplication
associativity. Theoretical results on the construction of projection matrices
with optimal computational accuracy are provided. Experimentally, we find
ESPACE enables 50% compression of GPT3, Llama2, and Nemotron4 models with small
accuracy degradation, as low as a 0.18 perplexity increase on GPT3-22B. At
lower compression rates of 20% to 40%, ESPACE drives GPT3 models to
outperforming their baseline, by up to a 0.38 decrease in perplexity for
GPT3-8B. ESPACE also reduces GEMM execution time and prefill inference latency
on existing hardware. Comparison with related works on compressing Llama2-7B
via matrix factorization shows that ESPACE is a first step in advancing the
state-of-the-art in tensor decomposition compression of LLMs."
Discovering distinctive elements of biomedical datasets for high-performance exploration,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"The human brain represents an object by small elements and distinguishes two
objects based on the difference in elements. Discovering the distinctive
elements of high-dimensional datasets is therefore critical in numerous
perception-driven biomedical and clinical studies. However, currently there is
no available method for reliable extraction of distinctive elements of
high-dimensional biomedical and clinical datasets. Here we present an
unsupervised deep learning technique namely distinctive element analysis (DEA),
which extracts the distinctive data elements using high-dimensional correlative
information of the datasets. DEA at first computes a large number of
distinctive parts of the data, then filters and condenses the parts into DEA
elements by employing a unique kernel-driven triple-optimization network. DEA
has been found to improve the accuracy by up to 45% in comparison to the
traditional techniques in applications such as disease detection from medical
images, gene ranking and cell recognition from single cell RNA sequence
(scRNA-seq) datasets. Moreover, DEA allows user-guided manipulation of the
intermediate calculation process and thus offers intermediate results with
better interpretability."
Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback,cs.LG,Machine Learning,2024-10-07,"While large language models (LLMs) show impressive decision-making abilities,
current methods lack a mechanism for automatic self-improvement from errors
during task execution. We propose LEAP, an iterative fine-tuning framework that
continually improves LLM agents using feedback from AI expert teachers. Our key
insight is to equip the expert teachers with a privileged state -- information
that is available during training but hidden at test time. This allows even
weak experts to provide precise guidance, significantly improving the student
agent's performance without access to privileged information at test time. We
evaluate LEAP on diverse decision-making benchmarks, including text-based games
(ALFWorld), web navigation (WebShop), and interactive coding (Intercode Bash).
Our experiments show that LEAP (1) outperforms behavior cloning and ReAct
baselines (2) enables weak student models (e.g., Llama3-8B) to exceed the
performance of strong teacher models (GPT4-o), and (3) allows weak models to
self-improve using privileged versions of themselves. We also provide a
theoretical analysis showing that LEAP's success hinges on balancing privileged
information with the student's realizability, which we empirically validate.
Our code is available at https://leap-llm.github.io"
2FAST-2LAMAA: A Lidar-Inertial Localisation and Mapping Framework for Non-Static Environments,cs.RO,Robotics,2024-10-07,"This document presents a framework for lidar-inertial localisation and
mapping named 2Fast-2Lamaa. The method revolves around two main steps which are
the inertial-aided undistortion of the lidar data and the scan-to-map
registration using a distance-field representation of the environment. The
initialisation-free undistortion uses inertial data to constrain the continuous
trajectory of the sensor during the lidar scan. The eleven DoFs that fully
characterise the trajectory are estimated by minimising lidar point-to-line and
point-to-plane distances in a non-linear least-square formulation. The
registration uses a map that provides a distance field for the environment
based on Gaussian Process regression. The pose of an undistorted lidar scan is
optimised to minimise the distance field queries of its points with respect to
the map. After registration, the new geometric information is efficiently
integrated into the map. The soundness of 2Fast-2Lamaa is demonstrated over
several datasets (qualitative evaluation only). The real-time implementation is
made publicly available at https://github.com/UTS-RI/2fast2lamaa."
Continuous Ensemble Weather Forecasting with Diffusion models,cs.LG,Machine Learning,2024-10-07,"Weather forecasting has seen a shift in methods from numerical simulations to
data-driven systems. While initial research in the area focused on
deterministic forecasting, recent works have used diffusion models to produce
skillful ensemble forecasts. These models are trained on a single forecasting
step and rolled out autoregressively. However, they are computationally
expensive and accumulate errors for high temporal resolution due to the many
rollout steps. We address these limitations with Continuous Ensemble
Forecasting, a novel and flexible method for sampling ensemble forecasts in
diffusion models. The method can generate temporally consistent ensemble
trajectories completely in parallel, with no autoregressive steps. Continuous
Ensemble Forecasting can also be combined with autoregressive rollouts to yield
forecasts at an arbitrary fine temporal resolution without sacrificing
accuracy. We demonstrate that the method achieves competitive results for
global weather forecasting with good probabilistic properties."
A Functional Extension of Semi-Structured Networks,cs.LG,Machine Learning,2024-10-07,"Semi-structured networks (SSNs) merge the structures familiar from additive
models with deep neural networks, allowing the modeling of interpretable
partial feature effects while capturing higher-order non-linearities at the
same time. A significant challenge in this integration is maintaining the
interpretability of the additive model component. Inspired by large-scale
biomechanics datasets, this paper explores extending SSNs to functional data.
Existing methods in functional data analysis are promising but often not
expressive enough to account for all interactions and non-linearities and do
not scale well to large datasets. Although the SSN approach presents a
compelling potential solution, its adaptation to functional data remains
complex. In this work, we propose a functional SSN method that retains the
advantageous properties of classical functional regression approaches while
also improving scalability. Our numerical experiments demonstrate that this
approach accurately recovers underlying signals, enhances predictive
performance, and performs favorably compared to competing methods."
Diffusion Imitation from Observation,cs.LG,Machine Learning,2024-10-07,"Learning from observation (LfO) aims to imitate experts by learning from
state-only demonstrations without requiring action labels. Existing adversarial
imitation learning approaches learn a generator agent policy to produce state
transitions that are indistinguishable to a discriminator that learns to
classify agent and expert state transitions. Despite its simplicity in
formulation, these methods are often sensitive to hyperparameters and brittle
to train. Motivated by the recent success of diffusion models in generative
modeling, we propose to integrate a diffusion model into the adversarial
imitation learning from observation framework. Specifically, we employ a
diffusion model to capture expert and agent transitions by generating the next
state, given the current state. Then, we reformulate the learning objective to
train the diffusion model as a binary classifier and use it to provide
""realness"" rewards for policy learning. Our proposed framework, Diffusion
Imitation from Observation (DIFO), demonstrates superior performance in various
continuous control domains, including navigation, locomotion, manipulation, and
games. Project page: https://nturobotlearninglab.github.io/DIFO"
Designing a Classifier for Active Fire Detection from Multispectral Satellite Imagery Using Neural Architecture Search,cs.LG,Machine Learning,2024-10-07,"This paper showcases the use of a reinforcement learning-based Neural
Architecture Search (NAS) agent to design a small neural network to perform
active fire detection on multispectral satellite imagery. Specifically, we aim
to design a neural network that can determine if a single multispectral pixel
is a part of a fire, and do so within the constraints of a Low Earth Orbit
(LEO) nanosatellite with a limited power budget, to facilitate on-board
processing of sensor data. In order to use reinforcement learning, a reward
function is needed. We supply this reward function in the shape of a regression
model that predicts the F1 score obtained by a particular architecture,
following quantization to INT8 precision, from purely architectural features.
This model is trained by collecting a random sample of neural network
architectures, training these architectures, and collecting their
classification performance statistics. Besides the F1 score, we also include
the total number of trainable parameters in our reward function to limit the
size of the designed model and ensure it fits within the resource constraints
imposed by nanosatellite platforms. Finally, we deployed the best neural
network to the Google Coral Micro Dev Board and evaluated its inference latency
and power consumption. This neural network consists of 1,716 trainable
parameters, takes on average 984{\mu}s to inference, and consumes around 800mW
to perform inference. These results show that our reinforcement learning-based
NAS approach can be successfully applied to novel problems not tackled before."
Refining Counterfactual Explanations With Joint-Distribution-Informed Shapley Towards Actionable Minimality,cs.LG,Machine Learning,2024-10-07,"Counterfactual explanations (CE) identify data points that closely resemble
the observed data but produce different machine learning (ML) model outputs,
offering critical insights into model decisions. Despite the diverse scenarios,
goals and tasks to which they are tailored, existing CE methods often lack
actionable efficiency because of unnecessary feature changes included within
the explanations that are presented to users and stakeholders. We address this
problem by proposing a method that minimizes the required feature changes while
maintaining the validity of CE, without imposing restrictions on models or CE
algorithms, whether instance- or group-based. The key innovation lies in
computing a joint distribution between observed and counterfactual data and
leveraging it to inform Shapley values for feature attributions (FA). We
demonstrate that optimal transport (OT) effectively derives this distribution,
especially when the alignment between observed and counterfactual data is
unclear in used CE methods. Additionally, a counterintuitive finding is
uncovered: it may be misleading to rely on an exact alignment defined by the CE
generation mechanism in conducting FA. Our proposed method is validated on
extensive experiments across multiple datasets, showcasing its effectiveness in
refining CE towards greater actionable efficiency."
STOP! Camera Spoofing via the in-Vehicle IP Network,cs.CR,Cryptography and Security,2024-10-07,"Autonomous driving and advanced driver assistance systems (ADAS) rely on
cameras to control the driving. In many prior approaches an attacker aiming to
stop the vehicle had to send messages on the specialized and better-defended
CAN bus. We suggest an easier alternative: manipulate the IP-based network
communication between the camera and the ADAS logic, inject fake images of stop
signs or red lights into the video stream, and let the ADAS stop the car
safely. We created an attack tool that successfully exploits the GigE Vision
protocol.
  Then we analyze two classes of passive anomaly detectors to identify such
attacks: protocol-based detectors and video-based detectors. We implemented
multiple detectors of both classes and evaluated them on data collected from
our test vehicle and also on data from the public BDD corpus. Our results show
that such detectors are effective against naive adversaries, but sophisticated
adversaries can evade detection.
  Finally, we propose a novel class of active defense mechanisms that randomly
adjust camera parameters during the video transmission, and verify that the
received images obey the requested adjustments. Within this class we focus on a
specific implementation, the width-varying defense, which randomly modifies the
width of every frame. Beyond its function as an anomaly detector, this defense
is also a protective measure against certain attacks: by distorting injected
image patches it prevents their recognition by the ADAS logic. We demonstrate
the effectiveness of the width-varying defense through theoretical analysis and
by an extensive evaluation of several types of attack in a wide range of
realistic road driving conditions. The best the attack was able to achieve
against this defense was injecting a stop sign for a duration of 0.2 seconds,
with a success probability of 0.2%, whereas stopping a vehicle requires about
2.5 seconds."
Haste Makes Waste: A Simple Approach for Scaling Graph Neural Networks,cs.LG,Machine Learning,2024-10-07,"Graph neural networks (GNNs) have demonstrated remarkable success in graph
representation learning, and various sampling approaches have been proposed to
scale GNNs to applications with large-scale graphs. A class of promising GNN
training algorithms take advantage of historical embeddings to reduce the
computation and memory cost while maintaining the model expressiveness of GNNs.
However, they incur significant computation bias due to the stale feature
history. In this paper, we provide a comprehensive analysis of their staleness
and inferior performance on large-scale problems. Motivated by our discoveries,
we propose a simple yet highly effective training algorithm (REST) to
effectively reduce feature staleness, which leads to significantly improved
performance and convergence across varying batch sizes. The proposed algorithm
seamlessly integrates with existing solutions, boasting easy implementation,
while comprehensive experiments underscore its superior performance and
efficiency on large-scale benchmarks. Specifically, our improvements to
state-of-the-art historical embedding methods result in a 2.7% and 3.6%
performance enhancement on the ogbn-papers100M and ogbn-products dataset
respectively, accompanied by notably accelerated convergence."
Enhanced Super-Resolution Training via Mimicked Alignment for Real-World Scenes,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Image super-resolution methods have made significant strides with deep
learning techniques and ample training data. However, they face challenges due
to inherent misalignment between low-resolution (LR) and high-resolution (HR)
pairs in real-world datasets. In this study, we propose a novel plug-and-play
module designed to mitigate these misalignment issues by aligning LR inputs
with HR images during training. Specifically, our approach involves mimicking a
novel LR sample that aligns with HR while preserving the degradation
characteristics of the original LR samples. This module seamlessly integrates
with any SR model, enhancing robustness against misalignment. Importantly, it
can be easily removed during inference, therefore without introducing any
parameters on the conventional SR models. We comprehensively evaluate our
method on synthetic and real-world datasets, demonstrating its effectiveness
across a spectrum of SR models, including traditional CNNs and state-of-the-art
Transformers. The source codes will be publicly made available at
https://github.com/omarAlezaby/Mimicked_Ali ."
Improving Predictor Reliability with Selective Recalibration,cs.LG,Machine Learning,2024-10-07,"A reliable deep learning system should be able to accurately express its
confidence with respect to its predictions, a quality known as calibration. One
of the most effective ways to produce reliable confidence estimates with a
pre-trained model is by applying a post-hoc recalibration method. Popular
recalibration methods like temperature scaling are typically fit on a small
amount of data and work in the model's output space, as opposed to the more
expressive feature embedding space, and thus usually have only one or a handful
of parameters. However, the target distribution to which they are applied is
often complex and difficult to fit well with such a function. To this end we
propose \textit{selective recalibration}, where a selection model learns to
reject some user-chosen proportion of the data in order to allow the
recalibrator to focus on regions of the input space that can be well-captured
by such a model. We provide theoretical analysis to motivate our algorithm, and
test our method through comprehensive experiments on difficult medical imaging
and zero-shot classification tasks. Our results show that selective
recalibration consistently leads to significantly lower calibration error than
a wide range of selection and recalibration baselines."
Synthesizing Interpretable Control Policies through Large Language Model Guided Search,cs.AI,Artificial Intelligence,2024-10-07,"The combination of Large Language Models (LLMs), systematic evaluation, and
evolutionary algorithms has enabled breakthroughs in combinatorial optimization
and scientific discovery. We propose to extend this powerful combination to the
control of dynamical systems, generating interpretable control policies capable
of complex behaviors. With our novel method, we represent control policies as
programs in standard languages like Python. We evaluate candidate controllers
in simulation and evolve them using a pre-trained LLM. Unlike conventional
learning-based control techniques, which rely on black box neural networks to
encode control policies, our approach enhances transparency and
interpretability. We still take advantage of the power of large AI models, but
leverage it at the policy design phase, ensuring that all system components
remain interpretable and easily verifiable at runtime. Additionally, the use of
standard programming languages makes it straightforward for humans to finetune
or adapt the controllers based on their expertise and intuition. We illustrate
our method through its application to the synthesis of an interpretable control
policy for the pendulum swing-up and the ball in cup tasks. We make the code
available at
https://github.com/muellerlab/synthesizing_interpretable_control_policies.git"
SharpSLAM: 3D Object-Oriented Visual SLAM with Deblurring for Agile Drones,cs.RO,Robotics,2024-10-07,"The paper focuses on the algorithm for improving the quality of 3D
reconstruction and segmentation in DSP-SLAM by enhancing the RGB image quality.
SharpSLAM algorithm developed by us aims to decrease the influence of high
dynamic motion on visual object-oriented SLAM through image deblurring,
improving all aspects of object-oriented SLAM, including localization, mapping,
and object reconstruction.
  The experimental results revealed noticeable improvement in object detection
quality, with F-score increased from 82.9% to 86.2% due to the higher number of
features and corresponding map points. The RMSE of signed distance function has
also decreased from 17.2 to 15.4 cm. Furthermore, our solution has enhanced
object positioning, with an increase in the IoU from 74.5% to 75.7%. SharpSLAM
algorithm has the potential to highly improve the quality of 3D reconstruction
and segmentation in DSP-SLAM and to impact a wide range of fields, including
robotics, autonomous vehicles, and augmented reality."
Deep learning-based Visual Measurement Extraction within an Adaptive Digital Twin Framework from Limited Data Using Transfer Learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Digital Twins technology is revolutionizing decision-making in scientific
research by integrating models and simulations with real-time data. Unlike
traditional Structural Health Monitoring methods, which rely on computationally
intensive Digital Image Correlation and have limitations in real-time data
integration, this research proposes a novel approach using Artificial
Intelligence. Specifically, Convolutional Neural Networks are employed to
analyze structural behaviors in real-time by correlating Digital Image
Correlation speckle pattern images with deformation fields. Initially focusing
on two-dimensional speckle patterns, the research extends to three-dimensional
applications using stereo-paired images for comprehensive deformation analysis.
This method overcomes computational challenges by utilizing a mix of
synthetically generated and authentic speckle pattern images for training the
Convolutional Neural Networks. The models are designed to be robust and
versatile, offering a promising alternative to traditional measurement
techniques and paving the way for advanced applications in three-dimensional
modeling. This advancement signifies a shift towards more efficient and dynamic
structural health monitoring by leveraging the power of Artificial Intelligence
for real-time simulation and analysis."
Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Climate change communication on social media increasingly employs
microtargeting strategies to effectively reach and influence specific
demographic groups. This study presents a post-hoc analysis of microtargeting
practices within climate campaigns by leveraging large language models (LLMs)
to examine Facebook advertisements. Our analysis focuses on two key aspects:
demographic targeting and fairness. We evaluate the ability of LLMs to
accurately predict the intended demographic targets, such as gender and age
group, achieving an overall accuracy of 88.55%. Furthermore, we instruct the
LLMs to generate explanations for their classifications, providing transparent
reasoning behind each decision. These explanations reveal the specific thematic
elements used to engage different demographic segments, highlighting distinct
strategies tailored to various audiences. Our findings show that young adults
are primarily targeted through messages emphasizing activism and environmental
consciousness, while women are engaged through themes related to caregiving
roles and social advocacy. In addition to evaluating the effectiveness of LLMs
in detecting microtargeted messaging, we conduct a comprehensive fairness
analysis to identify potential biases in model predictions. Our findings
indicate that while LLMs perform well overall, certain biases exist,
particularly in the classification of senior citizens and male audiences. By
showcasing the efficacy of LLMs in dissecting and explaining targeted
communication strategies and by highlighting fairness concerns, this study
provides a valuable framework for future research aimed at enhancing
transparency, accountability, and inclusivity in social media-driven climate
campaigns."
Fine-Tuning CLIP's Last Visual Projector: A Few-Shot Cornucopia,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"We consider the problem of adapting a contrastively pretrained
vision-language model like CLIP (Radford et al., 2021) for few-shot
classification. The existing literature addresses this problem by learning a
linear classifier of the frozen visual features, optimizing word embeddings, or
learning external feature adapters. This paper introduces an alternative way
for CLIP adaptation without adding 'external' parameters to optimize. We find
that simply fine-tuning the last projection matrix of the vision encoder leads
to strong performance compared to the existing baselines. Furthermore, we show
that regularizing training with the distance between the fine-tuned and
pretrained matrices adds reliability for adapting CLIP through this layer.
Perhaps surprisingly, this approach, coined ProLIP, yields performances on par
or better than state of the art on 11 few-shot classification benchmarks,
few-shot domain generalization, cross-dataset transfer and test-time
adaptation. Code will be made available at
https://github.com/astra-vision/ProLIP ."
Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Data is a crucial element in large language model (LLM) alignment. Recent
studies have explored using LLMs for efficient data collection. However,
LLM-generated data often suffers from quality issues, with underrepresented or
absent aspects and low-quality datapoints. To address these problems, we
propose Data Advisor, an enhanced LLM-based method for generating data that
takes into account the characteristics of the desired dataset. Starting from a
set of pre-defined principles in hand, Data Advisor monitors the status of the
generated data, identifies weaknesses in the current dataset, and advises the
next iteration of data generation accordingly. Data Advisor can be easily
integrated into existing data generation methods to enhance data quality and
coverage. Experiments on safety alignment of three representative LLMs (i.e.,
Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in
enhancing model safety against various fine-grained safety issues without
sacrificing model utility."
Grounding Partially-Defined Events in Multimodal Data,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"How are we able to learn about complex current events just from short
snippets of video? While natural language enables straightforward ways to
represent under-specified, partially observable events, visual data does not
facilitate analogous methods and, consequently, introduces unique challenges in
event understanding. With the growing prevalence of vision-capable AI agents,
these systems must be able to model events from collections of unstructured
video data. To tackle robust event modeling in multimodal settings, we
introduce a multimodal formulation for partially-defined events and cast the
extraction of these events as a three-stage span retrieval task. We propose a
corresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours
of densely annotated current event videos and 1,168 text documents, containing
22.8K labeled event-centric entities. We propose a collection of LLM-driven
approaches to the task of multimodal event analysis, and evaluate them on
MultiVENT-G. Results illustrate the challenges that abstract event
understanding poses and demonstrates promise in event-centric video-language
systems."
Brain Mapping with Dense Features: Grounding Cortical Semantic Selectivity in Natural Images With Vision Transformers,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Advances in large-scale artificial neural networks have facilitated novel
insights into the functional topology of the brain. Here, we leverage this
approach to study how semantic categories are organized in the human visual
cortex. To overcome the challenge presented by the co-occurrence of multiple
categories in natural images, we introduce BrainSAIL (Semantic Attribution and
Image Localization), a method for isolating specific neurally-activating visual
concepts in images. BrainSAIL exploits semantically consistent, dense spatial
features from pre-trained vision models, building upon their demonstrated
ability to robustly predict neural activity. This method derives clean,
spatially dense embeddings without requiring any additional training, and
employs a novel denoising process that leverages the semantic consistency of
images under random augmentations. By unifying the space of whole-image
embeddings and dense visual features and then applying voxel-wise encoding
models to these features, we enable the identification of specific subregions
of each image which drive selectivity patterns in different areas of the higher
visual cortex. We validate BrainSAIL on cortical regions with known category
selectivity, demonstrating its ability to accurately localize and disentangle
selectivity to diverse visual concepts. Next, we demonstrate BrainSAIL's
ability to characterize high-level visual selectivity to scene properties and
low-level visual features such as depth, luminance, and saturation, providing
insights into the encoding of complex visual information. Finally, we use
BrainSAIL to directly compare the feature selectivity of different brain
encoding models across different regions of interest in visual cortex. Our
innovative method paves the way for significant advances in mapping and
decomposing high-level visual representations in the human brain."
PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs,cs.LG,Machine Learning,2024-10-07,"Quantization is essential for deploying Large Language Models (LLMs) by
enhancing memory efficiency and inference speed. Existing methods for
activation quantization mainly address channel-wise outliers, often neglecting
token-wise outliers, leading to reliance on costly per-token dynamic
quantization. To address this, we introduce PrefixQuant, a novel technique that
isolates outlier tokens offline without re-training. Specifically, PrefixQuant
identifies high-frequency outlier tokens and prefixes them in the KV cache,
preventing the generation of outlier tokens during inference and simplifying
quantization. To our knowledge, PrefixQuant is the first to enable efficient
per-tensor static quantization to outperform expensive per-token dynamic
quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and
4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization
achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5
common-sense reasoning tasks, outperforming previous per-token dynamic
quantization methods like QuaRot with 0.98 perplexity improvement and +5.98
points accuracy. Additionally, the inference speed of W4A4 quantized models
using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot
models by 1.2x to 1.3x. Our code is available at
\url{https://github.com/ChenMnZ/PrefixQuant}."
TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"As the application of Large Language Models (LLMs) expands, the demand for
reliable evaluations increases. Existing LLM evaluation benchmarks primarily
rely on static datasets, making it challenging to assess model performance in
dynamic interactions with users. Moreover, these benchmarks often depend on
specific background knowledge, complicating the measurement of a model's
logical reasoning capabilities. Other dynamic evaluation methods based on
strong models or manual efforts may introduce biases and incur high costs and
time demands, hindering large-scale application. To address these issues, we
propose TurtleBench. TurtleBench collects real user guesses from our online
Turtle Soup Puzzle platform that we developed. This approach allows for the
relatively dynamic generation of evaluation datasets, mitigating the risk of
model cheating while aligning assessments more closely with genuine user needs
for reasoning capabilities, thus enhancing the reliability of evaluations.
TurtleBench includes 1,532 user guesses along with the correctness of guesses
after annotation. Using this dataset, we thoroughly evaluated nine of the most
advanced LLMs available today. Notably, the OpenAI o1 series models did not
achieve leading results in these evaluations. We propose several hypotheses for
further research, such as ""the latent reasoning of o1 utilizes trivial
Chain-of-Thought (CoT) techniques"" and ""increasing CoT length not only provides
reasoning benefits but also incurs noise costs."""
TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and Grounding with 16x Fewer Tokens,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Reading dense text and locating objects within images are fundamental
abilities for Large Vision-Language Models (LVLMs) tasked with advanced jobs.
Previous LVLMs, including superior proprietary models like GPT-4o, have
struggled to excel in both tasks simultaneously. Moreover, previous LVLMs with
fine-grained perception cost thousands of tokens per image, making them
resource-intensive. We present TextHawk2, a bilingual LVLM featuring efficient
fine-grained perception and demonstrating cutting-edge performance across
general-purpose, OCR, and grounding tasks with 16 times fewer image tokens.
Critical improvements include: (1) Token Compression: Building on the efficient
architecture of its predecessor, TextHawk2 significantly reduces the number of
tokens per image by 16 times, facilitating training and deployment of the
TextHawk series with minimal resources. (2) Visual Encoder Reinforcement: We
enhance the visual encoder through LVLM co-training, unlocking its potential
for previously unseen tasks like Chinese OCR and grounding. (3) Data Diversity:
We maintain a comparable scale of 100 million samples while diversifying the
sources of pre-training data. We assess TextHawk2 across multiple benchmarks,
where it consistently delivers superior performance and outperforms
closed-source models of similar scale, such as achieving 78.4% accuracy on
OCRBench, 81.4% accuracy on ChartQA, 89.6% ANLS on DocVQA, and 88.1%
accuracy@0.5 on RefCOCOg-test."
DART: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Text-conditioned human motion generation, which allows for user interaction
through natural language, has become increasingly popular. Existing methods
typically generate short, isolated motions based on a single input sentence.
However, human motions are continuous and can extend over long periods,
carrying rich semantics. Creating long, complex motions that precisely respond
to streams of text descriptions, particularly in an online and real-time
setting, remains a significant challenge. Furthermore, incorporating spatial
constraints into text-conditioned motion generation presents additional
challenges, as it requires aligning the motion semantics specified by text
descriptions with geometric information, such as goal locations and 3D scene
geometry. To address these limitations, we propose DART, a Diffusion-based
Autoregressive motion primitive model for Real-time Text-driven motion control.
Our model, DART, effectively learns a compact motion primitive space jointly
conditioned on motion history and text inputs using latent diffusion models. By
autoregressively generating motion primitives based on the preceding history
and current text input, DART enables real-time, sequential motion generation
driven by natural language descriptions. Additionally, the learned motion
primitive space allows for precise spatial motion control, which we formulate
either as a latent noise optimization problem or as a Markov decision process
addressed through reinforcement learning. We present effective algorithms for
both approaches, demonstrating our model's versatility and superior performance
in various motion synthesis tasks. Experiments show our method outperforms
existing baselines in motion realism, efficiency, and controllability. Video
results are available on the project page: https://zkf1997.github.io/DART/."
GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Diffusion-based 2D virtual try-on (VTON) techniques have recently
demonstrated strong performance, while the development of 3D VTON has largely
lagged behind. Despite recent advances in text-guided 3D scene editing,
integrating 2D VTON into these pipelines to achieve vivid 3D VTON remains
challenging. The reasons are twofold. First, text prompts cannot provide
sufficient details in describing clothing. Second, 2D VTON results generated
from different viewpoints of the same 3D scene lack coherence and spatial
relationships, hence frequently leading to appearance inconsistencies and
geometric distortions. To resolve these problems, we introduce an
image-prompted 3D VTON method (dubbed GS-VTON) which, by leveraging 3D Gaussian
Splatting (3DGS) as the 3D representation, enables the transfer of pre-trained
knowledge from 2D VTON models to 3D while improving cross-view consistency. (1)
Specifically, we propose a personalized diffusion model that utilizes low-rank
adaptation (LoRA) fine-tuning to incorporate personalized information into
pre-trained 2D VTON models. To achieve effective LoRA training, we introduce a
reference-driven image editing approach that enables the simultaneous editing
of multi-view images while ensuring consistency. (2) Furthermore, we propose a
persona-aware 3DGS editing framework to facilitate effective editing while
maintaining consistent cross-view appearance and high-quality 3D geometry. (3)
Additionally, we have established a new 3D VTON benchmark, 3D-VTONBench, which
facilitates comprehensive qualitative and quantitative 3D VTON evaluations.
Through extensive experiments and comparative analyses with existing methods,
the proposed \OM has demonstrated superior fidelity and advanced editing
capabilities, affirming its effectiveness for 3D VTON."
Differential Transformer,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Transformer tends to overallocate attention to irrelevant context. In this
work, we introduce Diff Transformer, which amplifies attention to the relevant
context while canceling noise. Specifically, the differential attention
mechanism calculates attention scores as the difference between two separate
softmax attention maps. The subtraction cancels noise, promoting the emergence
of sparse attention patterns. Experimental results on language modeling show
that Diff Transformer outperforms Transformer in various settings of scaling up
model size and training tokens. More intriguingly, it offers notable advantages
in practical applications, such as long-context modeling, key information
retrieval, hallucination mitigation, in-context learning, and reduction of
activation outliers. By being less distracted by irrelevant context, Diff
Transformer can mitigate hallucination in question answering and text
summarization. For in-context learning, Diff Transformer not only enhances
accuracy but is also more robust to order permutation, which was considered as
a chronic robustness issue. The results position Diff Transformer as a highly
effective and promising architecture to advance large language models."
Proprioceptive State Estimation for Quadruped Robots using Invariant Kalman Filtering and Scale-Variant Robust Cost Functions,cs.RO,Robotics,2024-10-07,"Accurate state estimation is crucial for legged robot locomotion, as it
provides the necessary information to allow control and navigation. However, it
is also challenging, especially in scenarios with uneven and slippery terrain.
This paper presents a new Invariant Extended Kalman filter for legged robot
state estimation using only proprioceptive sensors. We formulate the
methodology by combining recent advances in state estimation theory with the
use of robust cost functions in the measurement update. We tested our
methodology on quadruped robots through experiments and public datasets,
showing that we can obtain a pose drift up to 40% lower in trajectories
covering a distance of over 450m, in comparison with a state-of-the-art
Invariant Extended Kalman filter."
SePPO: Semi-Policy Preference Optimization for Diffusion Alignment,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Reinforcement learning from human feedback (RLHF) methods are emerging as a
way to fine-tune diffusion models (DMs) for visual generation. However,
commonly used on-policy strategies are limited by the generalization capability
of the reward model, while off-policy approaches require large amounts of
difficult-to-obtain paired human-annotated data, particularly in visual
generation tasks. To address the limitations of both on- and off-policy RLHF,
we propose a preference optimization method that aligns DMs with preferences
without relying on reward models or paired human-annotated data. Specifically,
we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO
leverages previous checkpoints as reference models while using them to generate
on-policy reference samples, which replace ""losing images"" in preference pairs.
This approach allows us to optimize using only off-policy ""winning images.""
Furthermore, we design a strategy for reference model selection that expands
the exploration in the policy space. Notably, we do not simply treat reference
samples as negative examples for learning. Instead, we design an anchor-based
criterion to assess whether the reference samples are likely to be winning or
losing images, allowing the model to selectively learn from the generated
reference samples. This approach mitigates performance degradation caused by
the uncertainty in reference sample quality. We validate SePPO across both
text-to-image and text-to-video benchmarks. SePPO surpasses all previous
approaches on the text-to-image benchmarks and also demonstrates outstanding
performance on the text-to-video benchmarks. Code will be released in
https://github.com/DwanZhang-AI/SePPO."
Diffusion Model Predictive Control,cs.LG,Machine Learning,2024-10-07,"We propose Diffusion Model Predictive Control (D-MPC), a novel MPC approach
that learns a multi-step action proposal and a multi-step dynamics model, both
using diffusion models, and combines them for use in online MPC. On the popular
D4RL benchmark, we show performance that is significantly better than existing
model-based offline planning methods using MPC and competitive with
state-of-the-art (SOTA) model-based and model-free reinforcement learning
methods. We additionally illustrate D-MPC's ability to optimize novel reward
functions at run time and adapt to novel dynamics, and highlight its advantages
compared to existing diffusion-based planning baselines."
Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Text-to-video (T2V) models like Sora have made significant strides in
visualizing complex prompts, which is increasingly viewed as a promising path
towards constructing the universal world simulator. Cognitive psychologists
believe that the foundation for achieving this goal is the ability to
understand intuitive physics. However, the capacity of these models to
accurately represent intuitive physics remains largely unexplored. To bridge
this gap, we introduce PhyGenBench, a comprehensive \textbf{Phy}sics
\textbf{Gen}eration \textbf{Ben}chmark designed to evaluate physical
commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully
crafted prompts across 27 distinct physical laws, spanning four fundamental
domains, which could comprehensively assesses models' understanding of physical
commonsense. Alongside PhyGenBench, we propose a novel evaluation framework
called PhyGenEval. This framework employs a hierarchical evaluation structure
utilizing appropriate advanced vision-language models and large language models
to assess physical commonsense. Through PhyGenBench and PhyGenEval, we can
conduct large-scale automated assessments of T2V models' understanding of
physical commonsense, which align closely with human feedback. Our evaluation
results and in-depth analysis demonstrate that current models struggle to
generate videos that comply with physical commonsense. Moreover, simply scaling
up models or employing prompt engineering techniques is insufficient to fully
address the challenges presented by PhyGenBench (e.g., dynamic scenarios). We
hope this study will inspire the community to prioritize the learning of
physical commonsense in these models beyond entertainment applications. We will
release the data and codes at https://github.com/OpenGVLab/PhyGenBench"
GLEE: A Unified Framework and Benchmark for Language-based Economic Environments,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Large Language Models (LLMs) show significant potential in economic and
strategic interactions, where communication via natural language is often
prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic
human behavior? Do they tend to reach an efficient and fair outcome? What is
the role of natural language in the strategic interaction? How do
characteristics of the economic environment influence these dynamics? These
questions become crucial concerning the economic and societal implications of
integrating LLM-based agents into real-world data-driven systems, such as
online retail platforms and recommender systems. While the ML community has
been exploring the potential of LLMs in such multi-agent setups, varying
assumptions, design choices and evaluation criteria across studies make it
difficult to draw robust and meaningful conclusions. To address this, we
introduce a benchmark for standardizing research on two-player, sequential,
language-based games. Inspired by the economic literature, we define three base
families of games with consistent parameterization, degrees of freedom and
economic measures to evaluate agents' performance (self-gain), as well as the
game outcome (efficiency and fairness). We develop an open-source framework for
interaction simulation and analysis, and utilize it to collect a dataset of LLM
vs. LLM interactions across numerous game configurations and an additional
dataset of human vs. LLM interactions. Through extensive experimentation, we
demonstrate how our framework and dataset can be used to: (i) compare the
behavior of LLM-based agents to human players in various economic contexts;
(ii) evaluate agents in both individual and collective performance measures;
and (iii) quantify the effect of the economic characteristics of the
environments on the behavior of agents."
Causal Micro-Narratives,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"We present a novel approach to classify causal micro-narratives from text.
These narratives are sentence-level explanations of the cause(s) and/or
effect(s) of a target subject. The approach requires only a subject-specific
ontology of causes and effects, and we demonstrate it with an application to
inflation narratives. Using a human-annotated dataset spanning historical and
contemporary US news articles for training, we evaluate several large language
models (LLMs) on this multi-label classification task. The best-performing
model--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative
detection and 0.71 on narrative classification. Comprehensive error analysis
reveals challenges arising from linguistic ambiguity and highlights how model
errors often mirror human annotator disagreements. This research establishes a
framework for extracting causal micro-narratives from real-world data, with
wide-ranging applications to social science research."
LoTLIP: Improving Language-Image Pre-training for Long Text Understanding,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Understanding long text is of great demands in practice but beyond the reach
of most language-image pre-training (LIP) models. In this work, we empirically
confirm that the key reason causing such an issue is that the training images
are usually paired with short captions, leaving certain tokens easily
overshadowed by salient tokens. Towards this problem, our initial attempt is to
relabel the data with long captions, however, directly learning with which may
lead to performance degradation in understanding short text (e.g., in the image
classification task). Then, after incorporating corner tokens to aggregate
diverse textual information, we manage to help the model catch up to its
original level of short text understanding yet greatly enhance its capability
of long text understanding. We further look into whether the model can
continuously benefit from longer captions and notice a clear trade-off between
the performance and the efficiency. Finally, we validate the effectiveness of
our approach using a self-constructed large-scale dataset, which consists of
100M long caption oriented text-image pairs. It is noteworthy that, on the task
of long-text image retrieval, we beat the competitor using long captions with
11.1% improvement (i.e., from 72.62% to 83.72%). We will release the code, the
model, and the new dataset to facilitate the reproducibility and further
research. The project page is available at https://wuw2019.github.io/lot-lip."
SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"To induce desired behaviors in large language models (LLMs) for
interaction-driven tasks, the instruction-tuning stage typically trains LLMs on
instruction-response pairs using the next-token prediction (NTP) loss. Previous
work aiming to improve instruction-tuning performance often emphasizes the need
for higher-quality supervised fine-tuning (SFT) datasets, which typically
involves expensive data filtering with proprietary LLMs or labor-intensive data
generation by human annotators. However, these approaches do not fully leverage
the datasets' intrinsic properties, resulting in high computational and labor
costs, thereby limiting scalability and performance gains. In this paper, we
propose SFTMix, a novel recipe that elevates instruction-tuning performance
beyond the conventional NTP paradigm, without the need for well-curated
datasets. Observing that LLMs exhibit uneven confidence across the semantic
representation space, we argue that examples with different confidence levels
should play distinct roles during the instruction-tuning process. Based on this
insight, SFTMix leverages training dynamics to identify examples with varying
confidence levels, then applies a Mixup-based regularization to mitigate
overfitting on confident examples while propagating supervision signals to
improve learning on relatively unconfident ones. This approach enables SFTMix
to significantly outperform NTP across a wide range of instruction-following
and healthcare domain-specific SFT tasks, demonstrating its adaptability to
diverse LLM families and scalability to datasets of any size. Comprehensive
ablation studies further verify the robustness of SFTMix's design choices,
underscoring its versatility in consistently enhancing performance across
different LLMs and datasets in broader natural language processing
applications."
Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents,cs.AI,Artificial Intelligence,2024-10-07,"Multimodal large language models (MLLMs) are transforming the capabilities of
graphical user interface (GUI) agents, facilitating their transition from
controlled simulations to complex, real-world applications across various
platforms. However, the effectiveness of these agents hinges on the robustness
of their grounding capability. Current GUI agents predominantly utilize
text-based representations such as HTML or accessibility trees, which, despite
their utility, often introduce noise, incompleteness, and increased
computational overhead. In this paper, we advocate a human-like embodiment for
GUI agents that perceive the environment entirely visually and directly take
pixel-level operations on the GUI. The key is visual grounding models that can
accurately map diverse referring expressions of GUI elements to their
coordinates on the GUI across different platforms. We show that a simple
recipe, which includes web-based synthetic data and slight adaptation of the
LLaVA architecture, is surprisingly effective for training such visual
grounding models. We collect the largest dataset for GUI visual grounding so
far, containing 10M GUI elements and their referring expressions over 1.3M
screenshots, and use it to train UGround, a strong universal visual grounding
model for GUI agents. Empirical results on six benchmarks spanning three
categories (grounding, offline agent, and online agent) show that 1) UGround
substantially outperforms existing visual grounding models for GUI agents, by
up to 20% absolute, and 2) agents with UGround outperform state-of-the-art
agents, despite the fact that existing agents use additional text-based input
while ours only uses visual perception. These results provide strong support
for the feasibility and promises of GUI agents that navigate the digital world
as humans do."
LLMs Are In-Context Reinforcement Learners,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Large Language Models (LLMs) can learn new tasks through in-context
supervised learning (i.e., ICL). This work studies if this ability extends to
in-context reinforcement learning (ICRL), where models are not given gold
labels in context, but only their past predictions and rewards. We show that a
naive application of ICRL fails miserably, and identify the root cause as a
fundamental deficiency at exploration, which leads to quick model degeneration.
We propose an algorithm to address this deficiency by increasing test-time
compute, as well as a compute-bound approximation. We use several challenging
classification tasks to empirically show that our ICRL algorithms lead to
effective learning from rewards alone, and analyze the characteristics of this
ability and our methods. Overall, our results reveal remarkable ICRL abilities
in LLMs."
TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Vision-Language Models (VLMs) have shown impressive performance in vision
tasks, but adapting them to new domains often requires expensive fine-tuning.
Prompt tuning techniques, including textual, visual, and multimodal prompting,
offer efficient alternatives by leveraging learnable prompts. However, their
application to Vision-Language Segmentation Models (VLSMs) and evaluation under
significant domain shifts remain unexplored. This work presents an open-source
benchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal
prompt tuning techniques into VLSMs, making prompt tuning usable for downstream
segmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt
tuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$
different combinations. We test various prompt tuning on $8$ diverse medical
datasets, including $3$ radiology datasets (breast tumor, echocardiograph,
chest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin
cancer), and two natural domain segmentation datasets. Our study found that
textual prompt tuning struggles under significant domain shifts, from
natural-domain images to medical data. Furthermore, visual prompt tuning, with
fewer hyperparameters than multimodal prompt tuning, often achieves performance
competitive to multimodal approaches, making it a valuable first attempt. Our
work advances the understanding and applicability of different prompt-tuning
techniques for robust domain-specific segmentation. The source code is
available at https://github.com/naamiinepal/tunevlseg."
CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Explaining Artificial Intelligence (AI) decisions is a major challenge
nowadays in AI, in particular when applied to sensitive scenarios like medicine
and law. However, the need to explain the rationale behind decisions is a main
issue also for human-based deliberation as it is important to justify
\textit{why} a certain decision has been taken. Resident medical doctors for
instance are required not only to provide a (possibly correct) diagnosis, but
also to explain how they reached a certain conclusion. Developing new tools to
aid residents to train their explanation skills is therefore a central
objective of AI in education. In this paper, we follow this direction, and we
present, to the best of our knowledge, the first multilingual dataset for
Medical Question Answering where correct and incorrect diagnoses for a clinical
case are enriched with a natural language explanation written by doctors. These
explanations have been manually annotated with argument components (i.e.,
premise, claim) and argument relations (i.e., attack, support), resulting in
the Multilingual CasiMedicos-Arg dataset which consists of 558 clinical cases
in four languages (English, Spanish, French, Italian) with explanations, where
we annotated 5021 claims, 2313 premises, 2431 support relations, and 1106
attack relations. We conclude by showing how competitive baselines perform over
this challenging dataset for the argument mining task."
DiffuseReg: Denoising Diffusion Model for Obtaining Deformation Fields in Unsupervised Deformable Image Registration,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Deformable image registration aims to precisely align medical images from
different modalities or times. Traditional deep learning methods, while
effective, often lack interpretability, real-time observability and adjustment
capacity during registration inference. Denoising diffusion models present an
alternative by reformulating registration as iterative image denoising.
However, existing diffusion registration approaches do not fully harness
capabilities, neglecting the critical sampling phase that enables continuous
observability during the inference. Hence, we introduce DiffuseReg, an
innovative diffusion-based method that denoises deformation fields instead of
images for improved transparency. We also propose a novel denoising network
upon Swin Transformer, which better integrates moving and fixed images with
diffusion time step throughout the denoising process. Furthermore, we enhance
control over the denoising registration process with a novel similarity
consistency regularization. Experiments on ACDC datasets demonstrate DiffuseReg
outperforms existing diffusion registration methods by 1.32 in Dice score. The
sampling process in DiffuseReg enables real-time output observability and
adjustment unmatched by previous deep models."
SimO Loss: Anchor-Free Contrastive Loss for Fine-Grained Supervised Contrastive Learning,cs.LG,Machine Learning,2024-10-07,"We introduce a novel anchor-free contrastive learning (AFCL) method
leveraging our proposed Similarity-Orthogonality (SimO) loss. Our approach
minimizes a semi-metric discriminative loss function that simultaneously
optimizes two key objectives: reducing the distance and orthogonality between
embeddings of similar inputs while maximizing these metrics for dissimilar
inputs, facilitating more fine-grained contrastive learning. The AFCL method,
powered by SimO loss, creates a fiber bundle topological structure in the
embedding space, forming class-specific, internally cohesive yet orthogonal
neighborhoods. We validate the efficacy of our method on the CIFAR-10 dataset,
providing visualizations that demonstrate the impact of SimO loss on the
embedding space. Our results illustrate the formation of distinct, orthogonal
class neighborhoods, showcasing the method's ability to create well-structured
embeddings that balance class separation with intra-class variability. This
work opens new avenues for understanding and leveraging the geometric
properties of learned representations in various machine learning tasks."
SymmetryLens: A new candidate paradigm for unsupervised symmetry learning via locality and equivariance,cs.LG,Machine Learning,2024-10-07,"We develop a new, unsupervised symmetry learning method that starts with raw
data, and gives the minimal (discrete) generator of an underlying Lie group of
symmetries, together with a symmetry equivariant representation of the data.
The method is able to learn the pixel translation operator from a dataset with
only an approximate translation symmetry, and can learn quite different types
of symmetries which are not apparent to the naked eye, equally well. The method
is based on the formulation of an information-theoretic loss function that
measures both the degree to which the dataset is symmetric under a given
candidate symmetry, and also, the degree of locality of the samples in the
dataset with respect to this symmetry. We demonstrate that this coupling
between symmetry and locality, together with a special optimization technique
developed for entropy estimation, results in a highly stable system that gives
reproducible results. The symmetry actions we consider are group
representations, however, we believe the approach has the potential to be
generalized to more general, nonlinear actions of non-commutative Lie groups."
GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models,cs.LG,Machine Learning,2024-10-07,"Recent advancements in Large Language Models (LLMs) have sparked interest in
their formal reasoning capabilities, particularly in mathematics. The GSM8K
benchmark is widely used to assess the mathematical reasoning of models on
grade-school-level questions. While the performance of LLMs on GSM8K has
significantly improved in recent years, it remains unclear whether their
mathematical reasoning capabilities have genuinely advanced, raising questions
about the reliability of the reported metrics. To address these concerns, we
conduct a large-scale study on several SOTA open and closed models. To overcome
the limitations of existing evaluations, we introduce GSM-Symbolic, an improved
benchmark created from symbolic templates that allow for the generation of a
diverse set of questions. GSM-Symbolic enables more controllable evaluations,
providing key insights and more reliable metrics for measuring the reasoning
capabilities of models.Our findings reveal that LLMs exhibit noticeable
variance when responding to different instantiations of the same question.
Specifically, the performance of all models declines when only the numerical
values in the question are altered in the GSM-Symbolic benchmark. Furthermore,
we investigate the fragility of mathematical reasoning in these models and show
that their performance significantly deteriorates as the number of clauses in a
question increases. We hypothesize that this decline is because current LLMs
cannot perform genuine logical reasoning; they replicate reasoning steps from
their training data. Adding a single clause that seems relevant to the question
causes significant performance drops (up to 65%) across all state-of-the-art
models, even though the clause doesn't contribute to the reasoning chain needed
for the final answer. Overall, our work offers a more nuanced understanding of
LLMs' capabilities and limitations in mathematical reasoning."
The Dawn of Video Generation: Preliminary Explorations with SORA-like Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"High-quality video generation, encompassing text-to-video (T2V),
image-to-video (I2V), and video-to-video (V2V) generation, holds considerable
significance in content creation to benefit anyone express their inherent
creativity in new ways and world simulation to modeling and understanding the
world. Models like SORA have advanced generating videos with higher resolution,
more natural motion, better vision-language alignment, and increased
controllability, particularly for long video sequences. These improvements have
been driven by the evolution of model architectures, shifting from UNet to more
scalable and parameter-rich DiT models, along with large-scale data expansion
and refined training strategies. However, despite the emergence of DiT-based
closed-source and open-source models, a comprehensive investigation into their
capabilities and limitations remains lacking. Furthermore, the rapid
development has made it challenging for recent benchmarks to fully cover
SORA-like models and recognize their significant advancements. Additionally,
evaluation metrics often fail to align with human preferences."
ETGL-DDPG: A Deep Deterministic Policy Gradient Algorithm for Sparse Reward Continuous Control,cs.LG,Machine Learning,2024-10-07,"We consider deep deterministic policy gradient (DDPG) in the context of
reinforcement learning with sparse rewards. To enhance exploration, we
introduce a search procedure, \emph{${\epsilon}{t}$-greedy}, which generates
exploratory options for exploring less-visited states. We prove that search
using $\epsilon t$-greedy has polynomial sample complexity under mild MDP
assumptions. To more efficiently use the information provided by rewarded
transitions, we develop a new dual experience replay buffer framework,
\emph{GDRB}, and implement \emph{longest n-step returns}. The resulting
algorithm, \emph{ETGL-DDPG}, integrates all three techniques: \bm{$\epsilon
t$}-greedy, \textbf{G}DRB, and \textbf{L}ongest $n$-step, into DDPG. We
evaluate ETGL-DDPG on standard benchmarks and demonstrate that it outperforms
DDPG, as well as other state-of-the-art methods, across all tested
sparse-reward continuous environments. Ablation studies further highlight how
each strategy individually enhances the performance of DDPG in this setting."
Cookbook: A framework for improving LLM generative abilities via programmatic data generating templates,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Fine-tuning large language models (LLMs) on instruction datasets is a common
way to improve their generative capabilities. However, instruction datasets can
be expensive and time-consuming to manually curate, and while LLM-generated
data is less labor-intensive, it may violate user privacy agreements or terms
of service of LLM providers. Therefore, we seek a way of constructing
instruction datasets with samples that are not generated by humans or LLMs but
still improve LLM generative capabilities. In this work, we introduce Cookbook,
a framework that programmatically generates training data consisting of simple
patterns over random tokens, resulting in a scalable, cost-effective approach
that avoids legal and privacy issues. First, Cookbook uses a template -- a data
generating Python function -- to produce training data that encourages the
model to learn an explicit pattern-based rule that corresponds to a desired
task. We find that fine-tuning on Cookbook-generated data is able to improve
performance on its corresponding task by up to 52.7 accuracy points. Second,
since instruction datasets improve performance on multiple downstream tasks
simultaneously, Cookbook algorithmically learns how to mix data from various
templates to optimize performance on multiple tasks. On the standard multi-task
GPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated
dataset attains the best accuracy on average compared to other 7B parameter
instruction-tuned models and is the best performing model on 3 out of 8 tasks.
Finally, we analyze when and why Cookbook improves performance and present a
metric that allows us to verify that the improvement is largely explained by
the model's generations adhering better to template rules."
Precise Model Benchmarking with Only a Few Observations,cs.LG,Machine Learning,2024-10-07,"How can we precisely estimate a large language model's (LLM) accuracy on
questions belonging to a specific topic within a larger question-answering
dataset? The standard direct estimator, which averages the model's accuracy on
the questions in each subgroup, may exhibit high variance for subgroups
(topics) with small sample sizes. Synthetic regression modeling, which
leverages the model's accuracy on questions about other topics, may yield
biased estimates that are too unreliable for large subgroups. We prescribe a
simple yet effective solution: an empirical Bayes (EB) estimator that balances
direct and regression estimates for each subgroup separately, improving the
precision of subgroup-level estimates of model performance. Our experiments on
multiple datasets show that this approach consistently provides more precise
estimates of the LLM performance compared to the direct and regression
approaches, achieving substantial reductions in the mean squared error.
Confidence intervals for EB estimates also have near-nominal coverage and are
narrower compared to those for the direct estimator. Additional experiments on
tabular and vision data validate the benefits of this EB approach."
Density estimation with LLMs: a geometric investigation of in-context learning trajectories,cs.LG,Machine Learning,2024-10-07,"Large language models (LLMs) demonstrate remarkable emergent abilities to
perform in-context learning across various tasks, including time series
forecasting. This work investigates LLMs' ability to estimate probability
density functions (PDFs) from data observed in-context; such density estimation
(DE) is a fundamental task underlying many probabilistic modeling problems. We
leverage the Intensive Principal Component Analysis (InPCA) to visualize and
analyze the in-context learning dynamics of LLaMA-2 models. Our main finding is
that these LLMs all follow similar learning trajectories in a low-dimensional
InPCA space, which are distinct from those of traditional density estimation
methods like histograms and Gaussian kernel density estimation (KDE). We
interpret the LLaMA in-context DE process as a KDE with an adaptive kernel
width and shape. This custom kernel model captures a significant portion of
LLaMA's behavior despite having only two parameters. We further speculate on
why LLaMA's kernel width and shape differs from classical algorithms, providing
insights into the mechanism of in-context probabilistic reasoning in LLMs."
Organizing Unstructured Image Collections using Natural Language,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Organizing unstructured visual data into semantic clusters is a key challenge
in computer vision. Traditional deep clustering (DC) approaches focus on a
single partition of data, while multiple clustering (MC) methods address this
limitation by uncovering distinct clustering solutions. The rise of large
language models (LLMs) and multimodal LLMs (MLLMs) has enhanced MC by allowing
users to define clustering criteria in natural language. However, manually
specifying criteria for large datasets is impractical. In this work, we
introduce the task Semantic Multiple Clustering (SMC) that aims to
automatically discover clustering criteria from large image collections,
uncovering interpretable substructures without requiring human input. Our
framework, Text Driven Semantic Multiple Clustering (TeDeSC), uses text as a
proxy to concurrently reason over large image collections, discover
partitioning criteria, expressed in natural language, and reveal semantic
substructures. To evaluate TeDeSC, we introduce the COCO-4c and Food-4c
benchmarks, each containing four grouping criteria and ground-truth
annotations. We apply TeDeSC to various applications, such as discovering
biases and analyzing social media image popularity, demonstrating its utility
as a tool for automatically organizing image collections and revealing novel
insights."
Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"In this paper, we propose a new method to enhance compositional understanding
in pre-trained vision and language models (VLMs) without sacrificing
performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches
often improve compositional reasoning at the cost of degrading multi-modal
capabilities, primarily due to the use of global hard negative (HN) loss, which
contrasts global representations of images and texts. This global HN loss
pushes HN texts that are highly similar to the original ones, damaging the
model's multi-modal representations. To overcome this limitation, we propose
Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard
negative loss and selective calibrated regularization. These innovations
provide fine-grained negative supervision while preserving the model's
representational integrity. Our extensive evaluations across diverse benchmarks
for both compositionality and multi-modal tasks show that FSC-CLIP not only
achieves compositionality on par with state-of-the-art models but also retains
strong multi-modal capabilities. Code is available at:
https://github.com/ytaek-oh/fsc-clip."
Studying and Mitigating Biases in Sign Language Understanding Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Ensuring that the benefits of sign language technologies are distributed
equitably among all community members is crucial. Thus, it is important to
address potential biases and inequities that may arise from the design or use
of these resources. Crowd-sourced sign language datasets, such as the ASL
Citizen dataset, are great resources for improving accessibility and preserving
linguistic diversity, but they must be used thoughtfully to avoid reinforcing
existing biases.
  In this work, we utilize the rich information about participant demographics
and lexical features present in the ASL Citizen dataset to study and document
the biases that may result from models trained on crowd-sourced sign datasets.
Further, we apply several bias mitigation techniques during model training, and
find that these techniques reduce performance disparities without decreasing
accuracy. With the publication of this work, we release the demographic
information about the participants in the ASL Citizen dataset to encourage
future bias mitigation work in this space."
Beyond FVD: Enhanced Evaluation Metrics for Video Generation Quality,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"The Fr\'echet Video Distance (FVD) is a widely adopted metric for evaluating
video generation distribution quality. However, its effectiveness relies on
critical assumptions. Our analysis reveals three significant limitations: (1)
the non-Gaussianity of the Inflated 3D Convnet (I3D) feature space; (2) the
insensitivity of I3D features to temporal distortions; (3) the impractical
sample sizes required for reliable estimation. These findings undermine FVD's
reliability and show that FVD falls short as a standalone metric for video
generation evaluation. After extensive analysis of a wide range of metrics and
backbone architectures, we propose JEDi, the JEPA Embedding Distance, based on
features derived from a Joint Embedding Predictive Architecture, measured using
Maximum Mean Discrepancy with polynomial kernel. Our experiments on multiple
open-source datasets show clear evidence that it is a superior alternative to
the widely used FVD metric, requiring only 16% of the samples to reach its
steady value, while increasing alignment with human evaluation by 34%, on
average."
RespLLM: Unifying Audio and Text with Multimodal LLMs for Generalized Respiratory Health Prediction,cs.LG,Machine Learning,2024-10-07,"The high incidence and mortality rates associated with respiratory diseases
underscores the importance of early screening. Machine learning models can
automate clinical consultations and auscultation, offering vital support in
this area. However, the data involved, spanning demographics, medical history,
symptoms, and respiratory audio, are heterogeneous and complex. Existing
approaches are insufficient and lack generalizability, as they typically rely
on limited training data, basic fusion techniques, and task-specific models. In
this paper, we propose RespLLM, a novel multimodal large language model (LLM)
framework that unifies text and audio representations for respiratory health
prediction. RespLLM leverages the extensive prior knowledge of pretrained LLMs
and enables effective audio-text fusion through cross-modal attentions.
Instruction tuning is employed to integrate diverse data from multiple sources,
ensuring generalizability and versatility of the model. Experiments on five
real-world datasets demonstrate that RespLLM outperforms leading baselines by
an average of 4.6% on trained tasks, 7.9% on unseen datasets, and facilitates
zero-shot predictions for new tasks. Our work lays the foundation for
multimodal models that can perceive, listen to, and understand heterogeneous
data, paving the way for scalable respiratory health diagnosis."
RevisEval: Improving LLM-as-a-Judge via Response-Adapted References,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"With significant efforts in recent studies, LLM-as-a-Judge has become a
cost-effective alternative to human evaluation for assessing the text
generation quality in a wide range of tasks. However, there still remains a
reliability gap between LLM-as-a-Judge and human evaluation. One important
reason is the lack of guided oracles in the evaluation process. Motivated by
the role of reference pervasively used in classic text evaluation, we introduce
RevisEval, a novel text generation evaluation paradigm via the response-adapted
references. RevisEval is driven by the key observation that an ideal reference
should maintain the necessary relevance to the response to be evaluated.
Specifically, RevisEval leverages the text revision capabilities of large
language models (LLMs) to adaptively revise the response, then treat the
revised text as the reference (response-adapted reference) for the subsequent
evaluation. Extensive experiments demonstrate that RevisEval outperforms
traditional reference-free and reference-based evaluation paradigms that use
LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.
More importantly, our response-adapted references can further boost the
classical text metrics, e.g., BLEU and BERTScore, compared to traditional
references and even rival the LLM-as-a-Judge. A detailed analysis is also
conducted to confirm RevisEval's effectiveness in bias reduction, the impact of
inference cost, and reference relevance."
Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective,cs.LG,Machine Learning,2024-10-07,"Training language models currently requires pre-determining a fixed compute
budget because the typical cosine learning rate schedule depends on the total
number of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a
constant learning rate to produce a main branch of iterates that can in
principle continue indefinitely without a pre-specified compute budget. Then,
given any compute budget, one can branch out from the main branch at a proper
at any time with a rapidly decaying learning rate to produce a strong model.
Empirically, WSD generates a non-traditional loss curve: the loss remains
elevated during the stable phase but sharply declines during the decay phase.
Towards explaining this phenomenon, we conjecture that pretraining loss
exhibits a river valley landscape, which resembles a deep valley with a river
at its bottom. Under this assumption, we show that during the stable phase, the
iterate undergoes large oscillations due to the high learning rate, yet it
progresses swiftly along the river. During the decay phase, the rapidly
dropping learning rate minimizes the iterate's oscillations, moving it closer
to the river and revealing true optimization progress. Therefore, the sustained
high learning rate phase and fast decaying phase are responsible for progress
in the river and the mountain directions respectively, and are both critical.
Our analysis predicts phenomenons consistent with empirical observations and
shows that this landscape can emerge from pretraining on a simple bi-gram
dataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that
reuses previous checkpoints' decay phases and keeps only one main branch, where
we resume from a decayed checkpoint. WSD-S empirically outperforms WSD and
Cyclic-Cosine in obtaining multiple language model checkpoints across various
compute budgets in a single run for parameters scaling from 0.1B to 1.2B."
LADEV: A Language-Driven Testing and Evaluation Platform for Vision-Language-Action Models in Robotic Manipulation,cs.RO,Robotics,2024-10-07,"Building on the advancements of Large Language Models (LLMs) and Vision
Language Models (VLMs), recent research has introduced Vision-Language-Action
(VLA) models as an integrated solution for robotic manipulation tasks. These
models take camera images and natural language task instructions as input and
directly generate control actions for robots to perform specified tasks,
greatly improving both decision-making capabilities and interaction with human
users. However, the data-driven nature of VLA models, combined with their lack
of interpretability, makes the assurance of their effectiveness and robustness
a challenging task. This highlights the need for a reliable testing and
evaluation platform. For this purpose, in this work, we propose LADEV, a
comprehensive and efficient platform specifically designed for evaluating VLA
models. We first present a language-driven approach that automatically
generates simulation environments from natural language inputs, mitigating the
need for manual adjustments and significantly improving testing efficiency.
Then, to further assess the influence of language input on the VLA models, we
implement a paraphrase mechanism that produces diverse natural language task
instructions for testing. Finally, to expedite the evaluation process, we
introduce a batch-style method for conducting large-scale testing of VLA
models. Using LADEV, we conducted experiments on several state-of-the-art VLA
models, demonstrating its effectiveness as a tool for evaluating these models.
Our results showed that LADEV not only enhances testing efficiency but also
establishes a solid baseline for evaluating VLA models, paving the way for the
development of more intelligent and advanced robotic systems."
State Estimation of Marine Vessels Affected by Waves by Unmanned Aerial Vehicles,cs.RO,Robotics,2024-10-07,"A novel approach for robust state estimation of marine vessels in rough water
is proposed in this paper to enable tight collaboration between Unmanned Aerial
Vehicles (UAVs) and a marine vessel, such as cooperative landing or object
manipulation, regardless of weather conditions. Our study of marine vessel (in
our case Unmanned Surface Vehicle (USV)) dynamics influenced by strong wave
motion has resulted in a novel nonlinear mathematical USV model with 6 degrees
of freedom (DOFs), which is required for precise USV state estimation and
motion prediction. The proposed state estimation approach fuses data from
multiple sensors onboard the UAV and the USV to enable redundancy and
robustness under varying weather conditions of real-world applications. The
proposed approach provides estimated states of the USV with 6 DOFs and predicts
its future states to enable tight control of both vehicles on a receding
control horizon. The proposed approach was extensively tested in the realistic
Gazebo simulator and successfully experimentally validated in many real-world
experiments representing different application scenarios, including agile
landing on an oscillating and moving USV. A comparative study indicates that
the proposed approach significantly surpassed the current state-of-the-art."
Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Machine Translation (MT) evaluation metrics assess translation quality
automatically. Recently, researchers have employed MT metrics for various new
use cases, such as data filtering and translation re-ranking. However, most MT
metrics return assessments as scalar scores that are difficult to interpret,
posing a challenge to making informed design choices. Moreover, MT metrics'
capabilities have historically been evaluated using correlation with human
judgment, which, despite its efficacy, falls short of providing intuitive
insights into metric performance, especially in terms of new metric use cases.
To address these issues, we introduce an interpretable evaluation framework for
MT metrics. Within this framework, we evaluate metrics in two scenarios that
serve as proxies for the data filtering and translation re-ranking use cases.
Furthermore, by measuring the performance of MT metrics using Precision,
Recall, and F-score, we offer clearer insights into their capabilities than
correlation with human judgments. Finally, we raise concerns regarding the
reliability of manually curated data following the Direct Assessments+Scalar
Quality Metrics (DA+SQM) guidelines, reporting a notably low agreement with
Multidimensional Quality Metrics (MQM) annotations."
MARs: Multi-view Attention Regularizations for Patch-based Feature Recognition of Space Terrain,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"The visual detection and tracking of surface terrain is required for
spacecraft to safely land on or navigate within close proximity to celestial
objects. Current approaches rely on template matching with pre-gathered
patch-based features, which are expensive to obtain and a limiting factor in
perceptual capability. While recent literature has focused on in-situ detection
methods to enhance navigation and operational autonomy, robust description is
still needed. In this work, we explore metric learning as the lightweight
feature description mechanism and find that current solutions fail to address
inter-class similarity and multi-view observational geometry. We attribute this
to the view-unaware attention mechanism and introduce Multi-view Attention
Regularizations (MARs) to constrain the channel and spatial attention across
multiple feature views, regularizing the what and where of attention focus. We
thoroughly analyze many modern metric learning losses with and without MARs and
demonstrate improved terrain-feature recognition performance by upwards of 85%.
We additionally introduce the Luna-1 dataset, consisting of Moon crater
landmarks and reference navigation frames from NASA mission data to support
future research in this difficult task. Luna-1 and source code are publicly
available at https://droneslab.github.io/mars/."
Enhancing Equity in Large Language Models for Medical Applications,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Recent advancements have highlighted the potential of large language models
(LLMs) in medical applications, notably in automating Clinical Trial Matching
for translational research and providing medical question-answering for
clinical decision support. However, our study reveals significant inequities in
the use of LLMs, particularly for individuals from specific racial, gender, and
underrepresented groups influenced by social determinants of health. These
disparities could worsen existing health inequities if LLMs are broadly adopted
in healthcare. To address this, we propose and evaluate a novel framework,
EquityGuard, designed to detect and mitigate biases in LLM-based medical
applications. EquityGuard incorporates a Bias Detection Mechanism capable of
identifying and correcting unfair predictions, thus enhancing outcomes and
promoting equity across diverse population groups."
Interactive Event Sifting using Bayesian Graph Neural Networks,cs.LG,Machine Learning,2024-10-07,"Forensic analysts often use social media imagery and texts to understand
important events. A primary challenge is the initial sifting of irrelevant
posts. This work introduces an interactive process for training an
event-centric, learning-based multimodal classification model that automates
sanitization. We propose a method based on Bayesian Graph Neural Networks
(BGNNs) and evaluate active learning and pseudo-labeling formulations to reduce
the number of posts the analyst must manually annotate. Our results indicate
that BGNNs are useful for social-media data sifting for forensics
investigations of events of interest, the value of active learning and
pseudo-labeling varies based on the setting, and incorporating unlabelled data
from other events improves performance."
ReasoningRank: Teaching Student Models to Rank through Reasoning-Based Knowledge Distillation,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Reranking documents based on their relevance to a given query is critical in
information retrieval. Traditional reranking methods often focus on improving
the initial rankings but lack transparency, failing to explain why one document
is ranked higher. In this paper, we introduce ReasoningRank, a novel reranking
approach that enhances clarity by generating two types of reasoning: explicit
reasoning, which explains how a document addresses the query, and comparison
reasoning, which justifies the relevance of one document over another. We
leverage large language models (LLMs) as teacher models to generate these
explanations and distill this knowledge into smaller, more resource-efficient
student models. While the student models may not outperform LLMs in speed, they
significantly reduce the computational burden by requiring fewer resources,
making them more suitable for large-scale or resource-constrained settings.
These student models are trained to both generate meaningful reasoning and
rerank documents, achieving competitive performance across multiple datasets,
including MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank
improves reranking accuracy and provides valuable insights into the
decision-making process, offering a structured and interpretable solution for
reranking tasks."
A Simulation-Free Deep Learning Approach to Stochastic Optimal Control,cs.LG,Machine Learning,2024-10-07,"We propose a simulation-free algorithm for the solution of generic problems
in stochastic optimal control (SOC). Unlike existing methods, our approach does
not require the solution of an adjoint problem, but rather leverages Girsanov
theorem to directly calculate the gradient of the SOC objective on-policy. This
allows us to speed up the optimization of control policies parameterized by
neural networks since it completely avoids the expensive back-propagation step
through stochastic differential equations (SDEs) used in the Neural SDE
framework. In particular, it enables us to solve SOC problems in high dimension
and on long time horizons. We demonstrate the efficiency of our approach in
various domains of applications, including standard stochastic optimal control
problems, sampling from unnormalized distributions via construction of a
Schr\""odinger-F\""ollmer process, and fine-tuning of pre-trained diffusion
models. In all cases our method is shown to outperform the existing methods in
both the computing time and memory efficiency."
A Predictive and Optimization Approach for Enhanced Urban Mobility Using Spatiotemporal Data,cs.LG,Machine Learning,2024-10-07,"In modern urban centers, effective transportation management poses a
significant challenge, with traffic jams and inconsistent travel durations
greatly affecting commuters and logistics operations. This study introduces a
novel method for enhancing urban mobility by combining machine learning
algorithms with live traffic information. We developed predictive models for
journey time and congestion analysis using data from New York City's yellow
taxi trips. The research employed a spatiotemporal analysis framework to
identify traffic trends and implemented real-time route optimization using the
GraphHopper API. This system determines the most efficient paths based on
current conditions, adapting to changes in traffic flow. The methodology
utilizes Spark MLlib for predictive modeling and Spark Streaming for processing
data in real-time. By integrating historical data analysis with current traffic
inputs, our system shows notable enhancements in both travel time forecasts and
route optimization, demonstrating its potential for widespread application in
major urban areas. This research contributes to ongoing efforts aimed at
reducing urban congestion and improving transportation efficiency through
advanced data-driven methods."
Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Generative language models often struggle with specialized or less-discussed
knowledge. A potential solution is found in Retrieval-Augmented Generation
(RAG) models which act like retrieving information before generating responses.
In this study, we explore how the \textsc{Atlas} approach, a RAG model, decides
between what it already knows (parametric) and what it retrieves
(non-parametric). We use causal mediation analysis and controlled experiments
to examine how internal representations influence information processing. Our
findings disentangle the effects of parametric knowledge and the retrieved
context. They indicate that in cases where the model can choose between both
types of information (parametric and non-parametric), it relies more on the
context than the parametric knowledge. Furthermore, the analysis investigates
the computations involved in \emph{how} the model uses the information from the
context. We find that multiple mechanisms are active within the model and can
be detected with mediation analysis: first, the decision of \emph{whether the
context is relevant}, and second, how the encoder computes output
representations to support copying when relevant."
VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Embedding models have been crucial in enabling various downstream tasks such
as semantic similarity, information retrieval, and clustering. Recently, there
has been a surge of interest in developing universal text embedding models that
can generalize across tasks (e.g., MTEB). However, progress in learning
universal multimodal embedding models has been relatively slow despite their
importance. In this work, we aim to explore the potential for building
universal embeddings capable of handling a wide range of downstream tasks. Our
contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),
which covers 4 meta-tasks (i.e. classification, visual question answering,
multimodal retrieval, and visual grounding) and 36 datasets, including 20
training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->
Vector), a contrastive training framework that converts any state-of-the-art
vision-language model into an embedding model via training on MMEB. Unlike
previous models such as CLIP and BLIP, VLM2Vec can process any combination of
images and text to generate a fixed-dimensional vector based on task
instructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate
them on MMEB's evaluation split. Our results show that \model achieves an
absolute average improvement of 10% to 20% over existing multimodal embedding
models on both in-distribution and out-of-distribution datasets in MMEB."
MIBench: A Comprehensive Benchmark for Model Inversion Attack and Defense,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Model Inversion (MI) attacks aim at leveraging the output information of
target models to reconstruct privacy-sensitive training data, raising
widespread concerns on privacy threats of Deep Neural Networks (DNNs).
Unfortunately, in tandem with the rapid evolution of MI attacks, the lack of a
comprehensive, aligned, and reliable benchmark has emerged as a formidable
challenge. This deficiency leads to inadequate comparisons between different
attack methods and inconsistent experimental setups. In this paper, we
introduce the first practical benchmark for model inversion attacks and
defenses to address this critical gap, which is named \textit{MIBench}. This
benchmark serves as an extensible and reproducible modular-based toolbox and
currently integrates a total of 16 state-of-the-art attack and defense methods.
Moreover, we furnish a suite of assessment tools encompassing 9 commonly used
evaluation protocols to facilitate standardized and fair evaluation and
analysis. Capitalizing on this foundation, we conduct extensive experiments
from multiple perspectives to holistically compare and analyze the performance
of various methods across different scenarios, which overcomes the misalignment
issues and discrepancy prevalent in previous works. Based on the collected
attack methods and defense strategies, we analyze the impact of target
resolution, defense robustness, model predictive power, model architectures,
transferability and loss function. Our hope is that this \textit{MIBench} could
provide a unified, practical and extensible toolbox and is widely utilized by
researchers in the field to rigorously test and compare their novel methods,
ensuring equitable evaluations and thereby propelling further advancements in
the future development."
Real-Time Truly-Coupled Lidar-Inertial Motion Correction and Spatiotemporal Dynamic Object Detection,cs.RO,Robotics,2024-10-07,"Over the past decade, lidars have become a cornerstone of robotics state
estimation and perception thanks to their ability to provide accurate geometric
information about their surroundings in the form of 3D scans. Unfortunately,
most of nowadays lidars do not take snapshots of the environment but sweep the
environment over a period of time (typically around 100 ms). Such a
rolling-shutter-like mechanism introduces motion distortion into the collected
lidar scan, thus hindering downstream perception applications. In this paper,
we present a novel method for motion distortion correction of lidar data by
tightly coupling lidar with Inertial Measurement Unit (IMU) data. The
motivation of this work is a map-free dynamic object detection based on lidar.
The proposed lidar data undistortion method relies on continuous preintegrated
of IMU measurements that allow parameterising the sensors' continuous 6-DoF
trajectory using solely eleven discrete state variables (biases, initial
velocity, and gravity direction). The undistortion consists of feature-based
distance minimisation of point-to-line and point-to-plane residuals in a
non-linear least-square formulation. Given undistorted geometric data over a
short temporal window, the proposed pipeline computes the spatiotemporal normal
vector of each of the lidar points. The temporal component of the normals is a
proxy for the corresponding point's velocity, therefore allowing for
learning-free dynamic object classification without the need for registration
in a global reference frame. We demonstrate the soundness of the proposed
method and its different components using public datasets and compare them with
state-of-the-art lidar-inertial state estimation and dynamic object detection
algorithms."
CTC-GMM: CTC guided modality matching for fast and accurate streaming speech translation,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Models for streaming speech translation (ST) can achieve high accuracy and
low latency if they're developed with vast amounts of paired audio in the
source language and written text in the target language. Yet, these text labels
for the target language are often pseudo labels due to the prohibitive cost of
manual ST data labeling. In this paper, we introduce a methodology named
Connectionist Temporal Classification guided modality matching (CTC-GMM) that
enhances the streaming ST model by leveraging extensive machine translation
(MT) text data. This technique employs CTC to compress the speech sequence into
a compact embedding sequence that matches the corresponding text sequence,
allowing us to utilize matched {source-target} language text pairs from the MT
corpora to refine the streaming ST model further. Our evaluations with FLEURS
and CoVoST2 show that the CTC-GMM approach can increase translation accuracy
relatively by 13.9% and 6.4% respectively, while also boosting decoding speed
by 59.7% on GPU."
Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild,cs.LG,Machine Learning,2024-10-07,"As Large Language Models (LLMs) excel across tasks and specialized domains,
scaling LLMs based on existing models has garnered significant attention, which
faces the challenge of decreasing performance when combining disparate models.
Various techniques have been proposed for the aggregation of pre-trained LLMs,
including model merging, Mixture-of-Experts, and stacking. Despite their
merits, a comprehensive comparison and synergistic application of them to a
diverse model zoo is yet to be adequately addressed. In light of this research
gap, this paper introduces Model-GLUE, a holistic LLM scaling guideline. First,
our work starts with a benchmarking of existing LLM scaling techniques,
especially selective merging, and variants of mixture. Utilizing the insights
from the benchmark results, we formulate an strategy for the selection and
aggregation of a heterogeneous model zoo characterizing different architectures
and initialization. Our methodology involves the clustering of mergeable models
and optimal merging strategy selection, and the integration of clusters through
a model mixture. Finally, evidenced by our experiments on a diverse
Llama-2-based model zoo, Model-GLUE shows an average performance enhancement of
5.61%, achieved without additional training. Codes are available at:
https://github.com/Model-GLUE/Model-GLUE."
Leveraging Multimodal Diffusion Models to Accelerate Imaging with Side Information,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Diffusion models have found phenomenal success as expressive priors for
solving inverse problems, but their extension beyond natural images to more
structured scientific domains remains limited. Motivated by applications in
materials science, we aim to reduce the number of measurements required from an
expensive imaging modality of interest, by leveraging side information from an
auxiliary modality that is much cheaper to obtain. To deal with the
non-differentiable and black-box nature of the forward model, we propose a
framework to train a multimodal diffusion model over the joint modalities,
turning inverse problems with black-box forward models into simple linear
inpainting problems. Numerically, we demonstrate the feasibility of training
diffusion models over materials imagery data, and show that our approach
achieves superior image reconstruction by leveraging the available side
information, requiring significantly less amount of data from the expensive
microscopy modality."
BSG4Bot: Efficient Bot Detection based on Biased Heterogeneous Subgraphs,cs.LG,Machine Learning,2024-10-07,"The detection of malicious social bots has become a crucial task, as bots can
be easily deployed and manipulated to spread disinformation, promote conspiracy
messages, and more. Most existing approaches utilize graph neural networks
(GNNs)to capture both user profle and structural features,achieving promising
progress. However, they still face limitations including the expensive training
on large underlying graph, the performance degration when similar neighborhood
patterns' assumption preferred by GNNs is not satisfied, and the dynamic
features of bots in a highly adversarial context. Motivated by these
limitations, this paper proposes a method named BSG4Bot with an intuition that
GNNs training on Biased SubGraphs can improve both performance and time/space
efficiency in bot detection. Specifically, BSG4Bot first pre-trains a
classifier on node features efficiently to define the node similarities, and
constructs biased subgraphs by combining the similarities computed by the
pre-trained classifier and the node importances computed by Personalized
PageRank (PPR scores). BSG4Bot then introduces a heterogeneous GNN over the
constructed subgraphs to detect bots effectively and efficiently. The
relatively stable features, including the content category and temporal
activity features, are explored and incorporated into BSG4Bot after preliminary
verification on sample data. The extensive experimental studies show that
BSG4Bot outperforms the state-of-the-art bot detection methods, while only
needing nearly 1/5 training time."
Tuning-Free Bilevel Optimization: New Algorithms and Convergence Analysis,cs.LG,Machine Learning,2024-10-07,"Bilevel optimization has recently attracted considerable attention due to its
abundant applications in machine learning problems. However, existing methods
rely on prior knowledge of problem parameters to determine stepsizes, resulting
in significant effort in tuning stepsizes when these parameters are unknown. In
this paper, we propose two novel tuning-free algorithms, D-TFBO and S-TFBO.
D-TFBO employs a double-loop structure with stepsizes adaptively adjusted by
the ""inverse of cumulative gradient norms"" strategy. S-TFBO features a simpler
fully single-loop structure that updates three variables simultaneously with a
theory-motivated joint design of adaptive stepsizes for all variables. We
provide a comprehensive convergence analysis for both algorithms and show that
D-TFBO and S-TFBO respectively require $O(\frac{1}{\epsilon})$ and
$O(\frac{1}{\epsilon}\log^4(\frac{1}{\epsilon}))$ iterations to find an
$\epsilon$-accurate stationary point, (nearly) matching their well-tuned
counterparts using the information of problem parameters. Experiments on
various problems show that our methods achieve performance comparable to
existing well-tuned approaches, while being more robust to the selection of
initial stepsizes. To the best of our knowledge, our methods are the first to
completely eliminate the need for stepsize tuning, while achieving theoretical
guarantees."
LOTOS: Layer-wise Orthogonalization for Training Robust Ensembles,cs.LG,Machine Learning,2024-10-07,"Transferability of adversarial examples is a well-known property that
endangers all classification models, even those that are only accessible
through black-box queries. Prior work has shown that an ensemble of models is
more resilient to transferability: the probability that an adversarial example
is effective against most models of the ensemble is low. Thus, most ongoing
research focuses on improving ensemble diversity. Another line of prior work
has shown that Lipschitz continuity of the models can make models more robust
since it limits how a model's output changes with small input perturbations. In
this paper, we study the effect of Lipschitz continuity on transferability
rates. We show that although a lower Lipschitz constant increases the
robustness of a single model, it is not as beneficial in training robust
ensembles as it increases the transferability rate of adversarial examples
across models in the ensemble. Therefore, we introduce LOTOS, a new training
paradigm for ensembles, which counteracts this adverse effect. It does so by
promoting orthogonality among the top-$k$ sub-spaces of the transformations of
the corresponding affine layers of any pair of models in the ensemble. We
theoretically show that $k$ does not need to be large for convolutional layers,
which makes the computational overhead negligible. Through various experiments,
we show LOTOS increases the robust accuracy of ensembles of ResNet-18 models by
$6$ percentage points (p.p) against black-box attacks on CIFAR-10. It is also
capable of combining with the robustness of prior state-of-the-art methods for
training robust ensembles to enhance their robust accuracy by $10.7$ p.p."
Falcon Mamba: The First Competitive Attention-free 7B Language Model,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"In this technical report, we present Falcon Mamba 7B, a new base large
language model based on the novel Mamba architecture. Falcon Mamba 7B is
trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure
Mamba-based model, Falcon Mamba 7B surpasses leading open-weight models based
on Transformers, such as Mistral 7B, Llama3.1 8B, and Falcon2 11B. It is on par
with Gemma 7B and outperforms models with different architecture designs, such
as RecurrentGemma 9B and RWKV-v6 Finch 7B/14B. Currently, Falcon Mamba 7B is
the best-performing Mamba model in the literature at this scale, surpassing
both existing Mamba and hybrid Mamba-Transformer models, according to the Open
LLM Leaderboard. Due to its architecture, Falcon Mamba 7B is significantly
faster at inference and requires substantially less memory for long sequence
generation. Despite recent studies suggesting that hybrid Mamba-Transformer
models outperform pure architecture designs, we demonstrate that even the pure
Mamba design can achieve similar, or even superior results compared to the
Transformer and hybrid designs. We make the weights of our implementation of
Falcon Mamba 7B publicly available on
https://huggingface.co/tiiuae/falcon-mamba-7b, under a permissive license."
Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents,cs.AI,Artificial Intelligence,2024-10-07,"Recent research has explored the use of Large Language Models (LLMs) for
tackling complex graph reasoning tasks. However, due to the intricacies of
graph structures and the inherent limitations of LLMs in handling long text,
current approaches often fail to deliver satisfactory accuracy, even on
small-scale graphs and simple tasks. To address these challenges, we introduce
GraphAgent-Reasoner, a fine-tuning-free framework that utilizes a multi-agent
collaboration strategy for explicit and precise graph reasoning. Inspired by
distributed graph computation theory, our framework decomposes graph problems
into smaller, node-centric tasks that are distributed among multiple agents.
The agents collaborate to solve the overall problem, significantly reducing the
amount of information and complexity handled by a single LLM, thus enhancing
the accuracy of graph reasoning. By simply increasing the number of agents,
GraphAgent-Reasoner can efficiently scale to accommodate larger graphs with
over 1,000 nodes. Evaluated on the GraphInstruct dataset, our framework
demonstrates near-perfect accuracy on polynomial-time graph reasoning tasks,
significantly outperforming the best available models, both closed-source and
fine-tuned open-source variants. Our framework also demonstrates the capability
to handle real-world graph reasoning applications such as webpage importance
analysis."
"Assouad, Fano, and Le Cam with Interaction: A Unifying Lower Bound Framework and Characterization for Bandit Learnability",cs.LG,Machine Learning,2024-10-07,"In this paper, we develop a unified framework for lower bound methods in
statistical estimation and interactive decision making. Classical lower bound
techniques -- such as Fano's inequality, Le Cam's method, and Assouad's lemma
-- have been central to the study of minimax risk in statistical estimation,
yet they are insufficient for the analysis of methods that collect data in an
interactive manner. The recent minimax lower bounds for interactive decision
making via the Decision-Estimation Coefficient (DEC) appear to be genuinely
different from the classical methods. We propose a unified view of these
distinct methodologies through a general algorithmic lower bound method. We
further introduce a novel complexity measure, decision dimension, which
facilitates the derivation of new lower bounds for interactive decision making.
In particular, decision dimension provides a characterization of bandit
learnability for any structured bandit model class. Further, we characterize
the sample complexity of learning convex model class up to a polynomial gap
with the decision dimension, addressing the remaining gap between upper and
lower bounds in Foster et al. (2021, 2023)."
Human-Feedback Efficient Reinforcement Learning for Online Diffusion Model Finetuning,cs.LG,Machine Learning,2024-10-07,"Controllable generation through Stable Diffusion (SD) fine-tuning aims to
improve fidelity, safety, and alignment with human guidance. Existing
reinforcement learning from human feedback methods usually rely on predefined
heuristic reward functions or pretrained reward models built on large-scale
datasets, limiting their applicability to scenarios where collecting such data
is costly or difficult. To effectively and efficiently utilize human feedback,
we develop a framework, HERO, which leverages online human feedback collected
on the fly during model learning. Specifically, HERO features two key
mechanisms: (1) Feedback-Aligned Representation Learning, an online training
method that captures human feedback and provides informative learning signals
for fine-tuning, and (2) Feedback-Guided Image Generation, which involves
generating images from SD's refined initialization samples, enabling faster
convergence towards the evaluator's intent. We demonstrate that HERO is 4x more
efficient in online feedback for body part anomaly correction compared to the
best existing method. Additionally, experiments show that HERO can effectively
handle tasks like reasoning, counting, personalization, and reducing NSFW
content with only 0.5K online feedback."
Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"In the realm of dermatological diagnoses, where the analysis of dermatoscopic
and microscopic skin lesion images is pivotal for the accurate and early
detection of various medical conditions, the costs associated with creating
diverse and high-quality annotated datasets have hampered the accuracy and
generalizability of machine learning models. We propose an innovative
unsupervised augmentation solution that harnesses Generative Adversarial
Network (GAN) based models and associated techniques over their latent space to
generate controlled semiautomatically-discovered semantic variations in
dermatoscopic images. We created synthetic images to incorporate the semantic
variations and augmented the training data with these images. With this
approach, we were able to increase the performance of machine learning models
and set a new benchmark amongst non-ensemble based models in skin lesion
classification on the HAM10000 dataset; and used the observed analytics and
generated models for detailed studies on model explainability, affirming the
effectiveness of our solution."
LiDAR-GS:Real-time LiDAR Re-Simulation using Gaussian Splatting,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"LiDAR simulation plays a crucial role in closed-loop simulation for
autonomous driving. Although recent advancements, such as the use of
reconstructed mesh and Neural Radiance Fields (NeRF), have made progress in
simulating the physical properties of LiDAR, these methods have struggled to
achieve satisfactory frame rates and rendering quality. To address these
limitations, we present LiDAR-GS, the first LiDAR Gaussian Splatting method,
for real-time high-fidelity re-simulation of LiDAR sensor scans in public urban
road scenes. The vanilla Gaussian Splatting, designed for camera models, cannot
be directly applied to LiDAR re-simulation. To bridge the gap between passive
camera and active LiDAR, our LiDAR-GS designs a differentiable laser beam
splatting, grounded in the LiDAR range view model. This innovation allows for
precise surface splatting by projecting lasers onto micro cross-sections,
effectively eliminating artifacts associated with local affine approximations.
Additionally, LiDAR-GS leverages Neural Gaussian Fields, which further
integrate view-dependent clues, to represent key LiDAR properties that are
influenced by the incident angle and external factors. Combining these
practices with some essential adaptations, e.g., dynamic instances
decomposition, our approach succeeds in simultaneously re-simulating depth,
intensity, and ray-drop channels, achieving state-of-the-art results in both
rendering frame rate and quality on publically available large scene datasets.
Our source code will be made publicly available."
Hyper-Representations: Learning from Populations of Neural Networks,cs.LG,Machine Learning,2024-10-07,"This thesis addresses the challenge of understanding Neural Networks through
the lens of their most fundamental component: the weights, which encapsulate
the learned information and determine the model behavior. At the core of this
thesis is a fundamental question: Can we learn general, task-agnostic
representations from populations of Neural Network models? The key contribution
of this thesis to answer that question are hyper-representations, a
self-supervised method to learn representations of NN weights. Work in this
thesis finds that trained NN models indeed occupy meaningful structures in the
weight space, that can be learned and used. Through extensive experiments, this
thesis demonstrates that hyper-representations uncover model properties, such
as their performance, state of training, or hyperparameters. Moreover, the
identification of regions with specific properties in hyper-representation
space allows to sample and generate model weights with targeted properties.
This thesis demonstrates applications for fine-tuning, and transfer learning to
great success. Lastly, it presents methods that allow hyper-representations to
generalize beyond model sizes, architectures, and tasks. The practical
implications of that are profound, as it opens the door to foundation models of
Neural Networks, which aggregate and instantiate their knowledge across models
and architectures. Ultimately, this thesis contributes to the deeper
understanding of Neural Networks by investigating structures in their weights
which leads to more interpretable, efficient, and adaptable models. By laying
the groundwork for representation learning of NN weights, this research
demonstrates the potential to change the way Neural Networks are developed,
analyzed, and used."
AI-Enhanced Ethical Hacking: A Linux-Focused Experiment,cs.CR,Cryptography and Security,2024-10-07,"This technical report investigates the integration of generative AI (GenAI),
specifically ChatGPT, into the practice of ethical hacking through a
comprehensive experimental study and conceptual analysis. Conducted in a
controlled virtual environment, the study evaluates GenAI's effectiveness
across the key stages of penetration testing on Linux-based target machines
operating within a virtual local area network (LAN), including reconnaissance,
scanning and enumeration, gaining access, maintaining access, and covering
tracks. The findings confirm that GenAI can significantly enhance and
streamline the ethical hacking process while underscoring the importance of
balanced human-AI collaboration rather than the complete replacement of human
input. The report also critically examines potential risks such as misuse, data
biases, hallucination, and over-reliance on AI. This research contributes to
the ongoing discussion on the ethical use of AI in cybersecurity and highlights
the need for continued innovation to strengthen security defences."
MetaDD: Boosting Dataset Distillation with Neural Network Architecture-Invariant Generalization,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Dataset distillation (DD) entails creating a refined, compact distilled
dataset from a large-scale dataset to facilitate efficient training. A
significant challenge in DD is the dependency between the distilled dataset and
the neural network (NN) architecture used. Training a different NN architecture
with a distilled dataset distilled using a specific architecture often results
in diminished trainning performance for other architectures. This paper
introduces MetaDD, designed to enhance the generalizability of DD across
various NN architectures. Specifically, MetaDD partitions distilled data into
meta features (i.e., the data's common characteristics that remain consistent
across different NN architectures) and heterogeneous features (i.e., the data's
unique feature to each NN architecture). Then, MetaDD employs an
architecture-invariant loss function for multi-architecture feature alignment,
which increases meta features and reduces heterogeneous features in distilled
data. As a low-memory consumption component, MetaDD can be seamlessly
integrated into any DD methodology. Experimental results demonstrate that
MetaDD significantly improves performance across various DD methods. On the
Distilled Tiny-Imagenet with Sre2L (50 IPC), MetaDD achieves cross-architecture
NN accuracy of up to 30.1\%, surpassing the second-best method (GLaD) by 1.7\%."
SparsePO: Controlling Preference Alignment of LLMs via Sparse Token Masks,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Preference Optimization (PO) has proven an effective step for aligning
language models to human-desired behaviors. Current variants, following the
offline Direct Preference Optimization objective, have focused on a strict
setting where all tokens are contributing signals of KL divergence and rewards
to the loss function. However, human preference is not affected by each word in
a sequence equally but is often dependent on specific words or phrases, e.g.
existence of toxic terms leads to non-preferred responses. Based on this
observation, we argue that not all tokens should be weighted equally during PO
and propose a flexible objective termed SparsePO, that aims to automatically
learn to weight the KL divergence and reward corresponding to each token during
PO training. We propose two different variants of weight-masks that can either
be derived from the reference model itself or learned on the fly. Notably, our
method induces sparsity in the learned masks, allowing the model to learn how
to best weight reward and KL divergence contributions at the token level,
learning an optimal level of mask sparsity. Extensive experiments on multiple
domains, including sentiment control, dialogue, text summarization and
text-to-code generation, illustrate that our approach assigns meaningful
weights to tokens according to the target task, generates more responses with
the desired preference and improves reasoning tasks by up to 2 percentage
points compared to other token- and response-level PO methods."
IGroupSS-Mamba: Interval Group Spatial-Spectral Mamba for Hyperspectral Image Classification,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Hyperspectral image (HSI) classification has garnered substantial attention
in remote sensing fields. Recent Mamba architectures built upon the Selective
State Space Models (S6) have demonstrated enormous potential in long-range
sequence modeling. However, the high dimensionality of hyperspectral data and
information redundancy pose challenges to the application of Mamba in HSI
classification, suffering from suboptimal performance and computational
efficiency. In light of this, this paper investigates a lightweight Interval
Group Spatial-Spectral Mamba framework (IGroupSS-Mamba) for HSI classification,
which allows for multi-directional and multi-scale global spatial-spectral
information extraction in a grouping and hierarchical manner. Technically, an
Interval Group S6 Mechanism (IGSM) is developed as the core component, which
partitions high-dimensional features into multiple non-overlapping groups at
intervals, and then integrates a unidirectional S6 for each group with a
specific scanning direction to achieve non-redundant sequence modeling.
Compared to conventional applying multi-directional scanning to all bands, this
grouping strategy leverages the complementary strengths of different scanning
directions while decreasing computational costs. To adequately capture the
spatial-spectral contextual information, an Interval Group Spatial-Spectral
Block (IGSSB) is introduced, in which two IGSM-based spatial and spectral
operators are cascaded to characterize the global spatial-spectral relationship
along the spatial and spectral dimensions, respectively. IGroupSS-Mamba is
constructed as a hierarchical structure stacked by multiple IGSSB blocks,
integrating a pixel aggregation-based downsampling strategy for multiscale
spatial-spectral semantic learning from shallow to deep stages. Extensive
experiments demonstrate that IGroupSS-Mamba outperforms the state-of-the-art
methods."
Investigating large language models for their competence in extracting grammatically sound sentences from transcribed noisy utterances,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Selectively processing noisy utterances while effectively disregarding
speech-specific elements poses no considerable challenge for humans, as they
exhibit remarkable cognitive abilities to separate semantically significant
content from speech-specific noise (i.e. filled pauses, disfluencies, and
restarts). These abilities may be driven by mechanisms based on acquired
grammatical rules that compose abstract syntactic-semantic structures within
utterances. Segments without syntactic and semantic significance are
consistently disregarded in these structures. The structures, in tandem with
lexis, likely underpin language comprehension and thus facilitate effective
communication. In our study, grounded in linguistically motivated experiments,
we investigate whether large language models (LLMs) can effectively perform
analogical speech comprehension tasks. In particular, we examine the ability of
LLMs to extract well-structured utterances from transcriptions of noisy
dialogues. We conduct two evaluation experiments in the Polish language
scenario, using a~dataset presumably unfamiliar to LLMs to mitigate the risk of
data contamination. Our results show that not all extracted utterances are
correctly structured, indicating that either LLMs do not fully acquire
syntactic-semantic rules or they acquire them but cannot apply them
effectively. We conclude that the ability of LLMs to comprehend noisy
utterances is still relatively superficial compared to human proficiency in
processing them."
DreamSat: Towards a General 3D Model for Novel View Synthesis of Space Objects,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Novel view synthesis (NVS) enables to generate new images of a scene or
convert a set of 2D images into a comprehensive 3D model. In the context of
Space Domain Awareness, since space is becoming increasingly congested, NVS can
accurately map space objects and debris, improving the safety and efficiency of
space operations. Similarly, in Rendezvous and Proximity Operations missions,
3D models can provide details about a target object's shape, size, and
orientation, allowing for better planning and prediction of the target's
behavior. In this work, we explore the generalization abilities of these
reconstruction techniques, aiming to avoid the necessity of retraining for each
new scene, by presenting a novel approach to 3D spacecraft reconstruction from
single-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art
single-view reconstruction model, on a high-quality dataset of 190 high-quality
spacecraft models and integrating it into the DreamGaussian framework. We
demonstrate consistent improvements in reconstruction quality across multiple
metrics, including Contrastive Language-Image Pretraining (CLIP) score
(+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity
Index (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS)
(+0.16%) on a test set of 30 previously unseen spacecraft images. Our method
addresses the lack of domain-specific 3D reconstruction tools in the space
industry by leveraging state-of-the-art diffusion models and 3D Gaussian
splatting techniques. This approach maintains the efficiency of the
DreamGaussian framework while enhancing the accuracy and detail of spacecraft
reconstructions. The code for this work can be accessed on GitHub
(https://github.com/ARCLab-MIT/space-nvs)."
Human-in-the-loop Reasoning For Traffic Sign Detection: Collaborative Approach Yolo With Video-llava,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Traffic Sign Recognition (TSR) detection is a crucial component of autonomous
vehicles. While You Only Look Once (YOLO) is a popular real-time object
detection algorithm, factors like training data quality and adverse weather
conditions (e.g., heavy rain) can lead to detection failures. These failures
can be particularly dangerous when visual similarities between objects exist,
such as mistaking a 30 km/h sign for a higher speed limit sign. This paper
proposes a method that combines video analysis and reasoning, prompting with a
human-in-the-loop guide large vision model to improve YOLOs accuracy in
detecting road speed limit signs, especially in semi-real-world conditions. It
is hypothesized that the guided prompting and reasoning abilities of
Video-LLava can enhance YOLOs traffic sign detection capabilities. This
hypothesis is supported by an evaluation based on human-annotated accuracy
metrics within a dataset of recorded videos from the CARLA car simulator. The
results demonstrate that a collaborative approach combining YOLO with
Video-LLava and reasoning can effectively address challenging situations such
as heavy rain and overcast conditions that hinder YOLOs detection capabilities."
Towards a Modern and Lightweight Rendering Engine for Dynamic Robotic Simulations,cs.RO,Robotics,2024-10-07,"Interactive dynamic simulators are an accelerator for developing novel
robotic control algorithms and complex systems involving humans and robots. In
user training and synthetic data generation applications, a high-fidelity
visualization of the simulation is essential. Visual fidelity is dependent on
the quality of the computer graphics algorithms used to render the simulated
scene. Furthermore, the rendering algorithms must be implemented on the
graphics processing unit (GPU) to achieve real-time performance, requiring the
use of a graphics application programming interface (API). This paper presents
a performance-focused and lightweight rendering engine supporting the Vulkan
graphics API. The engine is designed to modernize the legacy rendering pipeline
of Asynchronous Multi-Body Framework (AMBF), a dynamic simulation framework
used extensively for interactive robotics simulation development. This new
rendering engine implements graphical features such as physically based
rendering (PBR), anti-aliasing, and ray-traced shadows, significantly improving
the image quality of AMBF. Computational experiments show that the engine can
render a simulated scene with over seven million triangles while maintaining
GPU computation times within two milliseconds."
On the Structure of Game Provenance and its Applications,cs.AI,Artificial Intelligence,2024-10-07,"Provenance in databases has been thoroughly studied for positive and for
recursive queries, then for first-order (FO) queries, i.e., having negation but
no recursion. Query evaluation can be understood as a two-player game where the
opponents argue whether or not a tuple is in the query answer. This
game-theoretic approach yields a natural provenance model for FO queries,
unifying how and why-not provenance. Here, we study the fine-grain structure of
game provenance. A game $G=(V,E)$ consists of positions $V$ and moves $E$ and
can be solved by computing the well-founded model of a single, unstratifiable
rule: \[ \text{win}(X) \leftarrow \text{move}(X, Y), \neg \, \text{win}(Y). \]
In the solved game $G^{\lambda}$, the value of a position $x\,{\in}\,V$ is
either won, lost, or drawn. This value is explained by the provenance
$\mathscr{P}$(x), i.e., certain (annotated) edges reachable from $x$. We
identify seven edge types that give rise to new kinds of provenance, i.e.,
potential, actual, and primary, and demonstrate that ""not all moves are created
equal"". We describe the new provenance types, show how they can be computed
while solving games, and discuss applications, e.g., for abstract argumentation
frameworks."
Reinforcement Learning Control for Autonomous Hydraulic Material Handling Machines with Underactuated Tools,cs.RO,Robotics,2024-10-07,"The precise and safe control of heavy material handling machines presents
numerous challenges due to the hard-to-model hydraulically actuated joints and
the need for collision-free trajectory planning with a free-swinging
end-effector tool. In this work, we propose an RL-based controller that
commands the cabin joint and the arm simultaneously. It is trained in a
simulation combining data-driven modeling techniques with first-principles
modeling. On the one hand, we employ a neural network model to capture the
highly nonlinear dynamics of the upper carriage turn hydraulic motor,
incorporating explicit pressure prediction to handle delays better. On the
other hand, we model the arm as velocity-controllable and the free-swinging
end-effector tool as a damped pendulum using first principles. This combined
model enhances our simulation environment, enabling the training of RL
controllers that can be directly transferred to the real machine. Designed to
reach steady-state Cartesian targets, the RL controller learns to leverage the
hydraulic dynamics to improve accuracy, maintain high speeds, and minimize
end-effector tool oscillations. Our controller, tested on a mid-size prototype
material handler, is more accurate than an inexperienced operator and causes
fewer tool oscillations. It demonstrates competitive performance even compared
to an experienced professional driver."
HyperINF: Unleashing the HyperPower of the Schulz's Method for Data Influence Estimation,cs.LG,Machine Learning,2024-10-07,"Influence functions provide a principled method to assess the contribution of
individual training samples to a specific target. Yet, their high computational
costs limit their applications on large-scale models and datasets. Existing
methods proposed for influence function approximation have significantly
reduced the computational overheads. However, they mostly suffer from
inaccurate estimation due to the lack of strong convergence guarantees from the
algorithm. The family of hyperpower methods are well-known for their rigorous
convergence guarantees on matrix inverse approximation, while the matrix
multiplication operation can involve intractable memory and computation costs
on large-scale models. We propose HyperINF, an efficient and accurate influence
function approximation method which leverages the hyperpower method,
specifically Schulz's iterative algorithm.
  To deal with the computation-intensive matrix multiplication, we incorporate
the generalized fisher information (GFIM) as a low-rank approximation of the
Hessian matrix, which reduces the memory and computation overheads to constant
costs independent of ranks on LoRA-tuned models.
  We first demonstrate the superior accuracy and stability of \method compared
to other baselines through a synthetic convergence simulation for matrix
inversion. We further validate the efficacy of \method through extensive
real-world data attribution tasks, including mislabeled data detection and data
selection for LLM and VLM fine-tuning.
  On LoRA-tuned models, HyperINF achieves superior downstream performance with
minimal memory and computational overhead, while other baselines suffer from
significant degradation. Our codebase is available at
https://github.com/Blackzxy/HyperINF."
Explanation sensitivity to the randomness of large language models: the case of journalistic text classification,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Large language models (LLMs) perform very well in several natural language
processing tasks but raise explainability challenges. In this paper, we examine
the effect of random elements in the training of LLMs on the explainability of
their predictions. We do so on a task of opinionated journalistic text
classification in French. Using a fine-tuned CamemBERT model and an explanation
method based on relevance propagation, we find that training with different
random seeds produces models with similar accuracy but variable explanations.
We therefore claim that characterizing the explanations' statistical
distribution is needed for the explainability of LLMs. We then explore a
simpler model based on textual features which offers stable explanations but is
less accurate. Hence, this simpler model corresponds to a different tradeoff
between accuracy and explainability. We show that it can be improved by
inserting features derived from CamemBERT's explanations. We finally discuss
new research directions suggested by our results, in particular regarding the
origin of the sensitivity observed in the training randomness."
ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"The advancements of language language models (LLMs) have piqued growing
interest in developing LLM-based language agents to automate scientific
discovery end-to-end, which has sparked both excitement and skepticism about
the true capabilities of such agents. In this work, we argue that for an agent
to fully automate scientific discovery, it must be able to complete all
essential tasks in the workflow. Thus, we call for rigorous assessment of
agents on individual tasks in a scientific workflow before making bold claims
on end-to-end automation. To this end, we present ScienceAgentBench, a new
benchmark for evaluating language agents for data-driven scientific discovery.
To ensure the scientific authenticity and real-world relevance of our
benchmark, we extract 102 tasks from 44 peer-reviewed publications in four
disciplines and engage nine subject matter experts to validate them. We unify
the target output for every task to a self-contained Python program file and
employ an array of evaluation metrics to examine the generated programs,
execution results, and costs. Each task goes through multiple rounds of manual
validation by annotators and subject matter experts to ensure its annotation
quality and scientific plausibility. We also propose two effective strategies
to mitigate data contamination concerns. Using our benchmark, we evaluate five
open-weight and proprietary LLMs, each with three frameworks: direct prompting,
OpenHands, and self-debug. Given three attempts for each task, the
best-performing agent can only solve 32.4% of the tasks independently and 34.3%
with expert-provided knowledge. These results underscore the limited capacities
of current language agents in generating code for data-driven discovery, let
alone end-to-end automation for scientific research."
HE-Nav: A High-Performance and Efficient Navigation System for Aerial-Ground Robots in Cluttered Environments,cs.RO,Robotics,2024-10-07,"Existing AGR navigation systems have advanced in lightly occluded scenarios
(e.g., buildings) by employing 3D semantic scene completion networks for voxel
occupancy prediction and constructing Euclidean Signed Distance Field (ESDF)
maps for collision-free path planning. However, these systems exhibit
suboptimal performance and efficiency in cluttered environments with severe
occlusions (e.g., dense forests or tall walls), due to limitations arising from
perception networks' low prediction accuracy and path planners' high
computational overhead. In this paper, we present HE-Nav, the first
high-performance and efficient navigation system tailored for AGRs operating in
cluttered environments. The perception module utilizes a lightweight semantic
scene completion network (LBSCNet), guided by a bird's eye view (BEV) feature
fusion and enhanced by an exquisitely designed SCB-Fusion module and attention
mechanism. This enables real-time and efficient obstacle prediction in
cluttered areas, generating a complete local map. Building upon this completed
map, our novel AG-Planner employs the energy-efficient kinodynamic A* search
algorithm to guarantee planning is energy-saving. Subsequent trajectory
optimization processes yield safe, smooth, dynamically feasible and ESDF-free
aerial-ground hybrid paths. Extensive experiments demonstrate that HE-Nav
achieved 7x energy savings in real-world situations while maintaining planning
success rates of 98% in simulation scenarios. Code and video are available on
our project page: https://jmwang0117.github.io/HE-Nav/."
Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data,cs.LG,Machine Learning,2024-10-07,"Foundation models have recently been shown to be strong data compressors.
However, when accounting for their excessive parameter count, their compression
ratios are actually inferior to standard compression algorithms. Moreover,
naively reducing the number of parameters may not necessarily help as it leads
to worse predictions and thus weaker compression. In this paper, we conduct a
large-scale empirical study to investigate whether there is a sweet spot where
competitive compression ratios with pre-trained vanilla transformers are
possible. To this end, we train families of models on 165GB of raw byte
sequences of either text, image, or audio data (and all possible combinations
of the three) and then compress 1GB of out-of-distribution (OOD) data from each
modality. We find that relatively small models (i.e., millions of parameters)
can outperform standard general-purpose compression algorithms (gzip, LZMA2)
and even domain-specific compressors (PNG, JPEG 2000, FLAC) - even when
factoring in parameter count. We achieve, e.g., the lowest compression ratio of
0.49 on OOD audio data (vs. 0.54 for FLAC). To study the impact of model- and
dataset scale, we conduct extensive ablations and hyperparameter sweeps, and we
investigate the effect of unimodal versus multimodal training. We find that
even small models can be trained to perform well on multiple modalities, but,
in contrast to previously reported results with large-scale foundation models,
transfer to unseen modalities is generally weak."
ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Current Large Language Models (LLMs) have shown strong reasoning capabilities
in commonsense question answering benchmarks, but the process underlying their
success remains largely opaque. As a consequence, recent approaches have
equipped LLMs with mechanisms for knowledge retrieval, reasoning and
introspection, not only to improve their capabilities but also to enhance the
interpretability of their outputs. However, these methods require additional
training, hand-crafted templates or human-written explanations. To address
these issues, we introduce ZEBRA, a zero-shot question answering framework that
combines retrieval, case-based reasoning and introspection and dispenses with
the need for additional training of the LLM. Given an input question, ZEBRA
retrieves relevant question-knowledge pairs from a knowledge base and generates
new knowledge by reasoning over the relationships in these pairs. This
generated knowledge is then used to answer the input question, improving the
model's performance and interpretability. We evaluate our approach across 8
well-established commonsense reasoning benchmarks, demonstrating that ZEBRA
consistently outperforms strong LLMs and previous knowledge integration
approaches, achieving an average accuracy improvement of up to 4.5 points."
TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention,cs.LG,Machine Learning,2024-10-07,"Large language models (LLMs) have driven significant advancements across
diverse NLP tasks, with long-context models gaining prominence for handling
extended inputs. However, the expanding key-value (KV) cache size required by
Transformer architectures intensifies the memory constraints, particularly
during the decoding phase, creating a significant bottleneck. Existing sparse
attention mechanisms designed to address this bottleneck have two limitations:
(1) they often fail to reliably identify the most relevant tokens for
attention, and (2) they overlook the spatial coherence of token selection
across consecutive Transformer layers, which can lead to performance
degradation and substantial overhead in token selection. This paper introduces
TidalDecode, a simple yet effective algorithm and system for fast and accurate
LLM decoding through position persistent sparse attention. TidalDecode
leverages the spatial coherence of tokens selected by existing sparse attention
methods and introduces a few token selection layers that perform full attention
to identify the tokens with the highest attention scores, while all other
layers perform sparse attention with the pre-selected tokens. This design
enables TidalDecode to substantially reduce the overhead of token selection for
sparse attention without sacrificing the quality of the generated results.
Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely
matches the generative performance of full attention methods while reducing the
LLM decoding latency by up to 2.1x."
xLSTM-FER: Enhancing Student Expression Recognition with Extended Vision Long Short-Term Memory Network,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Student expression recognition has become an essential tool for assessing
learning experiences and emotional states. This paper introduces xLSTM-FER, a
novel architecture derived from the Extended Long Short-Term Memory (xLSTM),
designed to enhance the accuracy and efficiency of expression recognition
through advanced sequence processing capabilities for student facial expression
recognition. xLSTM-FER processes input images by segmenting them into a series
of patches and leveraging a stack of xLSTM blocks to handle these patches.
xLSTM-FER can capture subtle changes in real-world students' facial expressions
and improve recognition accuracy by learning spatial-temporal relationships
within the sequence. Experiments on CK+, RAF-DF, and FERplus demonstrate the
potential of xLSTM-FER in expression recognition tasks, showing better
performance compared to state-of-the-art methods on standard datasets. The
linear computational and memory complexity of xLSTM-FER make it particularly
suitable for handling high-resolution images. Moreover, the design of xLSTM-FER
allows for efficient processing of non-sequential inputs such as images without
additional computation."
Function Gradient Approximation with Random Shallow ReLU Networks with Control Applications,cs.LG,Machine Learning,2024-10-07,"Neural networks are widely used to approximate unknown functions in control.
A common neural network architecture uses a single hidden layer (i.e. a shallow
network), in which the input parameters are fixed in advance and only the
output parameters are trained. The typical formal analysis asserts that if
output parameters exist to approximate the unknown function with sufficient
accuracy, then desired control performance can be achieved. A long-standing
theoretical gap was that no conditions existed to guarantee that, for the fixed
input parameters, required accuracy could be obtained by training the output
parameters. Our recent work has partially closed this gap by demonstrating that
if input parameters are chosen randomly, then for any sufficiently smooth
function, with high-probability there are output parameters resulting in
$O((1/m)^{1/2})$ approximation errors, where $m$ is the number of neurons.
However, some applications, notably continuous-time value function
approximation, require that the network approximates the both the unknown
function and its gradient with sufficient accuracy. In this paper, we show that
randomly generated input parameters and trained output parameters result in
gradient errors of $O((\log(m)/m)^{1/2})$, and additionally, improve the
constants from our prior work. We show how to apply the result to policy
evaluation problems."
Control-oriented Clustering of Visual Latent Representation,cs.LG,Machine Learning,2024-10-07,"We initiate a study of the geometry of the visual representation space -- the
information channel from the vision encoder to the action decoder -- in an
image-based control pipeline learned from behavior cloning. Inspired by the
phenomenon of neural collapse (NC) in image classification, we investigate
whether a similar law of clustering emerges in the visual representation space.
Since image-based control is a regression task without explicitly defined
classes, the central piece of the puzzle lies in determining according to what
implicit classes the visual features cluster, if such a law exists. Focusing on
image-based planar pushing, we posit the most important role of the visual
representation in a control task is to convey a goal to the action decoder. We
then classify training samples of expert demonstrations into eight
""control-oriented"" classes based on (a) the relative pose between the object
and the target in the input or (b) the relative pose of the object induced by
expert actions in the output, where one class corresponds to one relative pose
orthant (REPO). Across four different instantiations of architecture, we report
the prevalent emergence of control-oriented clustering in the visual
representation space according to the eight REPOs. Beyond empirical
observation, we show such a law of clustering can be leveraged as an
algorithmic tool to improve test-time performance when training a policy with
limited expert demonstrations. Particularly, we pretrain the vision encoder
using NC as a regularization to encourage control-oriented clustering of the
visual features. Surprisingly, such an NC-pretrained vision encoder, when
finetuned end-to-end with the action decoder, boosts the test-time performance
by 10% to 35% in the low-data regime. Real-world vision-based planar pushing
experiments confirmed the surprising advantage of control-oriented visual
representation pretraining."
Improving Object Detection via Local-global Contrastive Learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Visual domain gaps often impact object detection performance. Image-to-image
translation can mitigate this effect, where contrastive approaches enable
learning of the image-to-image mapping under unsupervised regimes. However,
existing methods often fail to handle content-rich scenes with multiple object
instances, which manifests in unsatisfactory detection performance. Sensitivity
to such instance-level content is typically only gained through object
annotations, which can be expensive to obtain. Towards addressing this issue,
we present a novel image-to-image translation method that specifically targets
cross-domain object detection. We formulate our approach as a contrastive
learning framework with an inductive prior that optimises the appearance of
object instances through spatial attention masks, implicitly delineating the
scene into foreground regions associated with the target object instances and
background non-object regions. Instead of relying on object annotations to
explicitly account for object instances during translation, our approach learns
to represent objects by contrasting local-global information. This affords
investigation of an under-explored challenge: obtaining performant detection,
under domain shifts, without relying on object annotations nor detector model
fine-tuning. We experiment with multiple cross-domain object detection settings
across three challenging benchmarks and report state-of-the-art performance.
Project page: https://local-global-detection.github.io"
SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Data curation is the problem of how to collect and organize samples into a
dataset that supports efficient learning. Despite the centrality of the task,
little work has been devoted towards a large-scale, systematic comparison of
various curation methods. In this work, we take steps towards a formal
evaluation of data curation strategies and introduce SELECT, the first
large-scale benchmark of curation strategies for image classification.
  In order to generate baseline methods for the SELECT benchmark, we create a
new dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K
to date. Our dataset extends ImageNet with 5 new training-data shifts, each
approximately the size of ImageNet-1K itself, and each assembled using a
distinct curation strategy. We evaluate our data curation baselines in two
ways: (i) using each training-data shift to train identical image
classification models from scratch (ii) using the data itself to fit a
pretrained self-supervised representation.
  Our findings show interesting trends, particularly pertaining to recent
methods for data curation such as synthetic data generation and lookup based on
CLIP embeddings. We show that although these strategies are highly competitive
for certain tasks, the curation strategy used to assemble the original
ImageNet-1K dataset remains the gold standard. We anticipate that our benchmark
can illuminate the path for new methods to further reduce the gap. We release
our checkpoints, code, documentation, and a link to our dataset at
https://github.com/jimmyxu123/SELECT."
Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Loss spikes, a phenomenon in which the loss value diverges suddenly, is a
fundamental issue in the pre-training of large language models. This paper
supposes that the non-uniformity of the norm of the parameters is one of the
causes of loss spikes. Here, in training of neural networks, the scale of the
gradients is required to be kept constant throughout the layers to avoid the
vanishing and exploding gradients problem. However, to meet these requirements
in the Transformer model, the norm of the model parameters must be non-uniform,
and thus, parameters whose norm is smaller are more sensitive to the parameter
update. To address this issue, we propose a novel technique, weight scaling as
reparameterization (WeSaR). WeSaR introduces a gate parameter per parameter
matrix and adjusts it to the value satisfying the requirements. Because of the
gate parameter, WeSaR sets the norm of the original parameters uniformly, which
results in stable training. Experimental results with the Transformer decoders
consisting of 130 million, 1.3 billion, and 13 billion parameters showed that
WeSaR stabilizes and accelerates training and that it outperformed compared
methods including popular initialization methods."
HE-Drive: Human-Like End-to-End Driving with Vision Language Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"In this paper, we propose HE-Drive: the first human-like-centric end-to-end
autonomous driving system to generate trajectories that are both temporally
consistent and comfortable. Recent studies have shown that imitation
learning-based planners and learning-based trajectory scorers can effectively
generate and select accuracy trajectories that closely mimic expert
demonstrations. However, such trajectory planners and scorers face the dilemma
of generating temporally inconsistent and uncomfortable trajectories. To solve
the above problems, Our HE-Drive first extracts key 3D spatial representations
through sparse perception, which then serves as conditional inputs for a
Conditional Denoising Diffusion Probabilistic Models (DDPMs)-based motion
planner to generate temporal consistency multi-modal trajectories. A
Vision-Language Models (VLMs)-guided trajectory scorer subsequently selects the
most comfortable trajectory from these candidates to control the vehicle,
ensuring human-like end-to-end driving. Experiments show that HE-Drive not only
achieves state-of-the-art performance (i.e., reduces the average collision rate
by 71% than VAD) and efficiency (i.e., 1.9X faster than SparseDrive) on the
challenging nuScenes and OpenScene datasets but also provides the most
comfortable driving experience on real-world data.For more information, visit
the project website: https://jmwang0117.github.io/HE-Drive/."
FreSh: Frequency Shifting for Accelerated Neural Representation Learning,cs.LG,Machine Learning,2024-10-07,"Implicit Neural Representations (INRs) have recently gained attention as a
powerful approach for continuously representing signals such as images, videos,
and 3D shapes using multilayer perceptrons (MLPs). However, MLPs are known to
exhibit a low-frequency bias, limiting their ability to capture high-frequency
details accurately. This limitation is typically addressed by incorporating
high-frequency input embeddings or specialized activation layers. In this work,
we demonstrate that these embeddings and activations are often configured with
hyperparameters that perform well on average but are suboptimal for specific
input signals under consideration, necessitating a costly grid search to
identify optimal settings. Our key observation is that the initial frequency
spectrum of an untrained model's output correlates strongly with the model's
eventual performance on a given target signal. Leveraging this insight, we
propose frequency shifting (or FreSh), a method that selects embedding
hyperparameters to align the frequency spectrum of the model's initial output
with that of the target signal. We show that this simple initialization
technique improves performance across various neural representation methods and
tasks, achieving results comparable to extensive hyperparameter sweeps but with
only marginal computational overhead compared to training a single model with
default hyperparameters."
A test suite of prompt injection attacks for LLM-based machine translation,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"LLM-based NLP systems typically work by embedding their input data into
prompt templates which contain instructions and/or in-context examples,
creating queries which are submitted to a LLM, and then parsing the LLM
response in order to generate the system outputs. Prompt Injection Attacks
(PIAs) are a type of subversion of these systems where a malicious user crafts
special inputs which interfere with the prompt templates, causing the LLM to
respond in ways unintended by the system designer.
  Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based
machine translation. Specifically, the task is to translate questions from the
TruthfulQA test suite, where an adversarial prompt is prepended to the
questions, instructing the system to ignore the translation instruction and
answer the questions instead.
  In this test suite, we extend this approach to all the language pairs of the
WMT 2024 General Machine Translation task. Moreover, we include additional
attack formats in addition to the one originally studied."
Named Clinical Entity Recognition Benchmark,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"This technical report introduces a Named Clinical Entity Recognition
Benchmark for evaluating language models in healthcare, addressing the crucial
natural language processing (NLP) task of extracting structured information
from clinical narratives to support applications like automated coding,
clinical trial cohort identification, and clinical decision support.
  The leaderboard provides a standardized platform for assessing diverse
language models, including encoder and decoder architectures, on their ability
to identify and classify clinical entities across multiple medical domains. A
curated collection of openly available clinical datasets is utilized,
encompassing entities such as diseases, symptoms, medications, procedures, and
laboratory measurements. Importantly, these entities are standardized according
to the Observational Medical Outcomes Partnership (OMOP) Common Data Model,
ensuring consistency and interoperability across different healthcare systems
and datasets, and a comprehensive evaluation of model performance. Performance
of models is primarily assessed using the F1-score, and it is complemented by
various assessment modes to provide comprehensive insights into model
performance. The report also includes a brief analysis of models evaluated to
date, highlighting observed trends and limitations.
  By establishing this benchmarking framework, the leaderboard aims to promote
transparency, facilitate comparative analyses, and drive innovation in clinical
entity recognition tasks, addressing the need for robust evaluation methods in
healthcare NLP."
Can LLMs plan paths with extra hints from solvers?,cs.AI,Artificial Intelligence,2024-10-07,"Large Language Models (LLMs) have shown remarkable capabilities in natural
language processing, mathematical problem solving, and tasks related to program
synthesis. However, their effectiveness in long-term planning and higher-order
reasoning has been noted to be limited and fragile. This paper explores an
approach for enhancing LLM performance in solving a classical robotic planning
task by integrating solver-generated feedback. We explore four different
strategies for providing feedback, including visual feedback, we utilize
fine-tuning, and we evaluate the performance of three different LLMs across a
10 standard and 100 more randomly generated planning problems. Our results
suggest that the solver-generated feedback improves the LLM's ability to solve
the moderately difficult problems, but the harder problems still remain out of
reach. The study provides detailed analysis of the effects of the different
hinting strategies and the different planning tendencies of the evaluated LLMs."
PhotoReg: Photometrically Registering 3D Gaussian Splatting Models,cs.RO,Robotics,2024-10-07,"Building accurate representations of the environment is critical for
intelligent robots to make decisions during deployment. Advances in
photorealistic environment models have enabled robots to develop
hyper-realistic reconstructions, which can be used to generate images that are
intuitive for human inspection. In particular, the recently introduced
\ac{3DGS}, which describes the scene with up to millions of primitive
ellipsoids, can be rendered in real time. \ac{3DGS} has rapidly gained
prominence. However, a critical unsolved problem persists: how can we fuse
multiple \ac{3DGS} into a single coherent model? Solving this problem will
enable robot teams to jointly build \ac{3DGS} models of their surroundings. A
key insight of this work is to leverage the {duality} between photorealistic
reconstructions, which render realistic 2D images from 3D structure, and
\emph{3D foundation models}, which predict 3D structure from image pairs. To
this end, we develop PhotoReg, a framework to register multiple photorealistic
\ac{3DGS} models with 3D foundation models. As \ac{3DGS} models are generally
built from monocular camera images, they have \emph{arbitrary scale}. To
resolve this, PhotoReg actively enforces scale consistency among the different
\ac{3DGS} models by considering depth estimates within these models. Then, the
alignment is iteratively refined with fine-grained photometric losses to
produce high-quality fused \ac{3DGS} models. We rigorously evaluate PhotoReg on
both standard benchmark datasets and our custom-collected datasets, including
with two quadruped robots. The code is released at
\url{ziweny11.github.io/photoreg}."
Systematic Literature Review of Vision-Based Approaches to Outdoor Livestock Monitoring with Lessons from Wildlife Studies,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Precision livestock farming (PLF) aims to improve the health and welfare of
livestock animals and farming outcomes through the use of advanced
technologies. Computer vision, combined with recent advances in machine
learning and deep learning artificial intelligence approaches, offers a
possible solution to the PLF ideal of 24/7 livestock monitoring that helps
facilitate early detection of animal health and welfare issues. However, a
significant number of livestock species are raised in large outdoor habitats
that pose technological challenges for computer vision approaches. This review
provides a comprehensive overview of computer vision methods and open
challenges in outdoor animal monitoring. We include research from both the
livestock and wildlife fields in the review because of the similarities in
appearance, behaviour, and habitat for many livestock and wildlife. We focus on
large terrestrial mammals, such as cattle, horses, deer, goats, sheep, koalas,
giraffes, and elephants. We use an image processing pipeline to frame our
discussion and highlight the current capabilities and open technical challenges
at each stage of the pipeline. The review found a clear trend towards the use
of deep learning approaches for animal detection, counting, and multi-species
classification. We discuss in detail the applicability of current vision-based
methods to PLF contexts and promising directions for future research."
GARField: Addressing the visual Sim-to-Real gap in garment manipulation with mesh-attached radiance fields,cs.RO,Robotics,2024-10-07,"While humans intuitively manipulate garments and other textiles items swiftly
and accurately, it is a significant challenge for robots. A factor crucial to
the human performance is the ability to imagine, a priori, the intended result
of the manipulation intents and hence develop predictions on the garment pose.
This allows us to plan from highly obstructed states, adapt our plans as we
collect more information and react swiftly to unforeseen circumstances. Robots,
on the other hand, struggle to establish such intuitions and form tight links
between plans and observations. This can be attributed in part to the high cost
of obtaining densely labelled data for textile manipulation, both in quality
and quantity. The problem of data collection is a long standing issue in
data-based approaches to garment manipulation. Currently, the generation of
high quality and labelled garment manipulation data is mainly attempted through
advanced data capture procedures that create simplified state estimations from
real-world observations. In this work, however, we propose to generate
real-world observations from given object states. To achieve this, we present
GARField (Garment Attached Radiance Field) a differentiable rendering
architecture allowing data generation from simulated states stored as triangle
meshes. Code will be available on https://ddonatien.github.io/garfield-website/"
Over-the-Air Federated Learning in Cell-Free MIMO with Long-term Power Constraint,cs.LG,Machine Learning,2024-10-07,"Wireless networks supporting artificial intelligence have gained significant
attention, with Over-the-Air Federated Learning emerging as a key application
due to its unique transmission and distributed computing characteristics. This
paper derives error bounds for Over-the-Air Federated Learning in a Cell-free
MIMO system and formulates an optimization problem to minimize optimality gap
via joint optimization of power control and beamforming. We introduce the
MOP-LOFPC algorithm, which employs Lyapunov optimization to decouple long-term
constraints across rounds while requiring only causal channel state
information. Experimental results demonstrate that MOP-LOFPC achieves a better
and more flexible trade-off between the model's training loss and adherence to
long-term power constraints compared to existing baselines."
Active Fine-Tuning of Generalist Policies,cs.LG,Machine Learning,2024-10-07,"Pre-trained generalist policies are rapidly gaining relevance in robot
learning due to their promise of fast adaptation to novel, in-domain tasks.
This adaptation often relies on collecting new demonstrations for a specific
task of interest and applying imitation learning algorithms, such as behavioral
cloning. However, as soon as several tasks need to be learned, we must decide
which tasks should be demonstrated and how often? We study this multi-task
problem and explore an interactive framework in which the agent adaptively
selects the tasks to be demonstrated. We propose AMF (Active Multi-task
Fine-tuning), an algorithm to maximize multi-task policy performance under a
limited demonstration budget by collecting demonstrations yielding the largest
information gain on the expert policy. We derive performance guarantees for AMF
under regularity assumptions and demonstrate its empirical effectiveness to
efficiently fine-tune neural policies in complex and high-dimensional
environments."
DEPT: Decoupled Embeddings for Pre-training Language Models,cs.LG,Machine Learning,2024-10-07,"Language Model pre-training benefits from a broader data mixture to enhance
performance across domains and languages. However, training on such
heterogeneous text corpora is complex, requiring extensive and cost-intensive
efforts. Since these data sources vary in lexical, syntactic, and semantic
aspects, they cause negative interference or the ""curse of multilinguality"". We
propose a novel pre-training framework to alleviate this curse. Our method,
DEPT, decouples the embedding layers from the transformer body while
simultaneously training the latter in multiple contexts. DEPT enables the model
to train without being bound to a shared global vocabulary. DEPT: (1) can train
robustly and effectively under significant data heterogeneity, (2) reduces the
parameter count of the token embeddings by up to 80% and the communication
costs by 675x for billion-scale models (3) enhances model generalization and
plasticity in adapting to new languages and domains, and (4) allows training
with custom optimized vocabulary per data source. We prove DEPT's potential by
performing the first vocabulary-agnostic federated multilingual pre-training of
a 1.3 billion-parameter model across high and low-resource languages, reducing
its parameter count by 409 million."
FRIDA: Free-Rider Detection using Privacy Attacks,cs.LG,Machine Learning,2024-10-07,"Federated learning is increasingly popular as it enables multiple parties
with limited datasets and resources to train a high-performing machine learning
model collaboratively. However, similarly to other collaborative systems,
federated learning is vulnerable to free-riders -- participants who do not
contribute to the training but still benefit from the shared model. Free-riders
not only compromise the integrity of the learning process but also slow down
the convergence of the global model, resulting in increased costs for the
honest participants.
  To address this challenge, we propose FRIDA: free-rider detection using
privacy attacks, a framework that leverages inference attacks to detect
free-riders. Unlike traditional methods that only capture the implicit effects
of free-riding, FRIDA directly infers details of the underlying training
datasets, revealing characteristics that indicate free-rider behaviour. Through
extensive experiments, we demonstrate that membership and property inference
attacks are effective for this purpose. Our evaluation shows that FRIDA
outperforms state-of-the-art methods, especially in non-IID settings."
Enhanced Multi-Robot SLAM System with Cross-Validation Matching and Exponential Threshold Keyframe Selection,cs.RO,Robotics,2024-10-07,"The evolving field of mobile robotics has indeed increased the demand for
simultaneous localization and mapping (SLAM) systems. To augment the
localization accuracy and mapping efficacy of SLAM, we refined the core module
of the SLAM system. Within the feature matching phase, we introduced
cross-validation matching to filter out mismatches. In the keyframe selection
strategy, an exponential threshold function is constructed to quantify the
keyframe selection process. Compared with a single robot, the multi-robot
collaborative SLAM (CSLAM) system substantially improves task execution
efficiency and robustness. By employing a centralized structure, we formulate a
multi-robot SLAM system and design a coarse-to-fine matching approach for
multi-map point cloud registration. Our system, built upon ORB-SLAM3, underwent
extensive evaluation utilizing the TUM RGB-D, EuRoC MAV, and TUM_VI datasets.
The experimental results demonstrate a significant improvement in the
positioning accuracy and mapping quality of our enhanced algorithm compared to
those of ORB-SLAM3, with a 12.90% reduction in the absolute trajectory error."
T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data,cs.LG,Machine Learning,2024-10-07,"Self-supervision is often used for pre-training to foster performance on a
downstream task by constructing meaningful representations of samples.
Self-supervised learning (SSL) generally involves generating different views of
the same sample and thus requires data augmentations that are challenging to
construct for tabular data. This constitutes one of the main challenges of
self-supervision for structured data. In the present work, we propose a novel
augmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on
a Joint Embedding Predictive Architecture (JEPA) and is akin to mask
reconstruction in the latent space. It involves predicting the latent
representation of one subset of features from the latent representation of a
different subset within the same sample, thereby learning rich representations
without augmentations. We use our method as a pre-training technique and train
several deep classifiers on the obtained representation. Our experimental
results demonstrate a substantial improvement in both classification and
regression tasks, outperforming models trained directly on samples in their
original data space. Moreover, T-JEPA enables some methods to consistently
outperform or match the performance of traditional methods likes Gradient
Boosted Decision Trees. To understand why, we extensively characterize the
obtained representations and show that T-JEPA effectively identifies relevant
features for downstream tasks without access to the labels. Additionally, we
introduce regularization tokens, a novel regularization method critical for
training of JEPA-based models on structured data."
Anticipating Human Behavior for Safe Navigation and Efficient Collaborative Manipulation with Mobile Service Robots,cs.RO,Robotics,2024-10-07,"The anticipation of human behavior is a crucial capability for robots to
interact with humans safely and efficiently. We employ a smart edge sensor
network to provide global observations along with future predictions and goal
information to integrate anticipatory behavior for the control of a mobile
manipulation robot. We present approaches to anticipate human behavior in the
context of safe navigation and a collaborative mobile manipulation task. First,
we anticipate human motion by employing projections of human trajectories from
smart edge sensor network observations into the planning map of a mobile robot.
Second, we anticipate human intentions in a collaborative furniture-carrying
task to achieve a given goal. Our experiments indicate that anticipating human
behavior allows for safer navigation and more efficient collaboration. Finally,
we showcase an integrated system that anticipates human behavior and
collaborates with a human to achieve a target room layout, including the
placement of tables and chairs."
Towards a Categorical Foundation of Deep Learning: A Survey,cs.LG,Machine Learning,2024-10-07,"The unprecedented pace of machine learning research has lead to incredible
advances, but also poses hard challenges. At present, the field lacks strong
theoretical underpinnings, and many important achievements stem from ad hoc
design choices which are hard to justify in principle and whose effectiveness
often goes unexplained. Research debt is increasing and many papers are found
not to be reproducible.
  This thesis is a survey that covers some recent work attempting to study
machine learning categorically. Category theory is a branch of abstract
mathematics that has found successful applications in many fields, both inside
and outside mathematics. Acting as a lingua franca of mathematics and science,
category theory might be able to give a unifying structure to the field of
machine learning. This could solve some of the aforementioned problems.
  In this work, we mainly focus on the application of category theory to deep
learning. Namely, we discuss the use of categorical optics to model
gradient-based learning, the use of categorical algebras and integral
transforms to link classical computer science to neural networks, the use of
functors to link different layers of abstraction and preserve structure, and,
finally, the use of string diagrams to provide detailed representations of
neural network architectures."
Recent Advances of Multimodal Continual Learning: A Comprehensive Survey,cs.LG,Machine Learning,2024-10-07,"Continual learning (CL) aims to empower machine learning models to learn
continually from new data, while building upon previously acquired knowledge
without forgetting. As machine learning models have evolved from small to large
pre-trained architectures, and from supporting unimodal to multimodal data,
multimodal continual learning (MMCL) methods have recently emerged. The primary
challenge of MMCL is that it goes beyond a simple stacking of unimodal CL
methods, as such straightforward approaches often yield unsatisfactory
performance. In this work, we present the first comprehensive survey on MMCL.
We provide essential background knowledge and MMCL settings, as well as a
structured taxonomy of MMCL methods. We categorize existing MMCL methods into
four categories, i.e., regularization-based, architecture-based, replay-based,
and prompt-based methods, explaining their methodologies and highlighting their
key innovations. Additionally, to prompt further research in this field, we
summarize open MMCL datasets and benchmarks, and discuss several promising
future directions for investigation and development. We have also created a
GitHub repository for indexing relevant MMCL papers and open resources
available at https://github.com/LucyDYu/Awesome-Multimodal-Continual-Learning."
Towards the generation of hierarchical attack models from cybersecurity vulnerabilities using language models,cs.CR,Cryptography and Security,2024-10-07,"This paper investigates the use of a pre-trained language model and siamese
network to discern sibling relationships between text-based cybersecurity
vulnerability data. The ultimate purpose of the approach presented in this
paper is towards the construction of hierarchical attack models based on a set
of text descriptions characterising potential/observed vulnerabilities in a
given system. Due to the nature of the data, and the uncertainty sensitive
environment in which the problem is presented, a practically oriented soft
computing approach is necessary. Therefore, a key focus of this work is to
investigate practical questions surrounding the reliability of predicted links
towards the construction of such models, to which end conceptual and practical
challenges and solutions associated with the proposed approach are outlined,
such as dataset complexity and stability of predictions. Accordingly, the
contributions of this paper focus on producing neural networks using a
pre-trained language model for predicting sibling relationships between
cybersecurity vulnerabilities, then outlining how to apply this capability
towards the generation of hierarchical attack models. In addition, two data
sampling mechanisms for tackling data complexity, and a consensus mechanism for
reducing the amount of false positive predictions are outlined. Each of these
approaches is compared and contrasted using empirical results from three sets
of cybersecurity data to determine their effectiveness."
SkillMatch: Evaluating Self-supervised Learning of Skill Relatedness,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Accurately modeling the relationships between skills is a crucial part of
human resources processes such as recruitment and employee development. Yet, no
benchmarks exist to evaluate such methods directly. We construct and release
SkillMatch, a benchmark for the task of skill relatedness, based on expert
knowledge mining from millions of job ads. Additionally, we propose a scalable
self-supervised learning technique to adapt a Sentence-BERT model based on
skill co-occurrence in job ads. This new method greatly surpasses traditional
models for skill relatedness as measured on SkillMatch. By releasing SkillMatch
publicly, we aim to contribute a foundation for research towards increased
accuracy and transparency of skill-based recommendation systems."
Conditional Variational Autoencoders for Probabilistic Pose Regression,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Robots rely on visual relocalization to estimate their pose from camera
images when they lose track. One of the challenges in visual relocalization is
repetitive structures in the operation environment of the robot. This calls for
probabilistic methods that support multiple hypotheses for robot's pose. We
propose such a probabilistic method to predict the posterior distribution of
camera poses given an observed image. Our proposed training strategy results in
a generative model of camera poses given an image, which can be used to draw
samples from the pose posterior distribution. Our method is streamlined and
well-founded in theory and outperforms existing methods on localization in
presence of ambiguities."
Efficient Model-Based Reinforcement Learning Through Optimistic Thompson Sampling,cs.LG,Machine Learning,2024-10-07,"Learning complex robot behavior through interactions with the environment
necessitates principled exploration. Effective strategies should prioritize
exploring regions of the state-action space that maximize rewards, with
optimistic exploration emerging as a promising direction aligned with this idea
and enabling sample-efficient reinforcement learning. However, existing methods
overlook a crucial aspect: the need for optimism to be informed by a belief
connecting the reward and state. To address this, we propose a practical,
theoretically grounded approach to optimistic exploration based on Thompson
sampling. Our model structure is the first that allows for reasoning about
joint uncertainty over transitions and rewards. We apply our method on a set of
MuJoCo and VMAS continuous control tasks. Our experiments demonstrate that
optimistic exploration significantly accelerates learning in environments with
sparse rewards, action penalties, and difficult-to-explore regions.
Furthermore, we provide insights into when optimism is beneficial and emphasize
the critical role of model uncertainty in guiding exploration."
A Meta-Complexity Characterization of Quantum Cryptography,cs.CR,Cryptography and Security,2024-10-07,"We prove the first meta-complexity characterization of a quantum
cryptographic primitive. We show that one-way puzzles exist if and only if
there is some quantum samplable distribution of binary strings over which it is
hard to approximate Kolmogorov complexity. Therefore, we characterize one-way
puzzles by the average-case hardness of a uncomputable problem. This brings to
the quantum setting a recent line of work that characterizes classical
cryptography with the average-case hardness of a meta-complexity problem,
initiated by Liu and Pass. Moreover, since the average-case hardness of
Kolmogorov complexity over classically polynomial-time samplable distributions
characterizes one-way functions, this result poses one-way puzzles as a natural
generalization of one-way functions to the quantum setting. Furthermore, our
equivalence goes through probability estimation, giving us the additional
equivalence that one-way puzzles exist if and only if there is a quantum
samplable distribution over which probability estimation is hard. We also
observe that the oracle worlds of defined by Kretschmer et. al. rule out any
relativizing characterization of one-way puzzles by the hardness of a problem
in NP or QMA, which means that it may not be possible with current techniques
to characterize one-way puzzles with another meta-complexity problem."
RoWeeder: Unsupervised Weed Mapping through Crop-Row Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Precision agriculture relies heavily on effective weed management to ensure
robust crop yields. This study presents RoWeeder, an innovative framework for
unsupervised weed mapping that combines crop-row detection with a
noise-resilient deep learning model. By leveraging crop-row information to
create a pseudo-ground truth, our method trains a lightweight deep learning
model capable of distinguishing between crops and weeds, even in the presence
of noisy data. Evaluated on the WeedMap dataset, RoWeeder achieves an F1 score
of 75.3, outperforming several baselines. Comprehensive ablation studies
further validated the model's performance. By integrating RoWeeder with drone
technology, farmers can conduct real-time aerial surveys, enabling precise weed
management across large fields. The code is available at:
\url{https://github.com/pasqualedem/RoWeeder}."
"On the Rigour of Scientific Writing: Criteria, Analysis, and Insights",cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Rigour is crucial for scientific research as it ensures the reproducibility
and validity of results and findings. Despite its importance, little work
exists on modelling rigour computationally, and there is a lack of analysis on
whether these criteria can effectively signal or measure the rigour of
scientific papers in practice. In this paper, we introduce a bottom-up,
data-driven framework to automatically identify and define rigour criteria and
assess their relevance in scientific writing. Our framework includes rigour
keyword extraction, detailed rigour definition generation, and salient criteria
identification. Furthermore, our framework is domain-agnostic and can be
tailored to the evaluation of scientific rigour for different areas,
accommodating the distinct salient criteria across fields. We conducted
comprehensive experiments based on datasets collected from two high impact
venues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate the
effectiveness of our framework in modelling rigour. In addition, we analyse
linguistic patterns of rigour, revealing that framing certainty is crucial for
enhancing the perception of scientific rigour, while suggestion certainty and
probability uncertainty diminish it."
Comparison of marker-less 2D image-based methods for infant pose estimation,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"There are increasing efforts to automate clinical methods for early diagnosis
of developmental disorders, among them the General Movement Assessment (GMA), a
video-based tool to classify infant motor functioning. Optimal pose estimation
is a crucial part of the automated GMA. In this study we compare the
performance of available generic- and infant-pose estimators, and the choice of
viewing angle for optimal recordings, i.e., conventional diagonal view used in
GMA vs. top-down view. For this study, we used 4500 annotated video-frames from
75 recordings of infant spontaneous motor functions from 4 to 26 weeks. To
determine which available pose estimation method and camera angle yield the
best pose estimation accuracy on infants in a GMA related setting, the distance
to human annotations as well as the percentage of correct key-points (PCK) were
computed and compared. The results show that the best performing generic model
trained on adults, ViTPose, also performs best on infants. We see no
improvement from using specialized infant-pose estimators over the generic pose
estimators on our own infant dataset. However, when retraining a generic model
on our data, there is a significant improvement in pose estimation accuracy.
The pose estimation accuracy obtained from the top-down view is significantly
better than that obtained from the diagonal view, especially for the detection
of the hip key-points. The results also indicate only limited generalization
capabilities of infant-pose estimators to other infant datasets, which hints
that one should be careful when choosing infant pose estimators and using them
on infant datasets which they were not trained on. While the standard GMA
method uses a diagonal view for assessment, pose estimation accuracy
significantly improves using a top-down view. This suggests that a top-down
view should be included in recording setups for automated GMA research."
6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric Rendering,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Novel view synthesis has advanced significantly with the development of
neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However,
achieving high quality without compromising real-time rendering remains
challenging, particularly for physically-based ray tracing with view-dependent
effects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D
spatial-angular representation to better incorporate view-dependent effects,
but the Gaussian representation and control scheme are sub-optimal. In this
paper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS),
which enhances color and opacity representations and leverages the additional
directional information in the 6D space for optimized Gaussian control. Our
approach is fully compatible with the 3DGS framework and significantly improves
real-time radiance field rendering by better modeling view-dependent effects
and fine details. Experiments demonstrate that 6DGS significantly outperforms
3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction
of 66.5% Gaussian points compared to 3DGS."
L-C4: Language-Based Video Colorization for Creative and Consistent Color,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Automatic video colorization is inherently an ill-posed problem because each
monochrome frame has multiple optional color candidates. Previous
exemplar-based video colorization methods restrict the user's imagination due
to the elaborate retrieval process. Alternatively, conditional image
colorization methods combined with post-processing algorithms still struggle to
maintain temporal consistency. To address these issues, we present
Language-based video Colorization for Creative and Consistent Colors (L-C4) to
guide the colorization process using user-provided language descriptions. Our
model is built upon a pre-trained cross-modality generative model, leveraging
its comprehensive language understanding and robust color representation
abilities. We introduce the cross-modality pre-fusion module to generate
instance-aware text embeddings, enabling the application of creative colors.
Additionally, we propose temporally deformable attention to prevent flickering
or color shifts, and cross-clip fusion to maintain long-term color consistency.
Extensive experimental results demonstrate that L-C4 outperforms relevant
methods, achieving semantically accurate colors, unrestricted creative
correspondence, and temporally robust consistency."
Collaboration! Towards Robust Neural Methods for Routing Problems,cs.AI,Artificial Intelligence,2024-10-07,"Despite enjoying desirable efficiency and reduced reliance on domain
expertise, existing neural methods for vehicle routing problems (VRPs) suffer
from severe robustness issues -- their performance significantly deteriorates
on clean instances with crafted perturbations. To enhance robustness, we
propose an ensemble-based Collaborative Neural Framework (CNF) w.r.t. the
defense of neural VRP methods, which is crucial yet underexplored in the
literature. Given a neural VRP method, we adversarially train multiple models
in a collaborative manner to synergistically promote robustness against
attacks, while boosting standard generalization on clean instances. A neural
router is designed to adeptly distribute training instances among models,
enhancing overall load balancing and collaborative efficacy. Extensive
experiments verify the effectiveness and versatility of CNF in defending
against various attacks across different neural VRP methods. Notably, our
approach also achieves impressive out-of-distribution generalization on
benchmark instances."
Revealing Directions for Text-guided 3D Face Editing,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"3D face editing is a significant task in multimedia, aimed at the
manipulation of 3D face models across various control signals. The success of
3D-aware GAN provides expressive 3D models learned from 2D single-view images
only, encouraging researchers to discover semantic editing directions in its
latent space. However, previous methods face challenges in balancing quality,
efficiency, and generalization. To solve the problem, we explore the
possibility of introducing the strength of diffusion model into 3D-aware GANs.
In this paper, we present Face Clan, a fast and text-general approach for
generating and manipulating 3D faces based on arbitrary attribute descriptions.
To achieve disentangled editing, we propose to diffuse on the latent space
under a pair of opposite prompts to estimate the mask indicating the region of
interest on latent codes. Based on the mask, we then apply denoising to the
masked latent codes to reveal the editing direction. Our method offers a
precisely controllable manipulation method, allowing users to intuitively
customize regions of interest with the text description. Experiments
demonstrate the effectiveness and generalization of our Face Clan for various
pre-trained GANs. It offers an intuitive and wide application for text-guided
face editing that contributes to the landscape of multimedia content creation."
Activation Scaling for Steering and Interpreting Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Given the prompt ""Rome is in"", can we steer a language model to flip its
prediction of an incorrect token ""France"" to a correct token ""Italy"" by only
multiplying a few relevant activation vectors with scalars? We argue that
successfully intervening on a model is a prerequisite for interpreting its
internal workings. Concretely, we establish a three-term objective: a
successful intervention should flip the correct with the wrong token and vice
versa (effectiveness), and leave other tokens unaffected (faithfulness), all
while being sparse (minimality). Using gradient-based optimization, this
objective lets us learn (and later evaluate) a specific kind of efficient and
interpretable intervention: activation scaling only modifies the signed
magnitude of activation vectors to strengthen, weaken, or reverse the steering
directions already encoded in the model. On synthetic tasks, this intervention
performs comparably with steering vectors in terms of effectiveness and
faithfulness, but is much more minimal allowing us to pinpoint interpretable
model components. We evaluate activation scaling from different angles, compare
performance on different datasets, and make activation scalars a learnable
function of the activation vectors themselves to generalize to varying-length
prompts."
On Efficient Variants of Segment Anything Model: A Survey,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"The Segment Anything Model (SAM) is a foundational model for image
segmentation tasks, known for its strong generalization across diverse
applications. However, its impressive performance comes with significant
computational and resource demands, making it challenging to deploy in
resource-limited environments such as mobile devices. To address this, a
variety of SAM variants have been proposed to enhance efficiency without
sacrificing accuracy. This survey provides the first comprehensive review of
these efficient SAM variants. We begin by exploring the motivations driving
this research. We then present core techniques used in SAM and model
acceleration. This is followed by an in-depth analysis of various acceleration
strategies, categorized by approach. Finally, we offer a unified and extensive
evaluation of these methods, assessing their efficiency and accuracy on
representative benchmarks, and providing a clear comparison of their overall
performance."
Failure-Proof Non-Contrastive Self-Supervised Learning,cs.LG,Machine Learning,2024-10-07,"We identify sufficient conditions to avoid known failure modes, including
representation, dimensional, cluster and intracluster collapses, occurring in
non-contrastive self-supervised learning. Based on these findings, we propose a
principled design for the projector and loss function. We theoretically
demonstrate that this design introduces an inductive bias that promotes
learning representations that are both decorrelated and clustered without
explicit enforcing these properties and leading to improved generalization. To
the best of our knowledge, this is the first solution that achieves robust
training with respect to these failure modes while guaranteeing enhanced
generalization performance in downstream tasks. We validate our theoretical
findings on image datasets including SVHN, CIFAR10, CIFAR100 and ImageNet-100,
and show that our solution, dubbed FALCON, outperforms existing feature
decorrelation and cluster-based self-supervised learning methods in terms of
generalization to clustering and linear classification tasks."
Maximizing the practical achievability of quantum annealing attacks on factorization-based cryptography,cs.CR,Cryptography and Security,2024-10-07,"This work focuses on quantum methods for cryptanalysis of schemes based on
the integer factorization problem and the discrete logarithm problem. We
demonstrate how to practically solve the largest instances of the factorization
problem by improving an approach that combines quantum and classical
computations, assuming the use of the best publicly available special-class
quantum computer: the quantum annealer. We achieve new computational experiment
results by solving the largest instance of the factorization problem ever
announced as solved using quantum annealing, with a size of 29 bits. The core
idea of the improved approach is to leverage known sub-exponential classical
method to break the problem down into many smaller computations and perform the
most critical ones on a quantum computer. This approach does not reduce the
complexity class, but it assesses the pragmatic capabilities of an attacker. It
also marks a step forward in the development of hybrid methods, which in
practice may surpass classical methods in terms of efficiency sooner than
purely quantum computations will."
Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"In an era where maritime infrastructures are crucial, advanced situational
awareness solutions are increasingly important. The use of optical camera
systems can allow real-time usage of maritime footage. This thesis presents an
investigation into leveraging deep learning and computer vision to advance
real-time ship recognition and georeferencing for the improvement of maritime
situational awareness. A novel dataset, ShipSG, is introduced, containing 3,505
images and 11,625 ship masks with corresponding class and geographic position.
After an exploration of state-of-the-art, a custom real-time segmentation
architecture, ScatYOLOv8+CBAM, is designed for the NVIDIA Jetson AGX Xavier
embedded system. This architecture adds the 2D scattering transform and
attention mechanisms to YOLOv8, achieving an mAP of 75.46% and an 25.3 ms per
frame, outperforming state-of-the-art methods by over 5%. To improve small and
distant ship recognition in high-resolution images on embedded systems, an
enhanced slicing mechanism is introduced, improving mAP by 8% to 11%.
Additionally, a georeferencing method is proposed, achieving positioning errors
of 18 m for ships up to 400 m away and 44 m for ships between 400 m and 1200 m.
The findings are also applied in real-world scenarios, such as the detection of
abnormal ship behaviour, camera integrity assessment and 3D reconstruction. The
approach of this thesis outperforms existing methods and provides a framework
for integrating recognized and georeferenced ships into real-time systems,
enhancing operational effectiveness and decision-making for maritime
stakeholders. This thesis contributes to the maritime computer vision field by
establishing a benchmark for ship segmentation and georeferencing research,
demonstrating the viability of deep-learning-based recognition and
georeferencing methods for real-time maritime monitoring."
Detecting and Approximating Redundant Computational Blocks in Neural Networks,cs.LG,Machine Learning,2024-10-07,"Deep neural networks often learn similar internal representations, both
across different models and within their own layers. While inter-network
similarities have enabled techniques such as model stitching and merging,
intra-network similarities present new opportunities for designing more
efficient architectures. In this paper, we investigate the emergence of these
internal similarities across different layers in diverse neural architectures,
showing that similarity patterns emerge independently of the datataset used. We
introduce a simple metric, Block Redundancy, to detect redundant blocks,
providing a foundation for future architectural optimization methods. Building
on this, we propose Redundant Blocks Approximation (RBA), a general framework
that identifies and approximates one or more redundant computational blocks
using simpler transformations. We show that the transformation $\mathcal{T}$
between two representations can be efficiently computed in closed-form, and it
is enough to replace the redundant blocks from the network. RBA reduces model
parameters and time complexity while maintaining good performance. We validate
our method on classification tasks in the vision domain using a variety of
pretrained foundational models and datasets."
"Next state prediction gives rise to entangled, yet compositional representations of objects",cs.LG,Machine Learning,2024-10-07,"Compositional representations are thought to enable humans to generalize
across combinatorially vast state spaces. Models with learnable object slots,
which encode information about objects in separate latent codes, have shown
promise for this type of generalization but rely on strong architectural
priors. Models with distributed representations, on the other hand, use
overlapping, potentially entangled neural codes, and their ability to support
compositional generalization remains underexplored. In this paper we examine
whether distributed models can develop linearly separable representations of
objects, like slotted models, through unsupervised training on videos of object
interactions. We show that, surprisingly, models with distributed
representations often match or outperform models with object slots in
downstream prediction tasks. Furthermore, we find that linearly separable
object representations can emerge without object-centric priors, with auxiliary
objectives like next-state prediction playing a key role. Finally, we observe
that distributed models' object representations are never fully disentangled,
even if they are linearly separable: Multiple objects can be encoded through
partially overlapping neural populations while still being highly separable
with a linear classifier. We hypothesize that maintaining partially shared
codes enables distributed models to better compress object dynamics,
potentially enhancing generalization."
PRFusion: Toward Effective and Robust Multi-Modal Place Recognition with Image and Point Cloud Fusion,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Place recognition plays a crucial role in the fields of robotics and computer
vision, finding applications in areas such as autonomous driving, mapping, and
localization. Place recognition identifies a place using query sensor data and
a known database. One of the main challenges is to develop a model that can
deliver accurate results while being robust to environmental variations. We
propose two multi-modal place recognition models, namely PRFusion and
PRFusion++. PRFusion utilizes global fusion with manifold metric attention,
enabling effective interaction between features without requiring camera-LiDAR
extrinsic calibrations. In contrast, PRFusion++ assumes the availability of
extrinsic calibrations and leverages pixel-point correspondences to enhance
feature learning on local windows. Additionally, both models incorporate neural
diffusion layers, which enable reliable operation even in challenging
environments. We verify the state-of-the-art performance of both models on
three large-scale benchmarks. Notably, they outperform existing models by a
substantial margin of +3.0 AR@1 on the demanding Boreas dataset. Furthermore,
we conduct ablation studies to validate the effectiveness of our proposed
methods. The codes are available at: https://github.com/sijieaaa/PRFusion"
Training Interactive Agent in Large FPS Game Map with Rule-enhanced Reinforcement Learning,cs.AI,Artificial Intelligence,2024-10-07,"In the realm of competitive gaming, 3D first-person shooter (FPS) games have
gained immense popularity, prompting the development of game AI systems to
enhance gameplay. However, deploying game AI in practical scenarios still poses
challenges, particularly in large-scale and complex FPS games. In this paper,
we focus on the practical deployment of game AI in the online multiplayer
competitive 3D FPS game called Arena Breakout, developed by Tencent Games. We
propose a novel gaming AI system named Private Military Company Agent (PMCA),
which is interactable within a large game map and engages in combat with
players while utilizing tactical advantages provided by the surrounding
terrain.
  To address the challenges of navigation and combat in modern 3D FPS games, we
introduce a method that combines navigation mesh (Navmesh) and shooting-rule
with deep reinforcement learning (NSRL). The integration of Navmesh enhances
the agent's global navigation capabilities while shooting behavior is
controlled using rule-based methods to ensure controllability. NSRL employs a
DRL model to predict when to enable the navigation mesh, resulting in a diverse
range of behaviors for the game AI. Customized rewards for human-like behaviors
are also employed to align PMCA's behavior with that of human players."
OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"We present OmniBooth, an image generation framework that enables spatial
control with instance-level multi-modal customization. For all instances, the
multimodal instruction can be described through text prompts or image
references. Given a set of user-defined masks and associated text or image
guidance, our objective is to generate an image, where multiple objects are
positioned at specified coordinates and their attributes are precisely aligned
with the corresponding guidance. This approach significantly expands the scope
of text-to-image generation, and elevates it to a more versatile and practical
dimension in controllability. In this paper, our core contribution lies in the
proposed latent control signals, a high-dimensional spatial feature that
provides a unified representation to integrate the spatial, textual, and image
conditions seamlessly. The text condition extends ControlNet to provide
instance-level open-vocabulary generation. The image condition further enables
fine-grained control with personalized identity. In practice, our method
empowers users with more flexibility in controllable generation, as users can
choose multi-modal conditions from text or images as needed. Furthermore,
thorough experiments demonstrate our enhanced performance in image synthesis
fidelity and alignment across different tasks and datasets. Project page:
https://len-li.github.io/omnibooth-web/"
Goal-Conditioned Terminal Value Estimation for Real-time and Multi-task Model Predictive Control,cs.RO,Robotics,2024-10-07,"While MPC enables nonlinear feedback control by solving an optimal control
problem at each timestep, the computational burden tends to be significantly
large, making it difficult to optimize a policy within the control period. To
address this issue, one possible approach is to utilize terminal value learning
to reduce computational costs. However, the learned value cannot be used for
other tasks in situations where the task dynamically changes in the original
MPC setup. In this study, we develop an MPC framework with goal-conditioned
terminal value learning to achieve multitask policy optimization while reducing
computational time. Furthermore, by using a hierarchical control structure that
allows the upper-level trajectory planner to output appropriate
goal-conditioned trajectories, we demonstrate that a robot model is able to
generate diverse motions. We evaluate the proposed method on a bipedal inverted
pendulum robot model and confirm that combining goal-conditioned terminal value
learning with an upper-level trajectory planner enables real-time control;
thus, the robot successfully tracks a target trajectory on sloped terrain."
Intent Classification for Bank Chatbots through LLM Fine-Tuning,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"This study evaluates the application of large language models (LLMs) for
intent classification within a chatbot with predetermined responses designed
for banking industry websites. Specifically, the research examines the
effectiveness of fine-tuning SlovakBERT compared to employing multilingual
generative models, such as Llama 8b instruct and Gemma 7b instruct, in both
their pre-trained and fine-tuned versions. The findings indicate that
SlovakBERT outperforms the other models in terms of in-scope accuracy and
out-of-scope false positive rate, establishing it as the benchmark for this
application."
GRU-D Characterizes Age-Specific Temporal Missingness in MIMIC-IV,cs.LG,Machine Learning,2024-10-07,"Temporal missingness, defined as unobserved patterns in time series, and its
predictive potentials represent an emerging area in clinical machine learning.
We trained a gated recurrent unit with decay mechanisms, called GRU-D, for a
binary classification between elderly - and young patients. We extracted time
series for 5 vital signs from MIMIC-IV as model inputs. GRU-D was evaluated
with means of 0.780 AUROC and 0.810 AUPRC on bootstrapped data. Interpreting
trained model parameters, we found differences in blood pressure missingness
and respiratory rate missingness as important predictors learned by
parameterized hidden gated units. We successfully showed how GRU-D can be used
to reveal patterns in temporal missingness building the basis of novel research
directions."
Defense-as-a-Service: Black-box Shielding against Backdoored Graph Models,cs.LG,Machine Learning,2024-10-07,"With the trend of large graph learning models, business owners tend to employ
a model provided by a third party to deliver business services to users.
However, these models might be backdoored, and malicious users can submit
trigger-embedded inputs to manipulate the model predictions. Current graph
backdoor defenses have several limitations: 1) depending on model-related
details, 2) requiring additional model fine-tuning, and 3) relying upon extra
explainability tools, all of which are infeasible under stringent privacy
policies. To address those limitations, we propose GraphProt, which allows
resource-constrained business owners to rely on third parties to avoid backdoor
attacks on GNN-based graph classifiers. Our GraphProt is model-agnostic and
only relies on the input graph. The key insight is to leverage subgraph
information for prediction, thereby mitigating backdoor effects induced by
triggers. GraphProt comprises two components: clustering-based trigger
elimination and robust subgraph ensemble. Specifically, we first propose
feature-topology clustering that aims to remove most of the anomalous subgraphs
(triggers). Moreover, we design subgraph sampling strategies based on
feature-topology clustering to build a robust classifier via majority vote.
Experimental results across three backdoor attacks and six benchmark datasets
demonstrate that GraphProt significantly reduces the backdoor attack success
rate while preserving the model accuracy on regular graph classification tasks."
SoK: Towards Security and Safety of Edge AI,cs.CR,Cryptography and Security,2024-10-07,"Advanced AI applications have become increasingly available to a broad
audience, e.g., as centrally managed large language models (LLMs). Such
centralization is both a risk and a performance bottleneck - Edge AI promises
to be a solution to these problems. However, its decentralized approach raises
additional challenges regarding security and safety. In this paper, we argue
that both of these aspects are critical for Edge AI, and even more so, their
integration. Concretely, we survey security and safety threats, summarize
existing countermeasures, and collect open challenges as a call for more
research in this area."
Low-Rank Continual Personalization of Diffusion Models,cs.LG,Machine Learning,2024-10-07,"Recent personalization methods for diffusion models, such as Dreambooth,
allow fine-tuning pre-trained models to generate new concepts. However,
applying these techniques across multiple tasks in order to include, e.g.,
several new objects or styles, leads to mutual interference between their
adapters. While recent studies attempt to mitigate this issue by combining
trained adapters across tasks after fine-tuning, we adopt a more rigorous
regime and investigate the personalization of large diffusion models under a
continual learning scenario, where such interference leads to catastrophic
forgetting of previous knowledge. To that end, we evaluate the na\""ive
continual fine-tuning of customized models and compare this approach with three
methods for consecutive adapters' training: sequentially merging new adapters,
merging orthogonally initialized adapters, and updating only relevant
parameters according to the task. In our experiments, we show that the proposed
approaches mitigate forgetting when compared to the na\""ive approach."
D-PoSE: Depth as an Intermediate Representation for 3D Human Pose and Shape Estimation,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"We present D-PoSE (Depth as an Intermediate Representation for 3D Human Pose
and Shape Estimation), a one-stage method that estimates human pose and SMPL-X
shape parameters from a single RGB image. Recent works use larger models with
transformer backbones and decoders to improve the accuracy in human pose and
shape (HPS) benchmarks. D-PoSE proposes a vision based approach that uses the
estimated human depth-maps as an intermediate representation for HPS and
leverages training with synthetic data and the ground-truth depth-maps provided
with them for depth supervision during training. Although trained on synthetic
datasets, D-PoSE achieves state-of-the-art performance on the real-world
benchmark datasets, EMDB and 3DPW. Despite its simple lightweight design and
the CNN backbone, it outperforms ViT-based models that have a number of
parameters that is larger by almost an order of magnitude. D-PoSE code is
available at: https://github.com/nvasilik/D-PoSE"
ResTNet: Defense against Adversarial Policies via Transformer in Computer Go,cs.LG,Machine Learning,2024-10-07,"Although AlphaZero has achieved superhuman levels in Go, recent research has
highlighted its vulnerability in particular situations requiring a more
comprehensive understanding of the entire board. To address this challenge,
this paper introduces ResTNet, a network that interleaves residual networks and
Transformer. Our empirical experiments demonstrate several advantages of using
ResTNet. First, it not only improves playing strength but also enhances the
ability of global information. Second, it defends against an adversary Go
program, called cyclic-adversary, tailor-made for attacking AlphaZero
algorithms, significantly reducing the average probability of being attacked
rate from 70.44% to 23.91%. Third, it improves the accuracy from 59.15% to
80.01% in correctly recognizing ladder patterns, which are one of the
challenging patterns for Go AIs. Finally, ResTNet offers a potential
explanation of the decision-making process and can also be applied to other
games like Hex. To the best of our knowledge, ResTNet is the first to integrate
residual networks and Transformer in the context of AlphaZero for board games,
suggesting a promising direction for enhancing AlphaZero's global
understanding."
Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural Collapse,cs.LG,Machine Learning,2024-10-07,"Deep neural networks (DNNs) at convergence consistently represent the
training data in the last layer via a highly symmetric geometric structure
referred to as neural collapse. This empirical evidence has spurred a line of
theoretical research aimed at proving the emergence of neural collapse, mostly
focusing on the unconstrained features model. Here, the features of the
penultimate layer are free variables, which makes the model data-agnostic and,
hence, puts into question its ability to capture DNN training. Our work
addresses the issue, moving away from unconstrained features and studying DNNs
that end with at least two linear layers. We first prove generic guarantees on
neural collapse that assume (i) low training error and balancedness of the
linear layers (for within-class variability collapse), and (ii) bounded
conditioning of the features before the linear part (for orthogonality of
class-means, as well as their alignment with weight matrices). We then show
that such assumptions hold for gradient descent training with weight decay: (i)
for networks with a wide first layer, we prove low training error and
balancedness, and (ii) for solutions that are either nearly optimal or stable
under large learning rates, we additionally prove the bounded conditioning.
Taken together, our results are the first to show neural collapse in the
end-to-end training of DNNs."
Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Visual language pre-training (VLP) models have demonstrated significant
success across various domains, yet they remain vulnerable to adversarial
attacks. Addressing these adversarial vulnerabilities is crucial for enhancing
security in multimodal learning. Traditionally, adversarial methods targeting
VLP models involve simultaneously perturbing images and text. However, this
approach faces notable challenges: first, adversarial perturbations often fail
to translate effectively into real-world scenarios; second, direct
modifications to the text are conspicuously visible. To overcome these
limitations, we propose a novel strategy that exclusively employs image patches
for attacks, thus preserving the integrity of the original text. Our method
leverages prior knowledge from diffusion models to enhance the authenticity and
naturalness of the perturbations. Moreover, to optimize patch placement and
improve the efficacy of our attacks, we utilize the cross-attention mechanism,
which encapsulates intermodal interactions by generating attention maps to
guide strategic patch placements. Comprehensive experiments conducted in a
white-box setting for image-to-text scenarios reveal that our proposed method
significantly outperforms existing techniques, achieving a 100% attack success
rate. Additionally, it demonstrates commendable performance in transfer tasks
involving text-to-image configurations."
Improving the Sampling Strategy in KernelSHAP,cs.LG,Machine Learning,2024-10-07,"Shapley values are a popular model-agnostic explanation framework for
explaining predictions made by complex machine learning models. The framework
provides feature contribution scores that sum to the predicted response and
represent each feature's importance. The computation of exact Shapley values is
computationally expensive due to estimating an exponential amount of
non-trivial conditional expectations. The KernelSHAP framework enables us to
approximate the Shapley values using a sampled subset of weighted conditional
expectations. We propose three main novel contributions: a stabilizing
technique to reduce the variance of the weights in the current state-of-the-art
strategy, a novel weighing scheme that corrects the Shapley kernel weights
based on sampled subsets, and a straightforward strategy that includes the
important subsets and integrates them with the corrected Shapley kernel
weights. We compare these new approximation strategies against existing ones by
evaluating their Shapley value accuracy as a function of the number of subsets.
The results demonstrate that our sampling strategies significantly enhance the
accuracy of the approximated Shapley value explanations, making them more
reliable in practical applications. This work provides valuable insights and
practical recommendations for researchers and practitioners seeking to
implement Shapley value-based explainability of their models."
Improved detection of discarded fish species through BoxAL active learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"In recent years, powerful data-driven deep-learning techniques have been
developed and applied for automated catch registration. However, these methods
are dependent on the labelled data, which is time-consuming, labour-intensive,
expensive to collect and need expert knowledge. In this study, we present an
active learning technique, named BoxAL, which includes estimation of epistemic
certainty of the Faster R-CNN object-detection model. The method allows
selecting the most uncertain training images from an unlabeled pool, which are
then used to train the object-detection model. To evaluate the method, we used
an open-source image dataset obtained with a dedicated image-acquisition system
developed for commercial trawlers targeting demersal species. We demonstrated,
that our approach allows reaching the same object-detection performance as with
the random sampling using 400 fewer labelled images. Besides, mean AP score was
significantly higher at the last training iteration with 1100 training images,
specifically, 39.0&plusmn;1.6 and 34.8&plusmn;1.8 for certainty-based sampling
and random sampling, respectively. Additionally, we showed that epistemic
certainty is a suitable method to sample images that the current iteration of
the model cannot deal with yet. Our study additionally showed that the sampled
new data is more valuable for training than the remaining unlabeled data. Our
software is available on https://github.com/pieterblok/boxal."
Leveraging Grammar Induction for Language Understanding and Generation,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Grammar induction has made significant progress in recent years. However, it
is not clear how the application of induced grammar could enhance practical
performance in downstream tasks. In this work, we introduce an unsupervised
grammar induction method for language understanding and generation. We
construct a grammar parser to induce constituency structures and dependency
relations, which is simultaneously trained on downstream tasks without
additional syntax annotations. The induced grammar features are subsequently
incorporated into Transformer as a syntactic mask to guide self-attention. We
evaluate and apply our method to multiple machine translation tasks and natural
language understanding tasks. Our method demonstrates superior performance
compared to the original Transformer and other models enhanced with external
parsers. Experimental results indicate that our method is effective in both
from-scratch and pre-trained scenarios. Additionally, our research highlights
the contribution of explicitly modeling the grammatical structure of texts to
neural network models."
AnyAttack: Towards Large-scale Self-supervised Generation of Targeted Adversarial Examples for Vision-Language Models,cs.LG,Machine Learning,2024-10-07,"Due to their multimodal capabilities, Vision-Language Models (VLMs) have
found numerous impactful applications in real-world scenarios. However, recent
studies have revealed that VLMs are vulnerable to image-based adversarial
attacks, particularly targeted adversarial images that manipulate the model to
generate harmful content specified by the adversary. Current attack methods
rely on predefined target labels to create targeted adversarial attacks, which
limits their scalability and applicability for large-scale robustness
evaluations. In this paper, we propose AnyAttack, a self-supervised framework
that generates targeted adversarial images for VLMs without label supervision,
allowing any image to serve as a target for the attack. To address the
limitation of existing methods that require label supervision, we introduce a
contrastive loss that trains a generator on a large-scale unlabeled image
dataset, LAION-400M dataset, for generating targeted adversarial noise. This
large-scale pre-training endows our method with powerful transferability across
a wide range of VLMs. Extensive experiments on five mainstream open-source VLMs
(CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4) across three multimodal tasks
(image-text retrieval, multimodal classification, and image captioning)
demonstrate the effectiveness of our attack. Additionally, we successfully
transfer AnyAttack to multiple commercial VLMs, including Google's Gemini,
Claude's Sonnet, and Microsoft's Copilot. These results reveal an unprecedented
risk to VLMs, highlighting the need for effective countermeasures."
TeX-NeRF: Neural Radiance Fields from Pseudo-TeX Vision,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Neural radiance fields (NeRF) has gained significant attention for its
exceptional visual effects. However, most existing NeRF methods reconstruct 3D
scenes from RGB images captured by visible light cameras. In practical
scenarios like darkness, low light, or bad weather, visible light cameras
become ineffective. Therefore, we propose TeX-NeRF, a 3D reconstruction method
using only infrared images, which introduces the object material emissivity as
a priori, preprocesses the infrared images using Pseudo-TeX vision, and maps
the temperatures (T), emissivities (e), and textures (X) of the scene into the
saturation (S), hue (H), and value (V) channels of the HSV color space,
respectively. Novel view synthesis using the processed images has yielded
excellent results. Additionally, we introduce 3D-TeX Datasets, the first
dataset comprising infrared images and their corresponding Pseudo-TeX vision
images. Experiments demonstrate that our method not only matches the quality of
scene reconstruction achieved with high-quality RGB images but also provides
accurate temperature estimations for objects in the scene."
On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent,cs.LG,Machine Learning,2024-10-07,"The Adam optimizer is widely used for transformer optimization in practice,
which makes understanding the underlying optimization mechanisms an important
problem. However, due to the Adam's complexity, theoretical analysis of how it
optimizes transformers remains a challenging task. Fortunately, Sign Gradient
Descent (SignGD) serves as an effective surrogate for Adam. Despite its
simplicity, theoretical understanding of how SignGD optimizes transformers
still lags behind. In this work, we study how SignGD optimizes a two-layer
transformer -- consisting of a softmax attention layer with trainable query-key
parameterization followed by a linear layer -- on a linearly separable noisy
dataset. We identify four stages in the training dynamics, each exhibiting
intriguing behaviors. Based on the training dynamics, we prove the fast
convergence but poor generalization of the learned transformer on the noisy
dataset. We also show that Adam behaves similarly to SignGD in terms of both
optimization and generalization in this setting. Additionally, we find that the
poor generalization of SignGD is not solely due to data noise, suggesting that
both SignGD and Adam requires high-quality data for real-world tasks. Finally,
experiments on synthetic and real-world datasets empirically support our
theoretical results."
Predictive Spliner: Data-Driven Overtaking in Autonomous Racing Using Opponent Trajectory Prediction,cs.RO,Robotics,2024-10-07,"Head-to-head racing against opponents is a challenging and emerging topic in
the domain of autonomous racing. We propose Predictive Spliner, a data-driven
overtaking planner that learns the behavior of opponents through Gaussian
Process (GP) regression, which is then leveraged to compute viable overtaking
maneuvers in future sections of the racing track. Experimentally validated on a
1:10 scale autonomous racing platform using Light Detection and Ranging (LiDAR)
information to perceive the opponent, Predictive Spliner outperforms
State-of-the-Art (SotA) algorithms by overtaking opponents at up to 83.1% of
its own speed, being on average 8.4% faster than the previous best-performing
method. Additionally, it achieves an average success rate of 84.5%, which is
47.6% higher than the previous best-performing method. The method maintains
computational efficiency with a Central Processing Unit (CPU) load of 22.79%
and a computation time of 8.4 ms, evaluated on a Commercial off-the-Shelf
(CotS) Intel i7-1165G7, making it suitable for real-time robotic applications.
These results highlight the potential of Predictive Spliner to enhance the
performance and safety of autonomous racing vehicles. The code for Predictive
Spliner is available at: https://github.com/ForzaETH/predictive-spliner."
Art Forgery Detection using Kolmogorov Arnold and Convolutional Neural Networks,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Art authentication has historically established itself as a task requiring
profound connoisseurship of one particular artist. Nevertheless, famous art
forgers such as Wolfgang Beltracchi were able to deceive dozens of art experts.
In recent years Artificial Intelligence algorithms have been successfully
applied to various image processing tasks. In this work, we leverage the
growing improvements in AI to present an art authentication framework for the
identification of the forger Wolfgang Beltracchi. Differently from existing
literature on AI-aided art authentication, we focus on a specialized model of a
forger, rather than an artist, flipping the approach of traditional AI methods.
We use a carefully compiled dataset of known artists forged by Beltracchi and a
set of known works by the forger to train a multiclass image classification
model based on EfficientNet. We compare the results with Kolmogorov Arnold
Networks (KAN) which, to the best of our knowledge, have never been tested in
the art domain. The results show a general agreement between the different
models' predictions on artworks flagged as forgeries, which are then closely
studied using visual analysis."
Mastering Chinese Chess AI (Xiangqi) Without Search,cs.LG,Machine Learning,2024-10-07,"We have developed a high-performance Chinese Chess AI that operates without
reliance on search algorithms. This AI has demonstrated the capability to
compete at a level commensurate with the top 0.1\% of human players. By
eliminating the search process typically associated with such systems, this AI
achieves a Queries Per Second (QPS) rate that exceeds those of systems based on
the Monte Carlo Tree Search (MCTS) algorithm by over a thousandfold and
surpasses those based on the AlphaBeta pruning algorithm by more than a
hundredfold. The AI training system consists of two parts: supervised learning
and reinforcement learning. Supervised learning provides an initial human-like
Chinese chess AI, while reinforcement learning, based on supervised learning,
elevates the strength of the entire AI to a new level. Based on this training
system, we carried out enough ablation experiments and discovered that 1. The
same parameter amount of Transformer architecture has a higher performance than
CNN on Chinese chess; 2. Possible moves of both sides as features can greatly
improve the training process; 3. Selective opponent pool, compared to pure
self-play training, results in a faster improvement curve and a higher strength
limit. 4. Value Estimation with Cutoff(VECT) improves the original PPO
algorithm training process and we will give the explanation."
Unsupervised Skill Discovery for Robotic Manipulation through Automatic Task Generation,cs.RO,Robotics,2024-10-07,"Learning skills that interact with objects is of major importance for robotic
manipulation. These skills can indeed serve as an efficient prior for solving
various manipulation tasks. We propose a novel Skill Learning approach that
discovers composable behaviors by solving a large and diverse number of
autonomously generated tasks. Our method learns skills allowing the robot to
consistently and robustly interact with objects in its environment. The
discovered behaviors are embedded in primitives which can be composed with
Hierarchical Reinforcement Learning to solve unseen manipulation tasks. In
particular, we leverage Asymmetric Self-Play to discover behaviors and
Multiplicative Compositional Policies to embed them. We compare our method to
Skill Learning baselines and find that our skills are more interactive.
Furthermore, the learned skills can be used to solve a set of unseen
manipulation tasks, in simulation as well as on a real robotic platform."
TimeCNN: Refining Cross-Variable Interaction on Time Point for Time Series Forecasting,cs.LG,Machine Learning,2024-10-07,"Time series forecasting is extensively applied across diverse domains.
Transformer-based models demonstrate significant potential in modeling
cross-time and cross-variable interaction. However, we notice that the
cross-variable correlation of multivariate time series demonstrates
multifaceted (positive and negative correlations) and dynamic progression over
time, which is not well captured by existing Transformer-based models. To
address this issue, we propose a TimeCNN model to refine cross-variable
interactions to enhance time series forecasting. Its key innovation is
timepoint-independent, where each time point has an independent convolution
kernel, allowing each time point to have its independent model to capture
relationships among variables. This approach effectively handles both positive
and negative correlations and adapts to the evolving nature of variable
relationships over time. Extensive experiments conducted on 12 real-world
datasets demonstrate that TimeCNN consistently outperforms state-of-the-art
models. Notably, our model achieves significant reductions in computational
requirements (approximately 60.46%) and parameter count (about 57.50%), while
delivering inference speeds 3 to 4 times faster than the benchmark iTransformer
model"
PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"In the field of image editing, three core challenges persist:
controllability, background preservation, and efficiency. Inversion-based
methods rely on time-consuming optimization to preserve the features of the
initial images, which results in low efficiency due to the requirement for
extensive network inference. Conversely, inversion-free methods lack
theoretical support for background similarity, as they circumvent the issue of
maintaining initial features to achieve efficiency. As a consequence, none of
these methods can achieve both high efficiency and background consistency. To
tackle the challenges and the aforementioned disadvantages, we introduce
PostEdit, a method that incorporates a posterior scheme to govern the diffusion
sampling process. Specifically, a corresponding measurement term related to
both the initial features and Langevin dynamics is introduced to optimize the
estimated image generated by the given target prompt. Extensive experimental
results indicate that the proposed PostEdit achieves state-of-the-art editing
performance while accurately preserving unedited regions. Furthermore, the
method is both inversion- and training-free, necessitating approximately 1.5
seconds and 18 GB of GPU memory to generate high-quality results."
A Simple Image Segmentation Framework via In-Context Examples,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Recently, there have been explorations of generalist segmentation models that
can effectively tackle a variety of image segmentation tasks within a unified
in-context learning framework. However, these methods still struggle with task
ambiguity in in-context segmentation, as not all in-context examples can
accurately convey the task information. In order to address this issue, we
present SINE, a simple image Segmentation framework utilizing in-context
examples. Our approach leverages a Transformer encoder-decoder structure, where
the encoder provides high-quality image representations, and the decoder is
designed to yield multiple task-specific output masks to effectively eliminate
task ambiguity. Specifically, we introduce an In-context Interaction module to
complement in-context information and produce correlations between the target
image and the in-context example and a Matching Transformer that uses fixed
matching and a Hungarian algorithm to eliminate differences between different
tasks. In addition, we have further perfected the current evaluation system for
in-context image segmentation, aiming to facilitate a holistic appraisal of
these models. Experiments on various segmentation tasks show the effectiveness
of the proposed method."
Strong Model Collapse,cs.LG,Machine Learning,2024-10-07,"Within the scaling laws paradigm, which underpins the training of large
neural networks like ChatGPT and Llama, we consider a supervised regression
setting and establish the existance of a strong form of the model collapse
phenomenon, a critical performance degradation due to synthetic data in the
training corpus. Our results show that even the smallest fraction of synthetic
data (e.g., as little as 1\% of the total training dataset) can still lead to
model collapse: larger and larger training sets do not enhance performance. We
further investigate whether increasing model size, an approach aligned with
current trends in training large language models, exacerbates or mitigates
model collapse. In a simplified regime where neural networks are approximated
via random projections of tunable size, we both theoretically and empirically
show that larger models can amplify model collapse. Interestingly, our theory
also indicates that, beyond the interpolation threshold (which can be extremely
high for very large datasets), larger models may mitigate the collapse,
although they do not entirely prevent it. Our theoretical findings are
empirically verified through experiments on language models and feed-forward
neural networks for images."
Rationale-Aware Answer Verification by Pairwise Self-Evaluation,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Answer verification identifies correct solutions among candidates generated
by large language models (LLMs). Current approaches typically train verifier
models by labeling solutions as correct or incorrect based solely on whether
the final answer matches the gold answer. However, this approach neglects any
flawed rationale in the solution yielding the correct answer, undermining the
verifier's ability to distinguish between sound and flawed rationales. We
empirically show that in StrategyQA, only 19% of LLM-generated solutions with
correct answers have valid rationales, thus leading to an unreliable verifier.
Furthermore, we demonstrate that training a verifier on valid rationales
significantly improves its ability to distinguish valid and flawed rationale.
To make a better verifier without extra human supervision, we introduce REPS
(Rationale Enhancement through Pairwise Selection), a method for selecting
valid rationales from candidates by iteratively applying pairwise
self-evaluation using the same LLM that generates the solutions. Verifiers
trained on solutions selected by REPS outperform those trained using
conventional training methods on three reasoning benchmarks (ARC-Challenge,
DROP, and StrategyQA). Our results suggest that training reliable verifiers
requires ensuring the validity of rationales in addition to the correctness of
the final answers, which would be critical for models assisting humans in
solving complex reasoning tasks."
As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative Feedback Loss,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Direct Preference Optimization (DPO) has emerged as a more computationally
efficient alternative to Reinforcement Learning from Human Feedback (RLHF) with
Proximal Policy Optimization (PPO), eliminating the need for reward models and
online sampling. Despite these benefits, DPO and its variants remain sensitive
to hyper-parameters and prone to instability, particularly on mathematical
datasets. We argue that these issues arise from the unidirectional
likelihood-derivative negative feedback inherent in the log-likelihood loss
function. To address this, we propose a novel LLM alignment loss that
establishes a stable Bidirectional Negative Feedback (BNF) during optimization.
Our proposed BNF loss eliminates the need for pairwise contrastive losses and
does not require any extra tunable hyper-parameters or pairwise preference
data, streamlining the alignment pipeline to be as simple as supervised
fine-tuning. We conduct extensive experiments across two challenging QA
benchmarks and four reasoning benchmarks. The experimental results show that
BNF achieves comparable performance to the best methods on QA benchmarks, while
its performance decrease on the four reasoning benchmarks is significantly
lower compared to the best methods, thus striking a better balance between
value alignment and reasoning ability. In addition, we further validate the
performance of BNF on non-pairwise datasets, and conduct in-depth analysis of
log-likelihood and logit shifts across different preference optimization
methods."
Multimodal Fusion Strategies for Mapping Biophysical Landscape Features,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Multimodal aerial data are used to monitor natural systems, and machine
learning can significantly accelerate the classification of landscape features
within such imagery to benefit ecology and conservation. It remains
under-explored, however, how these multiple modalities ought to be fused in a
deep learning model. As a step towards filling this gap, we study three
strategies (Early fusion, Late fusion, and Mixture of Experts) for fusing
thermal, RGB, and LiDAR imagery using a dataset of spatially-aligned
orthomosaics in these three modalities. In particular, we aim to map three
ecologically-relevant biophysical landscape features in African savanna
ecosystems: rhino middens, termite mounds, and water. The three fusion
strategies differ in whether the modalities are fused early or late, and if
late, whether the model learns fixed weights per modality for each class or
generates weights for each class adaptively, based on the input. Overall, the
three methods have similar macro-averaged performance with Late fusion
achieving an AUC of 0.698, but their per-class performance varies strongly,
with Early fusion achieving the best recall for middens and water and Mixture
of Experts achieving the best recall for mounds."
A Planar-Symmetric SO(3) Representation for Learning Grasp Detection,cs.RO,Robotics,2024-10-07,"Planar-symmetric hands, such as parallel grippers, are widely adopted in both
research and industrial fields. Their symmetry, however, introduces ambiguity
and discontinuity in the SO(3) representation, which hinders both the training
and inference of neural-network-based grasp detectors. We propose a novel SO(3)
representation that can parametrize a pair of planar-symmetric poses with a
single parameter set by leveraging the 2D Bingham distribution. We also detail
a grasp detector based on our representation, which provides a more consistent
rotation output. An intensive evaluation with multiple grippers and objects in
both the simulation and the real world quantitatively shows our approach's
contribution."
Taming Gradient Oversmoothing and Expansion in Graph Neural Networks,cs.LG,Machine Learning,2024-10-07,"Oversmoothing has been claimed as a primary bottleneck for multi-layered
graph neural networks (GNNs). Multiple analyses have examined how and why
oversmoothing occurs. However, none of the prior work addressed how
optimization is performed under the oversmoothing regime. In this work, we show
the presence of $\textit{gradient oversmoothing}$ preventing optimization
during training. We further analyze that GNNs with residual connections, a
well-known solution to help gradient flow in deep architecture, introduce
$\textit{gradient expansion}$, a phenomenon of the gradient explosion in
diverse directions. Therefore, adding residual connections cannot be a solution
for making a GNN deep. Our analysis reveals that constraining the Lipschitz
bound of each layer can neutralize the gradient expansion. To this end, we
provide a simple yet effective normalization method to prevent the gradient
expansion. An empirical study shows that the residual GNNs with hundreds of
layers can be efficiently trained with the proposed normalization without
compromising performance. Additional studies show that the empirical
observations corroborate our theoretical analysis."
Trained Models Tell Us How to Make Them Robust to Spurious Correlation without Group Annotation,cs.LG,Machine Learning,2024-10-07,"Classifiers trained with Empirical Risk Minimization (ERM) tend to rely on
attributes that have high spurious correlation with the target. This can
degrade the performance on underrepresented (or 'minority') groups that lack
these attributes, posing significant challenges for both out-of-distribution
generalization and fairness objectives. Many studies aim to enhance robustness
to spurious correlation, but they sometimes depend on group annotations for
training. Additionally, a common limitation in previous research is the
reliance on group-annotated validation datasets for model selection. This
constrains their applicability in situations where the nature of the spurious
correlation is not known, or when group labels for certain spurious attributes
are not available. To enhance model robustness with minimal group annotation
assumptions, we propose Environment-based Validation and Loss-based Sampling
(EVaLS). It uses the losses from an ERM-trained model to construct a balanced
dataset of high-loss and low-loss samples, mitigating group imbalance in data.
This significantly enhances robustness to group shifts when equipped with a
simple post-training last layer retraining. By using environment inference
methods to create diverse environments with correlation shifts, EVaLS can
potentially eliminate the need for group annotation in validation data. In this
context, the worst environment accuracy acts as a reliable surrogate throughout
the retraining process for tuning hyperparameters and finding a model that
performs well across diverse group shifts. EVaLS effectively achieves group
robustness, showing that group annotation is not necessary even for validation.
It is a fast, straightforward, and effective approach that reaches near-optimal
worst group accuracy without needing group annotations, marking a new chapter
in the robustness of trained models against spurious correlation."
CAT: Concept-level backdoor ATtacks for Concept Bottleneck Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Despite the transformative impact of deep learning across multiple domains,
the inherent opacity of these models has driven the development of Explainable
Artificial Intelligence (XAI). Among these efforts, Concept Bottleneck Models
(CBMs) have emerged as a key approach to improve interpretability by leveraging
high-level semantic information. However, CBMs, like other machine learning
models, are susceptible to security threats, particularly backdoor attacks,
which can covertly manipulate model behaviors. Understanding that the community
has not yet studied the concept level backdoor attack of CBM, because of
""Better the devil you know than the devil you don't know."", we introduce CAT
(Concept-level Backdoor ATtacks), a methodology that leverages the conceptual
representations within CBMs to embed triggers during training, enabling
controlled manipulation of model predictions at inference time. An enhanced
attack pattern, CAT+, incorporates a correlation function to systematically
select the most effective and stealthy concept triggers, thereby optimizing the
attack's impact. Our comprehensive evaluation framework assesses both the
attack success rate and stealthiness, demonstrating that CAT and CAT+ maintain
high performance on clean data while achieving significant targeted effects on
backdoored datasets. This work underscores the potential security risks
associated with CBMs and provides a robust testing methodology for future
security assessments."
MINER: Mining the Underlying Pattern of Modality-Specific Neurons in Multimodal Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"In recent years, multimodal large language models (MLLMs) have significantly
advanced, integrating more modalities into diverse applications. However, the
lack of explainability remains a major barrier to their use in scenarios
requiring decision transparency. Current neuron-level explanation paradigms
mainly focus on knowledge localization or language- and domain-specific
analyses, leaving the exploration of multimodality largely unaddressed. To
tackle these challenges, we propose MINER, a transferable framework for mining
modality-specific neurons (MSNs) in MLLMs, which comprises four stages: (1)
modality separation, (2) importance score calculation, (3) importance score
aggregation, (4) modality-specific neuron selection. Extensive experiments
across six benchmarks and two representative MLLMs show that (I) deactivating
ONLY 2% of MSNs significantly reduces MLLMs performance (0.56 to 0.24 for
Qwen2-VL, 0.69 to 0.31 for Qwen2-Audio), (II) different modalities mainly
converge in the lower layers, (III) MSNs influence how key information from
various modalities converges to the last token, (IV) two intriguing phenomena
worth further investigation, i.e., semantic probing and semantic telomeres. The
source code is available at this URL."
Resource-Efficient Multiview Perception: Integrating Semantic Masking with Masked Autoencoders,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Multiview systems have become a key technology in modern computer vision,
offering advanced capabilities in scene understanding and analysis. However,
these systems face critical challenges in bandwidth limitations and
computational constraints, particularly for resource-limited camera nodes like
drones. This paper presents a novel approach for communication-efficient
distributed multiview detection and tracking using masked autoencoders (MAEs).
We introduce a semantic-guided masking strategy that leverages pre-trained
segmentation models and a tunable power function to prioritize informative
image regions. This approach, combined with an MAE, reduces communication
overhead while preserving essential visual information. We evaluate our method
on both virtual and real-world multiview datasets, demonstrating comparable
performance in terms of detection and tracking performance metrics compared to
state-of-the-art techniques, even at high masking ratios. Our selective masking
algorithm outperforms random masking, maintaining higher accuracy and precision
as the masking ratio increases. Furthermore, our approach achieves a
significant reduction in transmission data volume compared to baseline methods,
thereby balancing multiview tracking performance with communication efficiency."
Learning Interpretable Hierarchical Dynamical Systems Models from Time Series Data,cs.LG,Machine Learning,2024-10-07,"In science, we are often interested in obtaining a generative model of the
underlying system dynamics from observed time series. While powerful methods
for dynamical systems reconstruction (DSR) exist when data come from a single
domain, how to best integrate data from multiple dynamical regimes and leverage
it for generalization is still an open question. This becomes particularly
important when individual time series are short, and group-level information
may help to fill in for gaps in single-domain data. At the same time, averaging
is not an option in DSR, as it will wipe out crucial dynamical properties
(e.g., limit cycles in one domain vs. chaos in another). Hence, a framework is
needed that enables to efficiently harvest group-level (multi-domain)
information while retaining all single-domain dynamical characteristics. Here
we provide such a hierarchical approach and showcase it on popular DSR
benchmarks, as well as on neuroscientific and medical time series. In addition
to faithful reconstruction of all individual dynamical regimes, our
unsupervised methodology discovers common low-dimensional feature spaces in
which datasets with similar dynamics cluster. The features spanning these
spaces were further dynamically highly interpretable, surprisingly in often
linear relation to control parameters that govern the dynamics of the
underlying system. Finally, we illustrate transfer learning and generalization
to new parameter regimes."
Learning Efficient and Effective Trajectories for Differential Equation-based Image Restoration,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"The differential equation-based image restoration approach aims to establish
learnable trajectories connecting high-quality images to a tractable
distribution, e.g., low-quality images or a Gaussian distribution. In this
paper, we reformulate the trajectory optimization of this kind of method,
focusing on enhancing both reconstruction quality and efficiency. Initially, we
navigate effective restoration paths through a reinforcement learning process,
gradually steering potential trajectories toward the most precise options.
Additionally, to mitigate the considerable computational burden associated with
iterative sampling, we propose cost-aware trajectory distillation to streamline
complex paths into several manageable steps with adaptable sizes. Moreover, we
fine-tune a foundational diffusion model (FLUX) with 12B parameters by using
our algorithms, producing a unified framework for handling 7 kinds of image
restoration tasks. Extensive experiments showcase the significant superiority
of the proposed method, achieving a maximum PSNR improvement of 2.1 dB over
state-of-the-art methods, while also greatly enhancing visual perceptual
quality. Project page: \url{https://zhu-zhiyu.github.io/FLUX-IR/}."
FedBiP: Heterogeneous One-Shot Federated Learning with Personalized Latent Diffusion Models,cs.LG,Machine Learning,2024-10-07,"One-Shot Federated Learning (OSFL), a special decentralized machine learning
paradigm, has recently gained significant attention. OSFL requires only a
single round of client data or model upload, which reduces communication costs
and mitigates privacy threats compared to traditional FL. Despite these
promising prospects, existing methods face challenges due to client data
heterogeneity and limited data quantity when applied to real-world OSFL
systems. Recently, Latent Diffusion Models (LDM) have shown remarkable
advancements in synthesizing high-quality images through pretraining on
large-scale datasets, thereby presenting a potential solution to overcome these
issues. However, directly applying pretrained LDM to heterogeneous OSFL results
in significant distribution shifts in synthetic data, leading to performance
degradation in classification models trained on such data. This issue is
particularly pronounced in rare domains, such as medical imaging, which are
underrepresented in LDM's pretraining data. To address this challenge, we
propose Federated Bi-Level Personalization (FedBiP), which personalizes the
pretrained LDM at both instance-level and concept-level. Hereby, FedBiP
synthesizes images following the client's local data distribution without
compromising the privacy regulations. FedBiP is also the first approach to
simultaneously address feature space heterogeneity and client data scarcity in
OSFL. Our method is validated through extensive experiments on three OSFL
benchmarks with feature space heterogeneity, as well as on challenging medical
and satellite image datasets with label heterogeneity. The results demonstrate
the effectiveness of FedBiP, which substantially outperforms other OSFL
methods."
Data-driven Diffusion Models for Enhancing Safety in Autonomous Vehicle Traffic Simulations,cs.RO,Robotics,2024-10-07,"Safety-critical traffic scenarios are integral to the development and
validation of autonomous driving systems. These scenarios provide crucial
insights into vehicle responses under high-risk conditions rarely encountered
in real-world settings. Recent advancements in critical scenario generation
have demonstrated the superiority of diffusion-based approaches over
traditional generative models in terms of effectiveness and realism. However,
current diffusion-based methods fail to adequately address the complexity of
driver behavior and traffic density information, both of which significantly
influence driver decision-making processes. In this work, we present a novel
approach to overcome these limitations by introducing adversarial guidance
functions for diffusion models that incorporate behavior complexity and traffic
density, thereby enhancing the generation of more effective and realistic
safety-critical traffic scenarios. The proposed method is evaluated on two
evaluation metrics: effectiveness and realism.The proposed method is evaluated
on two evaluation metrics: effectiveness and realism, demonstrating better
efficacy as compared to other state-of-the-art methods."
LPZero: Language Model Zero-cost Proxy Search from Zero,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"In spite of the outstanding performance, Neural Architecture Search (NAS) is
criticized for massive computation. Recently, Zero-shot NAS has emerged as a
promising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce
computational demands. Despite this, existing ZC proxies heavily rely on expert
knowledge and incur significant trial-and-error costs. Particularly in NLP
tasks, most existing ZC proxies fail to surpass the performance of the naive
baseline. To address these challenges, we introduce a novel framework,
\textbf{LPZero}, which is the first to automatically design ZC proxies for
various tasks, achieving higher ranking consistency than human-designed
proxies. Specifically, we model the ZC proxy as a symbolic equation and
incorporate a unified proxy search space that encompasses existing ZC proxies,
which are composed of a predefined set of mathematical symbols. To
heuristically search for the best ZC proxy, LPZero incorporates genetic
programming to find the optimal symbolic composition. We propose a
\textit{Rule-based Pruning Strategy (RPS),} which preemptively eliminates
unpromising proxies, thereby mitigating the risk of proxy degradation.
Extensive experiments on FlexiBERT, GPT-2, and LLaMA-7B demonstrate LPZero's
superior ranking ability and performance on downstream tasks compared to
current approaches."
Timer-XL: Long-Context Transformers for Unified Time Series Forecasting,cs.LG,Machine Learning,2024-10-07,"We present Timer-XL, a generative Transformer for unified time series
forecasting. To uniformly predict 1D and 2D time series, we generalize next
token prediction, predominantly adopted for causal generation of 1D sequences,
to multivariate next token prediction. The proposed paradigm uniformly
formulates various forecasting scenarios as a long-context generation problem.
We opt for the generative Transformer, which can capture global-range and
causal dependencies while providing contextual flexibility, to implement
unified forecasting on univariate series characterized by non-stationarity,
multivariate time series with complicated dynamics and correlations, and
covariate-informed contexts that include both endogenous and exogenous
variables. Technically, we propose a universal TimeAttention to facilitate
generative Transformers on time series, which can effectively capture
fine-grained intra- and inter-series dependencies of flattened time series
tokens (patches) and is further strengthened by position embeddings in both
temporal and variable dimensions. Timer-XL achieves state-of-the-art
performance across challenging forecasting benchmarks through a unified
approach. As a large time series model, it demonstrates notable model
transferability by large-scale pre-training, as well as contextual flexibility
in token lengths, positioning it as a one-for-all forecaster."
Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Very High Resolution (VHR) geospatial image analysis is crucial for
humanitarian assistance in both natural and anthropogenic crises, as it allows
to rapidly identify the most critical areas that need support. Nonetheless,
manually inspecting large areas is time-consuming and requires domain
expertise. Thanks to their accuracy, generalization capabilities, and highly
parallelizable workload, Deep Neural Networks (DNNs) provide an excellent way
to automate this task. Nevertheless, there is a scarcity of VHR data pertaining
to conflict situations, and consequently, of studies on the effectiveness of
DNNs in those scenarios. Motivated by this, our work extensively studies the
applicability of a collection of state-of-the-art Convolutional Neural Networks
(CNNs) originally developed for natural disasters damage assessment in a war
scenario. To this end, we build an annotated dataset with pre- and
post-conflict images of the Ukrainian city of Mariupol. We then explore the
transferability of the CNN models in both zero-shot and learning scenarios,
demonstrating their potential and limitations. To the best of our knowledge,
this is the first study to use sub-meter resolution imagery to assess building
damage in combat zones."
Improving Image Clustering with Artifacts Attenuation via Inference-Time Attention Engineering,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"The goal of this paper is to improve the performance of pretrained Vision
Transformer (ViT) models, particularly DINOv2, in image clustering task without
requiring re-training or fine-tuning. As model size increases, high-norm
artifacts anomaly appears in the patches of multi-head attention. We observe
that this anomaly leads to reduced accuracy in zero-shot image clustering.
These artifacts are characterized by disproportionately large values in the
attention map compared to other patch tokens. To address these artifacts, we
propose an approach called Inference-Time Attention Engineering (ITAE), which
manipulates attention function during inference. Specifically, we identify the
artifacts by investigating one of the Query-Key-Value (QKV) patches in the
multi-head attention and attenuate their corresponding attention values inside
the pretrained models. ITAE shows improved clustering accuracy on multiple
datasets by exhibiting more expressive features in latent space. Our findings
highlight the potential of ITAE as a practical solution for reducing artifacts
in pretrained ViT models and improving model performance in clustering tasks
without the need for re-training or fine-tuning."
Transforming Color: A Novel Image Colorization Method,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"This paper introduces a novel method for image colorization that utilizes a
color transformer and generative adversarial networks (GANs) to address the
challenge of generating visually appealing colorized images. Conventional
approaches often struggle with capturing long-range dependencies and producing
realistic colorizations. The proposed method integrates a transformer
architecture to capture global information and a GAN framework to improve
visual quality. In this study, a color encoder that utilizes a random normal
distribution to generate color features is applied. These features are then
integrated with grayscale image features to enhance the overall representation
of the images. Our method demonstrates superior performance compared with
existing approaches by utilizing the capacity of the transformer, which can
capture long-range dependencies and generate a realistic colorization of the
GAN. Experimental results show that the proposed network significantly
outperforms other state-of-the-art colorization techniques, highlighting its
potential for image colorization. This research opens new possibilities for
precise and visually compelling image colorization in domains such as digital
restoration and historical image analysis."
DAPE V2: Process Attention Score as Feature Map for Length Extrapolation,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"The attention mechanism is a fundamental component of the Transformer model,
contributing to interactions among distinct tokens, in contrast to earlier
feed-forward neural networks. In general, the attention scores are determined
simply by the key-query products. However, this work's occasional trial
(combining DAPE and NoPE) of including additional MLPs on attention scores
without position encoding indicates that the classical key-query multiplication
may limit the performance of Transformers. In this work, we conceptualize
attention as a feature map and apply the convolution operator (for neighboring
attention scores across different heads) to mimic the processing methods in
computer vision. Specifically, the main contribution of this paper is
identifying and interpreting the Transformer length extrapolation problem as a
result of the limited expressiveness of the naive query and key dot product,
and we successfully translate the length extrapolation issue into a
well-understood feature map processing problem. The novel insight, which can be
adapted to various attention-related models, reveals that the current
Transformer architecture has the potential for further evolution. Extensive
experiments demonstrate that treating attention as a feature map and applying
convolution as a processing method significantly enhances Transformer
performance."
EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos with Procedural Texts,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Mistake action detection from egocentric videos is crucial for developing
intelligent archives that detect workers' errors and provide feedback. Previous
studies have been limited to specific domains, focused on detecting mistakes
from videos without procedural texts, and analyzed whether actions are
mistakes. To address these limitations, in this paper, we propose the EgoOops
dataset, which includes egocentric videos, procedural texts, and three types of
annotations: video-text alignment, mistake labels, and descriptions for
mistakes. EgoOops covers five procedural domains and includes 50 egocentric
videos. The video-text alignment allows the model to detect mistakes based on
both videos and procedural texts. The mistake labels and descriptions enable
detailed analysis of real-world mistakes. Based on EgoOops, we tackle two
tasks: video-text alignment and mistake detection. For video-text alignment, we
enhance the recent StepFormer model with an additional loss for fine-tuning.
Based on the alignment results, we propose a multi-modal classifier to predict
mistake labels. In our experiments, the proposed methods achieve higher
performance than the baselines. In addition, our ablation study demonstrates
the effectiveness of combining videos and texts. We will release the dataset
and codes upon publication."
Representing the Under-Represented: Cultural and Core Capability Benchmarks for Developing Thai Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"The rapid advancement of large language models (LLMs) has highlighted the
need for robust evaluation frameworks that assess their core capabilities, such
as reasoning, knowledge, and commonsense, leading to the inception of certain
widely-used benchmark suites such as the H6 benchmark. However, these benchmark
suites are primarily built for the English language, and there exists a lack
thereof for under-represented languages, in terms of LLM development, such as
Thai. On the other hand, developing LLMs for Thai should also include enhancing
the cultural understanding as well as core capabilities. To address these dual
challenge in Thai LLM research, we propose two key benchmarks: Thai-H6 and Thai
Cultural and Linguistic Intelligence Benchmark (ThaiCLI). Through a thorough
evaluation of various LLMs with multi-lingual capabilities, we provide a
comprehensive analysis of the proposed benchmarks and how they contribute to
Thai LLM development. Furthermore, we will make both the datasets and
evaluation code publicly available to encourage further research and
development for Thai LLMs."
GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph for Long Document QA,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"In the past, Retrieval-Augmented Generation (RAG) methods split text into
chunks to enable language models to handle long documents. Recent tree-based
RAG methods are able to retrieve detailed information while preserving global
context. However, with the advent of more powerful LLMs, such as Llama 3.1,
which offer better comprehension and support for longer inputs, we found that
even recent tree-based RAG methods perform worse than directly feeding the
entire document into Llama 3.1, although RAG methods still hold an advantage in
reducing computational costs. In this paper, we propose a new retrieval method,
called LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph
(GARLIC), which outperforms previous state-of-the-art baselines, including
Llama 3.1, while retaining the computational efficiency of RAG methods. Our
method introduces several improvements: (1) Rather than using a tree structure,
we construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many
summarization, where the graph edges are derived from attention mechanisms, and
each node focuses on a single event or very few events. (2) We introduce a
novel retrieval method that leverages the attention weights of LLMs rather than
dense embedding similarity. Our method allows for searching the graph along
multiple paths and can terminate at any depth. (3) We use the LLM to control
the retrieval process, enabling it to dynamically adjust the amount and depth
of information retrieved for different queries. Experimental results show that
our method outperforms previous state-of-the-art baselines, including Llama
3.1, on two single-document and two multi-document QA datasets, while
maintaining similar computational complexity to traditional RAG methods."
Analysis of Hybrid Compositions in Animation Film with Weakly Supervised Learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"We present an approach for the analysis of hybrid visual compositions in
animation in the domain of ephemeral film. We combine ideas from
semi-supervised and weakly supervised learning to train a model that can
segment hybrid compositions without requiring pre-labeled segmentation masks.
We evaluate our approach on a set of ephemeral films from 13 film archives.
Results demonstrate that the proposed learning strategy yields a performance
close to a fully supervised baseline. On a qualitative level the performed
analysis provides interesting insights on hybrid compositions in animation
film."
Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Having been trained on massive pretraining data, large language models have
shown excellent performance on many knowledge-intensive tasks. However,
pretraining data tends to contain misleading and even conflicting information,
and it is intriguing to understand how LLMs handle these noisy data during
training. In this study, we systematically analyze LLMs' learning preferences
for data with conflicting knowledge. We find that pretrained LLMs establish
learning preferences similar to humans, i.e., preferences towards formal texts
and texts with fewer spelling errors, resulting in faster learning and more
favorable treatment of knowledge in data with such features when facing
conflicts. This finding is generalizable across models and languages and is
more evident in larger models. An in-depth analysis reveals that LLMs tend to
trust data with features that signify consistency with the majority of data,
and it is possible to instill new preferences and erase old ones by
manipulating the degree of consistency with the majority data."
Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Multimodal Large Language Models (MLLMs) have emerged as a central focus in
both industry and academia, but often suffer from biases introduced by visual
and language priors, which can lead to multimodal hallucination. These biases
arise from the visual encoder and the Large Language Model (LLM) backbone,
affecting the attention mechanism responsible for aligning multimodal inputs.
Existing decoding-based mitigation methods focus on statistical correlations
and overlook the causal relationships between attention mechanisms and model
output, limiting their effectiveness in addressing these biases. To tackle this
issue, we propose a causal inference framework termed CausalMM that applies
structural causal modeling to MLLMs, treating modality priors as a confounder
between attention mechanisms and output. Specifically, by employing backdoor
adjustment and counterfactual reasoning at both the visual and language
attention levels, our method mitigates the negative effects of modality priors
and enhances the alignment of MLLM's inputs and outputs, with a maximum score
improvement of 65.3% on 6 VLind-Bench indicators and 164 points on MME
Benchmark compared to conventional methods. Extensive experiments validate the
effectiveness of our approach while being a plug-and-play solution. Our code is
available at: https://github.com/The-Martyr/CausalMM"
Fast Training of Sinusoidal Neural Fields via Scaling Initialization,cs.LG,Machine Learning,2024-10-07,"Neural fields are an emerging paradigm that represent data as continuous
functions parameterized by neural networks. Despite many advantages, neural
fields often have a high training cost, which prevents a broader adoption. In
this paper, we focus on a popular family of neural fields, called sinusoidal
neural fields (SNFs), and study how it should be initialized to maximize the
training speed. We find that the standard initialization scheme for SNFs --
designed based on the signal propagation principle -- is suboptimal. In
particular, we show that by simply multiplying each weight (except for the last
layer) by a constant, we can accelerate SNF training by 10$\times$. This
method, coined $\textit{weight scaling}$, consistently provides a significant
speedup over various data domains, allowing the SNFs to train faster than more
recently proposed architectures. To understand why the weight scaling works
well, we conduct extensive theoretical and empirical analyses which reveal that
the weight scaling not only resolves the spectral bias quite effectively but
also enjoys a well-conditioned optimization trajectory."
MM-R$^3$: On (In-)Consistency of Multi-modal Large Language Models (MLLMs),cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"With the advent of Large Language Models (LLMs) and Multimodal
(Visio-lingual) LLMs, a flurry of research has emerged, analyzing the
performance of such models across a diverse array of tasks. While most studies
focus on evaluating the capabilities of state-of-the-art (SoTA) MLLM models
through task accuracy (e.g., Visual Question Answering, grounding) across
various datasets, our work explores the related but complementary aspect of
consistency - the ability of an MLLM model to produce semantically similar or
identical responses to semantically similar queries. We note that consistency
is a fundamental prerequisite (necessary but not sufficient condition) for
robustness and trust in MLLMs. Humans, in particular, are known to be highly
consistent (even if not always accurate) in their responses, and consistency is
inherently expected from AI systems. Armed with this perspective, we propose
the MM-R$^3$ benchmark, which analyses the performance in terms of consistency
and accuracy in SoTA MLLMs with three tasks: Question Rephrasing, Image
Restyling, and Context Reasoning. Our analysis reveals that consistency does
not always align with accuracy, indicating that models with higher accuracy are
not necessarily more consistent, and vice versa. Furthermore, we propose a
simple yet effective mitigation strategy in the form of an adapter module
trained to minimize inconsistency across prompts. With our proposed strategy,
we are able to achieve absolute improvements of 5.7% and 12.5%, on average on
widely used MLLMs such as BLIP-2 and LLaVa 1.5M in terms of consistency over
their existing counterparts."
Granular Ball Twin Support Vector Machine,cs.LG,Machine Learning,2024-10-07,"On Efficient and Scalable Computation of the Nonparametric Maximum Likelihood
Estimator in Mixture ModelsTwin support vector machine (TSVM) is an emerging
machine learning model with versatile applicability in classification and
regression endeavors. Nevertheless, TSVM confronts noteworthy challenges: $(i)$
the imperative demand for matrix inversions presents formidable obstacles to
its efficiency and applicability on large-scale datasets; $(ii)$ the omission
of the structural risk minimization (SRM) principle in its primal formulation
heightens the vulnerability to overfitting risks; and $(iii)$ the TSVM exhibits
a high susceptibility to noise and outliers, and also demonstrates instability
when subjected to resampling. In view of the aforementioned challenges, we
propose the granular ball twin support vector machine (GBTSVM). GBTSVM takes
granular balls, rather than individual data points, as inputs to construct a
classifier. These granular balls, characterized by their coarser granularity,
exhibit robustness to resampling and reduced susceptibility to the impact of
noise and outliers. We further propose a novel large-scale granular ball twin
support vector machine (LS-GBTSVM). LS-GBTSVM's optimization formulation
ensures two critical facets: $(i)$ it eliminates the need for matrix
inversions, streamlining the LS-GBTSVM's computational efficiency, and $(ii)$
it incorporates the SRM principle through the incorporation of regularization
terms, effectively addressing the issue of overfitting. The proposed LS-GBTSVM
exemplifies efficiency, scalability for large datasets, and robustness against
noise and outliers. We conduct a comprehensive evaluation of the GBTSVM and
LS-GBTSVM models on benchmark datasets from UCI, KEEL, and NDC datasets. Our
experimental findings and statistical analyses affirm the superior
generalization prowess of the proposed GBTSVM and LS-GBTSVM models."
Double Oracle Neural Architecture Search for Game Theoretic Deep Learning Models,cs.LG,Machine Learning,2024-10-07,"In this paper, we propose a new approach to train deep learning models using
game theory concepts including Generative Adversarial Networks (GANs) and
Adversarial Training (AT) where we deploy a double-oracle framework using best
response oracles. GAN is essentially a two-player zero-sum game between the
generator and the discriminator. The same concept can be applied to AT with
attacker and classifier as players. Training these models is challenging as a
pure Nash equilibrium may not exist and even finding the mixed Nash equilibrium
is difficult as training algorithms for both GAN and AT have a large-scale
strategy space. Extending our preliminary model DO-GAN, we propose the methods
to apply the double oracle framework concept to Adversarial Neural Architecture
Search (NAS for GAN) and Adversarial Training (NAS for AT) algorithms. We first
generalize the players' strategies as the trained models of generator and
discriminator from the best response oracles. We then compute the
meta-strategies using a linear program. For scalability of the framework where
multiple network models of best responses are stored in the memory, we prune
the weakly-dominated players' strategies to keep the oracles from becoming
intractable. Finally, we conduct experiments on MNIST, CIFAR-10 and
TinyImageNet for DONAS-GAN. We also evaluate the robustness under FGSM and PGD
attacks on CIFAR-10, SVHN and TinyImageNet for DONAS-AT. We show that all our
variants have significant improvements in both subjective qualitative
evaluation and quantitative metrics, compared with their respective base
architectures."
WTCL-Dehaze: Rethinking Real-world Image Dehazing via Wavelet Transform and Contrastive Learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Images captured in hazy outdoor conditions often suffer from colour
distortion, low contrast, and loss of detail, which impair high-level vision
tasks. Single image dehazing is essential for applications such as autonomous
driving and surveillance, with the aim of restoring image clarity. In this
work, we propose WTCL-Dehaze an enhanced semi-supervised dehazing network that
integrates Contrastive Loss and Discrete Wavelet Transform (DWT). We
incorporate contrastive regularization to enhance feature representation by
contrasting hazy and clear image pairs. Additionally, we utilize DWT for
multi-scale feature extraction, effectively capturing high-frequency details
and global structures. Our approach leverages both labelled and unlabelled data
to mitigate the domain gap and improve generalization. The model is trained on
a combination of synthetic and real-world datasets, ensuring robust performance
across different scenarios. Extensive experiments demonstrate that our proposed
algorithm achieves superior performance and improved robustness compared to
state-of-the-art single image dehazing methods on both benchmark datasets and
real-world images."
Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM,cs.AI,Artificial Intelligence,2024-10-07,"This work presents an interpretable decision-making framework for autonomous
vehicles that integrates traffic regulations, norms, and safety guidelines
comprehensively and enables seamless adaptation to different regions. While
traditional rule-based methods struggle to incorporate the full scope of
traffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on
Retrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic
rules and guidelines from extensive regulation documents and relevant records
based on the ego vehicle's situation. Given the semantic complexity of the
retrieved rules, we also design a reasoning module powered by a Large Language
Model (LLM) to interpret these rules, differentiate between mandatory rules and
safety guidelines, and assess actions on legal compliance and safety.
Additionally, the reasoning is designed to be interpretable, enhancing both
transparency and reliability. The framework demonstrates robust performance on
both hypothesized and real-world cases across diverse scenarios, along with the
ability to adapt to different regions with ease."
"A Comprehensive Study on GDPR-Oriented Analysis of Privacy Policies: Taxonomy, Corpus and GDPR Concept Classifiers",cs.CR,Cryptography and Security,2024-10-07,"Machine learning based classifiers that take a privacy policy as the input
and predict relevant concepts are useful in different applications such as
(semi-)automated compliance analysis against requirements of the EU GDPR. In
all past studies, such classifiers produce a concept label per segment (e.g.,
sentence or paragraph) and their performances were evaluated by using a dataset
of labeled segments without considering the privacy policy they belong to.
However, such an approach could overestimate the performance in real-world
settings, where all segments in a new privacy policy are supposed to be unseen.
Additionally, we also observed other research gaps, including the lack of a
more complete GDPR taxonomy and the less consideration of hierarchical
information in privacy policies. To fill such research gaps, we developed a
more complete GDPR taxonomy, created the first corpus of labeled privacy
policies with hierarchical information, and conducted the most comprehensive
performance evaluation of GDPR concept classifiers for privacy policies. Our
work leads to multiple novel findings, including the confirmed
inappropriateness of splitting training and test sets at the segment level, the
benefits of considering hierarchical information, and the limitations of the
""one size fits all"" approach, and the significance of testing cross-corpus
generalizability."
ImProver: Agent-Based Automated Proof Optimization,cs.AI,Artificial Intelligence,2024-10-07,"Large language models (LLMs) have been used to generate formal proofs of
mathematical theorems in proofs assistants such as Lean. However, we often want
to optimize a formal proof with respect to various criteria, depending on its
downstream use. For example, we may want a proof to adhere to a certain style,
or to be readable, concise, or modularly structured. Having suitably optimized
proofs is also important for learning tasks, especially since human-written
proofs may not optimal for that purpose. To this end, we study a new problem of
automated proof optimization: rewriting a proof so that it is correct and
optimizes for an arbitrary criterion, such as length or readability. As a first
method for automated proof optimization, we present ImProver, a
large-language-model agent that rewrites proofs to optimize arbitrary
user-defined metrics in Lean. We find that naively applying LLMs to proof
optimization falls short, and we incorporate various improvements into
ImProver, such as the use of symbolic Lean context in a novel Chain-of-States
technique, as well as error-correction and retrieval. We test ImProver on
rewriting real-world undergraduate, competition, and research-level mathematics
theorems, finding that ImProver is capable of rewriting proofs so that they are
substantially shorter, more modular, and more readable."
Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"As an essential task in information extraction (IE), Event-Event Causal
Relation Extraction (ECRE) aims to identify and classify the causal
relationships between event mentions in natural language texts. However,
existing research on ECRE has highlighted two critical challenges, including
the lack of document-level modeling and causal hallucinations. In this paper,
we propose a Knowledge-guided binary Question Answering (KnowQA) method with
event structures for ECRE, consisting of two stages: Event Structure
Construction and Binary Question Answering. We conduct extensive experiments
under both zero-shot and fine-tuning settings with large language models (LLMs)
on the MECI and MAVEN-ERE datasets. Experimental results demonstrate the
usefulness of event structures on document-level ECRE and the effectiveness of
KnowQA by achieving state-of-the-art on the MECI dataset. We observe not only
the effectiveness but also the high generalizability and low inconsistency of
our method, particularly when with complete event structures after fine-tuning
the models."
Intriguing Properties of Large Language and Vision Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Recently, large language and vision models (LLVMs) have received significant
attention and development efforts due to their remarkable generalization
performance across a wide range of tasks requiring perception and cognitive
abilities. A key factor behind their success is their simple architecture,
which consists of a vision encoder, a projector, and a large language model
(LLM). Despite their achievements in advanced reasoning tasks, their
performance on fundamental perception-related tasks (e.g., MMVP) remains
surprisingly low. This discrepancy raises the question of how LLVMs truly
perceive images and exploit the advantages of the vision encoder. To address
this, we systematically investigate this question regarding several aspects:
permutation invariance, robustness, math reasoning, alignment preserving and
importance, by evaluating the most common LLVM's families (i.e., LLaVA) across
10 evaluation benchmarks. Our extensive experiments reveal several intriguing
properties of current LLVMs: (1) they internally process the image in a global
manner, even when the order of visual patch sequences is randomly permuted; (2)
they are sometimes able to solve math problems without fully perceiving
detailed numerical information; (3) the cross-modal alignment is overfitted to
complex reasoning tasks, thereby, causing them to lose some of the original
perceptual capabilities of their vision encoder; (4) the representation space
in the lower layers (<25%) plays a crucial role in determining performance and
enhancing visual understanding. Lastly, based on the above observations, we
suggest potential future directions for building better LLVMs and constructing
more challenging evaluation benchmarks."
LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Generating Natural Language Explanations (NLEs) for model predictions on
medical images, particularly those depicting thoracic pathologies, remains a
critical and challenging task. Existing methodologies often struggle due to
general models' insufficient domain-specific medical knowledge and privacy
concerns associated with retrieval-based augmentation techniques. To address
these issues, we propose a novel Vision-Language framework augmented with a
Knowledge Graph (KG)-based datastore, which enhances the model's understanding
by incorporating additional domain-specific medical knowledge essential for
generating accurate and informative NLEs. Our framework employs a KG-based
retrieval mechanism that not only improves the precision of the generated
explanations but also preserves data privacy by avoiding direct data retrieval.
The KG datastore is designed as a plug-and-play module, allowing for seamless
integration with various model architectures. We introduce and evaluate three
distinct frameworks within this paradigm: KG-LLaVA, which integrates the
pre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining
MedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts
LLaVA by incorporating the Bio-ViT-L vision model. These frameworks are
validated on the MIMIC-NLE dataset, where they achieve state-of-the-art
results, underscoring the effectiveness of KG augmentation in generating
high-quality NLEs for thoracic pathologies."
PSA: Private Set Alignment for Secure and Collaborative Analytics on Large-Scale Data,cs.CR,Cryptography and Security,2024-10-07,"Enforcement of privacy regulation is essential for collaborative data
analytics. In this work, we address a scenario in which two companies expect to
securely join their datasets with respect to their common customers to maximize
data insights. Apart from the necessary protection of raw data, it becomes more
challenging to protect the identities and attributes of common customers, as it
requires participants to align their records associated with common customers
without knowing who they are. We proposed a solution, dubbed PSA, for this
scenario, which is effectively applicable to real-world use cases, such as
evaluating advertising conversion using data from both publishers and
merchants. The contributions of this work are threefold: 1. We defined the
notion of PSA with two levels of privacy protection and proposed novel PSA
protocols based on the modified oblivious switching network, which leverages
efficient symmetric key operations and offline precomputation to save online
run time. 2. We implemented and benchmarked the proposed protocols in different
network conditions by joining two datasets, each at the scale of one million
records, in 35.5 sec on a single thread with a network bandwidth of 500 Mbps,
resulting in an X100 improvement over the existing Homomorphic based protocols.
3. We give new proof for an algorithm of quasi-linear complexity that
constructs an oblivious switching network to achieve a target permutation
distinct from the existing one in the literature."
Evaluating the Generalization Ability of Spatiotemporal Model in Urban Scenario,cs.LG,Machine Learning,2024-10-07,"Spatiotemporal neural networks have shown great promise in urban scenarios by
effectively capturing temporal and spatial correlations. However, urban
environments are constantly evolving, and current model evaluations are often
limited to traffic scenarios and use data mainly collected only a few weeks
after training period to evaluate model performance. The generalization ability
of these models remains largely unexplored. To address this, we propose a
Spatiotemporal Out-of-Distribution (ST-OOD) benchmark, which comprises six
urban scenario: bike-sharing, 311 services, pedestrian counts, traffic speed,
traffic flow, ride-hailing demand, and bike-sharing, each with in-distribution
(same year) and out-of-distribution (next years) settings. We extensively
evaluate state-of-the-art spatiotemporal models and find that their performance
degrades significantly in out-of-distribution settings, with most models
performing even worse than a simple Multi-Layer Perceptron (MLP). Our findings
suggest that current leading methods tend to over-rely on parameters to overfit
training data, which may lead to good performance on in-distribution data but
often results in poor generalization. We also investigated whether dropout
could mitigate the negative effects of overfitting. Our results showed that a
slight dropout rate could significantly improve generalization performance on
most datasets, with minimal impact on in-distribution performance. However,
balancing in-distribution and out-of-distribution performance remains a
challenging problem. We hope that the proposed benchmark will encourage further
research on this critical issue."
TableRAG: Million-Token Table Understanding with Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Recent advancements in language models (LMs) have notably enhanced their
ability to reason with tabular data, primarily through program-aided mechanisms
that manipulate and analyze tables. However, these methods often require the
entire table as input, leading to scalability challenges due to the positional
bias or context length constraints. In response to these challenges, we
introduce TableRAG, a Retrieval-Augmented Generation (RAG) framework
specifically designed for LM-based table understanding. TableRAG leverages
query expansion combined with schema and cell retrieval to pinpoint crucial
information before providing it to the LMs. This enables more efficient data
encoding and precise retrieval, significantly reducing prompt lengths and
mitigating information loss. We have developed two new million-token benchmarks
from the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's
effectiveness at scale. Our results demonstrate that TableRAG's retrieval
design achieves the highest retrieval quality, leading to the new
state-of-the-art performance on large-scale table understanding."
Diffusion Models in 3D Vision: A Survey,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"In recent years, 3D vision has become a crucial field within computer vision,
powering a wide range of applications such as autonomous driving, robotics,
augmented reality (AR), and medical imaging. This field relies on the accurate
perception, understanding, and reconstruction of 3D scenes from 2D data sources
like images and videos. Diffusion models, originally designed for 2D generative
tasks, offer the potential for more flexible, probabilistic approaches that can
better capture the variability and uncertainty present in real-world 3D data.
However, traditional methods often struggle with efficiency and scalability. In
this paper, we review the state-of-the-art approaches that leverage diffusion
models for 3D visual tasks, including but not limited to 3D object generation,
shape completion, point cloud reconstruction, and scene understanding. We
provide an in-depth discussion of the underlying mathematical principles of
diffusion models, outlining their forward and reverse processes, as well as the
various architectural advancements that enable these models to work with 3D
datasets. We also discuss the key challenges in applying diffusion models to 3D
vision, such as handling occlusions and varying point densities, and the
computational demands of high-dimensional data. Finally, we discuss potential
solutions, including improving computational efficiency, enhancing multimodal
fusion, and exploring the use of large-scale pretraining for better
generalization across 3D tasks. This paper serves as a foundation for future
exploration and development in this rapidly evolving field."
TLDR: Token-Level Detective Reward Model for Large Vision Language Models,cs.LG,Machine Learning,2024-10-07,"Although reward models have been successful in improving multimodal large
language models, the reward models themselves remain brutal and contain minimal
information. Notably, existing reward models only mimic human annotations by
assigning only one binary feedback to any text, no matter how long the text is.
In the realm of multimodal language models, where models are required to
process both images and texts, a naive reward model may learn implicit biases
toward texts and become less grounded in images. In this paper, we propose a
$\textbf{T}$oken-$\textbf{L}$evel $\textbf{D}$etective $\textbf{R}$eward Model
($\textbf{TLDR}$) to provide fine-grained annotations to each text token. We
first introduce a perturbation-based method to generate synthetic hard
negatives and their token-level labels to train TLDR models. Then we show the
rich usefulness of TLDR models both in assisting off-the-shelf models to
self-correct their generations, and in serving as a hallucination evaluation
tool. Finally, we show that TLDR models can significantly speed up human
annotation by 3 times to acquire a broader range of high-quality vision
language data."
PredFormer: Transformers Are Effective Spatial-Temporal Predictive Learners,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Spatiotemporal predictive learning methods generally fall into two
categories: recurrent-based approaches, which face challenges in
parallelization and performance, and recurrent-free methods, which employ
convolutional neural networks (CNNs) as encoder-decoder architectures. These
methods benefit from strong inductive biases but often at the expense of
scalability and generalization. This paper proposes PredFormer, a pure
transformer-based framework for spatiotemporal predictive learning. Motivated
by the Vision Transformers (ViT) design, PredFormer leverages carefully
designed Gated Transformer blocks, following a comprehensive analysis of 3D
attention mechanisms, including full-, factorized-, and interleaved-
spatial-temporal attention. With its recurrent-free, transformer-based design,
PredFormer is both simple and efficient, significantly outperforming previous
methods by large margins. Extensive experiments on synthetic and real-world
datasets demonstrate that PredFormer achieves state-of-the-art performance. On
Moving MNIST, PredFormer achieves a 51.3% reduction in MSE relative to SimVP.
For TaxiBJ, the model decreases MSE by 33.1% and boosts FPS from 533 to 2364.
Additionally, on WeatherBench, it reduces MSE by 11.1% while enhancing FPS from
196 to 404. These performance gains in both accuracy and efficiency demonstrate
PredFormer's potential for real-world applications. The source code will be
released at https://github.com/yyyujintang/PredFormer."
Efficient transformer with reinforced position embedding for language models,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"In this paper, we propose an efficient transformer architecture that uses
reinforced positional embedding to obtain superior performance with half the
number of encoder decoder layers. We demonstrate that concatenating positional
encoding with trainable token embeddings, normalizing columns in the token
embedding matrix, and using the normalized token embedding matrix as the value
of the attention layer improve the training and validation loss and the
training time in an encoder-decoder Transformer model for a Portuguese-English
translation task with 10 epochs or 12 hours of training across 10 trials. Our
method, with roughly a threefold parameter reduction compared to the baseline
model, yields a mean training loss of 1.21, a mean validation loss of 1.51, and
an average training time of 1352.27 seconds per epoch, surpassing the baseline
model with the same embedding dimension that employs addition of positional
encoding and token embeddings, which achieves a mean training loss of 1.96, a
validation loss of 2.18, and an average training time of 4297.79 seconds per
epoch. Additionally, we evaluated our proposed architecture and the baseline
across 14 diverse translation datasets from TensorFlow. The results indicate
that our method consistently achieves lower or comparable training and
validation losses, suggesting enhanced learning efficiency."
Forgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-context Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Numerous recent works target to extend effective context length for language
models and various methods, tasks and benchmarks exist to measure model's
effective memorization length. However, through thorough investigations, we
find limitations for currently existing evaluations on model's memorization
capability. We provide an extensive survey for limitations in this work and
propose a new method called forgetting curve to measure the memorization
capability of long-context models. We show that forgetting curve has the
advantage of being robust to the tested corpus and the experimental settings,
of not relying on prompts and can be applied to any model size.
  We apply our forgetting curve to a large variety of models involving both
transformer and RNN/SSM based architectures. Our measurement provides empirical
evidence for the effectiveness of transformer extension techniques while raises
questions for the effective length of RNN/SSM based models. We also examine the
difference between our measurement and existing benchmarks as well as popular
metrics for various models. Our code and results can be found at
https://github.com/1azybug/ForgettingCurve."
ProtoNAM: Prototypical Neural Additive Models for Interpretable Deep Tabular Learning,cs.LG,Machine Learning,2024-10-07,"Generalized additive models (GAMs) have long been a powerful white-box tool
for the intelligible analysis of tabular data, revealing the influence of each
feature on the model predictions. Despite the success of neural networks (NNs)
in various domains, their application as NN-based GAMs in tabular data analysis
remains suboptimal compared to tree-based ones, and the opacity of encoders in
NN-GAMs also prevents users from understanding how networks learn the
functions. In this work, we propose a new deep tabular learning method, termed
Prototypical Neural Additive Model (ProtoNAM), which introduces prototypes into
neural networks in the framework of GAMs. With the introduced prototype-based
feature activation, ProtoNAM can flexibly model the irregular mapping from
tabular features to the outputs while maintaining the explainability of the
final prediction. We also propose a gradient-boosting inspired hierarchical
shape function modeling method, facilitating the discovery of complex feature
patterns and bringing transparency into the learning process of each network
layer. Our empirical evaluations demonstrate that ProtoNAM outperforms all
existing NN-based GAMs, while providing additional insights into the shape
function learned for each feature. The source code of ProtoNAM is available at
\url{https://github.com/Teddy-XiongGZ/ProtoNAM}."
A Strategy for Label Alignment in Deep Neural Networks,cs.LG,Machine Learning,2024-10-07,"One recent research demonstrated successful application of the label
alignment property for unsupervised domain adaptation in a linear regression
settings. Instead of regularizing representation learning to be domain
invariant, the research proposed to regularize the linear regression model to
align with the top singular vectors of the data matrix from the target domain.
In this work we expand upon this idea and generalize it to the case of deep
learning, where we derive an alternative formulation of the original adaptation
algorithm exploiting label alignment suitable for deep neural network. We also
perform experiments to demonstrate that our approach achieves comparable
performance to mainstream unsupervised domain adaptation methods while having
stabler convergence. All experiments and implementations in our work can be
found at the following codebase:
\url{https://github.com/xuanrui-work/DeepLabelAlignment}."
ACDC: Autoregressive Coherent Multimodal Generation using Diffusion Correction,cs.LG,Machine Learning,2024-10-07,"Autoregressive models (ARMs) and diffusion models (DMs) represent two leading
paradigms in generative modeling, each excelling in distinct areas: ARMs in
global context modeling and long-sequence generation, and DMs in generating
high-quality local contexts, especially for continuous data such as images and
short videos. However, ARMs often suffer from exponential error accumulation
over long sequences, leading to physically implausible results, while DMs are
limited by their local context generation capabilities. In this work, we
introduce Autoregressive Coherent multimodal generation with Diffusion
Correction (ACDC), a zero-shot approach that combines the strengths of both
ARMs and DMs at the inference stage without the need for additional
fine-tuning. ACDC leverages ARMs for global context generation and
memory-conditioned DMs for local correction, ensuring high-quality outputs by
correcting artifacts in generated multimodal tokens. In particular, we propose
a memory module based on large language models (LLMs) that dynamically adjusts
the conditioning texts for the DMs, preserving crucial global context
information. Our experiments on multimodal tasks, including coherent
multi-frame story generation and autoregressive video generation, demonstrate
that ACDC effectively mitigates the accumulation of errors and significantly
enhances the quality of generated outputs, achieving superior performance while
remaining agnostic to specific ARM and DM architectures. Project page:
https://acdc2025.github.io/"
Domains as Objectives: Domain-Uncertainty-Aware Policy Optimization through Explicit Multi-Domain Convex Coverage Set Learning,cs.RO,Robotics,2024-10-07,"The problem of uncertainty is a feature of real world robotics problems and
any control framework must contend with it in order to succeed in real
applications tasks. Reinforcement Learning is no different, and epistemic
uncertainty arising from model uncertainty or misspecification is a challenge
well captured by the sim-to-real gap. A simple solution to this issue is domain
randomization (DR), which unfortunately can result in conservative agents. As a
remedy to this conservativeness, the use of universal policies that take
additional information about the randomized domain has risen as an alternative
solution, along with recurrent neural network-based controllers.
Uncertainty-aware universal policies present a particularly compelling solution
able to account for system identification uncertainties during deployment. In
this paper, we reveal that the challenge of efficiently optimizing
uncertainty-aware policies can be fundamentally reframed as solving the convex
coverage set (CCS) problem within a multi-objective reinforcement learning
(MORL) context. By introducing a novel Markov decision process (MDP) framework
where each domain's performance is treated as an independent objective, we
unify the training of uncertainty-aware policies with MORL approaches. This
connection enables the application of MORL algorithms for domain randomization
(DR), allowing for more efficient policy optimization. To illustrate this, we
focus on the linear utility function, which aligns with the expectation in DR
formulations, and propose a series of algorithms adapted from the MORL
literature to solve the CCS, demonstrating their ability to enhance the
performance of uncertainty-aware policies."
$\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Understanding and accurately following instructions is critical for large
language models (LLMs) to be effective across diverse tasks. In this work, we
rigorously examine the key factors that enable models to generalize to unseen
instructions, providing insights to guide the collection of data for
instruction-tuning. Through controlled experiments, inspired by the
Turing-complete Markov algorithm, we demonstrate that such generalization
$\textbf{only emerges}$ when training data is diversified enough across
semantic domains. Our findings also reveal that merely diversifying within
limited domains fails to ensure robust generalization. In contrast,
cross-domain data diversification, even under constrained data budgets,
significantly enhances a model's adaptability. We further extend our analysis
to real-world scenarios, including fine-tuning of
$\textit{$\textbf{specialist}$}$ and $\textit{$\textbf{generalist}$}$ models.
In both cases, we demonstrate that 1) better performance can be achieved by
increasing the diversity of an established dataset while keeping the data size
constant, and 2) when scaling up the data, diversifying the semantics of
instructions is more effective than simply increasing the quantity of similar
data. Our research provides important insights for dataset collation,
particularly when optimizing model performance by expanding training data for
both specialist and generalist scenarios. We show that careful consideration of
data diversification is key: training specialist models with data extending
beyond their core domain leads to significant performance improvements, while
generalist models benefit from diverse data mixtures that enhance their overall
instruction-following capabilities across a wide range of applications. Our
results highlight the critical role of strategic diversification and offer
clear guidelines for improving data quality."
H-SIREN: Improving implicit neural representations with hyperbolic periodic functions,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Implicit neural representations (INR) have been recently adopted in various
applications ranging from computer vision tasks to physics simulations by
solving partial differential equations. Among existing INR-based works,
multi-layer perceptrons with sinusoidal activation functions find widespread
applications and are also frequently treated as a baseline for the development
of better activation functions for INR applications. Recent investigations
claim that the use of sinusoidal activation functions could be sub-optimal due
to their limited supported frequency set as well as their tendency to generate
over-smoothed solutions. We provide a simple solution to mitigate such an issue
by changing the activation function at the first layer from $\sin(x)$ to
$\sin(\sinh(2x))$. We demonstrate H-SIREN in various computer vision and fluid
flow problems, where it surpasses the performance of several state-of-the-art
INRs."
Rule-based Data Selection for Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"The quality of training data significantly impacts the performance of large
language models (LLMs). There are increasing studies using LLMs to rate and
select data based on several human-crafted metrics (rules). However, these
conventional rule-based approaches often depend too heavily on human
heuristics, lack effective metrics for assessing rules, and exhibit limited
adaptability to new tasks. In our study, we introduce an innovative rule-based
framework that utilizes the orthogonality of score vectors associated with
rules as a novel metric for rule evaluations. Our approach includes an
automated pipeline that first uses LLMs to generate a diverse set of rules,
encompassing various rating dimensions to evaluate data quality. Then it rates
a batch of data based on these rules and uses the determinantal point process
(DPP) from random matrix theory to select the most orthogonal score vectors,
thereby identifying a set of independent rules. These rules are subsequently
used to evaluate all data, selecting samples with the highest average scores
for downstream tasks such as LLM training. We verify the effectiveness of our
method through two experimental setups: 1) comparisons with ground truth
ratings and 2) benchmarking LLMs trained with the chosen data. Our
comprehensive experiments cover a range of scenarios, including general
pre-training and domain-specific fine-tuning in areas such as IMDB, Medical,
Math, and Code. The outcomes demonstrate that our DPP-based rule rating method
consistently outperforms other approaches, including rule-free rating, uniform
sampling, importance resampling, and QuRating, in terms of both rating
precision and model performance."
"Tight Stability, Convergence, and Robustness Bounds for Predictive Coding Networks",cs.LG,Machine Learning,2024-10-07,"Energy-based learning algorithms, such as predictive coding (PC), have
garnered significant attention in the machine learning community due to their
theoretical properties, such as local operations and biologically plausible
mechanisms for error correction. In this work, we rigorously analyze the
stability, robustness, and convergence of PC through the lens of dynamical
systems theory. We show that, first, PC is Lyapunov stable under mild
assumptions on its loss and residual energy functions, which implies intrinsic
robustness to small random perturbations due to its well-defined
energy-minimizing dynamics. Second, we formally establish that the PC updates
approximate quasi-Newton methods by incorporating higher-order curvature
information, which makes them more stable and able to converge with fewer
iterations compared to models trained via backpropagation (BP). Furthermore,
using this dynamical framework, we provide new theoretical bounds on the
similarity between PC and other algorithms, i.e., BP and target propagation
(TP), by precisely characterizing the role of higher-order derivatives. These
bounds, derived through detailed analysis of the Hessian structures, show that
PC is significantly closer to quasi-Newton updates than TP, providing a deeper
understanding of the stability and efficiency of PC compared to conventional
learning methods."
Learning How Hard to Think: Input-Adaptive Allocation of LM Computation,cs.LG,Machine Learning,2024-10-07,"Computationally intensive decoding procedures--including search, reranking,
and self-critique--can improve the quality of language model (LM) outputs in
problems spanning code generation, numerical reasoning, and dialog. Existing
work typically applies the same decoding procedure for every input to an LM.
But not all inputs require the same amount of computation to process. Can we
allocate decoding computation adaptively, using more resources to answer
questions whose answers will be harder to compute? We present an approach that
predicts the distribution of rewards given an input and computation budget,
then allocates additional computation to inputs for which it is predicted to be
most useful. We apply this approach in two decoding procedures: first, an
adaptive best-of-k procedure that dynamically selects the number of samples to
generate as input to a reranker; second, a routing procedure that dynamically
responds to a query using a decoding procedure that is expensive but accurate,
or one that is cheaper but less capable. Across a suite of programming,
mathematics, and dialog tasks, we show that accurate computation-allocation
procedures can be learned, and reduce computation by up to 50% at no cost to
response quality, or improve quality by up to 10% at a fixed computational
budget."
Generating CAD Code with Vision-Language Models for 3D Designs,cs.LG,Machine Learning,2024-10-07,"Generative AI has transformed the fields of Design and Manufacturing by
providing efficient and automated methods for generating and modifying 3D
objects. One approach involves using Large Language Models (LLMs) to generate
Computer- Aided Design (CAD) scripting code, which can then be executed to
render a 3D object; however, the resulting 3D object may not meet the specified
requirements. Testing the correctness of CAD generated code is challenging due
to the complexity and structure of 3D objects (e.g., shapes, surfaces, and
dimensions) that are not feasible in code. In this paper, we introduce
CADCodeVerify, a novel approach to iteratively verify and improve 3D objects
generated from CAD code. Our approach works by producing ameliorative feedback
by prompting a Vision-Language Model (VLM) to generate and answer a set of
validation questions to verify the generated object and prompt the VLM to
correct deviations. To evaluate CADCodeVerify, we introduce, CADPrompt, the
first benchmark for CAD code generation, consisting of 200 natural language
prompts paired with expert-annotated scripting code for 3D objects to benchmark
progress. Our findings show that CADCodeVerify improves VLM performance by
providing visual feedback, enhancing the structure of the 3D objects, and
increasing the success rate of the compiled program. When applied to GPT-4,
CADCodeVerify achieved a 7.30% reduction in Point Cloud distance and a 5.0%
improvement in success rate compared to prior work"
Neural Fourier Modelling: A Highly Compact Approach to Time-Series Analysis,cs.LG,Machine Learning,2024-10-07,"Neural time-series analysis has traditionally focused on modeling data in the
time domain, often with some approaches incorporating equivalent Fourier domain
representations as auxiliary spectral features. In this work, we shift the main
focus to frequency representations, modeling time-series data fully and
directly in the Fourier domain. We introduce Neural Fourier Modelling (NFM), a
compact yet powerful solution for time-series analysis. NFM is grounded in two
key properties of the Fourier transform (FT): (i) the ability to model
finite-length time series as functions in the Fourier domain, treating them as
continuous-time elements in function space, and (ii) the capacity for data
manipulation (such as resampling and timespan extension) within the Fourier
domain. We reinterpret Fourier-domain data manipulation as frequency
extrapolation and interpolation, incorporating this as a core learning
mechanism in NFM, applicable across various tasks. To support flexible
frequency extension with spectral priors and effective modulation of frequency
representations, we propose two learning modules: Learnable Frequency Tokens
(LFT) and Implicit Neural Fourier Filters (INFF). These modules enable compact
and expressive modeling in the Fourier domain. Extensive experiments
demonstrate that NFM achieves state-of-the-art performance on a wide range of
tasks (forecasting, anomaly detection, and classification), including
challenging time-series scenarios with previously unseen sampling rates at test
time. Moreover, NFM is highly compact, requiring fewer than 40K parameters in
each task, with time-series lengths ranging from 100 to 16K."
Proceedings of the First International Workshop on Next-Generation Language Models for Knowledge Representation and Reasoning (NeLaMKRR 2024),cs.AI,Artificial Intelligence,2024-10-07,"Reasoning is an essential component of human intelligence as it plays a
fundamental role in our ability to think critically, support responsible
decisions, and solve challenging problems. Traditionally, AI has addressed
reasoning in the context of logic-based representations of knowledge. However,
the recent leap forward in natural language processing, with the emergence of
language models based on transformers, is hinting at the possibility that these
models exhibit reasoning abilities, particularly as they grow in size and are
trained on more data. Despite ongoing discussions about what reasoning is in
language models, it is still not easy to pin down to what extent these models
are actually capable of reasoning.
  The goal of this workshop is to create a platform for researchers from
different disciplines and/or AI perspectives, to explore approaches and
techniques with the aim to reconcile reasoning between language models using
transformers and using logic-based representations. The specific objectives
include analyzing the reasoning abilities of language models measured alongside
KR methods, injecting KR-style reasoning abilities into language models
(including by neuro-symbolic means), and formalizing the kind of reasoning
language models carry out. This exploration aims to uncover how language models
can effectively integrate and leverage knowledge and reasoning with it, thus
improving their application and utility in areas where precision and
reliability are a key requirement."
"The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?",cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Large Language Models (LLMs) have shown capabilities close to human
performance in various analytical tasks, leading researchers to use them for
time and labor-intensive analyses. However, their capability to handle highly
specialized and open-ended tasks in domains like policy studies remains in
question. This paper investigates the efficiency and accuracy of LLMs in
specialized tasks through a structured user study focusing on Human-LLM
partnership. The study, conducted in two stages-Topic Discovery and Topic
Assignment-integrates LLMs with expert annotators to observe the impact of LLM
suggestions on what is usually human-only analysis. Results indicate that
LLM-generated topic lists have significant overlap with human generated topic
lists, with minor hiccups in missing document-specific topics. However, LLM
suggestions may significantly improve task completion speed, but at the same
time introduce anchoring bias, potentially affecting the depth and nuance of
the analysis, raising a critical question about the trade-off between increased
efficiency and the risk of biased analysis."
MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Recent large language models (LLMs) have demonstrated versatile capabilities
in long-context scenarios. Although some recent benchmarks have been developed
to evaluate the long-context capabilities of LLMs, there is a lack of
benchmarks evaluating the mathematical reasoning abilities of LLMs over long
contexts, which is crucial for LLMs' application in real-world scenarios. In
this paper, we introduce MathHay, an automated benchmark designed to assess the
long-context mathematical reasoning capabilities of LLMs. Unlike previous
benchmarks like Needle in a Haystack, which focus primarily on information
retrieval within long texts, MathHay demands models with both
information-seeking and complex mathematical reasoning abilities. We conduct
extensive experiments on MathHay to assess the long-context mathematical
reasoning abilities of eight top-performing LLMs. Even the best-performing
model, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over
long contexts, achieving only 51.26% accuracy at 128K tokens. This highlights
the significant room for improvement on the MathHay benchmark."
A Clifford Algebraic Approach to E(n)-Equivariant High-order Graph Neural Networks,cs.LG,Machine Learning,2024-10-07,"Designing neural network architectures that can handle data symmetry is
crucial. This is especially important for geometric graphs whose properties are
equivariance under Euclidean transformations. Current equivariant graph neural
networks (EGNNs), particularly those using message passing, have a limitation
in expressive power. Recent high-order graph neural networks can overcome this
limitation, yet they lack equivariance properties, representing a notable
drawback in certain applications in chemistry and physical sciences. In this
paper, we introduce the Clifford Group Equivariant Graph Neural Networks
(CG-EGNNs), a novel EGNN that enhances high-order message passing by
integrating high-order local structures in the context of Clifford algebras. As
a key benefit of using Clifford algebras, CG-EGNN can learn functions that
capture equivariance from positional features. By adopting the high-order
message passing mechanism, CG-EGNN gains richer information from neighbors,
thus improving model performance. Furthermore, we establish the universality
property of the $k$-hop message passing framework, showcasing greater
expressive power of CG-EGNNs with additional $k$-hop message passing mechanism.
We empirically validate that CG-EGNNs outperform previous methods on various
benchmarks including n-body, CMU motion capture, and MD17, highlighting their
effectiveness in geometric deep learning."
Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning,cs.LG,Machine Learning,2024-10-07,"Fine-tuning and in-context learning (ICL) are two prevalent methods in
imbuing large language models with task-specific knowledge. It is commonly
believed that fine-tuning can surpass ICL given sufficient training samples as
it allows the model to adjust its internal parameters based on the data.
However, this paper presents a counterintuitive finding: For tasks with
implicit patterns, ICL captures these patterns significantly better than
fine-tuning. We developed several datasets featuring implicit patterns, such as
sequences determining answers through parity or identifying reducible terms in
calculations. We then evaluated the models' understanding of these patterns
under both fine-tuning and ICL across models ranging from 0.5B to 7B
parameters. The results indicate that models employing ICL can quickly grasp
deep patterns and significantly improve accuracy. In contrast, fine-tuning,
despite utilizing thousands of times more training samples than ICL, achieved
only limited improvements. We also proposed circuit shift theory from a
mechanistic interpretability's view to explain why ICL wins."
Low-Rank Continual Pyramid Vision Transformer: Incrementally Segment Whole-Body Organs in CT with Light-Weighted Adaptation,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Deep segmentation networks achieve high performance when trained on specific
datasets. However, in clinical practice, it is often desirable that pretrained
segmentation models can be dynamically extended to enable segmenting new organs
without access to previous training datasets or without training from scratch.
This would ensure a much more efficient model development and deployment
paradigm accounting for the patient privacy and data storage issues. This
clinically preferred process can be viewed as a continual semantic segmentation
(CSS) problem. Previous CSS works would either experience catastrophic
forgetting or lead to unaffordable memory costs as models expand. In this work,
we propose a new continual whole-body organ segmentation model with
light-weighted low-rank adaptation (LoRA). We first train and freeze a pyramid
vision transformer (PVT) base segmentation model on the initial task, then
continually add light-weighted trainable LoRA parameters to the frozen model
for each new learning task. Through a holistically exploration of the
architecture modification, we identify three most important layers (i.e.,
patch-embedding, multi-head attention and feed forward layers) that are
critical in adapting to the new segmentation tasks, while retaining the
majority of the pretrained parameters fixed. Our proposed model continually
segments new organs without catastrophic forgetting and meanwhile maintaining a
low parameter increasing rate. Continually trained and tested on four datasets
covering different body parts of a total of 121 organs, results show that our
model achieves high segmentation accuracy, closely reaching the PVT and nnUNet
upper bounds, and significantly outperforms other regularization-based CSS
methods. When comparing to the leading architecture-based CSS method, our model
has a substantial lower parameter increasing rate while achieving comparable
performance."
Towards Measuring Goal-Directedness in AI Systems,cs.LG,Machine Learning,2024-10-07,"Recent advances in deep learning have brought attention to the possibility of
creating advanced, general AI systems that outperform humans across many tasks.
However, if these systems pursue unintended goals, there could be catastrophic
consequences. A key prerequisite for AI systems pursuing unintended goals is
whether they will behave in a coherent and goal-directed manner in the first
place, optimizing for some unknown goal; there exists significant research
trying to evaluate systems for said behaviors. However, the most rigorous
definitions of goal-directedness we currently have are difficult to compute in
real-world settings. Drawing upon this previous literature, we explore policy
goal-directedness within reinforcement learning (RL) environments. In our
findings, we propose a different family of definitions of the goal-directedness
of a policy that analyze whether it is well-modeled as near-optimal for many
(sparse) reward functions. We operationalize this preliminary definition of
goal-directedness and test it in toy Markov decision process (MDP)
environments. Furthermore, we explore how goal-directedness could be measured
in frontier large-language models (LLMs). Our contribution is a definition of
goal-directedness that is simpler and more easily computable in order to
approach the question of whether AI systems could pursue dangerous goals. We
recommend further exploration of measuring coherence and goal-directedness,
based on our findings."
On the Adversarial Risk of Test Time Adaptation: An Investigation into Realistic Test-Time Data Poisoning,cs.LG,Machine Learning,2024-10-07,"Test-time adaptation (TTA) updates the model weights during the inference
stage using testing data to enhance generalization. However, this practice
exposes TTA to adversarial risks. Existing studies have shown that when TTA is
updated with crafted adversarial test samples, also known as test-time poisoned
data, the performance on benign samples can deteriorate. Nonetheless, the
perceived adversarial risk may be overstated if the poisoned data is generated
under overly strong assumptions. In this work, we first review realistic
assumptions for test-time data poisoning, including white-box versus grey-box
attacks, access to benign data, attack budget, and more. We then propose an
effective and realistic attack method that better produces poisoned samples
without access to benign samples, and derive an effective in-distribution
attack objective. We also design two TTA-aware attack objectives. Our
benchmarks of existing attack methods reveal that the TTA methods are more
robust than previously believed. In addition, we analyze effective defense
strategies to help develop adversarially robust TTA methods."
Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting,cs.RO,Robotics,2024-10-07,"We propose a framework for active next best view and touch selection for
robotic manipulators using 3D Gaussian Splatting (3DGS). 3DGS is emerging as a
useful explicit 3D scene representation for robotics, as it has the ability to
represent scenes in a both photorealistic and geometrically accurate manner.
However, in real-world, online robotic scenes where the number of views is
limited given efficiency requirements, random view selection for 3DGS becomes
impractical as views are often overlapping and redundant. We address this issue
by proposing an end-to-end online training and active view selection pipeline,
which enhances the performance of 3DGS in few-view robotics settings. We first
elevate the performance of few-shot 3DGS with a novel semantic depth alignment
method using Segment Anything Model 2 (SAM2) that we supplement with Pearson
depth and surface normal loss to improve color and depth reconstruction of
real-world scenes. We then extend FisherRF, a next-best-view selection method
for 3DGS, to select views and touch poses based on depth uncertainty. We
perform online view selection on a real robot system during live 3DGS training.
We motivate our improvements to few-shot GS scenes, and extend depth-based
FisherRF to them, where we demonstrate both qualitative and quantitative
improvements on challenging robot scenes. For more information, please see our
project page at https://armlabstanford.github.io/next-best-sense."
CAR: Controllable Autoregressive Modeling for Visual Generation,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Controllable generation, which enables fine-grained control over generated
outputs, has emerged as a critical focus in visual generative models.
Currently, there are two primary technical approaches in visual generation:
diffusion models and autoregressive models. Diffusion models, as exemplified by
ControlNet and T2I-Adapter, offer advanced control mechanisms, whereas
autoregressive models, despite showcasing impressive generative quality and
scalability, remain underexplored in terms of controllability and flexibility.
In this study, we introduce Controllable AutoRegressive Modeling (CAR), a
novel, plug-and-play framework that integrates conditional control into
multi-scale latent variable modeling, enabling efficient control generation
within a pre-trained visual autoregressive model. CAR progressively refines and
captures control representations, which are injected into each autoregressive
step of the pre-trained model to guide the generation process. Our approach
demonstrates excellent controllability across various types of conditions and
delivers higher image quality compared to previous methods. Additionally, CAR
achieves robust generalization with significantly fewer training resources
compared to those required for pre-training the model. To the best of our
knowledge, we are the first to propose a control framework for pre-trained
autoregressive visual generation models."
A Universal Formulation for Path-Parametric Planning and Control,cs.RO,Robotics,2024-10-07,"This work presents a unified framework for path-parametric planning and
control. This formulation is universal as it standardizes the entire spectrum
of path-parametric techniques -- from traditional path following to more recent
contouring or progress-maximizing Model Predictive Control and Reinforcement
Learning -- under a single framework. The ingredients underlying this
universality are twofold: First, we present a compact and efficient technique
capable of computing singularity-free, smooth and differentiable moving frames.
Second, we derive a spatial path parameterization of the Cartesian coordinates
applicable to any arbitrary curve without prior assumptions on its parametric
speed or moving frame, and that perfectly interplays with the aforementioned
path parameterization method. The combination of these two ingredients leads to
a planning and control framework that brings togehter existing path-parametric
techniques in literature. Aiming to unify all these approaches, we open source
PACOR, a software library that implements the presented content, thereby
providing a self-contained toolkit for the formulation of path-parametric
planning and control methods."
Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"This paper explores optimal architectures for evaluating the outputs of large
language models (LLMs) using LLMs themselves. We propose a novel framework that
interprets LLMs as advocates within an ensemble of interacting agents, allowing
them to defend their answers and reach conclusions through a judge and jury
system. This approach offers a more dynamic and comprehensive evaluation
process compared to traditional human-based assessments or automated metrics.
We discuss the motivation behind this framework, its key components, and
comparative advantages. We also present a probabilistic model to evaluate the
error reduction achieved by iterative advocate systems. Finally, we outline
experiments to validate the effectiveness of multi-advocate architectures and
discuss future research directions."
Federated Learning Nodes Can Reconstruct Peers' Image Data,cs.LG,Machine Learning,2024-10-07,"Federated learning (FL) is a privacy-preserving machine learning framework
that enables multiple nodes to train models on their local data and
periodically average weight updates to benefit from other nodes' training. Each
node's goal is to collaborate with other nodes to improve the model's
performance while keeping its training data private. However, this framework
does not guarantee data privacy. Prior work has shown that the gradient-sharing
steps in FL can be vulnerable to data reconstruction attacks from an
honest-but-curious central server. In this work, we show that an
honest-but-curious node/client can also launch attacks to reconstruct peers'
image data in a centralized system, presenting a severe privacy risk. We
demonstrate that a single client can silently reconstruct other clients'
private images using diluted information available within consecutive updates.
We leverage state-of-the-art diffusion models to enhance the perceptual quality
and recognizability of the reconstructed images, further demonstrating the risk
of information leakage at a semantic level. This highlights the need for more
robust privacy-preserving mechanisms that protect against silent client-side
attacks during federated training."
"Knowledge Graph Based Agent for Complex, Knowledge-Intensive QA in Medicine",cs.AI,Artificial Intelligence,2024-10-07,"Biomedical knowledge is uniquely complex and structured, requiring distinct
reasoning strategies compared to other scientific disciplines like physics or
chemistry. Biomedical scientists do not rely on a single approach to reasoning;
instead, they use various strategies, including rule-based, prototype-based,
and case-based reasoning. This diversity calls for flexible approaches that
accommodate multiple reasoning strategies while leveraging in-domain knowledge.
We introduce KGARevion, a knowledge graph (KG) based agent designed to address
the complexity of knowledge-intensive medical queries. Upon receiving a query,
KGARevion generates relevant triplets by using the knowledge base of the LLM.
These triplets are then verified against a grounded KG to filter out erroneous
information and ensure that only accurate, relevant data contribute to the
final answer. Unlike RAG-based models, this multi-step process ensures
robustness in reasoning while adapting to different models of medical
reasoning. Evaluations on four gold-standard medical QA datasets show that
KGARevion improves accuracy by over 5.2%, outperforming 15 models in handling
complex medical questions. To test its capabilities, we curated three new
medical QA datasets with varying levels of semantic complexity, where KGARevion
achieved a 10.4% improvement in accuracy."
ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-07,"Active perception, a crucial human capability, involves setting a goal based
on the current understanding of the environment and performing actions to
achieve that goal. Despite significant efforts in evaluating Multimodal Large
Language Models (MLLMs), active perception has been largely overlooked. To
address this gap, we propose a novel benchmark named ActiView to evaluate
active perception in MLLMs. Since comprehensively assessing active perception
is challenging, we focus on a specialized form of Visual Question Answering
(VQA) that eases the evaluation yet challenging for existing MLLMs. Given an
image, we restrict the perceptual field of a model, requiring it to actively
zoom or shift its perceptual field based on reasoning to answer the question
successfully. We conduct extensive evaluation over 27 models, including
proprietary and open-source models, and observe that the ability to read and
comprehend multiple images simultaneously plays a significant role in enabling
active perception. Results reveal a significant gap in the active perception
capability of MLLMs, indicating that this area deserves more attention. We hope
that our benchmark could help develop methods for MLLMs to understand
multimodal inputs in more natural and holistic ways."
Contrastive Learning to Improve Retrieval for Real-world Fact Checking,cs.CL,Computation and Language (Natural Language Processing),2024-10-07,"Recent work on fact-checking addresses a realistic setting where models
incorporate evidence retrieved from the web to decide the veracity of claims. A
bottleneck in this pipeline is in retrieving relevant evidence: traditional
methods may surface documents directly related to a claim, but fact-checking
complex claims requires more inferences. For instance, a document about how a
vaccine was developed is relevant to addressing claims about what it might
contain, even if it does not address them directly. We present Contrastive
Fact-Checking Reranker (CFR), an improved retriever for this setting. By
leveraging the AVeriTeC dataset, which annotates subquestions for claims with
human written answers from evidence documents, we fine-tune Contriever with a
contrastive objective based on multiple training signals, including
distillation from GPT-4, evaluating subquestion answers, and gold labels in the
dataset. We evaluate our model on both retrieval and end-to-end veracity
judgments about claims. On the AVeriTeC dataset, we find a 6\% improvement in
veracity classification accuracy. We also show our gains can be transferred to
FEVER, ClaimDecomp, HotpotQA, and a synthetic dataset requiring retrievers to
make inferences."
Graph Fourier Neural Kernels (G-FuNK): Learning Solutions of Nonlinear Diffusive Parametric PDEs on Multiple Domains,cs.LG,Machine Learning,2024-10-06,"Predicting time-dependent dynamics of complex systems governed by non-linear
partial differential equations (PDEs) with varying parameters and domains is a
challenging task motivated by applications across various fields. We introduce
a novel family of neural operators based on our Graph Fourier Neural Kernels,
designed to learn solution generators for nonlinear PDEs in which the
highest-order term is diffusive, across multiple domains and parameters. G-FuNK
combines components that are parameter- and domain-adapted with others that are
not. The domain-adapted components are constructed using a weighted graph on
the discretized domain, where the graph Laplacian approximates the
highest-order diffusive term, ensuring boundary condition compliance and
capturing the parameter and domain-specific behavior. Meanwhile, the learned
components transfer across domains and parameters using our variant Fourier
Neural Operators. This approach naturally embeds geometric and directional
information, improving generalization to new test domains without need for
retraining the network. To handle temporal dynamics, our method incorporates an
integrated ODE solver to predict the evolution of the system. Experiments show
G-FuNK's capability to accurately approximate heat, reaction diffusion, and
cardiac electrophysiology equations across various geometries and anisotropic
diffusivity fields. G-FuNK achieves low relative errors on unseen domains and
fiber fields, significantly accelerating predictions compared to traditional
finite-element solvers."
AdaptDiff: Cross-Modality Domain Adaptation via Weak Conditional Semantic Diffusion for Retinal Vessel Segmentation,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Deep learning has shown remarkable performance in medical image segmentation.
However, despite its promise, deep learning has many challenges in practice due
to its inability to effectively transition to unseen domains, caused by the
inherent data distribution shift and the lack of manual annotations to guide
domain adaptation. To tackle this problem, we present an unsupervised domain
adaptation (UDA) method named AdaptDiff that enables a retinal vessel
segmentation network trained on fundus photography (FP) to produce satisfactory
results on unseen modalities (e.g., OCT-A) without any manual labels. For all
our target domains, we first adopt a segmentation model trained on the source
domain to create pseudo-labels. With these pseudo-labels, we train a
conditional semantic diffusion probabilistic model to represent the target
domain distribution. Experimentally, we show that even with low quality
pseudo-labels, the diffusion model can still capture the conditional semantic
information. Subsequently, we sample on the target domain with binary vessel
masks from the source domain to get paired data, i.e., target domain synthetic
images conditioned on the binary vessel map. Finally, we fine-tune the
pre-trained segmentation network using the synthetic paired data to mitigate
the domain gap. We assess the effectiveness of AdaptDiff on seven publicly
available datasets across three distinct modalities. Our results demonstrate a
significant improvement in segmentation performance across all unseen datasets.
Our code is publicly available at https://github.com/DeweiHu/AdaptDiff."
Mode-GS: Monocular Depth Guided Anchored 3D Gaussian Splatting for Robust Ground-View Scene Rendering,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"We present a novel-view rendering algorithm, Mode-GS, for ground-robot
trajectory datasets. Our approach is based on using anchored Gaussian splats,
which are designed to overcome the limitations of existing 3D Gaussian
splatting algorithms. Prior neural rendering methods suffer from severe splat
drift due to scene complexity and insufficient multi-view observation, and can
fail to fix splats on the true geometry in ground-robot datasets. Our method
integrates pixel-aligned anchors from monocular depths and generates Gaussian
splats around these anchors using residual-form Gaussian decoders. To address
the inherent scale ambiguity of monocular depth, we parameterize anchors with
per-view depth-scales and employ scale-consistent depth loss for online scale
calibration. Our method results in improved rendering performance, based on
PSNR, SSIM, and LPIPS metrics, in ground scenes with free trajectory patterns,
and achieves state-of-the-art rendering performance on the R3LIVE odometry
dataset and the Tanks and Temples dataset."
The Optimization Landscape of SGD Across the Feature Learning Strength,cs.LG,Machine Learning,2024-10-06,"We consider neural networks (NNs) where the final layer is down-scaled by a
fixed hyperparameter $\gamma$. Recent work has identified $\gamma$ as
controlling the strength of feature learning. As $\gamma$ increases, network
evolution changes from ""lazy"" kernel dynamics to ""rich"" feature-learning
dynamics, with a host of associated benefits including improved performance on
common tasks. In this work, we conduct a thorough empirical investigation of
the effect of scaling $\gamma$ across a variety of models and datasets in the
online training setting. We first examine the interaction of $\gamma$ with the
learning rate $\eta$, identifying several scaling regimes in the
$\gamma$-$\eta$ plane which we explain theoretically using a simple model. We
find that the optimal learning rate $\eta^*$ scales non-trivially with
$\gamma$. In particular, $\eta^* \propto \gamma^2$ when $\gamma \ll 1$ and
$\eta^* \propto \gamma^{2/L}$ when $\gamma \gg 1$ for a feed-forward network of
depth $L$. Using this optimal learning rate scaling, we proceed with an
empirical study of the under-explored ""ultra-rich"" $\gamma \gg 1$ regime. We
find that networks in this regime display characteristic loss curves, starting
with a long plateau followed by a drop-off, sometimes followed by one or more
additional staircase steps. We find networks of different large $\gamma$ values
optimize along similar trajectories up to a reparameterization of time. We
further find that optimal online performance is often found at large $\gamma$
and could be missed if this hyperparameter is not tuned. Our findings indicate
that analytical study of the large-$\gamma$ limit may yield useful insights
into the dynamics of representation learning in performant models."
Unpacking Failure Modes of Generative Policies: Runtime Monitoring of Consistency and Progress,cs.RO,Robotics,2024-10-06,"Robot behavior policies trained via imitation learning are prone to failure
under conditions that deviate from their training data. Thus, algorithms that
monitor learned policies at test time and provide early warnings of failure are
necessary to facilitate scalable deployment. We propose Sentinel, a runtime
monitoring framework that splits the detection of failures into two
complementary categories: 1) Erratic failures, which we detect using
statistical measures of temporal action consistency, and 2) task progression
failures, where we use Vision Language Models (VLMs) to detect when the policy
confidently and consistently takes actions that do not solve the task. Our
approach has two key strengths. First, because learned policies exhibit diverse
failure modes, combining complementary detectors leads to significantly higher
accuracy at failure detection. Second, using a statistical temporal action
consistency measure ensures that we quickly detect when multimodal, generative
policies exhibit erratic behavior at negligible computational cost. In
contrast, we only use VLMs to detect failure modes that are less
time-sensitive. We demonstrate our approach in the context of diffusion
policies trained on robotic mobile manipulation domains in both simulation and
the real world. By unifying temporal consistency detection and VLM runtime
monitoring, Sentinel detects 18% more failures than using either of the two
detectors alone and significantly outperforms baselines, thus highlighting the
importance of assigning specialized detectors to complementary categories of
failure. Qualitative results are made available at
https://sites.google.com/stanford.edu/sentinel."
Radial Basis Operator Networks,cs.LG,Machine Learning,2024-10-06,"Operator networks are designed to approximate nonlinear operators, which
provide mappings between infinite-dimensional spaces such as function spaces.
These networks are playing an increasingly important role in machine learning,
with their most notable contributions in the field of scientific computing.
Their significance stems from their ability to handle the type of data often
encountered in scientific applications. For instance, in climate modeling or
fluid dynamics, input data typically consists of discretized continuous fields
(like temperature distributions or velocity fields). We introduce the radial
basis operator network (RBON), which represents a significant advancement as
the first operator network capable of learning an operator in both the time
domain and frequency domain when adjusted to accept complex-valued inputs.
Despite the small, single hidden-layer structure, the RBON boasts small $L^2$
relative test error for both in- and out-of-distribution data (OOD) of less
than $1\times 10^{-7}$ in some benchmark cases. Moreover, the RBON maintains
small error on OOD data from entirely different function classes from the
training data."
Provable Weak-to-Strong Generalization via Benign Overfitting,cs.LG,Machine Learning,2024-10-06,"The classic teacher-student model in machine learning posits that a strong
teacher supervises a weak student to improve the student's capabilities. We
instead consider the inverted situation, where a weak teacher supervises a
strong student with imperfect pseudolabels. This paradigm was recently brought
forth by Burns et al.'23 and termed \emph{weak-to-strong generalization}. We
theoretically investigate weak-to-strong generalization for binary and
multilabel classification in a stylized overparameterized spiked covariance
model with Gaussian covariates where the weak teacher's pseudolabels are
asymptotically like random guessing. Under these assumptions, we provably
identify two asymptotic phases of the strong student's generalization after
weak supervision: (1) successful generalization and (2) random guessing. Our
techniques should eventually extend to weak-to-strong multiclass
classification. Towards doing so, we prove a tight lower tail inequality for
the maximum of correlated Gaussians, which may be of independent interest.
Understanding the multilabel setting reinforces the value of using logits for
weak supervision when they are available."
Is What You Ask For What You Get? Investigating Concept Associations in Text-to-Image Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Text-to-image (T2I) models are increasingly used in impactful real-life
applications. As such, there is a growing need to audit these models to ensure
that they generate desirable, task-appropriate images. However, systematically
inspecting the associations between prompts and generated content in a
human-understandable way remains challenging. To address this, we propose
\emph{Concept2Concept}, a framework where we characterize conditional
distributions of vision language models using interpretable concepts and
metrics that can be defined in terms of these concepts. This characterization
allows us to use our framework to audit models and prompt-datasets. To
demonstrate, we investigate several case studies of conditional distributions
of prompts, such as user defined distributions or empirical, real world
distributions. Lastly, we implement Concept2Concept as an open-source
interactive visualization tool facilitating use by non-technical end-users.
  Warning: This paper contains discussions of harmful content, including CSAM
and NSFW material, which may be disturbing to some readers."
A Cross-Lingual Meta-Learning Method Based on Domain Adaptation for Speech Emotion Recognition,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Best-performing speech models are trained on large amounts of data in the
language they are meant to work for. However, most languages have sparse data,
making training models challenging. This shortage of data is even more
prevalent in speech emotion recognition. Our work explores the model's
performance in limited data, specifically for speech emotion recognition.
Meta-learning specializes in improving the few-shot learning. As a result, we
employ meta-learning techniques on speech emotion recognition tasks, accent
recognition, and person identification. To this end, we propose a series of
improvements over the multistage meta-learning method. Unlike other works
focusing on smaller models due to the high computational cost of meta-learning
algorithms, we take a more practical approach. We incorporate a large
pre-trained backbone and a prototypical network, making our methods more
feasible and applicable. Our most notable contribution is an improved
fine-tuning technique during meta-testing that significantly boosts the
performance on out-of-distribution datasets. This result, together with
incremental improvements from several other works, helped us achieve accuracy
scores of 83.78% and 56.30% for Greek and Romanian speech emotion recognition
datasets not included in the training or validation splits in the context of
4-way 5-shot learning."
DeepLTL: Learning to Efficiently Satisfy Complex LTL Specifications,cs.AI,Artificial Intelligence,2024-10-06,"Linear temporal logic (LTL) has recently been adopted as a powerful formalism
for specifying complex, temporally extended tasks in reinforcement learning
(RL). However, learning policies that efficiently satisfy arbitrary
specifications not observed during training remains a challenging problem.
Existing approaches suffer from several shortcomings: they are often only
applicable to finite-horizon fragments of LTL, are restricted to suboptimal
solutions, and do not adequately handle safety constraints. In this work, we
propose a novel learning approach to address these concerns. Our method
leverages the structure of B\""uchi automata, which explicitly represent the
semantics of LTL specifications, to learn policies conditioned on sequences of
truth assignments that lead to satisfying the desired formulae. Experiments in
a variety of discrete and continuous domains demonstrate that our approach is
able to zero-shot satisfy a wide range of finite- and infinite-horizon
specifications, and outperforms existing methods in terms of both satisfaction
probability and efficiency."
Control Large Language Models via Divide and Conquer,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"This paper investigates controllable generation for large language models
(LLMs) with prompt-based control, focusing on Lexically Constrained Generation
(LCG). We systematically evaluate the performance of LLMs on satisfying lexical
constraints with prompt-based control, as well as their efficacy in downstream
applications. We conclude that LLMs face significant challenges in consistently
satisfying lexical constraints with prompt-based control. We identified three
key limitations of LLMs for LCG, including (1) position bias, where LLMs tend
to satisfy constraints that appear in specific positions within the input; (2)
low responsiveness to decoding parameters, which render minimal impact on
control of LLMs; and (3) struggle with handling the inherent complexity of
certain constraints (e.g., compound words). To address these issues, we
introduce a Divide and Conquer Generation strategy, effective for both
white-box and black-box LLMs, to enhance LLMs performance in LCG tasks, which
demonstrates over 90% improvement on success rate in the most challenging LCG
task. Our analysis provides valuable insights into the performance of LLMs in
LCG with prompt-based control, and our proposed strategy offers a pathway to
more sophisticated and customized text generation applications."
Punctuation Prediction for Polish Texts using Transformers,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Speech recognition systems typically output text lacking punctuation.
However, punctuation is crucial for written text comprehension. To tackle this
problem, Punctuation Prediction models are developed. This paper describes a
solution for Poleval 2022 Task 1: Punctuation Prediction for Polish Texts,
which scores 71.44 Weighted F1. The method utilizes a single HerBERT model
finetuned to the competition data and an external dataset."
Passage Retrieval of Polish Texts Using OKAPI BM25 and an Ensemble of Cross Encoders,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Passage Retrieval has traditionally relied on lexical methods like TF-IDF and
BM25. Recently, some neural network models have surpassed these methods in
performance. However, these models face challenges, such as the need for large
annotated datasets and adapting to new domains. This paper presents a winning
solution to the Poleval 2023 Task 3: Passage Retrieval challenge, which
involves retrieving passages of Polish texts in three domains: trivia, legal,
and customer support. However, only the trivia domain was used for training and
development data. The method used the OKAPI BM25 algorithm to retrieve
documents and an ensemble of publicly available multilingual Cross Encoders for
Reranking. Fine-tuning the reranker models slightly improved performance but
only in the training domain, while it worsened in other domains."
Towards Unsupervised Blind Face Restoration using Diffusion Prior,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Blind face restoration methods have shown remarkable performance,
particularly when trained on large-scale synthetic datasets with supervised
learning. These datasets are often generated by simulating low-quality face
images with a handcrafted image degradation pipeline. The models trained on
such synthetic degradations, however, cannot deal with inputs of unseen
degradations. In this paper, we address this issue by using only a set of input
images, with unknown degradations and without ground truth targets, to
fine-tune a restoration model that learns to map them to clean and contextually
consistent outputs. We utilize a pre-trained diffusion model as a generative
prior through which we generate high quality images from the natural image
distribution while maintaining the input image content through consistency
constraints. These generated images are then used as pseudo targets to
fine-tune a pre-trained restoration model. Unlike many recent approaches that
employ diffusion models at test time, we only do so during training and thus
maintain an efficient inference-time performance. Extensive experiments show
that the proposed approach can consistently improve the perceptual quality of
pre-trained blind face restoration models while maintaining great consistency
with the input contents. Our best model also achieves the state-of-the-art
results on both synthetic and real-world datasets."
Evaluation of Code LLMs on Geospatial Code Generation,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Software development support tools have been studied for a long time, with
recent approaches using Large Language Models (LLMs) for code generation. These
models can generate Python code for data science and machine learning
applications. LLMs are helpful for software engineers because they increase
productivity in daily work. An LLM can also serve as a ""mentor"" for
inexperienced software developers, and be a viable learning support.
High-quality code generation with LLMs can also be beneficial in geospatial
data science. However, this domain poses different challenges, and code
generation LLMs are typically not evaluated on geospatial tasks. Here, we show
how we constructed an evaluation benchmark for code generation models, based on
a selection of geospatial tasks. We categorised geospatial tasks based on their
complexity and required tools. Then, we created a dataset with tasks that test
model capabilities in spatial reasoning, spatial data processing, and
geospatial tools usage. The dataset consists of specific coding problems that
were manually created for high quality. For every problem, we proposed a set of
test scenarios that make it possible to automatically check the generated code
for correctness. In addition, we tested a selection of existing code generation
LLMs for code generation in the geospatial domain. We share our dataset and
reproducible evaluation code on a public GitHub repository, arguing that this
can serve as an evaluation benchmark for new LLMs in the future. Our dataset
will hopefully contribute to the development new models capable of solving
geospatial coding tasks with high accuracy. These models will enable the
creation of coding assistants tailored for geospatial applications."
LRQ-Fact: LLM-Generated Relevant Questions for Multimodal Fact-Checking,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Human fact-checkers have specialized domain knowledge that allows them to
formulate precise questions to verify information accuracy. However, this
expert-driven approach is labor-intensive and is not scalable, especially when
dealing with complex multimodal misinformation. In this paper, we propose a
fully-automated framework, LRQ-Fact, for multimodal fact-checking. Firstly, the
framework leverages Vision-Language Models (VLMs) and Large Language Models
(LLMs) to generate comprehensive questions and answers for probing multimodal
content. Next, a rule-based decision-maker module evaluates both the original
content and the generated questions and answers to assess the overall veracity.
Extensive experiments on two benchmarks show that LRQ-Fact improves detection
accuracy for multimodal misinformation. Moreover, we evaluate its
generalizability across different model backbones, offering valuable insights
for further refinement."
Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF,cs.LG,Machine Learning,2024-10-06,"Large Language Models (LLMs) have achieved remarkable success at tasks like
summarization that involve a single turn of interaction. However, they can
still struggle with multi-turn tasks like dialogue that require long-term
planning. Previous works on multi-turn dialogue extend single-turn
reinforcement learning from human feedback (RLHF) methods to the multi-turn
setting by treating all prior dialogue turns as a long context. Such approaches
suffer from covariate shift: the conversations in the training set have
previous turns generated by some reference policy, which means that low
training error may not necessarily correspond to good performance when the
learner is actually in the conversation loop. In response, we introduce
REgressing the RELative FUture (REFUEL), an efficient policy optimization
approach designed to address multi-turn RLHF in LLMs. REFUEL employs a single
model to estimate $Q$-values and trains on self-generated data, addressing the
covariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence
of regression tasks on iteratively collected datasets, enabling ease of
implementation. Theoretically, we prove that REFUEL can match the performance
of any policy covered by the training set. Empirically, we evaluate our
algorithm by using Llama-3.1-70B-it to simulate a user in conversation with our
model. REFUEL consistently outperforms state-of-the-art methods such as DPO and
REBEL across various settings. Furthermore, despite having only 8 billion
parameters, Llama-3-8B-it fine-tuned with REFUEL outperforms Llama-3.1-70B-it
on long multi-turn dialogues. Implementation of REFUEL can be found at
https://github.com/ZhaolinGao/REFUEL/, and models trained by REFUEL can be
found at https://huggingface.co/Cornell-AGI."
Distributed Inference on Mobile Edge and Cloud: An Early Exit based Clustering Approach,cs.LG,Machine Learning,2024-10-06,"Recent advances in Deep Neural Networks (DNNs) have demonstrated outstanding
performance across various domains. However, their large size is a challenge
for deployment on resource-constrained devices such as mobile, edge, and IoT
platforms. To overcome this, a distributed inference setup can be used where a
small-sized DNN (initial few layers) can be deployed on mobile, a bigger
version on the edge, and the full-fledged, on the cloud. A sample that has low
complexity (easy) could be then inferred on mobile, that has moderate
complexity (medium) on edge, and higher complexity (hard) on the cloud. As the
complexity of each sample is not known beforehand, the following question
arises in distributed inference: how to decide complexity so that it is
processed by enough layers of DNNs. We develop a novel approach named DIMEE
that utilizes Early Exit (EE) strategies developed to minimize inference
latency in DNNs. DIMEE aims to improve the accuracy, taking into account the
offloading cost from mobile to edge/cloud. Experimental validation on GLUE
datasets, encompassing various NLP tasks, shows that our method significantly
reduces the inference cost (> 43%) while maintaining a minimal drop in accuracy
(< 0.3%) compared to the case where all the inference is made in cloud."
VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"The recent developments in deep learning led to the integration of natural
language processing (NLP) with computer vision, resulting in powerful
integrated Vision and Language Models (VLMs). Despite their remarkable
capabilities, these models are frequently regarded as black boxes within the
machine learning research community. This raises a critical question: which
parts of an image correspond to specific segments of text, and how can we
decipher these associations? Understanding these connections is essential for
enhancing model transparency, interpretability, and trustworthiness. To answer
this question, we present an image-text aligned human visual attention dataset
that maps specific associations between image regions and corresponding text
segments. We then compare the internal heatmaps generated by VL models with
this dataset, allowing us to analyze and better understand the model's
decision-making process. This approach aims to enhance model transparency,
interpretability, and trustworthiness by providing insights into how these
models align visual and linguistic information. We conducted a comprehensive
study on text-guided visual saliency detection in these VL models. This study
aims to understand how different models prioritize and focus on specific visual
elements in response to corresponding text segments, providing deeper insights
into their internal mechanisms and improving our ability to interpret their
outputs."
Privacy's Peril: Unmasking the Unregulated Underground Market of Data Brokers and the Suggested Framework,cs.CR,Cryptography and Security,2024-10-06,"The internet is a common place for businesses to collect and store as much
client data as possible and computer storage capacity has increased
exponentially due to this trend. Businesses utilize this data to enhance
customer satisfaction, generate revenue, boost sales, and increase profile.
However, the emerging sector of data brokers is plagued with legal challenges.
In part I, we will look at what a data broker is, how it collects information,
the data industry, and some of the difficulties it encounters. In Part II, we
will look at potential options for regulating data brokers. All options are
provided in light of the EU General Data Protection Regulation (GDPR). In Part
III, we shall present our analysis and findings."
ProtocoLLM: Automatic Evaluation Framework of LLMs on Domain-Specific Scientific Protocol Formulation Tasks,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Automated generation of scientific protocols executable by robots can
significantly accelerate scientific research processes. Large Language Models
(LLMs) excel at Scientific Protocol Formulation Tasks (SPFT), but the
evaluation of their capabilities rely on human evaluation. Here, we propose a
flexible, automatic framework to evaluate LLM's capability on SPFT: ProtocoLLM.
This framework prompts the target model and GPT-4 to extract pseudocode from
biology protocols using only predefined lab actions and evaluates the output of
target model using LLAM-EVAL, the pseudocode generated by GPT-4 serving as a
baseline and Llama-3 acting as the evaluator. Our adaptable prompt-based
evaluation method, LLAM-EVAL, offers significant flexibility in terms of
evaluation model, material, criteria, and is free of cost. We evaluate GPT
variations, Llama, Mixtral, Gemma, Cohere, and Gemini. Overall, we find that
GPT and Cohere is a powerful scientific protocol formulators. We also introduce
BIOPROT 2.0, a dataset with biology protocols and corresponding pseudocodes,
which can aid LLMs in formulation and evaluation of SPFT. Our work is
extensible to assess LLMs on SPFT across various domains and other fields that
require protocol generation for specific goals."
Towards the first UD Treebank of Spoken Italian: the KIParla forest,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"The present project endeavors to enrich the linguistic resources available
for Italian by constructing a Universal Dependencies treebank for the KIParla
corpus (Mauri et al., 2019, Ballar\`e et al., 2020), an existing and well known
resource for spoken Italian."
Hammer: Robust Function-Calling for On-Device Language Models via Function Masking,cs.LG,Machine Learning,2024-10-06,"Large language models have demonstrated impressive value in performing as
autonomous agents when equipped with external tools and API calls. Nonetheless,
effectively harnessing their potential for executing complex tasks crucially
relies on enhancements in their function calling capabilities. This paper
identifies a critical gap in existing function calling models, where
performance varies significantly across benchmarks, often due to being misled
by specific naming conventions. To address such an issue, we introduce Hammer,
a novel family of foundation models specifically engineered for on-device
function calling. Hammer employs an augmented dataset that enhances models'
sensitivity to irrelevant functions and incorporates function masking
techniques to minimize misleading. Our empirical evaluations reveal that Hammer
not only outperforms larger models but also demonstrates robust generalization
across diverse benchmarks, achieving sota results. Our open source
contributions include a specialized dataset for irrelevance detection, a tuning
framework for enhanced generalization, and the Hammer models, establishing a
new standard for function calling performance."
Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Large language models (LLMs) have demonstrated significant potential in
clinical decision support. Yet LLMs still suffer from hallucinations and lack
fine-grained contextual medical knowledge, limiting their high-stake healthcare
applications such as clinical diagnosis. Traditional retrieval-augmented
generation (RAG) methods attempt to address these limitations but frequently
retrieve sparse or irrelevant information, undermining prediction accuracy. We
introduce KARE, a novel framework that integrates knowledge graph (KG)
community-level retrieval with LLM reasoning to enhance healthcare predictions.
KARE constructs a comprehensive multi-source KG by integrating biomedical
databases, clinical literature, and LLM-generated insights, and organizes it
using hierarchical graph community detection and summarization for precise and
contextually relevant information retrieval. Our key innovations include: (1) a
dense medical knowledge structuring approach enabling accurate retrieval of
relevant information; (2) a dynamic knowledge retrieval mechanism that enriches
patient contexts with focused, multi-faceted medical insights; and (3) a
reasoning-enhanced prediction framework that leverages these enriched contexts
to produce both accurate and interpretable clinical predictions. Extensive
experiments demonstrate that KARE outperforms leading models by up to
10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and
readmission predictions. In addition to its impressive prediction accuracy, our
framework leverages the reasoning capabilities of LLMs, enhancing the
trustworthiness of clinical predictions."
Upsample or Upweight? Balanced Training on Heavily Imbalanced Datasets,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Data availability across domains often follows a long-tail distribution: a
few domains have abundant data, while most face data scarcity. This imbalance
poses challenges in training language models uniformly across all domains. In
our study, we focus on multilingual settings, where data sizes vary
significantly between high- and low-resource languages. Common strategies to
address this include upsampling low-resource languages (Temperature Sampling)
or upweighting their loss (Scalarization). Although often considered
equivalent, this assumption has not been proven, which motivates our study.
Through both theoretical and empirical analysis, we identify the conditions
under which these approaches are equivalent and when they diverge.
Specifically, we demonstrate that these two methods are equivalent under full
gradient descent, but this equivalence breaks down with stochastic gradient
descent. Empirically, we observe that Temperature Sampling converges more
quickly but is prone to overfitting. We argue that this faster convergence is
likely due to the lower variance in gradient estimations, as shown
theoretically. Based on these insights, we propose Cooldown, a strategy that
reduces sampling temperature during training, accelerating convergence without
overfitting to low-resource languages. Our method is competitive with existing
data re-weighting and offers computational efficiency."
Robustness Reprogramming for Representation Learning,cs.LG,Machine Learning,2024-10-06,"This work tackles an intriguing and fundamental open challenge in
representation learning: Given a well-trained deep learning model, can it be
reprogrammed to enhance its robustness against adversarial or noisy input
perturbations without altering its parameters? To explore this, we revisit the
core feature transformation mechanism in representation learning and propose a
novel non-linear robust pattern matching technique as a robust alternative.
Furthermore, we introduce three model reprogramming paradigms to offer flexible
control of robustness under different efficiency requirements. Comprehensive
experiments and ablation studies across diverse learning models ranging from
basic linear model and MLPs to shallow and modern deep ConvNets demonstrate the
effectiveness of our approaches. This work not only opens a promising and
orthogonal direction for improving adversarial defenses in deep learning beyond
existing methods but also provides new insights into designing more resilient
AI systems with robust statistics."
Enhancing 3D Human Pose Estimation Amidst Severe Occlusion with Dual Transformer Fusion,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"In the field of 3D Human Pose Estimation from monocular videos, the presence
of diverse occlusion types presents a formidable challenge. Prior research has
made progress by harnessing spatial and temporal cues to infer 3D poses from 2D
joint observations. This paper introduces a Dual Transformer Fusion (DTF)
algorithm, a novel approach to obtain a holistic 3D pose estimation, even in
the presence of severe occlusions. Confronting the issue of occlusion-induced
missing joint data, we propose a temporal interpolation-based occlusion
guidance mechanism. To enable precise 3D Human Pose Estimation, our approach
leverages the innovative DTF architecture, which first generates a pair of
intermediate views. Each intermediate-view undergoes spatial refinement through
a self-refinement schema. Subsequently, these intermediate-views are fused to
yield the final 3D human pose estimation. The entire system is end-to-end
trainable. Through extensive experiments conducted on the Human3.6M and
MPI-INF-3DHP datasets, our method's performance is rigorously evaluated.
Notably, our approach outperforms existing state-of-the-art methods on both
datasets, yielding substantial improvements. The code is available here:
https://github.com/MehwishG/DTF."
Admissibility Over Winning: A New Approach to Reactive Synthesis in Robotics,cs.RO,Robotics,2024-10-06,"Reactive synthesis is a framework for modeling and automatically synthesizing
strategies in robotics, typically through computing a \emph{winning} strategy
in a 2-player game between the robot and the environment. Winning strategies,
however, do not always exist, even in some simple cases. In such situations, it
is still desirable for the robot to attempt its task rather than ""giving up"".
In this work, we explore the notion of admissibility to define strategies
beyond winning, tailored specifically for robotic systems. We introduce an
ordering of admissible strategies and define \emph{admissibly rational
strategies}, which aim to be winning and cooperative when possible, and
non-violating and hopeful when necessary. We present an efficient synthesis
algorithm and demonstrate that admissibly rational strategies produce desirable
behaviors through case studies."
EnsemW2S: Can an Ensemble of LLMs be Leveraged to Obtain a Stronger LLM?,cs.LG,Machine Learning,2024-10-06,"How can we harness the collective capabilities of multiple Large Language
Models (LLMs) to create an even more powerful model? This question forms the
foundation of our research, where we propose an innovative approach to
weak-to-strong (w2s) generalization-a critical problem in AI alignment. Our
work introduces an easy-to-hard (e2h) framework for studying the feasibility of
w2s generalization, where weak models trained on simpler tasks collaboratively
supervise stronger models on more complex tasks. This setup mirrors real-world
challenges, where direct human supervision is limited. To achieve this, we
develop a novel AdaBoost-inspired ensemble method, demonstrating that an
ensemble of weak supervisors can enhance the performance of stronger LLMs
across classification and generative tasks on difficult QA datasets. In several
cases, our ensemble approach matches the performance of models trained on
ground-truth data, establishing a new benchmark for w2s generalization. We
observe an improvement of up to 14% over existing baselines and average
improvements of 5% and 4% for binary classification and generative tasks,
respectively. This research points to a promising direction for enhancing AI
through collective supervision, especially in scenarios where labeled data is
sparse or insufficient."
Watermarking Decision Tree Ensembles,cs.LG,Machine Learning,2024-10-06,"Protecting the intellectual property of machine learning models is a hot
topic and many watermarking schemes for deep neural networks have been proposed
in the literature. Unfortunately, prior work largely neglected the
investigation of watermarking techniques for other types of models, including
decision tree ensembles, which are a state-of-the-art model for classification
tasks on non-perceptual data. In this paper, we present the first watermarking
scheme designed for decision tree ensembles, focusing in particular on random
forest models. We discuss watermark creation and verification, presenting a
thorough security analysis with respect to possible attacks. We finally perform
an experimental evaluation of the proposed scheme, showing excellent results in
terms of accuracy and security against the most relevant threats."
GAMformer: In-Context Learning for Generalized Additive Models,cs.LG,Machine Learning,2024-10-06,"Generalized Additive Models (GAMs) are widely recognized for their ability to
create fully interpretable machine learning models for tabular data.
Traditionally, training GAMs involves iterative learning algorithms, such as
splines, boosted trees, or neural networks, which refine the additive
components through repeated error reduction. In this paper, we introduce
GAMformer, the first method to leverage in-context learning to estimate shape
functions of a GAM in a single forward pass, representing a significant
departure from the conventional iterative approaches to GAM fitting. Building
on previous research applying in-context learning to tabular data, we
exclusively use complex, synthetic data to train GAMformer, yet find it
extrapolates well to real-world data. Our experiments show that GAMformer
performs on par with other leading GAMs across various classification
benchmarks while generating highly interpretable shape functions."
$\texttt{dattri}$: A Library for Efficient Data Attribution,cs.LG,Machine Learning,2024-10-06,"Data attribution methods aim to quantify the influence of individual training
samples on the prediction of artificial intelligence (AI) models. As training
data plays an increasingly crucial role in the modern development of
large-scale AI models, data attribution has found broad applications in
improving AI performance and safety. However, despite a surge of new data
attribution methods being developed recently, there lacks a comprehensive
library that facilitates the development, benchmarking, and deployment of
different data attribution methods. In this work, we introduce
$\texttt{dattri}$, an open-source data attribution library that addresses the
above needs. Specifically, $\texttt{dattri}$ highlights three novel design
features. Firstly, $\texttt{dattri}$ proposes a unified and easy-to-use API,
allowing users to integrate different data attribution methods into their
PyTorch-based machine learning pipeline with a few lines of code changed.
Secondly, $\texttt{dattri}$ modularizes low-level utility functions that are
commonly used in data attribution methods, such as Hessian-vector product,
inverse-Hessian-vector product or random projection, making it easier for
researchers to develop new data attribution methods. Thirdly, $\texttt{dattri}$
provides a comprehensive benchmark framework with pre-trained models and ground
truth annotations for a variety of benchmark settings, including generative AI
settings. We have implemented a variety of state-of-the-art efficient data
attribution methods that can be applied to large-scale neural network models,
and will continuously update the library in the future. Using the developed
$\texttt{dattri}$ library, we are able to perform a comprehensive and fair
benchmark analysis across a wide range of data attribution methods. The source
code of $\texttt{dattri}$ is available at https://github.com/TRAIS-Lab/dattri."
Bisimulation metric for Model Predictive Control,cs.LG,Machine Learning,2024-10-06,"Model-based reinforcement learning has shown promise for improving sample
efficiency and decision-making in complex environments. However, existing
methods face challenges in training stability, robustness to noise, and
computational efficiency. In this paper, we propose Bisimulation Metric for
Model Predictive Control (BS-MPC), a novel approach that incorporates
bisimulation metric loss in its objective function to directly optimize the
encoder. This time-step-wise direct optimization enables the learned encoder to
extract intrinsic information from the original state space while discarding
irrelevant details and preventing the gradients and errors from diverging.
BS-MPC improves training stability, robustness against input noise, and
computational efficiency by reducing training time. We evaluate BS-MPC on both
continuous control and image-based tasks from the DeepMind Control Suite,
demonstrating superior performance and robustness compared to state-of-the-art
baseline methods."
Learning De-Biased Representations for Remote-Sensing Imagery,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Remote sensing (RS) imagery, requiring specialized satellites to collect and
being difficult to annotate, suffers from data scarcity and class imbalance in
certain spectrums. Due to data scarcity, training any large-scale RS models
from scratch is unrealistic, and the alternative is to transfer pre-trained
models by fine-tuning or a more data-efficient method LoRA. Due to class
imbalance, transferred models exhibit strong bias, where features of the major
class dominate over those of the minor class. In this paper, we propose
debLoRA, a generic training approach that works with any LoRA variants to yield
debiased features. It is an unsupervised learning approach that can diversify
minor class features based on the shared attributes with major classes, where
the attributes are obtained by a simple step of clustering. To evaluate it, we
conduct extensive experiments in two transfer learning scenarios in the RS
domain: from natural to optical RS images, and from optical RS to
multi-spectrum RS images. We perform object classification and oriented object
detection tasks on the optical RS dataset DOTA and the SAR dataset FUSRS.
Results show that our debLoRA consistently surpasses prior arts across these RS
adaptation settings, yielding up to 3.3 and 4.7 percentage points gains on the
tail classes for natural to optical RS and optical RS to multi-spectrum RS
adaptations, respectively, while preserving the performance on head classes,
substantiating its efficacy and adaptability."
How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Recent advances in generative AI technologies like large language models have
boosted the incorporation of AI assistance in writing workflows, leading to the
rise of a new paradigm of human-AI co-creation in writing. To understand how
people perceive writings that are produced under this paradigm, in this paper,
we conduct an experimental study to understand whether and how the disclosure
of the level and type of AI assistance in the writing process would affect
people's perceptions of the writing on various aspects, including their
evaluation on the quality of the writing and their ranking of different
writings. Our results suggest that disclosing the AI assistance in the writing
process, especially if AI has provided assistance in generating new content,
decreases the average quality ratings for both argumentative essays and
creative stories. This decrease in the average quality ratings often comes with
an increased level of variations in different individuals' quality evaluations
of the same writing. Indeed, factors such as an individual's writing confidence
and familiarity with AI writing assistants are shown to moderate the impact of
AI assistance disclosure on their writing quality evaluations. We also find
that disclosing the use of AI assistance may significantly reduce the
proportion of writings produced with AI's content generation assistance among
the top-ranked writings."
Pullback Flow Matching on Data Manifolds,cs.LG,Machine Learning,2024-10-06,"We propose Pullback Flow Matching (PFM), a novel framework for generative
modeling on data manifolds. Unlike existing methods that assume or learn
restrictive closed-form manifold mappings for training Riemannian Flow Matching
(RFM) models, PFM leverages pullback geometry and isometric learning to
preserve the underlying manifold's geometry while enabling efficient generation
and precise interpolation in latent space. This approach not only facilitates
closed-form mappings on the data manifold but also allows for designable latent
spaces, using assumed metrics on both data and latent manifolds. By enhancing
isometric learning through Neural ODEs and proposing a scalable training
objective, we achieve a latent space more suitable for interpolation, leading
to improved manifold learning and generative performance. We demonstrate PFM's
effectiveness through applications in synthetic data, protein dynamics and
protein sequence data, generating novel proteins with specific properties. This
method shows strong potential for drug discovery and materials science, where
generating novel samples with specific properties is of great interest."
On Evaluating LLMs' Capabilities as Functional Approximators: A Bayesian Perspective,cs.LG,Machine Learning,2024-10-06,"Recent works have successfully applied Large Language Models (LLMs) to
function modeling tasks. However, the reasons behind this success remain
unclear. In this work, we propose a new evaluation framework to comprehensively
assess LLMs' function modeling abilities. By adopting a Bayesian perspective of
function modeling, we discover that LLMs are relatively weak in understanding
patterns in raw data, but excel at utilizing prior knowledge about the domain
to develop a strong understanding of the underlying function. Our findings
offer new insights about the strengths and limitations of LLMs in the context
of function modeling."
Multi-LED Classification as Pretext For Robot Heading Estimation,cs.RO,Robotics,2024-10-06,"We propose a self-supervised approach for visual robot detection and heading
estimation by learning to estimate the states (OFF or ON) of four independent
robot-mounted LEDs. Experimental results show a median image-space position
error of 14 px and relative heading MAE of 17 degrees, versus a supervised
upperbound scoring 10 px and 8 degrees, respectively."
In-Place Panoptic Radiance Field Segmentation with Perceptual Prior for 3D Scene Understanding,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Accurate 3D scene representation and panoptic understanding are essential for
applications such as virtual reality, robotics, and autonomous driving.
However, challenges persist with existing methods, including precise 2D-to-3D
mapping, handling complex scene characteristics like boundary ambiguity and
varying scales, and mitigating noise in panoptic pseudo-labels. This paper
introduces a novel perceptual-prior-guided 3D scene representation and panoptic
understanding method, which reformulates panoptic understanding within neural
radiance fields as a linear assignment problem involving 2D semantics and
instance recognition. Perceptual information from pre-trained 2D panoptic
segmentation models is incorporated as prior guidance, thereby synchronizing
the learning processes of appearance, geometry, and panoptic understanding
within neural radiance fields. An implicit scene representation and
understanding model is developed to enhance generalization across indoor and
outdoor scenes by extending the scale-encoded cascaded grids within a
reparameterized domain distillation framework. This model effectively manages
complex scene attributes and generates 3D-consistent scene representations and
panoptic understanding outcomes for various scenes. Experiments and ablation
studies under challenging conditions, including synthetic and real-world
scenes, demonstrate the proposed method's effectiveness in enhancing 3D scene
representation and panoptic segmentation accuracy."
TA3: Testing Against Adversarial Attacks on Machine Learning Models,cs.CR,Cryptography and Security,2024-10-06,"Adversarial attacks are major threats to the deployment of machine learning
(ML) models in many applications. Testing ML models against such attacks is
becoming an essential step for evaluating and improving ML models. In this
paper, we report the design and development of an interactive system for aiding
the workflow of Testing Against Adversarial Attacks (TA3). In particular, with
TA3, human-in-the-loop (HITL) enables human-steered attack simulation and
visualization-assisted attack impact evaluation. While the current version of
TA3 focuses on testing decision tree models against adversarial attacks based
on the One Pixel Attack Method, it demonstrates the importance of HITL in ML
testing and the potential application of HITL to the ML testing workflows for
other types of ML models and other types of adversarial attacks."
Casablanca: Data and Models for Multidialectal Arabic Speech Recognition,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"In spite of the recent progress in speech processing, the majority of world
languages and dialects remain uncovered. This situation only furthers an
already wide technological divide, thereby hindering technological and
socioeconomic inclusion. This challenge is largely due to the absence of
datasets that can empower diverse speech systems. In this paper, we seek to
mitigate this obstacle for a number of Arabic dialects by presenting
Casablanca, a large-scale community-driven effort to collect and transcribe a
multi-dialectal Arabic dataset. The dataset covers eight dialects: Algerian,
Egyptian, Emirati, Jordanian, Mauritanian, Moroccan, Palestinian, and Yemeni,
and includes annotations for transcription, gender, dialect, and
code-switching. We also develop a number of strong baselines exploiting
Casablanca. The project page for Casablanca is accessible at:
www.dlnlp.ai/speech/casablanca."
FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"In this paper, we introduce FAMMA, an open-source benchmark for financial
multilingual multimodal question answering (QA). Our benchmark aims to evaluate
the abilities of multimodal large language models (MLLMs) in answering
questions that require advanced financial knowledge and sophisticated
reasoning. It includes 1,758 meticulously collected question-answer pairs from
university textbooks and exams, spanning 8 major subfields in finance including
corporate finance, asset management, and financial engineering. Some of the QA
pairs are written in Chinese or French, while a majority of them are in
English. These questions are presented in a mixed format combining text and
heterogeneous image types, such as charts, tables, and diagrams. We evaluate a
range of state-of-the-art MLLMs on our benchmark, and our analysis shows that
FAMMA poses a significant challenge for these models. Even advanced systems
like GPT-4o and Claude-35-Sonnet achieve only 42\% accuracy. Additionally, the
open-source Qwen2-VL lags notably behind its proprietary counterparts. Lastly,
we explore GPT o1-style reasoning chains to enhance the models' reasoning
capabilities, which significantly improve error correction. Our FAMMA benchmark
will facilitate future research to develop expert systems in financial QA. The
leaderboard is available at https://famma-bench.github.io/famma/ ."
Look Around and Find Out: OOD Detection with Relative Angles,cs.LG,Machine Learning,2024-10-06,"Deep learning systems deployed in real-world applications often encounter
data that is different from their in-distribution (ID). A reliable system
should ideally abstain from making decisions in this out-of-distribution (OOD)
setting. Existing state-of-the-art methods primarily focus on feature
distances, such as k-th nearest neighbors and distances to decision boundaries,
either overlooking or ineffectively using in-distribution statistics. In this
work, we propose a novel angle-based metric for OOD detection that is computed
relative to the in-distribution structure. We demonstrate that the angles
between feature representations and decision boundaries, viewed from the mean
of in-distribution features, serve as an effective discriminative factor
between ID and OOD data. Our method achieves state-of-the-art performance on
CIFAR-10 and ImageNet benchmarks, reducing FPR95 by 0.88% and 7.74%
respectively. Our score function is compatible with existing feature space
regularization techniques, enhancing performance. Additionally, its
scale-invariance property enables creating an ensemble of models for OOD
detection via simple score summation."
Towards Secure Tuning: Mitigating Security Risks Arising from Benign Instruction Fine-Tuning,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Instruction Fine-Tuning (IFT) has become an essential method for adapting
base Large Language Models (LLMs) into variants for professional and private
use. However, researchers have raised concerns over a significant decrease in
LLMs' security following IFT, even when the IFT process involves entirely
benign instructions (termed Benign IFT). Our study represents a pioneering
effort to mitigate the security risks arising from Benign IFT. Specifically, we
conduct a Module Robustness Analysis, aiming to investigate how LLMs' internal
modules contribute to their security. Based on our analysis, we propose a novel
IFT strategy, called the Modular Layer-wise Learning Rate (ML-LR) strategy. In
our analysis, we implement a simple security feature classifier that serves as
a proxy to measure the robustness of modules (e.g. $Q$/$K$/$V$, etc.). Our
findings reveal that the module robustness shows clear patterns, varying
regularly with the module type and the layer depth. Leveraging these insights,
we develop a proxy-guided search algorithm to identify a robust subset of
modules, termed Mods$_{Robust}$. During IFT, the ML-LR strategy employs
differentiated learning rates for Mods$_{Robust}$ and the rest modules. Our
experimental results show that in security assessments, the application of our
ML-LR strategy significantly mitigates the rise in harmfulness of LLMs
following Benign IFT. Notably, our ML-LR strategy has little impact on the
usability or expertise of LLMs following Benign IFT. Furthermore, we have
conducted comprehensive analyses to verify the soundness and flexibility of our
ML-LR strategy."
Semi-Markovian Planning to Coordinate Aerial and Maritime Medical Evacuation Platforms,cs.AI,Artificial Intelligence,2024-10-06,"The transfer of patients between two aircraft using an underway watercraft
increases medical evacuation reach and flexibility in maritime environments.
The selection of any one of multiple underway watercraft for patient exchange
is complicated by participating aircraft utilization history and a
participating watercraft position and velocity. The selection problem is
modeled as a semi-Markov decision process with an action space including both
fixed land and moving watercraft exchange points. Monte Carlo tree search with
root parallelization is used to select optimal exchange points and determine
aircraft dispatch times. Model parameters are varied in simulation to identify
representative scenarios where watercraft exchange points reduce incident
response times. We find that an optimal policy with watercraft exchange points
outperforms an optimal policy without watercraft exchange points and a greedy
policy by 35% and 40%, respectively. In partnership with the United States
Army, we deploy for the first time the watercraft exchange point by executing a
mock patient transfer with a manikin between two HH-60M medical evacuation
helicopters and an underway Army Logistic Support Vessel south of the Hawaiian
island of Oahu. Both helicopters were dispatched in accordance with our
optimized decision strategy."
MC-CoT: A Modular Collaborative CoT Framework for Zero-shot Medical-VQA with LLM and MLLM Integration,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"In recent advancements, multimodal large language models (MLLMs) have been
fine-tuned on specific medical image datasets to address medical visual
question answering (Med-VQA) tasks. However, this common approach of
task-specific fine-tuning is costly and necessitates separate models for each
downstream task, limiting the exploration of zero-shot capabilities. In this
paper, we introduce MC-CoT, a modular cross-modal collaboration
Chain-of-Thought (CoT) framework designed to enhance the zero-shot performance
of MLLMs in Med-VQA by leveraging large language models (LLMs). MC-CoT improves
reasoning and information extraction by integrating medical knowledge and
task-specific guidance, where LLM provides various complex medical reasoning
chains and MLLM provides various observations of medical images based on
instructions of the LLM. Our experiments on datasets such as SLAKE, VQA-RAD,
and PATH-VQA show that MC-CoT surpasses standalone MLLMs and various
multimodality CoT frameworks in recall rate and accuracy. These findings
highlight the importance of incorporating background information and detailed
guidance in addressing complex zero-shot Med-VQA tasks."
Dynamic Post-Hoc Neural Ensemblers,cs.LG,Machine Learning,2024-10-06,"Ensemble methods are known for enhancing the accuracy and robustness of
machine learning models by combining multiple base learners. However, standard
approaches like greedy or random ensembles often fall short, as they assume a
constant weight across samples for the ensemble members. This can limit
expressiveness and hinder performance when aggregating the ensemble
predictions. In this study, we explore employing neural networks as ensemble
methods, emphasizing the significance of dynamic ensembling to leverage diverse
model predictions adaptively. Motivated by the risk of learning low-diversity
ensembles, we propose regularizing the model by randomly dropping base model
predictions during the training. We demonstrate this approach lower bounds the
diversity within the ensemble, reducing overfitting and improving
generalization capabilities. Our experiments showcase that the dynamic neural
ensemblers yield competitive results compared to strong baselines in computer
vision, natural language processing, and tabular data."
RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Large language models (LLMs) have brought a great breakthrough to the natural
language processing (NLP) community, while leading the challenge of handling
concurrent customer queries due to their high throughput demands. Data
multiplexing addresses this by merging multiple inputs into a single composite
input, allowing more efficient inference through a shared forward pass.
However, as distinguishing individuals from a composite input is challenging,
conventional methods typically require training the entire backbone, yet still
suffer from performance degradation. In this paper, we introduce RevMUX, a
parameter-efficient data multiplexing framework that incorporates a reversible
design in the multiplexer, which can be reused by the demultiplexer to perform
reverse operations and restore individual samples for classification. Extensive
experiments on four datasets and three types of LLM backbones demonstrate the
effectiveness of RevMUX for enhancing LLM inference efficiency while retaining
a satisfactory classification performance."
DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Despite the great success of Large Vision-Language Models (LVLMs), they
inevitably suffer from hallucination. As we know, both the visual encoder and
the Large Language Model (LLM) decoder in LVLMs are Transformer-based, allowing
the model to extract visual information and generate text outputs via attention
mechanisms. We find that the attention distribution of LLM decoder on image
tokens is highly consistent with the visual encoder and both distributions tend
to focus on particular background tokens rather than the referred objects in
the image. We attribute to the unexpected attention distribution to an inherent
flaw in the visual encoder itself, which misguides LLMs to over emphasize the
redundant information and generate object hallucination. To address the issue,
we propose DAMRO, a novel training-free strategy that $D$ive into $A$ttention
$M$echanism of LVLM to $R$educe $O$bject Hallucination. Specifically, our
approach employs classification token (CLS) of ViT to filter out high-attention
outlier tokens scattered in the background and then eliminate their influence
during decoding stage. We evaluate our method on LVLMs including LLaVA-1.5,
LLaVA-NeXT and InstructBLIP, using various benchmarks such as POPE, CHAIR, MME
and GPT-4V Aided Evaluation. The results demonstrate that our approach
significantly reduces the impact of these outlier tokens, thus effectively
alleviating the hallucination of LVLMs. The code of our method will be released
soon."
Realizing Video Summarization from the Path of Language-based Semantic Understanding,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"The recent development of Video-based Large Language Models (VideoLLMs), has
significantly advanced video summarization by aligning video features and, in
some cases, audio features with Large Language Models (LLMs). Each of these
VideoLLMs possesses unique strengths and weaknesses. Many recent methods have
required extensive fine-tuning to overcome the limitations of these models,
which can be resource-intensive. In this work, we observe that the strengths of
one VideoLLM can complement the weaknesses of another. Leveraging this insight,
we propose a novel video summarization framework inspired by the Mixture of
Experts (MoE) paradigm, which operates as an inference-time algorithm without
requiring any form of fine-tuning. Our approach integrates multiple VideoLLMs
to generate comprehensive and coherent textual summaries. It effectively
combines visual and audio content, provides detailed background descriptions,
and excels at identifying keyframes, which enables more semantically meaningful
retrieval compared to traditional computer vision approaches that rely solely
on visual information, all without the need for additional fine-tuning.
Moreover, the resulting summaries enhance performance in downstream tasks such
as summary video generation, either through keyframe selection or in
combination with text-to-image models. Our language-driven approach offers a
semantically rich alternative to conventional methods and provides flexibility
to incorporate newer VideoLLMs, enhancing adaptability and performance in video
summarization tasks."
ErrorRadar: Benchmarking Complex Mathematical Reasoning of Multimodal Large Language Models Via Error Detection,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"As the field of Multimodal Large Language Models (MLLMs) continues to evolve,
their potential to revolutionize artificial intelligence is particularly
promising, especially in addressing mathematical reasoning tasks. Current
mathematical benchmarks predominantly focus on evaluating MLLMs'
problem-solving ability, yet there is a crucial gap in addressing more complex
scenarios such as error detection, for enhancing reasoning capability in
complicated settings. To fill this gap, we formally formulate the new task:
multimodal error detection, and introduce ErrorRadar, the first benchmark
designed to assess MLLMs' capabilities in such a task. ErrorRadar evaluates two
sub-tasks: error step identification and error categorization, providing a
comprehensive framework for evaluating MLLMs' complex mathematical reasoning
ability. It consists of 2,500 high-quality multimodal K-12 mathematical
problems, collected from real-world student interactions in an educational
organization, with rigorous annotation and rich metadata such as problem type
and error category. Through extensive experiments, we evaluated both
open-source and closed-source representative MLLMs, benchmarking their
performance against educational expert evaluators. Results indicate significant
challenges still remain, as GPT-4o with best performance is still around 10%
behind human evaluation. The dataset will be available upon acceptance."
MECFormer: Multi-task Whole Slide Image Classification with Expert Consultation Network,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Whole slide image (WSI) classification is a crucial problem for cancer
diagnostics in clinics and hospitals. A WSI, acquired at gigapixel size, is
commonly tiled into patches and processed by multiple-instance learning (MIL)
models. Previous MIL-based models designed for this problem have only been
evaluated on individual tasks for specific organs, and the ability to handle
multiple tasks within a single model has not been investigated. In this study,
we propose MECFormer, a generative Transformer-based model designed to handle
multiple tasks within one model. To leverage the power of learning multiple
tasks simultaneously and to enhance the model's effectiveness in focusing on
each individual task, we introduce an Expert Consultation Network, a projection
layer placed at the beginning of the Transformer-based model. Additionally, to
enable flexible classification, autoregressive decoding is incorporated by a
language decoder for WSI classification. Through extensive experiments on five
datasets involving four different organs, one cancer classification task, and
four cancer subtyping tasks, MECFormer demonstrates superior performance
compared to individual state-of-the-art multiple-instance learning models."
LRHP: Learning Representations for Human Preferences via Preference Pairs,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"To improve human-preference alignment training, current research has
developed numerous preference datasets consisting of preference pairs labeled
as ""preferred"" or ""dispreferred"". These preference pairs are typically used to
encode human preferences into a single numerical value through reward modeling,
which acts as a reward signal during reinforcement learning from human feedback
(RLHF). However, representing these human preferences as a numerical value
complicates the analysis of these preferences and restricts their broader
applications other than RLHF. In contrast, in this work, we introduce a
preference representation learning task that aims to construct a richer and
more structured representation of human preferences. We further develop a more
generalizable framework, Learning Representations for Human Preferences via
preference pairs (namely LRHP), which extends beyond traditional reward
modeling to tackle this task. We verify the utility of preference
representations in two downstream tasks: preference data selection and
preference margin prediction. Building upon the human preferences in
representations, we achieve strong performance in both tasks, significantly
outperforming baselines."
Leveraging Large Language Models for Suicide Detection on Social Media with Limited Labels,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"The increasing frequency of suicidal thoughts highlights the importance of
early detection and intervention. Social media platforms, where users often
share personal experiences and seek help, could be utilized to identify
individuals at risk. However, the large volume of daily posts makes manual
review impractical. This paper explores the use of Large Language Models (LLMs)
to automatically detect suicidal content in text-based social media posts. We
propose a novel method for generating pseudo-labels for unlabeled data by
prompting LLMs, along with traditional classification fine-tuning techniques to
enhance label accuracy. To create a strong suicide detection model, we develop
an ensemble approach involving prompting with Qwen2-72B-Instruct, and using
fine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate
our approach on the dataset of the Suicide Ideation Detection on Social Media
Challenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we
conduct a comprehensive analysis to assess the impact of different models and
fine-tuning strategies on detection performance. Experimental results show that
the ensemble model significantly improves the detection accuracy, by 5% points
compared with the individual models. It achieves a weight F1 score of 0.770 on
the public test set, and 0.731 on the private test set, providing a promising
solution for identifying suicidal content in social media. Our analysis shows
that the choice of LLMs affects the prompting performance, with larger models
providing better accuracy. Our code and checkpoints are publicly available at
https://github.com/khanhvynguyen/Suicide_Detection_LLMs."
Adjusting Pretrained Backbones for Performativity,cs.LG,Machine Learning,2024-10-06,"With the widespread deployment of deep learning models, they influence their
environment in various ways. The induced distribution shifts can lead to
unexpected performance degradation in deployed models. Existing methods to
anticipate performativity typically incorporate information about the deployed
model into the feature vector when predicting future outcomes. While enjoying
appealing theoretical properties, modifying the input dimension of the
prediction task is often not practical. To address this, we propose a novel
technique to adjust pretrained backbones for performativity in a modular way,
achieving better sample efficiency and enabling the reuse of existing deep
learning assets. Focusing on performative label shift, the key idea is to train
a shallow adapter module to perform a Bayes-optimal label shift correction to
the backbone's logits given a sufficient statistic of the model to be deployed.
As such, our framework decouples the construction of input-specific feature
embeddings from the mechanism governing performativity. Motivated by dynamic
benchmarking as a use-case, we evaluate our approach under adversarial
sampling, for vision and language tasks. We show how it leads to smaller loss
along the retraining trajectory and enables us to effectively select among
candidate models to anticipate performance degradations. More broadly, our work
provides a first baseline for addressing performativity in deep learning."
AdaMemento: Adaptive Memory-Assisted Policy Optimization for Reinforcement Learning,cs.LG,Machine Learning,2024-10-06,"In sparse reward scenarios of reinforcement learning (RL), the memory
mechanism provides promising shortcuts to policy optimization by reflecting on
past experiences like humans. However, current memory-based RL methods simply
store and reuse high-value policies, lacking a deeper refining and filtering of
diverse past experiences and hence limiting the capability of memory. In this
paper, we propose AdaMemento, an adaptive memory-enhanced RL framework. Instead
of just memorizing positive past experiences, we design a memory-reflection
module that exploits both positive and negative experiences by learning to
predict known local optimal policies based on real-time states. To effectively
gather informative trajectories for the memory, we further introduce a
fine-grained intrinsic motivation paradigm, where nuances in similar states can
be precisely distinguished to guide exploration. The exploitation of past
experiences and exploration of new policies are then adaptively coordinated by
ensemble learning to approach the global optimum. Furthermore, we theoretically
prove the superiority of our new intrinsic motivation and ensemble mechanism.
From 59 quantitative and visualization experiments, we confirm that AdaMemento
can distinguish subtle states for better exploration and effectively exploiting
past experiences in memory, achieving significant improvement over previous
methods."
Generalizability analysis of deep learning predictions of human brain responses to augmented and semantically novel visual stimuli,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"The purpose of this work is to investigate the soundness and utility of a
neural network-based approach as a framework for exploring the impact of image
enhancement techniques on visual cortex activation. In a preliminary study, we
prepare a set of state-of-the-art brain encoding models, selected among the top
10 methods that participated in The Algonauts Project 2023 Challenge [16]. We
analyze their ability to make valid predictions about the effects of various
image enhancement techniques on neural responses. Given the impossibility of
acquiring the actual data due to the high costs associated with brain imaging
procedures, our investigation builds up on a series of experiments.
Specifically, we analyze the ability of brain encoders to estimate the cerebral
reaction to various augmentations by evaluating the response to augmentations
targeting objects (i.e., faces and words) with known impact on specific areas.
Moreover, we study the predicted activation in response to objects unseen
during training, exploring the impact of semantically out-of-distribution
stimuli. We provide relevant evidence for the generalization ability of the
models forming the proposed framework, which appears to be promising for the
identification of the optimal visual augmentation filter for a given task,
model-driven design strategies as well as for AR and VR applications."
Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Vision models excel in image classification but struggle to generalize to
unseen data, such as classifying images from unseen domains or discovering
novel categories. In this paper, we explore the relationship between logical
reasoning and deep learning generalization in visual classification. A logical
regularization termed L-Reg is derived which bridges a logical analysis
framework to image classification. Our work reveals that L-Reg reduces the
complexity of the model in terms of the feature distribution and classifier
weights. Specifically, we unveil the interpretability brought by L-Reg, as it
enables the model to extract the salient features, such as faces to persons,
for classification. Theoretical analysis and experiments demonstrate that L-Reg
enhances generalization across various scenarios, including multi-domain
generalization and generalized category discovery. In complex real-world
scenarios where images span unknown classes and unseen domains, L-Reg
consistently improves generalization, highlighting its practical efficacy."
Knowledge-Guided Dynamic Modality Attention Fusion Framework for Multimodal Sentiment Analysis,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Multimodal Sentiment Analysis (MSA) utilizes multimodal data to infer the
users' sentiment. Previous methods focus on equally treating the contribution
of each modality or statically using text as the dominant modality to conduct
interaction, which neglects the situation where each modality may become
dominant. In this paper, we propose a Knowledge-Guided Dynamic Modality
Attention Fusion Framework (KuDA) for multimodal sentiment analysis. KuDA uses
sentiment knowledge to guide the model dynamically selecting the dominant
modality and adjusting the contributions of each modality. In addition, with
the obtained multimodal representation, the model can further highlight the
contribution of dominant modality through the correlation evaluation loss.
Extensive experiments on four MSA benchmark datasets indicate that KuDA
achieves state-of-the-art performance and is able to adapt to different
scenarios of dominant modality."
A Large-Scale Exploit Instrumentation Study of AI/ML Supply Chain Attacks in Hugging Face Models,cs.CR,Cryptography and Security,2024-10-06,"The development of machine learning (ML) techniques has led to ample
opportunities for developers to develop and deploy their own models. Hugging
Face serves as an open source platform where developers can share and download
other models in an effort to make ML development more collaborative. In order
for models to be shared, they first need to be serialized. Certain Python
serialization methods are considered unsafe, as they are vulnerable to object
injection. This paper investigates the pervasiveness of these unsafe
serialization methods across Hugging Face, and demonstrates through an
exploitation approach, that models using unsafe serialization methods can be
exploited and shared, creating an unsafe environment for ML developers. We
investigate to what extent Hugging Face is able to flag repositories and files
using unsafe serialization methods, and develop a technique to detect malicious
models. Our results show that Hugging Face is home to a wide range of
potentially vulnerable models."
A Pluggable Common Sense-Enhanced Framework for Knowledge Graph Completion,cs.AI,Artificial Intelligence,2024-10-06,"Knowledge graph completion (KGC) tasks aim to infer missing facts in a
knowledge graph (KG) for many knowledge-intensive applications. However,
existing embedding-based KGC approaches primarily rely on factual triples,
potentially leading to outcomes inconsistent with common sense. Besides,
generating explicit common sense is often impractical or costly for a KG. To
address these challenges, we propose a pluggable common sense-enhanced KGC
framework that incorporates both fact and common sense for KGC. This framework
is adaptable to different KGs based on their entity concept richness and has
the capability to automatically generate explicit or implicit common sense from
factual triples. Furthermore, we introduce common sense-guided negative
sampling and a coarse-to-fine inference approach for KGs with rich entity
concepts. For KGs without concepts, we propose a dual scoring scheme involving
a relation-aware concept embedding mechanism. Importantly, our approach can be
integrated as a pluggable module for many knowledge graph embedding (KGE)
models, facilitating joint common sense and fact-driven training and inference.
The experiments illustrate that our framework exhibits good scalability and
outperforms existing models across various KGC tasks."
Fine-Grained Prediction of Reading Comprehension from Eye Movements,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Can human reading comprehension be assessed from eye movements in reading? In
this work, we address this longstanding question using large-scale eyetracking
data over textual materials that are geared towards behavioral analyses of
reading comprehension. We focus on a fine-grained and largely unaddressed task
of predicting reading comprehension from eye movements at the level of a single
question over a passage. We tackle this task using three new multimodal
language models, as well as a battery of prior models from the literature. We
evaluate the models' ability to generalize to new textual items, new
participants, and the combination of both, in two different reading regimes,
ordinary reading and information seeking. The evaluations suggest that although
the task is highly challenging, eye movements contain useful signals for
fine-grained prediction of reading comprehension. Code and data will be made
publicly available."
Learning to Solve Abstract Reasoning Problems with Neurosymbolic Program Synthesis and Task Generation,cs.AI,Artificial Intelligence,2024-10-06,"The ability to think abstractly and reason by analogy is a prerequisite to
rapidly adapt to new conditions, tackle newly encountered problems by
decomposing them, and synthesize knowledge to solve problems comprehensively.
We present TransCoder, a method for solving abstract problems based on neural
program synthesis, and conduct a comprehensive analysis of decisions made by
the generative module of the proposed architecture. At the core of TransCoder
is a typed domain-specific language, designed to facilitate feature engineering
and abstract reasoning. In training, we use the programs that failed to solve
tasks to generate new tasks and gather them in a synthetic dataset. As each
synthetic task created in this way has a known associated program (solution),
the model is trained on them in supervised mode. Solutions are represented in a
transparent programmatic form, which can be inspected and verified. We
demonstrate TransCoder's performance using the Abstract Reasoning Corpus
dataset, for which our framework generates tens of thousands of synthetic
problems with corresponding solutions and facilitates systematic progress in
learning."
Collapsed Language Models Promote Fairness,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"To mitigate societal biases implicitly encoded in recent successful
pretrained language models, a diverse array of approaches have been proposed to
encourage model fairness, focusing on prompting, data augmentation, regularized
fine-tuning, and more. Despite the development, it is nontrivial to reach a
principled understanding of fairness and an effective algorithm that can
consistently debias language models. In this work, by rigorous evaluations of
Neural Collapse -- a learning phenomenon happen in last-layer representations
and classifiers in deep networks -- on fairness-related words, we find that
debiased language models exhibit collapsed alignment between token
representations and word embeddings. More importantly, this observation
inspires us to design a principled fine-tuning method that can effectively
improve fairness in a wide range of debiasing methods, while still preserving
the performance of language models on standard natural language understanding
tasks. We attach our code at https://github.com/Xujxyang/Fairness-NC-main."
Revisiting In-context Learning Inference Circuit in Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"In-context Learning (ICL) is an emerging few-shot learning paradigm on
Language Models (LMs) with inner mechanisms un-explored. There are already
existing works describing the inner processing of ICL, while they struggle to
capture all the inference phenomena in large language models. Therefore, this
paper proposes a comprehensive circuit to model the inference dynamics and try
to explain the observed phenomena of ICL. In detail, we divide ICL inference
into 3 major operations: (1) Summarize: LMs encode every input text
(demonstrations and queries) into linear representation in the hidden states
with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge
the encoded representations of demonstrations with their corresponding label
tokens to produce joint representations of labels and demonstrations. (3)
Feature Retrieval and Copy: LMs search the joint representations similar to the
query representation on a task subspace, and copy the searched representations
into the query. Then, language model heads capture these copied label
representations to a certain extent and decode them into predicted labels. The
proposed inference circuit successfully captured many phenomena observed during
the ICL process, making it a comprehensive and practical explanation of the ICL
inference process. Moreover, ablation analysis by disabling the proposed steps
seriously damages the ICL performance, suggesting the proposed inference
circuit is a dominating mechanism. Additionally, we confirm and list some
bypass mechanisms that solve ICL tasks in parallel with the proposed circuit."
A Global Cybersecurity Standardization Framework for Healthcare Informatics,cs.CR,Cryptography and Security,2024-10-06,"Healthcare has witnessed an increased digitalization in the post-COVID world.
Technologies such as the medical internet of things and wearable devices are
generating a plethora of data available on the cloud anytime from anywhere.
This data can be analyzed using advanced artificial intelligence techniques for
diagnosis, prognosis, or even treatment of disease. This advancement comes with
a major risk to protecting and securing protected health information (PHI). The
prevailing regulations for preserving PHI are neither comprehensive nor easy to
implement. The study first identifies twenty activities crucial for privacy and
security, then categorizes them into five homogeneous categories namely:
$\complement_1$ (Policy and Compliance Management), $\complement_2$ (Employee
Training and Awareness), $\complement_3$ (Data Protection and Privacy Control),
$\complement_4$ (Monitoring and Response), and $\complement_5$ (Technology and
Infrastructure Security) and prioritizes these categories to provide a
framework for the implementation of privacy and security in a wise manner. The
framework utilized the Delphi Method to identify activities, criteria for
categorization, and prioritization. Categorization is based on the
Density-Based Spatial Clustering of Applications with Noise (DBSCAN), and
prioritization is performed using a Technique for Order of Preference by
Similarity to the Ideal Solution (TOPSIS). The outcomes conclude that
$\complement_3$ activities should be given first preference in implementation
and followed by $\complement_1$ and $\complement_2$ activities. Finally,
$\complement_4$ and $\complement_5$ should be implemented. The prioritized view
of identified clustered healthcare activities related to security and privacy,
are useful for healthcare policymakers and healthcare informatics
professionals."
Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Chain-of-Thought (CoT) has become a vital technique for enhancing the
performance of Large Language Models (LLMs), attracting increasing attention
from researchers. One stream of approaches focuses on the iterative enhancement
of LLMs by continuously verifying and refining their reasoning outputs for
desired quality. Despite its impressive results, this paradigm faces two
critical issues: (1) Simple verification methods: The current paradigm relies
solely on a single verification method. (2) Wrong Information Ignorance:
Traditional paradigms directly ignore wrong information during reasoning and
refine the logic paths from scratch each time. To address these challenges, we
propose Wrong-of-Thought (WoT), which includes two core modules: (1)
Multi-Perspective Verification: A multi-perspective verification method for
accurately refining the reasoning process and result, and (2) Wrong Information
Utilization: Utilizing wrong information to alert LLMs and reduce the
probability of LLMs making same mistakes. Experiments on 8 popular datasets and
5 LLMs demonstrate that WoT surpasses all previous baselines. In addition, WoT
exhibits powerful capabilities in difficult computation tasks."
Tensor-Train Point Cloud Compression and Efficient Approximate Nearest-Neighbor Search,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Nearest-neighbor search in large vector databases is crucial for various
machine learning applications. This paper introduces a novel method using
tensor-train (TT) low-rank tensor decomposition to efficiently represent point
clouds and enable fast approximate nearest-neighbor searches. We propose a
probabilistic interpretation and utilize density estimation losses like Sliced
Wasserstein to train TT decompositions, resulting in robust point cloud
compression. We reveal an inherent hierarchical structure within TT point
clouds, facilitating efficient approximate nearest-neighbor searches. In our
paper, we provide detailed insights into the methodology and conduct
comprehensive comparisons with existing methods. We demonstrate its
effectiveness in various scenarios, including out-of-distribution (OOD)
detection problems and approximate nearest-neighbor (ANN) search tasks."
Improved Off-policy Reinforcement Learning in Biological Sequence Design,cs.LG,Machine Learning,2024-10-06,"Designing biological sequences with desired properties is a significant
challenge due to the combinatorially vast search space and the high cost of
evaluating each candidate sequence. To address these challenges, reinforcement
learning (RL) methods, such as GFlowNets, utilize proxy models for rapid reward
evaluation and annotated data for policy training. Although these approaches
have shown promise in generating diverse and novel sequences, the limited
training data relative to the vast search space often leads to the
misspecification of proxy for out-of-distribution inputs. We introduce
$\delta$-Conservative Search, a novel off-policy search method for training
GFlowNets designed to improve robustness against proxy misspecification. The
key idea is to incorporate conservativeness, controlled by parameter $\delta$,
to constrain the search to reliable regions. Specifically, we inject noise into
high-score offline sequences by randomly masking tokens with a Bernoulli
distribution of parameter $\delta$ and then denoise masked tokens using the
GFlowNet policy. Additionally, $\delta$ is adaptively adjusted based on the
uncertainty of the proxy model for each data point. This enables the reflection
of proxy uncertainty to determine the level of conservativeness. Experimental
results demonstrate that our method consistently outperforms existing machine
learning methods in discovering high-score sequences across diverse
tasks-including DNA, RNA, protein, and peptide design-especially in large-scale
scenarios."
A Comprehensive Framework for Analyzing the Convergence of Adam: Bridging the Gap with SGD,cs.LG,Machine Learning,2024-10-06,"Adaptive Moment Estimation (Adam) is a cornerstone optimization algorithm in
deep learning, widely recognized for its flexibility with adaptive learning
rates and efficiency in handling large-scale data. However, despite its
practical success, the theoretical understanding of Adam's convergence has been
constrained by stringent assumptions, such as almost surely bounded stochastic
gradients or uniformly bounded gradients, which are more restrictive than those
typically required for analyzing stochastic gradient descent (SGD).
  In this paper, we introduce a novel and comprehensive framework for analyzing
the convergence properties of Adam. This framework offers a versatile approach
to establishing Adam's convergence. Specifically, we prove that Adam achieves
asymptotic (last iterate sense) convergence in both the almost sure sense and
the \(L_1\) sense under the relaxed assumptions typically used for SGD, namely
\(L\)-smoothness and the ABC inequality. Meanwhile, under the same assumptions,
we show that Adam attains non-asymptotic sample complexity bounds similar to
those of SGD."
An Attention-Based Algorithm for Gravity Adaptation Zone Calibration,cs.LG,Machine Learning,2024-10-06,"Accurate calibration of gravity adaptation zones is of great significance in
fields such as underwater navigation, geophysical exploration, and marine
engineering. With the increasing application of gravity field data in these
areas, traditional calibration methods based on single features are becoming
inadequate for capturing the complex characteristics of gravity fields and
addressing the intricate interrelationships among multidimensional data. This
paper proposes an attention-enhanced algorithm for gravity adaptation zone
calibration. By introducing an attention mechanism, the algorithm adaptively
fuses multidimensional gravity field features and dynamically assigns feature
weights, effectively solving the problems of multicollinearity and redundancy
inherent in traditional feature selection methods, significantly improving
calibration accuracy and robustness.In addition, a large-scale gravity field
dataset with over 10,000 sampling points was constructed, and Kriging
interpolation was used to enhance the spatial resolution of the data, providing
a reliable data foundation for model training and evaluation. We conducted both
qualitative and quantitative experiments on several classical machine learning
models (such as SVM, GBDT, and RF), and the results demonstrate that the
proposed algorithm significantly improves performance across these models,
outperforming other traditional feature selection methods. The method proposed
in this paper provides a new solution for gravity adaptation zone calibration,
showing strong generalization ability and potential for application in complex
environments. The code is available at \href{this link}
{https://github.com/hulnifox/RF-ATTN}."
SWEb: A Large Web Dataset for the Scandinavian Languages,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"This paper presents the hitherto largest pretraining dataset for the
Scandinavian languages: the Scandinavian WEb (SWEb), comprising over one
trillion tokens. The paper details the collection and processing pipeline, and
introduces a novel model-based text extractor that significantly reduces
complexity in comparison with rule-based approaches. We also introduce a new
cloze-style benchmark for evaluating language models in Swedish, and use this
test to compare models trained on the SWEb data to models trained on FineWeb,
with competitive results. All data, models and code are shared openly."
CopyLens: Dynamically Flagging Copyrighted Sub-Dataset Contributions to LLM Outputs,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Large Language Models (LLMs) have become pervasive due to their knowledge
absorption and text-generation capabilities. Concurrently, the copyright issue
for pretraining datasets has been a pressing concern, particularly when
generation includes specific styles. Previous methods either focus on the
defense of identical copyrighted outputs or find interpretability by individual
tokens with computational burdens. However, the gap between them exists, where
direct assessments of how dataset contributions impact LLM outputs are missing.
Once the model providers ensure copyright protection for data holders, a more
mature LLM community can be established. To address these limitations, we
introduce CopyLens, a new framework to analyze how copyrighted datasets may
influence LLM responses. Specifically, a two-stage approach is employed: First,
based on the uniqueness of pretraining data in the embedding space, token
representations are initially fused for potential copyrighted texts, followed
by a lightweight LSTM-based network to analyze dataset contributions. With such
a prior, a contrastive-learning-based non-copyright OOD detector is designed.
Our framework can dynamically face different situations and bridge the gap
between current copyright detection methods. Experiments show that CopyLens
improves efficiency and accuracy by 15.2% over our proposed baseline, 58.7%
over prompt engineering methods, and 0.21 AUC over OOD detection baselines."
MindScope: Exploring cognitive biases in large language models through Multi-Agent Systems,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Detecting cognitive biases in large language models (LLMs) is a fascinating
task that aims to probe the existing cognitive biases within these models.
Current methods for detecting cognitive biases in language models generally
suffer from incomplete detection capabilities and a restricted range of
detectable bias types. To address this issue, we introduced the 'MindScope'
dataset, which distinctively integrates static and dynamic elements. The static
component comprises 5,170 open-ended questions spanning 72 cognitive bias
categories. The dynamic component leverages a rule-based, multi-agent
communication framework to facilitate the generation of multi-round dialogues.
This framework is flexible and readily adaptable for various psychological
experiments involving LLMs. In addition, we introduce a multi-agent detection
method applicable to a wide range of detection tasks, which integrates
Retrieval-Augmented Generation (RAG), competitive debate, and a reinforcement
learning-based decision module. Demonstrating substantial effectiveness, this
method has shown to improve detection accuracy by as much as 35.10% compared to
GPT-4. Codes and appendix are available at
https://github.com/2279072142/MindScope."
Video Summarization Techniques: A Comprehensive Review,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"The rapid expansion of video content across a variety of industries,
including social media, education, entertainment, and surveillance, has made
video summarization an essential field of study. The current work is a survey
that explores the various approaches and methods created for video summarizing,
emphasizing both abstractive and extractive strategies. The process of
extractive summarization involves the identification of key frames or segments
from the source video, utilizing methods such as shot boundary recognition, and
clustering. On the other hand, abstractive summarization creates new content by
getting the essential content from the video, using machine learning models
like deep neural networks and natural language processing, reinforcement
learning, attention mechanisms, generative adversarial networks, and
multi-modal learning. We also include approaches that incorporate the two
methodologies, along with discussing the uses and difficulties encountered in
real-world implementations. The paper also covers the datasets used to
benchmark these techniques. This review attempts to provide a state-of-the-art
thorough knowledge of the current state and future directions of video
summarization research."
Attention Shift: Steering AI Away from Unsafe Content,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"This study investigates the generation of unsafe or harmful content in
state-of-the-art generative models, focusing on methods for restricting such
generations. We introduce a novel training-free approach using attention
reweighing to remove unsafe concepts without additional training during
inference. We compare our method against existing ablation methods, evaluating
the performance on both, direct and adversarial jailbreak prompts, using
qualitative and quantitative metrics. We hypothesize potential reasons for the
observed results and discuss the limitations and broader implications of
content restriction."
Optimising for the Unknown: Domain Alignment for Cephalometric Landmark Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Cephalometric Landmark Detection is the process of identifying key areas for
cephalometry. Each landmark is a single GT point labelled by a clinician. A
machine learning model predicts the probability locus of a landmark represented
by a heatmap. This work, for the 2024 CL-Detection MICCAI Challenge, proposes a
domain alignment strategy with a regional facial extraction module and an X-ray
artefact augmentation procedure. The challenge ranks our method's results as
the best in MRE of 1.186mm and third in the 2mm SDR of 82.04% on the online
validation leaderboard. The code is available at
https://github.com/Julian-Wyatt/OptimisingfortheUnknown."
Gdel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement,cs.AI,Artificial Intelligence,2024-10-06,"The rapid advancement of large language models (LLMs) has significantly
enhanced the capabilities of AI-driven agents across various tasks. However,
existing agentic systems, whether based on fixed pipeline algorithms or
pre-defined meta-learning frameworks, cannot search the whole agent design
space due to the restriction of human-designed components, and thus might miss
the globally optimal agent design. In this paper, we introduce G\""odel Agent, a
self-evolving framework inspired by the G\""odel machine, enabling agents to
recursively improve themselves without relying on predefined routines or fixed
optimization algorithms. G\""odel Agent leverages LLMs to dynamically modify its
own logic and behavior, guided solely by high-level objectives through
prompting. Experimental results on mathematical reasoning and complex agent
tasks demonstrate that implementation of G\""odel Agent can achieve continuous
self-improvement, surpassing manually crafted agents in performance,
efficiency, and generalizability."
TimeBridge: Non-Stationarity Matters for Long-term Time Series Forecasting,cs.LG,Machine Learning,2024-10-06,"Non-stationarity poses significant challenges for multivariate time series
forecasting due to the inherent short-term fluctuations and long-term trends
that can lead to spurious regressions or obscure essential long-term
relationships. Most existing methods either eliminate or retain
non-stationarity without adequately addressing its distinct impacts on
short-term and long-term modeling. Eliminating non-stationarity is essential
for avoiding spurious regressions and capturing local dependencies in
short-term modeling, while preserving it is crucial for revealing long-term
cointegration across variates. In this paper, we propose TimeBridge, a novel
framework designed to bridge the gap between non-stationarity and dependency
modeling in long-term time series forecasting. By segmenting input series into
smaller patches, TimeBridge applies Integrated Attention to mitigate short-term
non-stationarity and capture stable dependencies within each variate, while
Cointegrated Attention preserves non-stationarity to model long-term
cointegration across variates. Extensive experiments show that TimeBridge
consistently achieves state-of-the-art performance in both short-term and
long-term forecasting. Additionally, TimeBridge demonstrates exceptional
performance in financial forecasting on the CSI 500 and S&P 500 indices,
further validating its robustness and effectiveness. Code is available at
\url{https://github.com/Hank0626/TimeBridge}."
Automated Detection of Defects on Metal Surfaces using Vision Transformers,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Metal manufacturing often results in the production of defective products,
leading to operational challenges. Since traditional manual inspection is
time-consuming and resource-intensive, automatic solutions are needed. The
study utilizes deep learning techniques to develop a model for detecting metal
surface defects using Vision Transformers (ViTs). The proposed model focuses on
the classification and localization of defects using a ViT for feature
extraction. The architecture branches into two paths: classification and
localization. The model must approach high classification accuracy while
keeping the Mean Square Error (MSE) and Mean Absolute Error (MAE) as low as
possible in the localization process. Experimental results show that it can be
utilized in the process of automated defects detection, improve operational
efficiency, and reduce errors in metal manufacturing."
Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Diffusion-based text-to-image models have demonstrated impressive
achievements in diversity and aesthetics but struggle to generate images with
legible visual texts. Existing backbone models have limitations such as
misspelling, failing to generate texts, and lack of support for Chinese text,
but their development shows promising potential. In this paper, we propose a
series of methods, aiming to empower backbone models to generate visual texts
in English and Chinese. We first conduct a preliminary study revealing that
Byte Pair Encoding (BPE) tokenization and the insufficient learning of
cross-attention modules restrict the performance of the backbone models. Based
on these observations, we make the following improvements: (1) We design a
mixed granularity input strategy to provide more suitable text representations;
(2) We propose to augment the conventional training objective with three
glyph-aware training losses, which enhance the learning of cross-attention
modules and encourage the model to focus on visual texts. Through experiments,
we demonstrate that our methods can effectively empower backbone models to
generate semantic relevant, aesthetically appealing, and accurate visual text
images, while maintaining their fundamental image generation quality."
A Mathematical Explanation of UNet,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"The UNet architecture has transformed image segmentation. UNet's versatility
and accuracy have driven its widespread adoption, significantly advancing
fields reliant on machine learning problems with images. In this work, we give
a clear and concise mathematical explanation of UNet. We explain what is the
meaning and function of each of the components of UNet. We will show that UNet
is solving a control problem. We decompose the control variables using
multigrid methods. Then, operator-splitting techniques is used to solve the
problem, whose architecture exactly recovers the UNet architecture. Our result
shows that UNet is a one-step operator-splitting algorithm for the control
problem."
CAPEEN: Image Captioning with Early Exits and Knowledge Distillation,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Deep neural networks (DNNs) have made significant progress in recognizing
visual elements and generating descriptive text in image-captioning tasks.
However, their improved performance comes from increased computational burden
and inference latency. Early Exit (EE) strategies can be used to enhance their
efficiency, but their adaptation presents challenges in image captioning as it
requires varying levels of semantic information for accurate predictions. To
overcome this, we introduce CAPEEN to improve the performance of EE strategies
using knowledge distillation. Inference in CAPEEN is completed at intermediary
layers if prediction confidence exceeds a predefined value learned from the
training data. To account for real-world deployments, where target
distributions could drift from that of training samples, we introduce a variant
A-CAPEEN to adapt the thresholds on the fly using Multiarmed bandits framework.
Experiments on the MS COCO and Flickr30k datasets show that CAPEEN gains
speedup of 1.77x while maintaining competitive performance compared to the
final layer, and A-CAPEEN additionally offers robustness against distortions.
The source code is available at https://github.com/Div290/CapEEN"
CoVLM: Leveraging Consensus from Vision-Language Models for Semi-supervised Multi-modal Fake News Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"In this work, we address the real-world, challenging task of out-of-context
misinformation detection, where a real image is paired with an incorrect
caption for creating fake news. Existing approaches for this task assume the
availability of large amounts of labeled data, which is often impractical in
real-world, since it requires extensive manual intervention and domain
expertise. In contrast, since obtaining a large corpus of unlabeled image-text
pairs is much easier, here, we propose a semi-supervised protocol, where the
model has access to a limited number of labeled image-text pairs and a large
corpus of unlabeled pairs. Additionally, the occurrence of fake news being much
lesser compared to the real ones, the datasets tend to be highly imbalanced,
thus making the task even more challenging. Towards this goal, we propose a
novel framework, Consensus from Vision-Language Models (CoVLM), which generates
robust pseudo-labels for unlabeled pairs using thresholds derived from the
labeled data. This approach can automatically determine the right threshold
parameters of the model for selecting the confident pseudo-labels. Experimental
results on benchmark datasets across challenging conditions and comparisons
with state-of-the-art approaches demonstrate the effectiveness of our
framework."
DAdEE: Unsupervised Domain Adaptation in Early Exit PLMs,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Pre-trained Language Models (PLMs) exhibit good accuracy and generalization
ability across various tasks using self-supervision, but their large size
results in high inference latency. Early Exit (EE) strategies handle the issue
by allowing the samples to exit from classifiers attached to the intermediary
layers, but they do not generalize well, as exit classifiers can be sensitive
to domain changes. To address this, we propose Unsupervised Domain Adaptation
in EE framework (DADEE) that employs multi-level adaptation using knowledge
distillation. DADEE utilizes GAN-based adversarial adaptation at each layer to
achieve domain-invariant representations, reducing the domain gap between the
source and target domain across all layers. The attached exits not only speed
up inference but also enhance domain adaptation by reducing catastrophic
forgetting and mode collapse, making it more suitable for real-world scenarios.
Experiments on tasks such as sentiment analysis, entailment classification, and
natural language inference demonstrate that DADEE consistently outperforms not
only early exit methods but also various domain adaptation methods under domain
shift scenarios. The anonymized source code is available at
https://github.com/Div290/DAdEE."
Hyper-multi-step: The Truth Behind Difficult Long-context Tasks,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Long-context language models (LCLM), characterized by their extensive context
window, is becoming increasingly popular. Meanwhile, many long-context
benchmarks present challenging tasks that even the most advanced LCLMs struggle
to complete. However, the underlying sources of various challenging
long-context tasks have seldom been studied. To bridge this gap, we conduct
experiments to indicate their difficulty stems primarily from two basic issues:
""multi-matching retrieval,"" which requires the simultaneous retrieval of
multiple items, and ""logic-based retrieval,"" which necessitates logical
judgment within retrieval criteria. These two problems, while seemingly
straightforward, actually exceed the capabilities of LCLMs because they are
proven to be hyper-multi-step (demanding numerous steps to solve) in nature.
This finding could explain why LLMs struggle with more advanced long-context
tasks, providing a more accurate perspective for rethinking solutions for them."
Disentangling Regional Primitives for Image Generation,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"This paper presents a method to explain the internal representation structure
of a neural network for image generation. Specifically, our method disentangles
primitive feature components from the intermediate-layer feature of the neural
network, which ensures that each feature component is exclusively used to
generate a specific set of image regions. In this way, the generation of the
entire image can be considered as the superposition of different pre-encoded
primitive regional patterns, each being generated by a feature component. We
find that the feature component can be represented as an OR relationship
between the demands for generating different image regions, which is encoded by
the neural network. Therefore, we extend the Harsanyi interaction to represent
such an OR interaction to disentangle the feature component. Experiments show a
clear correspondence between each feature component and the generation of
specific image regions."
LiteVLoc: Map-Lite Visual Localization for Image Goal Navigation,cs.RO,Robotics,2024-10-06,"This paper presents LiteVLoc, a hierarchical visual localization framework
that uses a lightweight topo-metric map to represent the environment. The
method consists of three sequential modules that estimate camera poses in a
coarse-to-fine manner. Unlike mainstream approaches relying on detailed 3D
representations, LiteVLoc reduces storage overhead by leveraging learning-based
feature matching and geometric solvers for metric pose estimation. A novel
dataset for the map-free relocalization task is also introduced. Extensive
experiments including localization and navigation in both simulated and
real-world scenarios have validate the system's performance and demonstrated
its precision and efficiency for large-scale deployment. Code and data will be
made publicly available."
SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"In vision-language models (VLMs), visual tokens usually consume a significant
amount of computational overhead, despite their sparser information density
compared to text tokens. To address this, most existing methods learn a network
to prune redundant visual tokens and require additional training data.
Differently, we propose an efficient training-free token optimization mechanism
dubbed SparseVLM without extra parameters or fine-tuning costs. Concretely,
given that visual tokens complement text tokens in VLMs for linguistic
reasoning, we select visual-relevant text tokens to rate the significance of
vision tokens within the self-attention matrix extracted from the VLMs. Then we
progressively prune irrelevant tokens. To maximize sparsity while retaining
essential information, we introduce a rank-based strategy to adaptively
determine the sparsification ratio for each layer, alongside a token recycling
method that compresses pruned tokens into more compact representations.
Experimental results show that our SparseVLM improves the efficiency of various
VLMs across a range of image and video understanding tasks. In particular,
LLaVA equipped with SparseVLM reduces 61% to 67% FLOPs with a compression ratio
of 78% while maintaining 93% of the accuracy. Our code is available at
https://github.com/Gumpest/SparseVLMs."
Optimizing AI Reasoning: A Hamiltonian Dynamics Approach to Multi-Hop Question Answering,cs.AI,Artificial Intelligence,2024-10-06,"This paper introduces an innovative approach to analyzing and improving
multi-hop reasoning in AI systems by drawing inspiration from Hamiltonian
mechanics. We propose a novel framework that maps reasoning chains in embedding
spaces to Hamiltonian systems, allowing us to leverage powerful analytical
tools from classical physics. Our method defines a Hamiltonian function that
balances the progression of reasoning (kinetic energy) against the relevance to
the question at hand (potential energy). Using this framework, we analyze a
large dataset of reasoning chains from a multi-hop question-answering task,
revealing intriguing patterns that distinguish valid from invalid reasoning. We
show that valid reasoning chains have lower Hamiltonian energy and move in ways
that make the best trade-off between getting more information and answering the
right question. Furthermore, we demonstrate the application of this framework
to steer the creation of more efficient reasoning algorithms within AI systems.
Our results not only provide new insights into the nature of valid reasoning
but also open up exciting possibilities for physics-inspired approaches to
understanding and improving artificial intelligence."
"Blocks Architecture (BloArk): Efficient, Cost-Effective, and Incremental Dataset Architecture for Wikipedia Revision History",cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Wikipedia (Wiki) is one of the most widely used and publicly available
resources for natural language processing (NLP) applications. Wikipedia
Revision History (WikiRevHist) shows the order in which edits were made to any
Wiki page since its first modification. While the most up-to-date Wiki has been
widely used as a training source, WikiRevHist can also be valuable resources
for NLP applications. However, there are insufficient tools available to
process WikiRevHist without having substantial computing resources, making
additional customization, and spending extra time adapting others' works.
Therefore, we report Blocks Architecture (BloArk), an efficiency-focused data
processing architecture that reduces running time, computing resource
requirements, and repeated works in processing WikiRevHist dataset. BloArk
consists of three parts in its infrastructure: blocks, segments, and
warehouses. On top of that, we build the core data processing pipeline: builder
and modifier. The BloArk builder transforms the original WikiRevHist dataset
from XML syntax into JSON Lines (JSONL) format for improving the concurrent and
storage efficiency. The BloArk modifier takes previously-built warehouses to
operate incremental modifications for improving the utilization of existing
databases and reducing the cost of reusing others' works. In the end, BloArk
can scale up easily in both processing Wikipedia Revision History and
incrementally modifying existing dataset for downstream NLP use cases. The
source code, documentations, and example usages are publicly available online
and open-sourced under GPL-2.0 license."
Lens: Rethinking Multilingual Enhancement for Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Despite the growing global demand for large language models (LLMs) that serve
users from diverse linguistic backgrounds, most cutting-edge LLMs remain
predominantly English-centric. This creates a performance gap across languages,
restricting access to advanced AI services for non-English speakers. Current
methods to enhance multilingual capabilities largely rely on data-driven
post-training techniques, such as multilingual instruction tuning or continual
pre-training. However, these approaches encounter significant challenges,
including the scarcity of high-quality multilingual datasets and the limited
enhancement of multilingual capabilities. They often suffer from off-target
issues and catastrophic forgetting of central language abilities. To this end,
we propose Lens, a novel approach to enhance multilingual capabilities of LLMs
by leveraging their internal language representation spaces. Specially, Lens
operates by manipulating the hidden representations within the
language-agnostic and language-specific subspaces from top layers of LLMs.
Using the central language as a pivot, the target language is drawn closer to
it within the language-agnostic subspace, allowing it to inherit
well-established semantic representations. Meanwhile, in the language-specific
subspace, the representations of the target and central languages are pushed
apart, enabling the target language to express itself distinctly. Extensive
experiments on one English-centric and two multilingual LLMs demonstrate that
Lens effectively improves multilingual performance without sacrificing the
original central language capabilities of the backbone model, achieving
superior results with much fewer computational resources compared to existing
post-training approaches."
CiMaTe: Citation Count Prediction Effectively Leveraging the Main Text,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Prediction of the future citation counts of papers is increasingly important
to find interesting papers among an ever-growing number of papers. Although a
paper's main text is an important factor for citation count prediction, it is
difficult to handle in machine learning models because the main text is
typically very long; thus previous studies have not fully explored how to
leverage it. In this paper, we propose a BERT-based citation count prediction
model, called CiMaTe, that leverages the main text by explicitly capturing a
paper's sectional structure. Through experiments with papers from computational
linguistics and biology domains, we demonstrate the CiMaTe's effectiveness,
outperforming the previous methods in Spearman's rank correlation coefficient;
5.1 points in the computational linguistics domain and 1.8 points in the
biology domain."
Deformable NeRF using Recursively Subdivided Tetrahedra,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"While neural radiance fields (NeRF) have shown promise in novel view
synthesis, their implicit representation limits explicit control over object
manipulation. Existing research has proposed the integration of explicit
geometric proxies to enable deformation. However, these methods face two
primary challenges: firstly, the time-consuming and computationally demanding
tetrahedralization process; and secondly, handling complex or thin structures
often leads to either excessive, storage-intensive tetrahedral meshes or
poor-quality ones that impair deformation capabilities. To address these
challenges, we propose DeformRF, a method that seamlessly integrates the
manipulability of tetrahedral meshes with the high-quality rendering
capabilities of feature grid representations. To avoid ill-shaped tetrahedra
and tetrahedralization for each object, we propose a two-stage training
strategy. Starting with an almost-regular tetrahedral grid, our model initially
retains key tetrahedra surrounding the object and subsequently refines object
details using finer-granularity mesh in the second stage. We also present the
concept of recursively subdivided tetrahedra to create higher-resolution meshes
implicitly. This enables multi-resolution encoding while only necessitating the
storage of the coarse tetrahedral mesh generated in the first training stage.
We conduct a comprehensive evaluation of our DeformRF on both synthetic and
real-captured datasets. Both quantitative and qualitative results demonstrate
the effectiveness of our method for novel view synthesis and deformation tasks.
Project page: https://ustc3dv.github.io/DeformRF/"
Towards Understanding and Enhancing Security of Proof-of-Training for DNN Model Ownership Verification,cs.CR,Cryptography and Security,2024-10-06,"The great economic values of deep neural networks (DNNs) urge AI enterprises
to protect their intellectual property (IP) for these models. Recently,
proof-of-training (PoT) has been proposed as a promising solution to DNN IP
protection, through which AI enterprises can utilize the record of DNN training
process as their ownership proof. To prevent attackers from forging ownership
proof, a secure PoT scheme should be able to distinguish honest training
records from those forged by attackers. Although existing PoT schemes provide
various distinction criteria, these criteria are based on intuitions or
observations. The effectiveness of these criteria lacks clear and comprehensive
analysis, resulting in existing schemes initially deemed secure being swiftly
compromised by simple ideas. In this paper, we make the first move to identify
distinction criteria in the style of formal methods, so that their
effectiveness can be explicitly demonstrated. Specifically, we conduct
systematic modeling to cover a wide range of attacks and then theoretically
analyze the distinctions between honest and forged training records. The
analysis results not only induce a universal distinction criterion, but also
provide detailed reasoning to demonstrate its effectiveness in defending
against attacks covered by our model. Guided by the criterion, we propose a
generic PoT construction that can be instantiated into concrete schemes. This
construction sheds light on the realization that trajectory matching
algorithms, previously employed in data distillation, possess significant
advantages in PoT construction. Experimental results demonstrate that our
scheme can resist attacks that have compromised existing PoT schemes, which
corroborates its superiority in security."
Data Distribution Valuation,cs.LG,Machine Learning,2024-10-06,"Data valuation is a class of techniques for quantitatively assessing the
value of data for applications like pricing in data marketplaces. Existing data
valuation methods define a value for a discrete dataset. However, in many use
cases, users are interested in not only the value of the dataset, but that of
the distribution from which the dataset was sampled. For example, consider a
buyer trying to evaluate whether to purchase data from different vendors. The
buyer may observe (and compare) only a small preview sample from each vendor,
to decide which vendor's data distribution is most useful to the buyer and
purchase. The core question is how should we compare the values of data
distributions from their samples? Under a Huber characterization of the data
heterogeneity across vendors, we propose a maximum mean discrepancy (MMD)-based
valuation method which enables theoretically principled and actionable policies
for comparing data distributions from samples. We empirically demonstrate that
our method is sample-efficient and effective in identifying valuable data
distributions against several existing baselines, on multiple real-world
datasets (e.g., network intrusion detection, credit card fraud detection) and
downstream applications (classification, regression)."
Suspiciousness of Adversarial Texts to Human,cs.LG,Machine Learning,2024-10-06,"Adversarial examples pose a significant challenge to deep neural networks
(DNNs) across both image and text domains, with the intent to degrade model
performance through meticulously altered inputs. Adversarial texts, however,
are distinct from adversarial images due to their requirement for semantic
similarity and the discrete nature of the textual contents. This study delves
into the concept of human suspiciousness, a quality distinct from the
traditional focus on imperceptibility found in image-based adversarial
examples. Unlike images, where adversarial changes are meant to be
indistinguishable to the human eye, textual adversarial content must often
remain undetected or non-suspicious to human readers, even when the text's
purpose is to deceive NLP systems or bypass filters.
  In this research, we expand the study of human suspiciousness by analyzing
how individuals perceive adversarial texts. We gather and publish a novel
dataset of Likert-scale human evaluations on the suspiciousness of adversarial
sentences, crafted by four widely used adversarial attack methods and assess
their correlation with the human ability to detect machine-generated
alterations. Additionally, we develop a regression-based model to quantify
suspiciousness and establish a baseline for future research in reducing the
suspiciousness in adversarial text generation. We also demonstrate how the
regressor-generated suspicious scores can be incorporated into adversarial
generation methods to produce texts that are less likely to be perceived as
computer-generated. We make our human suspiciousness annotated data and our
code available."
DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided Stable Diffusion,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"The rapid progress of Deepfake technology has made face swapping highly
realistic, raising concerns about the malicious use of fabricated facial
content. Existing methods often struggle to generalize to unseen domains due to
the diverse nature of facial manipulations. In this paper, we revisit the
generation process and identify a universal principle: Deepfake images
inherently contain information from both source and target identities, while
genuine faces maintain a consistent identity. Building upon this insight, we
introduce DiffusionFake, a novel plug-and-play framework that reverses the
generative process of face forgeries to enhance the generalization of detection
models. DiffusionFake achieves this by injecting the features extracted by the
detection model into a frozen pre-trained Stable Diffusion model, compelling it
to reconstruct the corresponding target and source images. This guided
reconstruction process constrains the detection network to capture the source
and target related features to facilitate the reconstruction, thereby learning
rich and disentangled representations that are more resilient to unseen
forgeries. Extensive experiments demonstrate that DiffusionFake significantly
improves cross-domain generalization of various detector architectures without
introducing additional parameters during inference. Our Codes are available in
https://github.com/skJack/DiffusionFake.git."
A physics-based sensor simulation environment for lunar ground operations,cs.RO,Robotics,2024-10-06,"This contribution reports on a software framework that uses physically-based
rendering to simulate camera operation in lunar conditions. The focus is on
generating synthetic images qualitatively similar to those produced by an
actual camera operating on a vehicle traversing and/or actively interacting
with lunar terrain, e.g., for construction operations. The highlights of this
simulator are its ability to capture (i) light transport in lunar conditions
and (ii) artifacts related to the vehicle-terrain interaction, which might
include dust formation and transport. The simulation infrastructure is built
within an in-house developed physics engine called Chrono, which simulates the
dynamics of the deformable terrain-vehicle interaction, as well as fallout of
this interaction. The Chrono::Sensor camera model draws on ray tracing and
Hapke Photometric Functions. We analyze the performance of the simulator using
two virtual experiments featuring digital twins of NASA's VIPER rover
navigating a lunar environment, and of the NASA's RASSOR excavator engaged into
a digging operation. The sensor simulation solution presented can be used for
the design and testing of perception algorithms, or as a component of in-silico
experiments that pertain to large lunar operations, e.g., traversability,
construction tasks."
DABI: Evaluation of Data Augmentation Methods Using Downsampling in Bilateral Control-Based Imitation Learning with Images,cs.RO,Robotics,2024-10-06,"Autonomous robot manipulation is a complex and continuously evolving robotics
field. This paper focuses on data augmentation methods in imitation learning.
Imitation learning consists of three stages: data collection from experts,
learning model, and execution. However, collecting expert data requires manual
effort and is time-consuming. Additionally, as sensors have different data
acquisition intervals, preprocessing such as downsampling to match the lowest
frequency is necessary. Downsampling enables data augmentation and also
contributes to the stabilization of robot operations. In light of this
background, this paper proposes the Data Augmentation Method for Bilateral
Control-Based Imitation Learning with Images, called ""DABI"". DABI collects
robot joint angles, velocities, and torques at 1000 Hz, and uses images from
gripper and environmental cameras captured at 100 Hz as the basis for data
augmentation. This enables a tenfold increase in data. In this paper, we
collected just 5 expert demonstration datasets. We trained the bilateral
control Bi-ACT model with the unaltered dataset and two augmentation methods
for comparative experiments and conducted real-world experiments. The results
confirmed a significant improvement in success rates, thereby proving the
effectiveness of DABI. For additional material, please check
https://mertcookimg.github.io/dabi"
Algorithmic Capabilities of Random Transformers,cs.LG,Machine Learning,2024-10-06,"Trained transformer models have been found to implement interpretable
procedures for tasks like arithmetic and associative recall, but little is
understood about how the circuits that implement these procedures originate
during training. To what extent do they depend on the supervisory signal
provided to models, and to what extent are they attributable to behavior
already present in models at the beginning of training? To investigate these
questions, we investigate what functions can be learned by randomly initialized
transformers in which only the embedding layers are optimized, so that the only
input--output mappings learnable from data are those already implemented (up to
a choice of encoding scheme) by the randomly initialized model. We find that
these random transformers can perform a wide range of meaningful algorithmic
tasks, including modular arithmetic, in-weights and in-context associative
recall, decimal addition, parenthesis balancing, and even some aspects of
natural language text generation. Our results indicate that some algorithmic
capabilities are present in transformers (and accessible via appropriately
structured inputs) even before these models are trained. Code is available at
https://github.com/fjzzq2002/random_transformers."
VPI-Mlogs: A web-based machine learning solution for applications in petrophysics,cs.LG,Machine Learning,2024-10-06,"Machine learning is an important part of the data science field. In
petrophysics, machine learning algorithms and applications have been widely
approached. In this context, Vietnam Petroleum Institute (VPI) has researched
and deployed several effective prediction models, namely missing log
prediction, fracture zone and fracture density forecast, etc. As one of our
solutions, VPI-MLogs is a web-based deployment platform which integrates data
preprocessing, exploratory data analysis, visualisation and model execution.
Using the most popular data analysis programming language, Python, this
approach gives users a powerful tool to deal with the petrophysical logs
section. The solution helps to narrow the gap between common knowledge and
petrophysics insights. This article will focus on the web-based application
which integrates many solutions to grasp petrophysical data."
VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Text-to-image (T2I) diffusion models have revolutionized visual content
creation, but extending these capabilities to text-to-video (T2V) generation
remains a challenge, particularly in preserving temporal consistency. Existing
methods that aim to improve consistency often cause trade-offs such as reduced
imaging quality and impractical computational time. To address these issues we
introduce VideoGuide, a novel framework that enhances the temporal consistency
of pretrained T2V models without the need for additional training or
fine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model
(VDM) or itself as a guide during the early stages of inference, improving
temporal quality by interpolating the guiding model's denoised samples into the
sampling model's denoising process. The proposed method brings about
significant improvement in temporal consistency and image fidelity, providing a
cost-effective and practical solution that synergizes the strengths of various
video diffusion models. Furthermore, we demonstrate prior distillation,
revealing that base models can achieve enhanced text coherence by utilizing the
superior data prior of the guiding model through the proposed method. Project
Page: https://dohunlee1.github.io/videoguide.github.io/"
StreetSurfGS: Scalable Urban Street Surface Reconstruction with Planar-based Gaussian Splatting,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Reconstructing urban street scenes is crucial due to its vital role in
applications such as autonomous driving and urban planning. These scenes are
characterized by long and narrow camera trajectories, occlusion, complex object
relationships, and data sparsity across multiple scales. Despite recent
advancements, existing surface reconstruction methods, which are primarily
designed for object-centric scenarios, struggle to adapt effectively to the
unique characteristics of street scenes. To address this challenge, we
introduce StreetSurfGS, the first method to employ Gaussian Splatting
specifically tailored for scalable urban street scene surface reconstruction.
StreetSurfGS utilizes a planar-based octree representation and segmented
training to reduce memory costs, accommodate unique camera characteristics, and
ensure scalability. Additionally, to mitigate depth inaccuracies caused by
object overlap, we propose a guided smoothing strategy within regularization to
eliminate inaccurate boundary points and outliers. Furthermore, to address
sparse views and multi-scale challenges, we use a dual-step matching strategy
that leverages adjacent and long-term information. Extensive experiments
validate the efficacy of StreetSurfGS in both novel view synthesis and surface
reconstruction."
Enhancing Android Malware Detection: The Influence of ChatGPT on Decision-centric Task,cs.CR,Cryptography and Security,2024-10-06,"With the rise of large language models, such as ChatGPT, non-decisional
models have been applied to various tasks. Moreover, ChatGPT has drawn
attention to the traditional decision-centric task of Android malware
detection. Despite effective detection methods proposed by scholars, they face
low interpretability issues. Specifically, while these methods excel in
classifying applications as benign or malicious and can detect malicious
behavior, they often fail to provide detailed explanations for the decisions
they make. This challenge raises concerns about the reliability of existing
detection schemes and questions their true ability to understand complex data.
In this study, we investigate the influence of the non-decisional model,
ChatGPT, on the traditional decision-centric task of Android malware detection.
We choose three state-of-the-art solutions, Drebin, XMAL, and MaMaDroid,
conduct a series of experiments on publicly available datasets, and carry out a
comprehensive comparison and analysis. Our findings indicate that these
decision-driven solutions primarily rely on statistical patterns within
datasets to make decisions, rather than genuinely understanding the underlying
data. In contrast, ChatGPT, as a non-decisional model, excels in providing
comprehensive analysis reports, substantially enhancing interpretability.
Furthermore, we conduct surveys among experienced developers. The result
highlights developers' preference for ChatGPT, as it offers in-depth insights
and enhances efficiency and understanding of challenges. Meanwhile, these
studies and analyses offer profound insights, presenting developers with a
novel perspective on Android malware detection--enhancing the reliability of
detection results from a non-decisional perspective."
TIS-DPO: Token-level Importance Sampling for Direct Preference Optimization With Estimated Weights,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Direct Preference Optimization (DPO) has been widely adopted for preference
alignment of Large Language Models (LLMs) due to its simplicity and
effectiveness. However, DPO is derived as a bandit problem in which the whole
response is treated as a single arm, ignoring the importance differences
between tokens, which may affect optimization efficiency and make it difficult
to achieve optimal results. In this work, we propose that the optimal data for
DPO has equal expected rewards for each token in winning and losing responses,
as there is no difference in token importance. However, since the optimal
dataset is unavailable in practice, we propose using the original dataset for
importance sampling to achieve unbiased optimization. Accordingly, we propose a
token-level importance sampling DPO objective named TIS-DPO that assigns
importance weights to each token based on its reward. Inspired by previous
works, we estimate the token importance weights using the difference in
prediction probabilities from a pair of contrastive LLMs. We explore three
methods to construct these contrastive LLMs: (1) guiding the original LLM with
contrastive prompts, (2) training two separate LLMs using winning and losing
responses, and (3) performing forward and reverse DPO training with winning and
losing responses. Experiments show that TIS-DPO significantly outperforms
various baseline methods on harmlessness and helpfulness alignment and
summarization tasks. We also visualize the estimated weights, demonstrating
their ability to identify key token positions."
Latent Feature Mining for Predictive Model Enhancement with Large Language Models,cs.LG,Machine Learning,2024-10-06,"Predictive modeling often faces challenges due to limited data availability
and quality, especially in domains where collected features are weakly
correlated with outcomes and where additional feature collection is constrained
by ethical or practical difficulties. Traditional machine learning (ML) models
struggle to incorporate unobserved yet critical factors. In this work, we
introduce an effective approach to formulate latent feature mining as
text-to-text propositional logical reasoning. We propose FLAME (Faithful Latent
Feature Mining for Predictive Model Enhancement), a framework that leverages
large language models (LLMs) to augment observed features with latent features
and enhance the predictive power of ML models in downstream tasks. Our
framework is generalizable across various domains with necessary
domain-specific adaptation, as it is designed to incorporate contextual
information unique to each area, ensuring effective transfer to different areas
facing similar data availability challenges. We validate our framework with two
case studies: (1) the criminal justice system, a domain characterized by
limited and ethically challenging data collection; (2) the healthcare domain,
where patient privacy concerns and the complexity of medical data limit
comprehensive feature collection. Our results show that inferred latent
features align well with ground truth labels and significantly enhance the
downstream classifier."
Ordinal Preference Optimization: Aligning Human Preferences via NDCG,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Aligning Large Language Models (LLMs) with diverse human preferences is a
pivotal technique for controlling model behaviors and enhancing generation
quality. Reinforcement Learning from Human Feedback (RLHF), Direct Preference
Optimization (DPO), and their variants optimize language models by pairwise
comparisons. However, when multiple responses are available, these approaches
fall short of leveraging the extensive information in the ranking given by the
reward models or human feedback. In this work, we propose a novel listwise
approach named Ordinal Preference Optimization (OPO), which employs the
Normalized Discounted Cumulative Gain (NDCG), a widely-used ranking metric, to
better utilize relative proximity within ordinal multiple responses. We develop
an end-to-end preference optimization algorithm by approximating NDCG with a
differentiable surrogate loss. This approach builds a connection between
ranking models in information retrieval and the alignment problem. In aligning
multi-response datasets assigned with ordinal rewards, OPO outperforms existing
pairwise and listwise approaches on evaluation sets and general benchmarks like
AlpacaEval. Moreover, we demonstrate that increasing the pool of negative
samples can enhance model performance by reducing the adverse effects of
trivial negatives."
MVP-Bench: Can Large Vision--Language Models Conduct Multi-level Visual Perception Like Humans?,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"Humans perform visual perception at multiple levels, including low-level
object recognition and high-level semantic interpretation such as behavior
understanding. Subtle differences in low-level details can lead to substantial
changes in high-level perception. For example, substituting the shopping bag
held by a person with a gun suggests violent behavior, implying criminal or
violent activity. Despite significant advancements in various multimodal tasks,
Large Visual-Language Models (LVLMs) remain unexplored in their capabilities to
conduct such multi-level visual perceptions.
  To investigate the perception gap between LVLMs and humans, we introduce
MVP-Bench, the first visual-language benchmark systematically evaluating both
low- and high-level visual perception of LVLMs. We construct MVP-Bench across
natural and synthetic images to investigate how manipulated content influences
model perception. Using MVP-Bench, we diagnose the visual perception of 10
open-source and 2 closed-source LVLMs, showing that high-level perception tasks
significantly challenge existing LVLMs. The state-of-the-art GPT-4o only
achieves an accuracy of $56\%$ on Yes/No questions, compared with $74\%$ in
low-level scenarios. Furthermore, the performance gap between natural and
manipulated images indicates that current LVLMs do not generalize in
understanding the visual semantics of synthetic images as humans do. Our data
and code are publicly available at https://github.com/GuanzhenLi/MVP-Bench."
DeepONet for Solving PDEs: Generalization Analysis in Sobolev Training,cs.LG,Machine Learning,2024-10-06,"In this paper, we investigate the application of operator learning,
specifically DeepONet, to solve partial differential equations (PDEs). Unlike
function learning methods that require training separate neural networks for
each PDE, operator learning generalizes across different PDEs without
retraining. We focus on the performance of DeepONet in Sobolev training,
addressing two key questions: the approximation ability of deep branch and
trunk networks, and the generalization error in Sobolev norms. Our findings
highlight that deep branch networks offer significant performance benefits,
while trunk networks are best kept simple. Moreover, standard sampling methods
without adding derivative information in the encoding part are sufficient for
minimizing generalization error in Sobolev training, based on generalization
analysis. This paper fills a theoretical gap by providing error estimations for
a wide range of physics-informed machine learning models and applications."
Inference Scaling for Long-Context Retrieval Augmented Generation,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"The scaling of inference computation has unlocked the potential of
long-context large language models (LLMs) across diverse settings. For
knowledge-intensive tasks, the increased compute is often allocated to
incorporate more external knowledge. However, without effectively utilizing
such knowledge, solely expanding context does not always enhance performance.
In this work, we investigate inference scaling for retrieval augmented
generation (RAG), exploring strategies beyond simply increasing the quantity of
knowledge. We focus on two inference scaling strategies: in-context learning
and iterative prompting. These strategies provide additional flexibility to
scale test-time computation (e.g., by increasing retrieved documents or
generation steps), thereby enhancing LLMs' ability to effectively acquire and
utilize contextual information. We address two key questions: (1) How does RAG
performance benefit from the scaling of inference computation when optimally
configured? (2) Can we predict the optimal test-time compute allocation for a
given budget by modeling the relationship between RAG performance and inference
parameters? Our observations reveal that increasing inference computation leads
to nearly linear gains in RAG performance when optimally allocated, a
relationship we describe as the inference scaling laws for RAG. Building on
this, we further develop the computation allocation model to estimate RAG
performance across different inference configurations. The model predicts
optimal inference parameters under various computation constraints, which align
closely with the experimental results. By applying these optimal
configurations, we demonstrate that scaling inference compute on long-context
LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG."
Accelerating Inference of Networks in the Frequency Domain,cs.CV,Computer Vision and Pattern Recognition,2024-10-06,"It has been demonstrated that networks' parameters can be significantly
reduced in the frequency domain with a very small decrease in accuracy.
However, given the cost of frequency transforms, the computational complexity
is not significantly decreased. In this work, we propose performing network
inference in the frequency domain to speed up networks whose frequency
parameters are sparse. In particular, we propose a frequency inference chain
that is dual to the network inference in the spatial domain. In order to handle
the non-linear layers, we make a compromise to apply non-linear operations on
frequency data directly, which works effectively. Enabled by the frequency
inference chain and the strategy for non-linear layers, the proposed approach
completes the entire inference in the frequency domain. Unlike previous
approaches which require extra frequency or inverse transforms for all layers,
the proposed approach only needs the frequency transform and its inverse once
at the beginning and once at the end of a network. Comparisons with
state-of-the-art methods demonstrate that the proposed approach significantly
improves accuracy in the case of a high speedup ratio (over 100x). The source
code is available at \url{https://github.com/guanfangdong/FreqNet-Infer}."
ReTok: Replacing Tokenizer to Enhance Representation Efficiency in Large Language Model,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"Tokenizer is an essential component for large language models (LLMs), and a
tokenizer with a high compression rate can improve the model's representation
and processing efficiency. However, the tokenizer cannot ensure high
compression rate in all scenarios, and an increase in the average input and
output lengths will increases the training and inference costs of the model.
Therefore, it is crucial to find ways to improve the model's efficiency with
minimal cost while maintaining the model's performance. In this work, we
propose a method to improve model representation and processing efficiency by
replacing the tokenizers of LLMs. We propose replacing and reinitializing the
parameters of the model's input and output layers with the parameters of the
original model, and training these parameters while keeping other parameters
fixed. We conducted experiments on different LLMs, and the results show that
our method can maintain the performance of the model after replacing the
tokenizer, while significantly improving the decoding speed for long texts."
Gradient Routing: Masking Gradients to Localize Computation in Neural Networks,cs.LG,Machine Learning,2024-10-06,"Neural networks are trained primarily based on their inputs and outputs,
without regard for their internal mechanisms. These neglected mechanisms
determine properties that are critical for safety, like (i) transparency; (ii)
the absence of sensitive information or harmful capabilities; and (iii)
reliable generalization of goals beyond the training distribution. To address
this shortcoming, we introduce gradient routing, a training method that
isolates capabilities to specific subregions of a neural network. Gradient
routing applies data-dependent, weighted masks to gradients during
backpropagation. These masks are supplied by the user in order to configure
which parameters are updated by which data points. We show that gradient
routing can be used to (1) learn representations which are partitioned in an
interpretable way; (2) enable robust unlearning via ablation of a pre-specified
network subregion; and (3) achieve scalable oversight of a reinforcement
learner by localizing modules responsible for different behaviors. Throughout,
we find that gradient routing localizes capabilities even when applied to a
limited, ad-hoc subset of the data. We conclude that the approach holds promise
for challenging, real-world applications where quality data are scarce."
Leveraging Hierarchical Taxonomies in Prompt-based Continual Learning,cs.LG,Machine Learning,2024-10-06,"Drawing inspiration from human learning behaviors, this work proposes a novel
approach to mitigate catastrophic forgetting in Prompt-based Continual Learning
models by exploiting the relationships between continuously emerging class
data. We find that applying human habits of organizing and connecting
information can serve as an efficient strategy when training deep learning
models. Specifically, by building a hierarchical tree structure based on the
expanding set of labels, we gain fresh insights into the data, identifying
groups of similar classes could easily cause confusion. Additionally, we delve
deeper into the hidden connections between classes by exploring the original
pretrained model's behavior through an optimal transport-based approach. From
these insights, we propose a novel regularization loss function that encourages
models to focus more on challenging knowledge areas, thereby enhancing overall
performance. Experimentally, our method demonstrated significant superiority
over the most robust state-of-the-art models on various benchmarks."
Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion,cs.CR,Cryptography and Security,2024-10-06,"Ensuring the security of released large language models (LLMs) poses a
significant dilemma, as existing mechanisms either compromise ownership rights
or raise data privacy concerns. To address this dilemma, we introduce TaylorMLP
to protect the ownership of released LLMs and prevent their abuse.
Specifically, TaylorMLP preserves the ownership of LLMs by transforming the
weights of LLMs into parameters of Taylor-series. Instead of releasing the
original weights, developers can release the Taylor-series parameters with
users, thereby ensuring the security of LLMs. Moreover, TaylorMLP can prevent
abuse of LLMs by adjusting the generation speed. It can induce low-speed token
generation for the protected LLMs by increasing the terms in the Taylor-series.
This intentional delay helps LLM developers prevent potential large-scale
unauthorized uses of their models. Empirical experiments across five datasets
and three LLM architectures demonstrate that TaylorMLP induces over 4x increase
in latency, producing the tokens precisely matched with original LLMs.
Subsequent defensive experiments further confirm that TaylorMLP effectively
prevents users from reconstructing the weight values based on downstream
datasets."
Channel-Aware Throughput Maximization for Cooperative Data Fusion in CAV,cs.AI,Artificial Intelligence,2024-10-06,"Connected and autonomous vehicles (CAVs) have garnered significant attention
due to their extended perception range and enhanced sensing coverage. To
address challenges such as blind spots and obstructions, CAVs employ
vehicle-to-vehicle (V2V) communications to aggregate sensory data from
surrounding vehicles. However, cooperative perception is often constrained by
the limitations of achievable network throughput and channel quality. In this
paper, we propose a channel-aware throughput maximization approach to
facilitate CAV data fusion, leveraging a self-supervised autoencoder for
adaptive data compression. We formulate the problem as a mixed integer
programming (MIP) model, which we decompose into two sub-problems to derive
optimal data rate and compression ratio solutions under given link conditions.
An autoencoder is then trained to minimize bitrate with the determined
compression ratio, and a fine-tuning strategy is employed to further reduce
spectrum resource consumption. Experimental evaluation on the OpenCOOD platform
demonstrates the effectiveness of our proposed algorithm, showing more than
20.19\% improvement in network throughput and a 9.38\% increase in average
precision (AP@IoU) compared to state-of-the-art methods, with an optimal
latency of 19.99 ms."
Calibrating Expressions of Certainty,cs.CL,Computation and Language (Natural Language Processing),2024-10-06,"We present a novel approach to calibrating linguistic expressions of
certainty, e.g., ""Maybe"" and ""Likely"". Unlike prior work that assigns a single
score to each certainty phrase, we model uncertainty as distributions over the
simplex to capture their semantics more accurately. To accommodate this new
representation of certainty, we generalize existing measures of miscalibration
and introduce a novel post-hoc calibration method. Leveraging these tools, we
analyze the calibration of both humans (e.g., radiologists) and computational
models (e.g., language models) and provide interpretable suggestions to improve
their calibration."
Vehicle-in-Virtual-Environment Method for ADAS and Connected and Automated Driving Function Development/Demonstration/Evaluation,cs.RO,Robotics,2024-10-05,"The current approach for new Advanced Driver Assistance System (ADAS) and
Connected and Automated Driving (CAD) function development involves a
significant amount of public road testing which is inefficient due to the
number miles that need to be driven for rare and extreme events to take place,
thereby being very costly also, and unsafe as the rest of the road users become
involuntary test subjects. A new development, evaluation and demonstration
method for safe, efficient, and repeatable development, demonstration and
evaluation of ADAS and CAD functions called VehicleInVirtualEnvironment (VVE)
was recently introduced as a solution to this problem. The vehicle is operated
in a large, empty, and flat area during VVE while its localization and
perception sensor data is fed from the virtual environment with other traffic
and rare and extreme events being generated as needed. The virtual environment
can be easily configured and modified to construct different testing scenarios
on demand. This paper focuses on the VVE approach and introduces the coordinate
transformations needed to sync pose (location and orientation) in the virtual
and physical worlds and handling of localization and perception sensor data
using the highly realistic 3D simulation model of a recent autonomous shuttle
deployment site in Columbus, Ohio as the virtual world. As a further example
that uses multiple actors, the use of VVE for VehicleToVRU communication based
Vulnerable Road User (VRU) safety is presented in the paper using VVE
experiments and real pedestrian(s) in a safe and repeatable manner. VVE
experiments are used to demonstrate the efficacy of the method."
PANav: Toward Privacy-Aware Robot Navigation via Vision-Language Models,cs.RO,Robotics,2024-10-05,"Navigating robots discreetly in human work environments while considering the
possible privacy implications of robotic tasks presents significant challenges.
Such scenarios are increasingly common, for instance, when robots transport
sensitive objects that demand high levels of privacy in spaces crowded with
human activities. While extensive research has been conducted on robotic path
planning and social awareness, current robotic systems still lack the
functionality of privacy-aware navigation in public environments. To address
this, we propose a new framework for mobile robot navigation that leverages
vision-language models to incorporate privacy awareness into adaptive path
planning. Specifically, all potential paths from the starting point to the
destination are generated using the A* algorithm. Concurrently, the
vision-language model is used to infer the optimal path for privacy-awareness,
given the environmental layout and the navigational instruction. This approach
aims to minimize the robot's exposure to human activities and preserve the
privacy of the robot and its surroundings. Experimental results on the S3DIS
dataset demonstrate that our framework significantly enhances mobile robots'
privacy awareness of navigation in human-shared public environments.
Furthermore, we demonstrate the practical applicability of our framework by
successfully navigating a robotic platform through real-world office
environments. The supplementary video and code can be accessed via the
following link: https://sites.google.com/view/privacy-aware-nav."
Integrating Physics-Informed Deep Learning and Numerical Methods for Robust Dynamics Discovery and Parameter Estimation,cs.LG,Machine Learning,2024-10-05,"Incorporating a priori physics knowledge into machine learning leads to more
robust and interpretable algorithms. In this work, we combine deep learning
techniques and classic numerical methods for differential equations to solve
two challenging problems in dynamical systems theory: dynamics discovery and
parameter estimation. Results demonstrate the effectiveness of the proposed
approaches on a suite of test problems exhibiting oscillatory and chaotic
dynamics. When comparing the performance of various numerical schemes, such as
the Runge-Kutta and linear multistep families of methods, we observe promising
results in predicting the system dynamics and estimating physical parameters,
given appropriate choices of spatial and temporal discretization schemes and
numerical method orders."
Test-Time Adaptation for Keypoint-Based Spacecraft Pose Estimation Based on Predicted-View Synthesis,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Due to the difficulty of replicating the real conditions during training,
supervised algorithms for spacecraft pose estimation experience a drop in
performance when trained on synthetic data and applied to real operational
data. To address this issue, we propose a test-time adaptation approach that
leverages the temporal redundancy between images acquired during close
proximity operations. Our approach involves extracting features from sequential
spacecraft images, estimating their poses, and then using this information to
synthesise a reconstructed view. We establish a self-supervised learning
objective by comparing the synthesised view with the actual one. During
training, we supervise both pose estimation and image synthesis, while at
test-time, we optimise the self-supervised objective. Additionally, we
introduce a regularisation loss to prevent solutions that are not consistent
with the keypoint structure of the spacecraft. Our code is available at:
https://github.com/JotaBravo/spacecraft-tta."
Bootstrap Sampling Rate Greater than 1.0 May Improve Random Forest Performance,cs.LG,Machine Learning,2024-10-05,"Random forests utilize bootstrap sampling to create an individual training
set for each component tree. This involves sampling with replacement, with the
number of instances equal to the size of the original training set ($N$).
Research literature indicates that drawing fewer than $N$ observations can also
yield satisfactory results. The ratio of the number of observations in each
bootstrap sample to the total number of training instances is called the
bootstrap rate (BR). Sampling more than $N$ observations (BR $>$ 1) has been
explored in the literature only to a limited extent and has generally proven
ineffective. In this paper, we re-examine this approach using 36 diverse
datasets and consider BR values ranging from 1.2 to 5.0. Contrary to previous
findings, we show that such parameterization can result in statistically
significant improvements in classification accuracy compared to standard
settings (BR $\leq$ 1). Furthermore, we investigate what the optimal BR depends
on and conclude that it is more a property of the dataset than a dependence on
the random forest hyperparameters. Finally, we develop a binary classifier to
predict whether the optimal BR is $\leq$ 1 or $>$ 1 for a given dataset,
achieving between 81.88\% and 88.81\% accuracy, depending on the experiment
configuration."
Efficiently Identifying Low-Quality Language Subsets in Multilingual Datasets: A Case Study on a Large-Scale Multilingual Audio Dataset,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Curating datasets that span multiple languages is challenging. To make the
collection more scalable, researchers often incorporate one or more imperfect
classifiers in the process, like language identification models. These models,
however, are prone to failure, resulting in some language subsets being
unreliable for downstream tasks. We introduce a statistical test, the
Preference Proportion Test, for identifying such unreliable subsets. By
annotating only 20 samples for a language subset, we're able to identify
systematic transcription errors for 10 language subsets in a recent large
multilingual transcribed audio dataset, X-IPAPack (Zhu et al., 2024). We find
that filtering this low-quality data out when training models for the
downstream task of phonetic transcription brings substantial benefits, most
notably a 25.7% relative improvement on transcribing recordings in
out-of-distribution languages. Our method lays a path forward for systematic
and reliable multilingual dataset auditing."
Self-Supervised Anomaly Detection in the Wild: Favor Joint Embeddings Methods,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Accurate anomaly detection is critical in vision-based infrastructure
inspection, where it helps prevent costly failures and enhances safety.
Self-Supervised Learning (SSL) offers a promising approach by learning robust
representations from unlabeled data. However, its application in anomaly
detection remains underexplored. This paper addresses this gap by providing a
comprehensive evaluation of SSL methods for real-world anomaly detection,
focusing on sewer infrastructure. Using the Sewer-ML dataset, we evaluate
lightweight models such as ViT-Tiny and ResNet-18 across SSL frameworks,
including BYOL, Barlow Twins, SimCLR, DINO, and MAE, under varying class
imbalance levels. Through 250 experiments, we rigorously assess the performance
of these SSL methods to ensure a robust and comprehensive evaluation. Our
findings highlight the superiority of joint-embedding methods like SimCLR and
Barlow Twins over reconstruction-based approaches such as MAE, which struggle
to maintain performance under class imbalance. Furthermore, we find that the
SSL model choice is more critical than the backbone architecture. Additionally,
we emphasize the need for better label-free assessments of SSL representations,
as current methods like RankMe fail to adequately evaluate representation
quality, making cross-validation without labels infeasible. Despite the
remaining performance gap between SSL and supervised models, these findings
highlight the potential of SSL to enhance anomaly detection, paving the way for
further research in this underexplored area of SSL applications."
Enhancing Carbon Emission Reduction Strategies using OCO and ICOS data,cs.LG,Machine Learning,2024-10-05,"We propose a methodology to enhance local CO2 monitoring by integrating
satellite data from the Orbiting Carbon Observatories (OCO-2 and OCO-3) with
ground level observations from the Integrated Carbon Observation System (ICOS)
and weather data from the ECMWF Reanalysis v5 (ERA5). Unlike traditional
methods that downsample national data, our approach uses multimodal data fusion
for high-resolution CO2 estimations. We employ weighted K-nearest neighbor
(KNN) interpolation with machine learning models to predict ground level CO2
from satellite measurements, achieving a Root Mean Squared Error of 3.92 ppm.
Our results show the effectiveness of integrating diverse data sources in
capturing local emission patterns, highlighting the value of high-resolution
atmospheric transport models. The developed model improves the granularity of
CO2 monitoring, providing precise insights for targeted carbon mitigation
strategies, and represents a novel application of neural networks and KNN in
environmental monitoring, adaptable to various regions and temporal scales."
Unveiling the Impact of Local Homophily on GNN Fairness: In-Depth Analysis and New Benchmarks,cs.LG,Machine Learning,2024-10-05,"Graph Neural Networks (GNNs) often struggle to generalize when graphs exhibit
both homophily (same-class connections) and heterophily (different-class
connections). Specifically, GNNs tend to underperform for nodes with local
homophily levels that differ significantly from the global homophily level.
This issue poses a risk in user-centric applications where underrepresented
homophily levels are present. Concurrently, fairness within GNNs has received
substantial attention due to the potential amplification of biases via message
passing. However, the connection between local homophily and fairness in GNNs
remains underexplored. In this work, we move beyond global homophily and
explore how local homophily levels can lead to unfair predictions. We begin by
formalizing the challenge of fair predictions for underrepresented homophily
levels as an out-of-distribution (OOD) problem. We then conduct a theoretical
analysis that demonstrates how local homophily levels can alter predictions for
differing sensitive attributes. We additionally introduce three new GNN
fairness benchmarks, as well as a novel semi-synthetic graph generator, to
empirically study the OOD problem. Across extensive analysis we find that two
factors can promote unfairness: (a) OOD distance, and (b) heterophilous nodes
situated in homophilous graphs. In cases where these two conditions are met,
fairness drops by up to 24% on real world datasets, and 30% in semi-synthetic
datasets. Together, our theoretical insights, empirical analysis, and
algorithmic contributions unveil a previously overlooked source of unfairness
rooted in the graph's homophily information."
Reward Learning From Preference With Ties,cs.LG,Machine Learning,2024-10-05,"Reward learning plays a pivotal role in Reinforcement Learning from Human
Feedback (RLHF), ensuring the alignment of language models. The Bradley-Terry
(BT) model stands as the prevalent choice for capturing human preferences from
datasets containing pairs of chosen and rejected responses. In preference
modeling, the focus is not on absolute values but rather on the reward
difference between chosen and rejected responses, referred to as preference
strength. Thus, precise evaluation of preference strength holds paramount
importance in preference modeling. However, an easily overlooked factor
significantly affecting preference strength measurement is that human attitudes
towards two responses may not solely indicate a preference for one over the
other and ties are also a common occurrence. To address this, we propose the
adoption of the generalized Bradley-Terry model -- the Bradley-Terry model with
ties (BTT) -- to accommodate tied preferences, thus leveraging additional
information. We prove that even with the access to the true distributions of
prompt and response, disregarding ties can lead to a notable bias in preference
strength measurement. Comprehensive experiments further validate the advantages
of incorporating ties in preference modeling. Notably, fine-tuning with BTT
significantly outperforms fine-tuning with BT on synthetic preference datasets
with ties, labeled by state-of-the-art open-source LLMs."
Applying Hybrid Graph Neural Networks to Strengthen Credit Risk Analysis,cs.LG,Machine Learning,2024-10-05,"This paper presents a novel approach to credit risk prediction by employing
Graph Convolutional Neural Networks (GCNNs) to assess the creditworthiness of
borrowers. Leveraging the power of big data and artificial intelligence, the
proposed method addresses the challenges faced by traditional credit risk
assessment models, particularly in handling imbalanced datasets and extracting
meaningful features from complex relationships. The paper begins by
transforming raw borrower data into graph-structured data, where borrowers and
their relationships are represented as nodes and edges, respectively. A classic
subgraph convolutional model is then applied to extract local features,
followed by the introduction of a hybrid GCNN model that integrates both local
and global convolutional operators to capture a comprehensive representation of
node features. The hybrid model incorporates an attention mechanism to
adaptively select features, mitigating issues of over-smoothing and
insufficient feature consideration. The study demonstrates the potential of
GCNNs in improving the accuracy of credit risk prediction, offering a robust
solution for financial institutions seeking to enhance their lending
decision-making processes."
Locating Information Gaps and Narrative Inconsistencies Across Languages: A Case Study of LGBT People Portrayals on Wikipedia,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"To explain social phenomena and identify systematic biases, much research in
computational social science focuses on comparative text analyses. These
studies often rely on coarse corpus-level statistics or local word-level
analyses, mainly in English. We introduce the InfoGap method -- an efficient
and reliable approach to locating information gaps and inconsistencies in
articles at the fact level, across languages. We evaluate InfoGap by analyzing
LGBT people's portrayals, across 2.7K biography pages on English, Russian, and
French Wikipedias. We find large discrepancies in factual coverage across the
languages. Moreover, our analysis reveals that biographical facts carrying
negative connotations are more likely to be highlighted in Russian Wikipedia.
Crucially, InfoGap both facilitates large scale analyses, and pinpoints local
document- and fact-level information gaps, laying a new foundation for targeted
and nuanced comparative language analysis at scale."
"Black Boxes and Looking Glasses: Multilevel Symmetries, Reflection Planes, and Convex Optimization in Deep Networks",cs.LG,Machine Learning,2024-10-05,"We show that training deep neural networks (DNNs) with absolute value
activation and arbitrary input dimension can be formulated as equivalent convex
Lasso problems with novel features expressed using geometric algebra. This
formulation reveals geometric structures encoding symmetry in neural networks.
Using the equivalent Lasso form of DNNs, we formally prove a fundamental
distinction between deep and shallow networks: deep networks inherently favor
symmetric structures in their fitted functions, with greater depth enabling
multilevel symmetries, i.e., symmetries within symmetries. Moreover, Lasso
features represent distances to hyperplanes that are reflected across training
points. These reflection hyperplanes are spanned by training data and are
orthogonal to optimal weight vectors. Numerical experiments support theory and
demonstrate theoretically predicted features when training networks using
embeddings generated by Large Language Models."
Mechanistic Behavior Editing of Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Large Language Models trained on web-scale text acquire language generation
abilities that can solve a wide range of tasks, particularly when task
knowledge is refined into the generative prior using in-context examples.
However, spurious features learned from noisy data hinder their
generalizability. Supervised finetuning can introduce task specificity, but
introduce data inefficiency. Prior studies indicate that (i) noisy neural
circuitries coexist with generalizable ones within LLMs, and (ii) finetuning
typically enhances (or suppresses) existing abilities without introducing newer
ones. Building upon these, we propose TaRot, a novel method for task
adaptation. TaRot intervenes in the neural circuitries using learnable rotation
matrices that are optimized using Bayesian Optimization, on labelled samples in
the order of standard few-shot prompting examples. Experiments on multiple
classification and generation tasks using LLMs of varying sizes reveal the
efficacy of TaRot, improving upon both zero- as well as few-shot performance,
with average improvements (across models and tasks) of 23.81% and 11.15%,
respectively. The source code is available at
https://github.com/joykirat18/TaRot"
Language Model-Driven Data Pruning Enables Efficient Active Learning,cs.LG,Machine Learning,2024-10-05,"Active learning (AL) optimizes data labeling efficiency by selecting the most
informative instances for annotation. A key component in this procedure is an
acquisition function that guides the selection process and identifies the
suitable instances for labeling from the unlabeled pool. However, these
acquisition methods suffer from high computational costs with large unlabeled
data pools, posing a roadblock to their applicability on large datasets. To
address this challenge and bridge this gap, we introduce a novel plug-and-play
unlabeled data pruning strategy, ActivePrune, which leverages language models
to prune the unlabeled pool. ActivePrune implements a two-stage pruning
process: an initial fast evaluation using perplexity scores from an n-gram
language model, followed by a high-quality selection using metrics for data
quality computed through a quantized LLM. Additionally, to enhance the
diversity in the unlabeled pool, we propose a novel perplexity reweighting
method that systematically brings forward underrepresented instances for
selection in subsequent labeling iterations. Experiments on translation,
sentiment analysis, topic classification, and summarization tasks on four
diverse datasets and four active learning strategies demonstrate that
ActivePrune outperforms existing data pruning methods. Finally, we compare the
selection quality $\leftrightarrow$ efficiency tradeoff of the data pruning
methods and demonstrate that ActivePrune is computationally more efficient than
other LLM score-based pruning methods, and provides up to 74% reduction in the
end-to-end time required for active learning."
Evaluating Language Model Character Traits,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Language models (LMs) can exhibit human-like behaviour, but it is unclear how
to describe this behaviour without undue anthropomorphism. We formalise a
behaviourist view of LM character traits: qualities such as truthfulness,
sycophancy, or coherent beliefs and intentions, which may manifest as
consistent patterns of behaviour. Our theory is grounded in empirical
demonstrations of LMs exhibiting different character traits, such as accurate
and logically coherent beliefs, and helpful and harmless intentions. We find
that the consistency with which LMs exhibit certain character traits varies
with model size, fine-tuning, and prompting. In addition to characterising LM
character traits, we evaluate how these traits develop over the course of an
interaction. We find that traits such as truthfulness and harmfulness can be
stationary, i.e., consistent over an interaction, in certain contexts, but may
be reflective in different contexts, meaning they mirror the LM's behavior in
the preceding interaction. Our formalism enables us to describe LM behaviour
precisely in intuitive language, without undue anthropomorphism."
Fundamental Limitations on Subquadratic Alternatives to Transformers,cs.LG,Machine Learning,2024-10-05,"The Transformer architecture is widely deployed in many popular and impactful
Large Language Models. At its core is the attention mechanism for calculating
correlations between pairs of tokens. Performing an attention computation takes
quadratic time in the input size, and had become the time bottleneck for
transformer operations. In order to circumvent this, researchers have used a
variety of approaches, including designing heuristic algorithms for performing
attention computations faster, and proposing alternatives to the attention
mechanism which can be computed more quickly. For instance, state space models
such as Mamba were designed to replace attention with an almost linear time
alternative.
  In this paper, we prove that any such approach cannot perform important tasks
that Transformer is able to perform (assuming a popular conjecture from
fine-grained complexity theory). We focus on document similarity tasks, where
one is given as input many documents and would like to find a pair which is
(approximately) the most similar. We prove that Transformer is able to perform
this task, and we prove that this task cannot be performed in truly
subquadratic time by any algorithm. Thus, any model which can be evaluated in
subquadratic time - whether because of subquadratic-time heuristics for
attention, faster attention replacements like Mamba, or any other reason -
cannot perform this task. In other words, in order to perform tasks that
(implicitly or explicitly) involve document similarity, one may as well use
Transformer and cannot avoid its quadratic running time."
RoQLlama: A Lightweight Romanian Adapted Language Model,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"The remarkable achievements obtained by open-source large language models
(LLMs) in recent years have predominantly been concentrated on tasks involving
the English language. In this paper, we aim to advance the performance of
Llama2 models on Romanian tasks. We tackle the problem of reduced computing
resources by using QLoRA for training. We release RoQLlama-7b, a quantized LLM,
which shows equal or improved results compared to its full-sized counterpart
when tested on seven Romanian downstream tasks in the zero-shot setup. Also, it
consistently achieves higher average scores across all few-shot prompts.
Additionally, we introduce a novel Romanian dataset, namely RoMedQA, which
contains single-choice medical questions in Romanian."
Constructing Cloze Questions Generatively,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"We present a generative method called CQG for constructing cloze questions
from a given article using neural networks and WordNet, with an emphasis on
generating multigram distractors. Built on sense disambiguation, text-to-text
transformation, WordNet's synset taxonomies and lexical labels, CQG selects an
answer key for a given sentence, segments it into a sequence of instances,
generates instance-level distractor candidates (IDCs) using a transformer and
sibling synsets.It then removes inappropriate IDCs, ranks the remaining IDCs
based on contextual embedding similarities, as well as synset and lexical
relatedness, forms distractor candidates by combinatorially replacing instances
with the corresponding top-ranked IDCs, and checks if they are legitimate
phrases. Finally, it selects top-ranked distractor candidates based on
contextual semantic similarities to the answer key. Experiments show that this
method significantly outperforms SOTA results. Human judges also confirm the
high qualities of the generated distractors."
AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Creativity has long been considered one of the most difficult aspect of human
intelligence for AI to mimic. However, the rise of Large Language Models
(LLMs), like ChatGPT, has raised questions about whether AI can match or even
surpass human creativity. We present CREATIVITY INDEX as the first step to
quantify the linguistic creativity of a text by reconstructing it from existing
text snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that
the seemingly remarkable creativity of LLMs may be attributable in large part
to the creativity of human-written texts on the web. To compute CREATIVITY
INDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming
algorithm that can search verbatim and near-verbatim matches of text snippets
from a given document against the web. Experiments reveal that the CREATIVITY
INDEX of professional human authors is on average 66.2% higher than that of
LLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of
30.1%. In addition, we find that distinguished authors like Hemingway exhibit
measurably higher CREATIVITY INDEX compared to other human writers. Finally, we
demonstrate that CREATIVITY INDEX can be used as a surprisingly effective
criterion for zero-shot machine text detection, surpassing the strongest
existing zero-shot system, DetectGPT, by a significant margin of 30.2%, and
even outperforming the strongest supervised system, GhostBuster, in five out of
six domains."
DeFoG: Discrete Flow Matching for Graph Generation,cs.LG,Machine Learning,2024-10-05,"Graph generation is fundamental in diverse scientific applications, due to
its ability to reveal the underlying distribution of complex data, and
eventually generate new, realistic data points. Despite the success of
diffusion models in this domain, those face limitations in sampling efficiency
and flexibility, stemming from the tight coupling between the training and
sampling stages. To address this, we propose DeFoG, a novel framework using
discrete flow matching for graph generation. DeFoG employs a flow-based
approach that features an efficient linear interpolation noising process and a
flexible denoising process based on a continuous-time Markov chain formulation.
We leverage an expressive graph transformer and ensure desirable node
permutation properties to respect graph symmetry. Crucially, our framework
enables a disentangled design of the training and sampling stages, enabling
more effective and efficient optimization of model performance. We navigate
this design space by introducing several algorithmic improvements that boost
the model performance, consistently surpassing existing diffusion models. We
also theoretically demonstrate that, for general discrete data, discrete flow
models can faithfully replicate the ground truth distribution - a result that
naturally extends to graph data and reinforces DeFoG's foundations. Extensive
experiments show that DeFoG achieves state-of-the-art results on synthetic and
molecular datasets, improving both training and sampling efficiency over
diffusion models, and excels in conditional generation on a digital pathology
dataset."
Compositional Diffusion Models for Powered Descent Trajectory Generation with Flexible Constraints,cs.RO,Robotics,2024-10-05,"This work introduces TrajDiffuser, a compositional diffusion-based flexible
and concurrent trajectory generator for 6 degrees of freedom powered descent
guidance. TrajDiffuser is a statistical model that learns the multi-modal
distributions of a dataset of simulated optimal trajectories, each subject to
only one or few constraints that may vary for different trajectories. During
inference, the trajectory is generated simultaneously over time, providing
stable long-horizon planning, and constraints can be composed together,
increasing the model's generalizability and decreasing the training data
required. The generated trajectory is then used to initialize an optimizer,
increasing its robustness and speed."
Is deeper always better? Replacing linear mappings with deep learning networks in the Discriminative Lexicon Model,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Recently, deep learning models have increasingly been used in cognitive
modelling of language. This study asks whether deep learning can help us to
better understand the learning problem that needs to be solved by speakers,
above and beyond linear methods. We utilise the Discriminative Lexicon Model
(DLM, Baayen et al., 2019), which models comprehension and production with
mappings between numeric form and meaning vectors. While so far, these mappings
have been linear (Linear Discriminative Learning, LDL), in the present study we
replace them with deep dense neural networks (Deep Discriminative Learning,
DDL). We find that DDL affords more accurate mappings for large and diverse
datasets from English and Dutch, but not necessarily for Estonian and Taiwan
Mandarin. DDL outperforms LDL in particular for words with pseudo-morphological
structure such as slend+er. Applied to average reaction times, we find that DDL
is outperformed by frequency-informed linear mappings (FIL). However, DDL
trained in a frequency-informed way ('frequency-informed' deep learning, FIDDL)
substantially outperforms FIL. Finally, while linear mappings can very
effectively be updated from trial-to-trial to model incremental lexical
learning (Heitmeier et al., 2023), deep mappings cannot do so as effectively.
At present, both linear and deep mappings are informative for understanding
language."
Implicit to Explicit Entropy Regularization: Benchmarking ViT Fine-tuning under Noisy Labels,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Automatic annotation of large-scale datasets can introduce noisy training
data labels, which adversely affect the learning process of deep neural
networks (DNNs). Consequently, Noisy Labels Learning (NLL) has become a
critical research field for Convolutional Neural Networks (CNNs), though it
remains less explored for Vision Transformers (ViTs). In this study, we
evaluate the vulnerability of ViT fine-tuning to noisy labels and compare its
robustness with CNNs. We also investigate whether NLL methods developed for
CNNs are equally effective for ViTs. Using linear probing and MLP-K
fine-tuning, we benchmark two ViT backbones (ViT-B/16 and ViT-L/16) using three
commonly used classification losses: Cross Entropy (CE), Focal Loss (FL), and
Mean Absolute Error (MAE), alongside six robust NLL methods: GCE, SCE, NLNL,
APL, NCE+AGCE, and ANL-CE. The evaluation is conducted across six datasets
including MNIST, CIFAR-10/100, WebVision, Clothing1M, and Food-101N.
Furthermore, we explore whether implicit prediction entropy minimization
contributes to ViT robustness against noisy labels, noting a general trend of
prediction entropy reduction across most NLL methods. Building on this
observation, we examine whether explicit entropy minimization could enhance ViT
resilience to noisy labels. Our findings indicate that incorporating entropy
regularization enhances the performance of established loss functions such as
CE and FL, as well as the robustness of the six studied NLL methods across both
ViT backbones."
Advancements in Robotics Process Automation: A Novel Model with Enhanced Empirical Validation and Theoretical Insights,cs.RO,Robotics,2024-10-05,"Robotics Process Automation is revolutionizing business operations by
significantly enhancing efficiency, productivity, and operational excellence
across various industries. This manuscript delivers a comprehensive review of
recent advancements in RPA technologies and proposes a novel model designed to
elevate RPA capabilities."
Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Links are a fundamental part of information networks, turning isolated pieces
of knowledge into a network of information that is much richer than the sum of
its parts. However, adding a new link to the network is not trivial: it
requires not only the identification of a suitable pair of source and target
entities but also the understanding of the content of the source to locate a
suitable position for the link in the text. The latter problem has not been
addressed effectively, particularly in the absence of text spans in the source
that could serve as anchors to insert a link to the target entity. To bridge
this gap, we introduce and operationalize the task of entity insertion in
information networks. Focusing on the case of Wikipedia, we empirically show
that this problem is, both, relevant and challenging for editors. We compile a
benchmark dataset in 105 languages and develop a framework for entity insertion
called LocEI (Localized Entity Insertion) and its multilingual variant XLocEI.
We show that XLocEI outperforms all baseline models (including state-of-the-art
prompt-based ranking with LLMs such as GPT-4) and that it can be applied in a
zero-shot manner on languages not seen during training with minimal performance
drop. These findings are important for applying entity insertion models in
practice, e.g., to support editors in adding links across the more than 300
language versions of Wikipedia."
Enhancing Future Link Prediction in Quantum Computing Semantic Networks through LLM-Initiated Node Features,cs.LG,Machine Learning,2024-10-05,"Quantum computing is rapidly evolving in both physics and computer science,
offering the potential to solve complex problems and accelerate computational
processes. The development of quantum chips necessitates understanding the
correlations among diverse experimental conditions. Semantic networks built on
scientific literature, representing meaningful relationships between concepts,
have been used across various domains to identify knowledge gaps and novel
concept combinations. Neural network-based approaches have shown promise in
link prediction within these networks. This study proposes initializing node
features using LLMs to enhance node representations for link prediction tasks
in graph neural networks. LLMs can provide rich descriptions, reducing the need
for manual feature creation and lowering costs. Our method, evaluated using
various link prediction models on a quantum computing semantic network,
demonstrated efficacy compared to traditional node embedding techniques."
ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments,cs.RO,Robotics,2024-10-05,"Construction sites are challenging environments for autonomous systems due to
their unstructured nature and the presence of dynamic actors, such as workers
and machinery. This work presents a comprehensive panoptic scene understanding
solution designed to handle the complexities of such environments by
integrating 2D panoptic segmentation with 3D LiDAR mapping. Our system
generates detailed environmental representations in real-time by combining
semantic and geometric data, supported by Kalman Filter-based tracking for
dynamic object detection. We introduce a fine-tuning method that adapts large
pre-trained panoptic segmentation models for construction site applications
using a limited number of domain-specific samples. For this use case, we
release a first-of-its-kind dataset of 502 hand-labeled sample images with
panoptic annotations from construction sites. In addition, we propose a dynamic
panoptic mapping technique that enhances scene understanding in unstructured
environments. As a case study, we demonstrate the system's application for
autonomous navigation, utilizing real-time RRT* for reactive path planning in
dynamic scenarios. The dataset
(https://leggedrobotics.github.io/panoptic-scene-understanding.github.io/) and
code (https://github.com/leggedrobotics/rsl_panoptic_mapping) for training and
deployment are publicly available to support future research."
Towards Propositional KLM-Style Defeasible Standpoint Logics,cs.AI,Artificial Intelligence,2024-10-05,"The KLM approach to defeasible reasoning introduces a weakened form of
implication into classical logic. This allows one to incorporate exceptions to
general rules into a logical system, and for old conclusions to be withdrawn
upon learning new contradictory information. Standpoint logics are a group of
logics, introduced to the field of Knowledge Representation in the last 5
years, which allow for multiple viewpoints to be integrated into the same
ontology, even when certain viewpoints may hold contradicting beliefs. In this
paper, we aim to integrate standpoints into KLM propositional logic in a
restricted setting. We introduce the logical system of Defeasible Restricted
Standpoint Logic (DRSL) and define its syntax and semantics. Specifically, we
integrate ranked interpretations and standpoint structures, which provide the
semantics for propositional KLM and propositional standpoint logic
respectively, in order to introduce ranked standpoint structures for DRSL.
Moreover, we extend the non-monotonic entailment relation of rational closure
from the propositional KLM case to the DRSL case. The main contribution of this
paper is to characterize rational closure for DRSL both algorithmically and
semantically, showing that rational closure can be characterized through a
single representative ranked standpoint structure. Finally, we conclude that
the semantic and algorithmic characterizations of rational closure are
equivalent, and that entailment-checking for DRSL under rational closure is in
the same complexity class as entailment-checking for propositional KLM."
A Framework for Reproducible Benchmarking and Performance Diagnosis of SLAM Systems,cs.RO,Robotics,2024-10-05,"We propose SLAMFuse, an open-source SLAM benchmarking framework that provides
consistent crossplatform environments for evaluating multi-modal SLAM
algorithms, along with tools for data fuzzing, failure detection, and diagnosis
across different datasets. Our framework introduces a fuzzing mechanism to test
the resilience of SLAM algorithms against dataset perturbations. This enables
the assessment of pose estimation accuracy under varying conditions and
identifies critical perturbation thresholds. SLAMFuse improves diagnostics with
failure detection and analysis tools, examining algorithm behaviour against
dataset characteristics. SLAMFuse uses Docker to ensure reproducible testing
conditions across diverse datasets and systems by streamlining dependency
management. Emphasizing the importance of reproducibility and introducing
advanced tools for algorithm evaluation and performance diagnosis, our work
sets a new precedent for reliable benchmarking of SLAM systems. We provide
ready-to-use docker compatible versions of the algorithms and datasets used in
the experiments, together with guidelines for integrating and benchmarking new
algorithms. Code is available at https://github.com/nikolaradulov/slamfuse"
Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Resolving knowledge conflicts is a crucial challenge in Question Answering
(QA) tasks, as the internet contains numerous conflicting facts and opinions.
While some research has made progress in tackling ambiguous settings where
multiple valid answers exist, these approaches often neglect to provide source
citations, leaving users to evaluate the factuality of each answer. On the
other hand, existing work on citation generation has focused on unambiguous
settings with single answers, failing to address the complexity of real-world
scenarios. Despite the importance of both aspects, no prior research has
combined them, leaving a significant gap in the development of QA systems. In
this work, we bridge this gap by proposing the novel task of QA with source
citation in ambiguous settings, where multiple valid answers exist. To
facilitate research in this area, we create a comprehensive framework
consisting of: (1) five novel datasets, obtained by augmenting three existing
reading comprehension datasets with citation meta-data across various ambiguous
settings, such as distractors and paraphrasing; (2) the first ambiguous
multi-hop QA dataset featuring real-world, naturally occurring contexts; (3)
two new metrics to evaluate models' performances; and (4) several strong
baselines using rule-based, prompting, and finetuning approaches over five
large language models. We hope that this new task, datasets, metrics, and
baselines will inspire the community to push the boundaries of QA research and
develop more trustworthy and interpretable systems."
Persona Knowledge-Aligned Prompt Tuning Method for Online Debate,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Debate is the process of exchanging viewpoints or convincing others on a
particular issue. Recent research has provided empirical evidence that the
persuasiveness of an argument is determined not only by language usage but also
by communicator characteristics. Researchers have paid much attention to
aspects of languages, such as linguistic features and discourse structures, but
combining argument persuasiveness and impact with the social personae of the
audience has not been explored due to the difficulty and complexity. We have
observed the impressive simulation and personification capability of ChatGPT,
indicating a giant pre-trained language model may function as an individual to
provide personae and exert unique influences based on diverse background
knowledge. Therefore, we propose a persona knowledge-aligned framework for
argument quality assessment tasks from the audience side. This is the first
work that leverages the emergence of ChatGPT and injects such audience personae
knowledge into smaller language models via prompt tuning. The performance of
our pipeline demonstrates significant and consistent improvement compared to
competitive architectures."
Towards the Best Solution for Complex System Reliability: Can Statistics Outperform Machine Learning?,cs.LG,Machine Learning,2024-10-05,"Studying the reliability of complex systems using machine learning techniques
involves facing a series of technical and practical challenges, ranging from
the intrinsic nature of the system and data to the difficulties in modeling and
effectively deploying models in real-world scenarios. This study compares the
effectiveness of classical statistical techniques and machine learning methods
for improving complex system analysis in reliability assessments. We aim to
demonstrate that classical statistical algorithms often yield more precise and
interpretable results than black-box machine learning approaches in many
practical applications. The evaluation is conducted using both real-world data
and simulated scenarios. We report the results obtained from statistical
modeling algorithms, as well as from machine learning methods including neural
networks, K-nearest neighbors, and random forests."
Overview of Factify5WQA: Fact Verification through 5W Question-Answering,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Researchers have found that fake news spreads much times faster than real
news. This is a major problem, especially in today's world where social media
is the key source of news for many among the younger population. Fact
verification, thus, becomes an important task and many media sites contribute
to the cause. Manual fact verification is a tedious task, given the volume of
fake news online. The Factify5WQA shared task aims to increase research towards
automated fake news detection by providing a dataset with an aspect-based
question answering based fact verification method. Each claim and its
supporting document is associated with 5W questions that help compare the two
information sources. The objective performance measure in the task is done by
comparing answers using BLEU score to measure the accuracy of the answers,
followed by an accuracy measure of the classification. The task had submissions
using custom training setup and pre-trained language-models among others. The
best performing team posted an accuracy of 69.56%, which is a near 35%
improvement over the baseline."
Improving Distribution Alignment with Diversity-based Sampling,cs.LG,Machine Learning,2024-10-05,"Domain shifts are ubiquitous in machine learning, and can substantially
degrade a model's performance when deployed to real-world data. To address
this, distribution alignment methods aim to learn feature representations which
are invariant across domains, by minimising the discrepancy between the
distributions. However, the discrepancy estimates can be extremely noisy when
training via stochastic gradient descent (SGD), and shifts in the relative
proportions of different subgroups can lead to domain misalignments; these can
both stifle the benefits of the method. This paper proposes to improve these
estimates by inducing diversity in each sampled minibatch. This simultaneously
balances the data and reduces the variance of the gradients, thereby enhancing
the model's generalisation ability. We describe two options for diversity-based
data samplers, based on the k-determinantal point process (k-DPP) and the
k-means++ algorithm, which can function as drop-in replacements for a standard
random sampler. On a real-world domain shift task of bioacoustic event
detection, we show that both options 1) yield minibatches which are more
representative of the full dataset; 2) reduce the distance estimation error
between distributions, for a given sample size; and 3) improve
out-of-distribution accuracy for two distribution alignment algorithms, as well
as standard ERM."
Functional Homotopy: Smoothing Discrete Optimization via Continuous Parameters for LLM Jailbreak Attacks,cs.LG,Machine Learning,2024-10-05,"Optimization methods are widely employed in deep learning to identify and
mitigate undesired model responses. While gradient-based techniques have proven
effective for image models, their application to language models is hindered by
the discrete nature of the input space. This study introduces a novel
optimization approach, termed the \emph{functional homotopy} method, which
leverages the functional duality between model training and input generation.
By constructing a series of easy-to-hard optimization problems, we iteratively
solve these problems using principles derived from established homotopy
methods. We apply this approach to jailbreak attack synthesis for large
language models (LLMs), achieving a $20\%-30\%$ improvement in success rate
over existing methods in circumventing established safe open-source models such
as Llama-2 and Llama-3."
Early-Cycle Internal Impedance Enables ML-Based Battery Cycle Life Predictions Across Manufacturers,cs.LG,Machine Learning,2024-10-05,"Predicting the end-of-life (EOL) of lithium-ion batteries across different
manufacturers presents significant challenges due to variations in electrode
materials, manufacturing processes, cell formats, and a lack of generally
available data. Methods that construct features solely on voltage-capacity
profile data typically fail to generalize across cell chemistries. This study
introduces a methodology that combines traditional voltage-capacity features
with Direct Current Internal Resistance (DCIR) measurements, enabling more
accurate and generalizable EOL predictions. The use of early-cycle DCIR data
captures critical degradation mechanisms related to internal resistance growth,
enhancing model robustness. Models are shown to successfully predict the number
of cycles to EOL for unseen manufacturers of varied electrode composition with
a mean absolute error (MAE) of 150 cycles. This cross-manufacturer
generalizability reduces the need for extensive new data collection and
retraining, enabling manufacturers to optimize new battery designs using
existing datasets. Additionally, a novel DCIR-compatible dataset is released as
part of ongoing efforts to enrich the growing ecosystem of cycling data and
accelerate battery materials development."
SGD with memory: fundamental properties and stochastic acceleration,cs.LG,Machine Learning,2024-10-05,"An important open problem is the theoretically feasible acceleration of
mini-batch SGD-type algorithms on quadratic problems with power-law spectrum.
In the non-stochastic setting, the optimal exponent $\xi$ in the loss
convergence $L_t\sim C_Lt^{-\xi}$ is double that in plain GD and is achievable
using Heavy Ball (HB) with a suitable schedule; this no longer works in the
presence of mini-batch noise. We address this challenge by considering
first-order methods with an arbitrary fixed number $M$ of auxiliary velocity
vectors (*memory-$M$ algorithms*). We first prove an equivalence between two
forms of such algorithms and describe them in terms of suitable characteristic
polynomials. Then we develop a general expansion of the loss in terms of signal
and noise propagators. Using it, we show that losses of stationary stable
memory-$M$ algorithms always retain the exponent $\xi$ of plain GD, but can
have different constants $C_L$ depending on their effective learning rate that
generalizes that of HB. We prove that in memory-1 algorithms we can make $C_L$
arbitrarily small while maintaining stability. As a consequence, we propose a
memory-1 algorithm with a time-dependent schedule that we show heuristically
and experimentally to improve the exponent $\xi$ of plain SGD."
Distillation-Free One-Step Diffusion for Real-World Image Super-Resolution,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Diffusion models have been achieving excellent performance for real-world
image super-resolution (Real-ISR) with considerable computational costs.
Current approaches are trying to derive one-step diffusion models from
multi-step counterparts through knowledge distillation. However, these methods
incur substantial training costs and may constrain the performance of the
student model by the teacher's limitations. To tackle these issues, we propose
DFOSD, a Distillation-Free One-Step Diffusion model. Specifically, we propose a
noise-aware discriminator (NAD) to participate in adversarial training, further
enhancing the authenticity of the generated content. Additionally, we improve
the perceptual loss with edge-aware DISTS (EA-DISTS) to enhance the model's
ability to generate fine details. Our experiments demonstrate that, compared
with previous diffusion-based methods requiring dozens or even hundreds of
steps, our DFOSD attains comparable or even superior results in both
quantitative metrics and qualitative evaluations. Our DFOSD also abtains higher
performance and efficiency compared with other one-step diffusion methods. We
will release code and models at \url{https://github.com/JianzeLi-114/DFOSD}."
Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning,cs.LG,Machine Learning,2024-10-05,"While large language models (LLMs) have integrated images, adapting them to
graphs remains challenging, limiting their applications in materials and drug
design. This difficulty stems from the need for coherent autoregressive
generation across texts and graphs. To address this, we introduce Llamole, the
first multimodal LLM capable of interleaved text and graph generation, enabling
molecular inverse design with retrosynthetic planning. Llamole integrates a
base LLM with the Graph Diffusion Transformer and Graph Neural Networks for
multi-conditional molecular generation and reaction inference within texts,
while the LLM, with enhanced molecular understanding, flexibly controls
activation among the different graph modules. Additionally, Llamole integrates
A* search with LLM-based cost functions for efficient retrosynthetic planning.
We create benchmarking datasets and conduct extensive experiments to evaluate
Llamole against in-context learning and supervised fine-tuning. Llamole
significantly outperforms 14 adapted LLMs across 12 metrics for controllable
molecular design and retrosynthetic planning."
TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"We present TANGO, a framework for generating co-speech body-gesture videos.
Given a few-minute, single-speaker reference video and target speech audio,
TANGO produces high-fidelity videos with synchronized body gestures. TANGO
builds on Gesture Video Reenactment (GVR), which splits and retrieves video
clips using a directed graph structure - representing video frames as nodes and
valid transitions as edges. We address two key limitations of GVR: audio-motion
misalignment and visual artifacts in GAN-generated transition frames. In
particular, (i) we propose retrieving gestures using latent feature distance to
improve cross-modal alignment. To ensure the latent features could effectively
model the relationship between speech audio and gesture motion, we implement a
hierarchical joint embedding space (AuMoCLIP); (ii) we introduce the
diffusion-based model to generate high-quality transition frames. Our diffusion
model, Appearance Consistent Interpolation (ACInterp), is built upon
AnimateAnyone and includes a reference motion module and homography background
flow to preserve appearance consistency between generated and reference videos.
By integrating these components into the graph-based retrieval framework, TANGO
reliably produces realistic, audio-synchronized videos and outperforms all
existing generative and retrieval methods. Our codes and pretrained models are
available: \url{https://pantomatrix.github.io/TANGO/}"
Improving Portfolio Optimization Results with Bandit Networks,cs.AI,Artificial Intelligence,2024-10-05,"In Reinforcement Learning (RL), multi-armed Bandit (MAB) problems have found
applications across diverse domains such as recommender systems, healthcare,
and finance. Traditional MAB algorithms typically assume stationary reward
distributions, which limits their effectiveness in real-world scenarios
characterized by non-stationary dynamics. This paper addresses this limitation
by introducing and evaluating novel Bandit algorithms designed for
non-stationary environments. First, we present the Adaptive Discounted Thompson
Sampling (ADTS) algorithm, which enhances adaptability through relaxed
discounting and sliding window mechanisms to better respond to changes in
reward distributions. We then extend this approach to the Portfolio
Optimization problem by introducing the Combinatorial Adaptive Discounted
Thompson Sampling (CADTS) algorithm, which addresses computational challenges
within Combinatorial Bandits and improves dynamic asset allocation.
Additionally, we propose a novel architecture called Bandit Networks, which
integrates the outputs of ADTS and CADTS, thereby mitigating computational
limitations in stock selection. Through extensive experiments using real
financial market data, we demonstrate the potential of these algorithms and
architectures in adapting to dynamic environments and optimizing
decision-making processes. For instance, the proposed bandit network instances
present superior performance when compared to classic portfolio optimization
approaches, such as capital asset pricing model, equal weights, risk parity,
and Markovitz, with the best network presenting an out-of-sample Sharpe Ratio
20% higher than the best performing classical model."
A class of ternary codes with few weights,cs.CR,Cryptography and Security,2024-10-05,"Let $\ell^m$ be a power with $\ell$ a prime greater than $3$ and $m$ a
positive integer such that $3$ is a primitive root modulo $2\ell^m$. Let
$\mathbb{F}_3$ be the finite field of order $3$, and let $\mathbb{F}$ be the
$\ell^{m-1}(\ell-1)$-th extension field of $\mathbb{F}_3$. Denote by
$\text{Tr}$ the absolute trace map from $\mathbb{F}$ to $\mathbb{F}_3$. For any
$\alpha \in \mathbb{F}_3$ and $\beta \in\mathbb{F}$, let $D$ be the set of
nonzero solutions in $\mathbb{F}$ to the equation
$\text{Tr}(x^{\frac{q-1}{2\ell^m}} + \beta x) = \alpha$. In this paper, we
investigate a ternary code $\mathcal{C}$ of length $n$, defined by $\mathcal{C}
:= \{(\text{Tr}(d_1x), \text{Tr}(d_2x), \dots, \text{Tr}(d_nx)) : x \in
\mathbb{F}\}$ when we rewrite $D = \{d_1, d_2, \dots, d_n\}$. Using recent
results on explicit evaluations of exponential sums, the Weil bound, and
combinatorial techniques, we determine the Hamming weight distribution of the
code $\mathcal{C}$. Furthermore, we show that when $\alpha = \beta =0$, the
dual code of $\mathcal{C}$ is optimal with respect to the Hamming bound."
Equivariant Polynomial Functional Networks,cs.LG,Machine Learning,2024-10-05,"Neural Functional Networks (NFNs) have gained increasing interest due to
their wide range of applications, including extracting information from
implicit representations of data, editing network weights, and evaluating
policies. A key design principle of NFNs is their adherence to the permutation
and scaling symmetries inherent in the connectionist structure of the input
neural networks. Recent NFNs have been proposed with permutation and scaling
equivariance based on either graph-based message-passing mechanisms or
parameter-sharing mechanisms. However, graph-based equivariant NFNs suffer from
high memory consumption and long running times. On the other hand,
parameter-sharing-based NFNs built upon equivariant linear layers exhibit lower
memory consumption and faster running time, yet their expressivity is limited
due to the large size of the symmetric group of the input neural networks. The
challenge of designing a permutation and scaling equivariant NFN that maintains
low memory consumption and running time while preserving expressivity remains
unresolved. In this paper, we propose a novel solution with the development of
MAGEP-NFN (Monomial mAtrix Group Equivariant Polynomial NFN). Our approach
follows the parameter-sharing mechanism but differs from previous works by
constructing a nonlinear equivariant layer represented as a polynomial in the
input weights. This polynomial formulation enables us to incorporate additional
relationships between weights from different input hidden layers, enhancing the
model's expressivity while keeping memory consumption and running time low,
thereby addressing the aforementioned challenge. We provide empirical evidence
demonstrating that MAGEP-NFN achieves competitive performance and efficiency
compared to existing baselines."
Correlation-Aware Select and Merge Attention for Efficient Fine-Tuning and Context Length Extension,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Modeling long sequences is crucial for various large-scale models; however,
extending existing architectures to handle longer sequences presents
significant technical and resource challenges. In this paper, we propose an
efficient and flexible attention architecture that enables the extension of
context lengths in large language models with reduced computational resources
and fine-tuning time compared to other excellent methods. Specifically, we
introduce correlation-aware selection and merging mechanisms to facilitate
efficient sparse attention. In addition, we also propose a novel data
augmentation technique involving positional encodings to enhance generalization
to unseen positions. The results are as follows: First, using a single A100, we
achieve fine-tuning on Llama2-7B with a sequence length of 32K, which is more
efficient than other methods that rely on subsets for regression. Second, we
present a comprehensive method for extending context lengths across the
pre-training, fine-tuning, and inference phases. During pre-training, our
attention mechanism partially breaks translation invariance during token
selection, so we apply positional encodings only to the selected tokens. This
approach achieves relatively high performance and significant extrapolation
capabilities. For fine-tuning, we introduce Cyclic, Randomly Truncated, and
Dynamically Growing NTK Positional Embedding (CRD NTK). This design allows
fine-tuning with a sequence length of only 16K, enabling models such as
Llama2-7B and Mistral-7B to perform inference with context lengths of up to 1M
or even arbitrary lengths. Our method achieves 100\% accuracy on the passkey
task with a context length of 4M and maintains stable perplexity at a 1M
context length. This represents at least a 64-fold reduction in resource
requirements compared to traditional full-attention mechanisms, while still
achieving competitive performance."
Equivariant Neural Functional Networks for Transformers,cs.LG,Machine Learning,2024-10-05,"This paper systematically explores neural functional networks (NFN) for
transformer architectures. NFN are specialized neural networks that treat the
weights, gradients, or sparsity patterns of a deep neural network (DNN) as
input data and have proven valuable for tasks such as learnable optimizers,
implicit data representations, and weight editing. While NFN have been
extensively developed for MLP and CNN, no prior work has addressed their design
for transformers, despite the importance of transformers in modern deep
learning. This paper aims to address this gap by providing a systematic study
of NFN for transformers. We first determine the maximal symmetric group of the
weights in a multi-head attention module as well as a necessary and sufficient
condition under which two sets of hyperparameters of the multi-head attention
module define the same function. We then define the weight space of transformer
architectures and its associated group action, which leads to the design
principles for NFN in transformers. Based on these, we introduce
Transformer-NFN, an NFN that is equivariant under this group action.
Additionally, we release a dataset of more than 125,000 Transformers model
checkpoints trained on two datasets with two different tasks, providing a
benchmark for evaluating Transformer-NFN and encouraging further research on
transformer training and performance."
Learning on LoRAs: GL-Equivariant Processing of Low-Rank Weight Spaces for Large Finetuned Models,cs.LG,Machine Learning,2024-10-05,"Low-rank adaptations (LoRAs) have revolutionized the finetuning of large
foundation models, enabling efficient adaptation even with limited
computational resources. The resulting proliferation of LoRAs presents exciting
opportunities for applying machine learning techniques that take these low-rank
weights themselves as inputs. In this paper, we investigate the potential of
Learning on LoRAs (LoL), a paradigm where LoRA weights serve as input to
machine learning models. For instance, an LoL model that takes in LoRA weights
as inputs could predict the performance of the finetuned model on downstream
tasks, detect potentially harmful finetunes, or even generate novel model edits
without traditional training methods. We first identify the inherent parameter
symmetries of low rank decompositions of weights, which differ significantly
from the parameter symmetries of standard neural networks. To efficiently
process LoRA weights, we develop several symmetry-aware invariant or
equivariant LoL models, using tools such as canonicalization, invariant
featurization, and equivariant layers. We finetune thousands of text-to-image
diffusion models and language models to collect datasets of LoRAs. In numerical
experiments on these datasets, we show that our LoL architectures are capable
of processing low rank weight decompositions to predict CLIP score, finetuning
data attributes, finetuning data membership, and accuracy on downstream tasks."
Exploring Strengths and Weaknesses of Super-Resolution Attack in Deepfake Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Image manipulation is rapidly evolving, allowing the creation of credible
content that can be used to bend reality. Although the results of deepfake
detectors are promising, deepfakes can be made even more complicated to detect
through adversarial attacks. They aim to further manipulate the image to
camouflage deepfakes' artifacts or to insert signals making the image appear
pristine. In this paper, we further explore the potential of super-resolution
attacks based on different super-resolution techniques and with different
scales that can impact the performance of deepfake detectors with more or less
intensity. We also evaluated the impact of the attack on more diverse datasets
discovering that the super-resolution process is effective in hiding the
artifacts introduced by deepfake generation models but fails in hiding the
traces contained in fully synthetic images. Finally, we propose some changes to
the detectors' training process to improve their robustness to this kind of
attack."
RainbowPO: A Unified Framework for Combining Improvements in Preference Optimization,cs.AI,Artificial Intelligence,2024-10-05,"Recently, numerous preference optimization algorithms have been introduced as
extensions to the Direct Preference Optimization (DPO) family. While these
methods have successfully aligned models with human preferences, there is a
lack of understanding regarding the contributions of their additional
components. Moreover, fair and consistent comparisons are scarce, making it
difficult to discern which components genuinely enhance downstream performance.
In this work, we propose RainbowPO, a unified framework that demystifies the
effectiveness of existing DPO methods by categorizing their key components into
seven broad directions. We integrate these components into a single cohesive
objective, enhancing the performance of each individual element. Through
extensive experiments, we demonstrate that RainbowPO outperforms existing DPO
variants. Additionally, we provide insights to guide researchers in developing
new DPO methods and assist practitioners in their implementations."
Deep Transfer Learning Based Peer Review Aggregation and Meta-review Generation for Scientific Articles,cs.LG,Machine Learning,2024-10-05,"Peer review is the quality assessment of a manuscript by one or more peer
experts. Papers are submitted by the authors to scientific venues, and these
papers must be reviewed by peers or other authors. The meta-reviewers then
gather the peer reviews, assess them, and create a meta-review and decision for
each manuscript. As the number of papers submitted to these venues has grown in
recent years, it becomes increasingly challenging for meta-reviewers to collect
these peer evaluations on time while still maintaining the quality that is the
primary goal of meta-review creation. In this paper, we address two peer review
aggregation challenges a meta-reviewer faces: paper acceptance decision-making
and meta-review generation. Firstly, we propose to automate the process of
acceptance decision prediction by applying traditional machine learning
algorithms. We use pre-trained word embedding techniques BERT to process the
reviews written in natural language text. For the meta-review generation, we
propose a transfer learning model based on the T5 model. Experimental results
show that BERT is more effective than the other word embedding techniques, and
the recommendation score is an important feature for the acceptance decision
prediction. In addition, we figure out that fine-tuned T5 outperforms other
inference models. Our proposed system takes peer reviews and other relevant
features as input to produce a meta-review and make a judgment on whether or
not the paper should be accepted. In addition, experimental results show that
the acceptance decision prediction system of our task outperforms the existing
models, and the meta-review generation task shows significantly improved scores
compared to the existing models. For the statistical test, we utilize the
Wilcoxon signed-rank test to assess whether there is a statistically
significant improvement between paired observations."
IT$^3$: Idempotent Test-Time Training,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"This paper introduces Idempotent Test-Time Training (IT$^3$), a novel
approach to addressing the challenge of distribution shift. While
supervised-learning methods assume matching train and test distributions, this
is rarely the case for machine learning systems deployed in the real world.
Test-Time Training (TTT) approaches address this by adapting models during
inference, but they are limited by a domain specific auxiliary task. IT$^3$ is
based on the universal property of idempotence. An idempotent operator is one
that can be applied sequentially without changing the result beyond the initial
application, that is $f(f(x))=f(x)$. At training, the model receives an input
$x$ along with another signal that can either be the ground truth label $y$ or
a neutral ""don't know"" signal $0$. At test time, the additional signal can only
be $0$. When sequentially applying the model, first predicting $y_0 = f(x, 0)$
and then $y_1 = f(x, y_0)$, the distance between $y_0$ and $y_1$ measures
certainty and indicates out-of-distribution input $x$ if high. We use this
distance, that can be expressed as $||f(x, f(x, 0)) - f(x, 0)||$ as our TTT
loss during inference. By carefully optimizing this objective, we effectively
train $f(x,\cdot)$ to be idempotent, projecting the internal representation of
the input onto the training distribution. We demonstrate the versatility of our
approach across various tasks, including corrupted image classification,
aerodynamic predictions, tabular data with missing information, age prediction
from face, and large-scale aerial photo segmentation. Moreover, these tasks
span different architectures such as MLPs, CNNs, and GNNs."
LongGenBench: Long-context Generation Benchmark,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Current long-context benchmarks primarily focus on retrieval-based tests,
requiring Large Language Models (LLMs) to locate specific information within
extensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark.
Long-context generation refers to the ability of a language model to generate
coherent and contextually accurate text that spans across lengthy passages or
documents. While recent studies show strong performance on NIAH and other
retrieval-based long-context benchmarks, there is a significant lack of
benchmarks for evaluating long-context generation capabilities. To bridge this
gap and offer a comprehensive assessment, we introduce a synthetic benchmark,
LongGenBench, which allows for flexible configurations of customized generation
context lengths. LongGenBench advances beyond traditional benchmarks by
redesigning the format of questions and necessitating that LLMs respond with a
single, cohesive long-context answer. Upon extensive evaluation using
LongGenBench, we observe that: (1) both API accessed and open source models
exhibit performance degradation in long-context generation scenarios, ranging
from 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of
performance degradation, with the Gemini-1.5-Flash model showing the least
degradation among API accessed models, and the Qwen2 series exhibiting the
least degradation in LongGenBench among open source models."
CS4: Measuring the Creativity of Large Language Models Automatically by Controlling the Number of Story-Writing Constraints,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Evaluating the creativity of large language models (LLMs) in story writing is
difficult because LLM-generated stories could seemingly look creative but be
very similar to some existing stories in their huge and proprietary training
corpus. To overcome this challenge, we introduce a novel benchmark dataset with
varying levels of prompt specificity: CS4 ($\mathbf{C}$omparing the
$\mathbf{S}$kill of $\mathbf{C}$reating $\mathbf{S}$tories by
$\mathbf{C}$ontrolling the $\mathbf{S}$ynthesized $\mathbf{C}$onstraint
$\mathbf{S}$pecificity). By increasing the number of requirements/constraints
in the prompt, we can increase the prompt specificity and hinder LLMs from
retelling high-quality narratives in their training data. Consequently, CS4
empowers us to indirectly measure the LLMs' creativity without human
annotations.
  Our experiments on LLaMA, Gemma, and Mistral not only highlight the
creativity challenges LLMs face when dealing with highly specific prompts but
also reveal that different LLMs perform very differently under different
numbers of constraints and achieve different balances between the model's
instruction-following ability and narrative coherence. Additionally, our
experiments on OLMo suggest that Learning from Human Feedback (LHF) can help
LLMs select better stories from their training data but has limited influence
in boosting LLMs' ability to produce creative stories that are unseen in the
training corpora. The benchmark is released at
https://github.com/anirudhlakkaraju/cs4_benchmark."
Improving Generalization with Flat Hilbert Bayesian Inference,cs.LG,Machine Learning,2024-10-05,"We introduce Flat Hilbert Bayesian Inference (FHBI), an algorithm designed to
enhance generalization in Bayesian inference. Our approach involves an
iterative two-step procedure with an adversarial functional perturbation step
and a functional descent step within the reproducing kernel Hilbert spaces.
This methodology is supported by a theoretical analysis that extends previous
findings on generalization ability from finite-dimensional Euclidean spaces to
infinite-dimensional functional spaces. To evaluate the effectiveness of FHBI,
we conduct comprehensive comparisons against seven baseline methods on the
VTAB-1K benchmark, which encompasses 19 diverse datasets across various domains
with diverse semantics. Empirical results demonstrate that FHBI consistently
outperforms the baselines by notable margins, highlighting its practical
efficacy."
Consistent Autoformalization for Constructing Mathematical Libraries,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Autoformalization is the task of automatically translating mathematical
content written in natural language to a formal language expression. The
growing language interpretation capabilities of Large Language Models (LLMs),
including in formal languages, are lowering the barriers for autoformalization.
However, LLMs alone are not capable of consistently and reliably delivering
autoformalization, in particular as the complexity and specialization of the
target domain grows. As the field evolves into the direction of systematically
applying autoformalization towards large mathematical libraries, the need to
improve syntactic, terminological and semantic control increases. This paper
proposes the coordinated use of three mechanisms, most-similar retrieval
augmented generation (MS-RAG), denoising steps, and auto-correction with syntax
error feedback (Auto-SEF) to improve autoformalization quality. The empirical
analysis, across different models, demonstrates that these mechanisms can
deliver autoformalizaton results which are syntactically, terminologically and
semantically more consistent. These mechanisms can be applied across different
LLMs and have shown to deliver improve results across different model types."
Parametric Taylor series based latent dynamics identification neural networks,cs.LG,Machine Learning,2024-10-05,"Numerical solving parameterised partial differential equations (P-PDEs) is
highly practical yet computationally expensive, driving the development of
reduced-order models (ROMs). Recently, methods that combine latent space
identification techniques with deep learning algorithms (e.g., autoencoders)
have shown great potential in describing the dynamical system in the lower
dimensional latent space, for example, LaSDI, gLaSDI and GPLaSDI.
  In this paper, a new parametric latent identification of nonlinear dynamics
neural networks, P-TLDINets, is introduced, which relies on a novel neural
network structure based on Taylor series expansion and ResNets to learn the
ODEs that govern the reduced space dynamics. During the training process,
Taylor series-based Latent Dynamic Neural Networks (TLDNets) and identified
equations are trained simultaneously to generate a smoother latent space. In
order to facilitate the parameterised study, a $k$-nearest neighbours (KNN)
method based on an inverse distance weighting (IDW) interpolation scheme is
introduced to predict the identified ODE coefficients using local information.
Compared to other latent dynamics identification methods based on autoencoders,
P-TLDINets remain the interpretability of the model. Additionally, it
circumvents the building of explicit autoencoders, avoids dependency on
specific grids, and features a more lightweight structure, which is easy to
train with high generalisation capability and accuracy. Also, it is capable of
using different scales of meshes. P-TLDINets improve training speeds nearly
hundred times compared to GPLaSDI and gLaSDI, maintaining an $L_2$ error below
$2\%$ compared to high-fidelity models."
Accelerating Diffusion Models with One-to-Many Knowledge Distillation,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Significant advancements in image generation have been made with diffusion
models. Nevertheless, when contrasted with previous generative models,
diffusion models face substantial computational overhead, leading to failure in
real-time generation. Recent approaches have aimed to accelerate diffusion
models by reducing the number of sampling steps through improved sampling
techniques or step distillation. However, the methods to diminish the
computational cost for each timestep remain a relatively unexplored area.
Observing the fact that diffusion models exhibit varying input distributions
and feature distributions at different timesteps, we introduce one-to-many
knowledge distillation (O2MKD), which distills a single teacher diffusion model
into multiple student diffusion models, where each student diffusion model is
trained to learn the teacher's knowledge for a subset of continuous timesteps.
Experiments on CIFAR10, LSUN Church, CelebA-HQ with DDPM and COCO30K with
Stable Diffusion show that O2MKD can be applied to previous knowledge
distillation and fast sampling methods to achieve significant acceleration.
Codes will be released in Github."
Harnessing Task Overload for Scalable Jailbreak Attacks on Large Language Models,cs.CR,Cryptography and Security,2024-10-05,"Large Language Models (LLMs) remain vulnerable to jailbreak attacks that
bypass their safety mechanisms. Existing attack methods are fixed or
specifically tailored for certain models and cannot flexibly adjust attack
strength, which is critical for generalization when attacking models of various
sizes. We introduce a novel scalable jailbreak attack that preempts the
activation of an LLM's safety policies by occupying its computational
resources. Our method involves engaging the LLM in a resource-intensive
preliminary task - a Character Map lookup and decoding process - before
presenting the target instruction. By saturating the model's processing
capacity, we prevent the activation of safety protocols when processing the
subsequent instruction. Extensive experiments on state-of-the-art LLMs
demonstrate that our method achieves a high success rate in bypassing safety
measures without requiring gradient access, manual prompt engineering. We
verified our approach offers a scalable attack that quantifies attack strength
and adapts to different model scales at the optimal strength. We shows safety
policies of LLMs might be more susceptible to resource constraints. Our
findings reveal a critical vulnerability in current LLM safety designs,
highlighting the need for more robust defense strategies that account for
resource-intense condition."
DiDOTS: Knowledge Distillation from Large-Language-Models for Dementia Obfuscation in Transcribed Speech,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Dementia is a sensitive neurocognitive disorder affecting tens of millions of
people worldwide and its cases are expected to triple by 2050. Alarmingly,
recent advancements in dementia classification make it possible for adversaries
to violate affected individuals' privacy and infer their sensitive condition
from speech transcriptions. Existing obfuscation methods in text have never
been applied for dementia and depend on the availability of large labeled
datasets which are challenging to collect for sensitive medical attributes. In
this work, we bridge this research gap and tackle the above issues by
leveraging Large-Language-Models (LLMs) with diverse prompt designs (zero-shot,
few-shot, and knowledge-based) to obfuscate dementia in speech transcripts. Our
evaluation shows that LLMs are more effective dementia obfuscators compared to
competing methods. However, they have billions of parameters which renders them
hard to train, store and share, and they are also fragile suffering from
hallucination, refusal and contradiction effects among others. To further
mitigate these, we propose a novel method, DiDOTS. DiDOTS distills knowledge
from LLMs using a teacher-student paradigm and parameter-efficient fine-tuning.
DiDOTS has one order of magnitude fewer parameters compared to its teacher LLM
and can be fine-tuned using three orders of magnitude less parameters compared
to full fine-tuning. Our evaluation shows that compared to prior work DiDOTS
retains the performance of LLMs achieving 1.3x and 2.2x improvement in privacy
performance on two datasets, while humans rate it as better in preserving
utility even when compared to state-of-the-art paraphrasing models."
Unsupervised Assessment of Landscape Shifts Based on Persistent Entropy and Topological Preservation,cs.LG,Machine Learning,2024-10-05,"Concept drift typically refers to the analysis of changes in data
distribution. A drift in the input data can have negative consequences on a
learning predictor and the system's stability. The majority of concept drift
methods emphasize the analysis of statistical changes in non-stationary data
over time. In this context, we consider another perspective, where the concept
drift also integrates substantial changes in the topological characteristics of
the data stream. In this article, we introduce a novel framework for monitoring
changes in multi-dimensional data streams. We explore a generalization of the
standard concept drift focusing on the changes in the topological
characteristics of the data. Our developed approach is based on persistent
entropy and topology-preserving projections in a continual learning scenario.
The framework operates in both unsupervised and supervised environments. To
demonstrate the utility of the proposed framework, we analyze the model across
three scenarios using data streams generated with MNIST samples. The obtained
results reveal the potential of applying topological data analysis for shift
detection and encourage further research in this area."
Artistic Portrait Drawing with Vector Strokes,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"In this paper, we present a method, VectorPD, for converting a given human
face image into a vector portrait sketch. VectorPD supports different levels of
abstraction by simply controlling the number of strokes. Since vector graphics
are composed of different shape primitives, it is challenging for rendering
complex faces to accurately express facial details and structure. To address
this, VectorPD employs a novel two-round optimization mechanism. We first
initialize the strokes with facial keypoints, and generate a basic portrait
sketch by a CLIP-based Semantic Loss. Then we complete the face structure
through VGG-based Structure Loss, and propose a novel Crop-based Shadow Loss to
enrich the shadow details of the sketch, achieving a visually pleasing portrait
sketch. Quantitative and qualitative evaluations both demonstrate that the
portrait sketches generated by VectorPD can produce better visual effects than
existing state-of-the-art methods, maintaining as much fidelity as possible at
different levels of abstraction."
Fast Object Detection with a Machine Learning Edge Device,cs.RO,Robotics,2024-10-05,"This machine learning study investigates a lowcost edge device integrated
with an embedded system having computer vision and resulting in an improved
performance in inferencing time and precision of object detection and
classification. A primary aim of this study focused on reducing inferencing
time and low-power consumption and to enable an embedded device of a
competition-ready autonomous humanoid robot and to support real-time object
recognition, scene understanding, visual navigation, motion planning, and
autonomous navigation of the robot. This study compares processors for
inferencing time performance between a central processing unit (CPU), a
graphical processing unit (GPU), and a tensor processing unit (TPU). CPUs,
GPUs, and TPUs are all processors that can be used for machine learning tasks.
Related to the aim of supporting an autonomous humanoid robot, there was an
additional effort to observe whether or not there was a significant difference
in using a camera having monocular vision versus stereo vision capability. TPU
inference time results for this study reflect a 25% reduction in time over the
GPU, and a whopping 87.5% reduction in inference time compared to the CPU. Much
information in this paper is contributed to the final selection of Google's
Coral brand, Edge TPU device. The Arduino Nano 33 BLE Sense Tiny ML Kit was
also considered for comparison but due to initial incompatibilities and in the
interest of time to complete this study, a decision was made to review the kit
in a future experiment."
IV-Mixed Sampler: Leveraging Image Diffusion Models for Enhanced Video Synthesis,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"The multi-step sampling mechanism, a key feature of visual diffusion models,
has significant potential to replicate the success of OpenAI's Strawberry in
enhancing performance by increasing the inference computational cost.
Sufficient prior studies have demonstrated that correctly scaling up
computation in the sampling process can successfully lead to improved
generation quality, enhanced image editing, and compositional generalization.
While there have been rapid advancements in developing inference-heavy
algorithms for improved image generation, relatively little work has explored
inference scaling laws in video diffusion models (VDMs). Furthermore, existing
research shows only minimal performance gains that are perceptible to the naked
eye. To address this, we design a novel training-free algorithm IV-Mixed
Sampler that leverages the strengths of image diffusion models (IDMs) to assist
VDMs surpass their current capabilities. The core of IV-Mixed Sampler is to use
IDMs to significantly enhance the quality of each video frame and VDMs ensure
the temporal coherence of the video during the sampling process. Our
experiments have demonstrated that IV-Mixed Sampler achieves state-of-the-art
performance on 4 benchmarks including UCF-101-FVD, MSR-VTT-FVD,
Chronomagic-Bench-150, and Chronomagic-Bench-1649. For example, the open-source
Animatediff with IV-Mixed Sampler reduces the UMT-FVD score from 275.2 to
228.6, closing to 223.1 from the closed-source Pika-2.0."
Preference Optimization as Probabilistic Inference,cs.LG,Machine Learning,2024-10-05,"Existing preference optimization methods are mainly designed for directly
learning from human feedback with the assumption that paired examples
(preferred vs. dis-preferred) are available. In contrast, we propose a method
that can leverage unpaired preferred or dis-preferred examples, and works even
when only one type of feedback (positive or negative) is available. This
flexibility allows us to apply it in scenarios with varying forms of feedback
and models, including training generative language models based on human
feedback as well as training policies for sequential decision-making problems,
where learned (value) functions are available. Our approach builds upon the
probabilistic framework introduced in (Dayan and Hinton, 1997), which proposes
to use expectation-maximization (EM) to directly optimize the probability of
preferred outcomes (as opposed to classic expected reward maximization). To
obtain a practical algorithm, we identify and address a key limitation in
current EM-based methods: when applied to preference optimization, they solely
maximize the likelihood of preferred examples, while neglecting dis-preferred
samples. We show how one can extend EM algorithms to explicitly incorporate
dis-preferred outcomes, leading to a novel, theoretically grounded, preference
optimization algorithm that offers an intuitive and versatile way to learn from
both positive and negative feedback."
Towards Effective Counter-Responses: Aligning Human Preferences with Strategies to Combat Online Trolling,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Trolling in online communities typically involves disruptive behaviors such
as provoking anger and manipulating discussions, leading to a polarized
atmosphere and emotional distress. Robust moderation is essential for
mitigating these negative impacts and maintaining a healthy and constructive
community atmosphere. However, effectively addressing trolls is difficult
because their behaviors vary widely and require different response strategies
(RSs) to counter them. This diversity makes it challenging to choose an
appropriate RS for each specific situation. To address this challenge, our
research investigates whether humans have preferred strategies tailored to
different types of trolling behaviors. Our findings reveal a correlation
between the types of trolling encountered and the preferred RS. In this paper,
we introduce a methodology for generating counter-responses to trolls by
recommending appropriate RSs, supported by a dataset aligning these strategies
with human preferences across various troll contexts. The experimental results
demonstrate that our proposed approach guides constructive discussion and
reduces the negative effects of trolls, thereby enhancing the online community
environment."
Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"We introduce a novel Multi-modal Guided Real-World Face Restoration (MGFR)
technique designed to improve the quality of facial image restoration from
low-quality inputs. Leveraging a blend of attribute text prompts, high-quality
reference images, and identity information, MGFR can mitigate the generation of
false facial attributes and identities often associated with generative face
restoration methods. By incorporating a dual-control adapter and a two-stage
training strategy, our method effectively utilizes multi-modal prior
information for targeted restoration tasks. We also present the Reface-HQ
dataset, comprising over 23,000 high-resolution facial images across 5,000
identities, to address the need for reference face training images. Our
approach achieves superior visual quality in restoring facial details under
severe degradation and allows for controlled restoration processes, enhancing
the accuracy of identity preservation and attribute correction. Including
negative quality samples and attribute prompts in the training further refines
the model's ability to generate detailed and perceptually accurate images."
Toxic Subword Pruning for Dialogue Response Generation on Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"How to defend large language models (LLMs) from generating toxic content is
an important research area. Yet, most research focused on various model
training techniques to remediate LLMs by updating their weights. A typical
related research area is safety alignment. This however is often costly and
tedious and can expose the model to even more problems such as catastrophic
forgetting if the trainings are not carefully handled by experienced NLP
practitioners. We thus propose a simple yet effective and novel algorithm,
namely \textbf{Tox}ic Subword \textbf{Prun}ing (ToxPrune) to prune the subword
contained by the toxic words from BPE in trained LLMs. In contrast to the
previous work that demonstrates pruning BPE tokens as harmful to the task of
machine translation, we surprisingly found its usefulness in preventing toxic
content from being generated on LLMs. Fortunately, our findings suggest that
ToxPrune simultaneously improves the toxic language model NSFW-3B on the task
of dialogue response generation obviously. We surprisingly found that ToxPrune
can even obviously improve official Llama-3.1-6B in the metric of dialogue
diversity. Extensive automatic results and human evaluation indicate that
ToxPrune could be helpful for both remediating toxic LLMs and improving
non-toxic LLMs on the task of dialogue response generation.\footnote{We plan to
release the resources to facilitate future work.}"
Applying Quantum Autoencoders for Time Series Anomaly Detection,cs.LG,Machine Learning,2024-10-05,"Anomaly detection is an important problem with applications in various
domains such as fraud detection, pattern recognition or medical diagnosis.
Several algorithms have been introduced using classical computing approaches.
However, using quantum computing for solving anomaly detection problems in time
series data is a widely unexplored research field.
  This paper explores the application of quantum autoencoders to time series
anomaly detection. We investigate two primary techniques for classifying
anomalies: (1) Analyzing the reconstruction error generated by the quantum
autoencoder and (2) latent representation analysis. Our simulated experimental
results, conducted across various ansaetze, demonstrate that quantum
autoencoders consistently outperform classical deep learning-based autoencoders
across multiple datasets. Specifically, quantum autoencoders achieve superior
anomaly detection performance while utilizing 60-230 times fewer parameters and
requiring five times fewer training iterations. In addition, we implement our
quantum encoder on real quantum hardware. Our experimental results demonstrate
that quantum autoencoders achieve anomaly detection performance on par with
their simulated counterparts."
Neuro-Symbolic Entity Alignment via Variational Inference,cs.AI,Artificial Intelligence,2024-10-05,"Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying
equivalent entity pairs. Existing methods can be categorized into symbolic and
neural models. Symbolic models, while precise, struggle with substructure
heterogeneity and sparsity, whereas neural models, although effective,
generally lack interpretability and cannot handle uncertainty. We propose
NeuSymEA, a probabilistic neuro-symbolic framework that combines the strengths
of both methods. NeuSymEA models the joint probability of all possible pairs'
truth scores in a Markov random field, regulated by a set of rules, and
optimizes it with the variational EM algorithm. In the E-step, a neural model
parameterizes the truth score distributions and infers missing alignments. In
the M-step, the rule weights are updated based on the observed and inferred
alignments. To facilitate interpretability, we further design a
path-ranking-based explainer upon this framework that generates supporting
rules for the inferred alignments. Experiments on benchmarks demonstrate that
NeuSymEA not only significantly outperforms baselines in terms of effectiveness
and robustness, but also provides interpretable results."
DAMMI:Daily Activities in a Psychologically Annotated Multi-Modal IoT dataset,cs.AI,Artificial Intelligence,2024-10-05,"The growth in the elderly population and the shift in the age pyramid have
increased the demand for healthcare and well-being services. To address this
concern, alongside the rising cost of medical care, the concept of ageing at
home has emerged, driven by recent advances in medical and technological
solutions. Experts in computer science, communication technology, and
healthcare have collaborated to develop affordable health solutions by
employing sensors in living environments, wearable devices, and smartphones, in
association with advanced data mining and intelligent systems with learning
capabilities, to monitor, analyze, and predict the health status of elderly
individuals. However, implementing intelligent healthcare systems and
developing analytical techniques requires testing and evaluating algorithms on
real-world data. Despite the need, there is a shortage of publicly available
datasets that meet these requirements. To address this gap, we present the
DAMMI dataset in this work, designed to support researchers in the field. The
dataset includes daily activity data of an elderly individual collected via
home-installed sensors, smartphone data, and a wristband over 146 days. It also
contains daily psychological reports provided by a team of psychologists.
Furthermore, the data collection spans significant events such as the COVID-19
pandemic, New Year's holidays, and the religious month of Ramadan, offering
additional opportunities for analysis. In this paper, we outline detailed
information about the data collection system, the types of data recorded, and
pre-processed event logs. This dataset is intended to assist professionals in
IoT and data mining in evaluating and implementing their research ideas."
From Incomplete Coarse-Grained to Complete Fine-Grained: A Two-Stage Framework for Spatiotemporal Data Reconstruction,cs.LG,Machine Learning,2024-10-05,"With the rapid development of various sensing devices, spatiotemporal data is
becoming increasingly important nowadays. However, due to sensing costs and
privacy concerns, the collected data is often incomplete and coarse-grained,
limiting its application to specific tasks. To address this, we propose a new
task called spatiotemporal data reconstruction, which aims to infer complete
and fine-grained data from sparse and coarse-grained observations. To achieve
this, we introduce a two-stage data inference framework, DiffRecon, grounded in
the Denoising Diffusion Probabilistic Model (DDPM). In the first stage, we
present Diffusion-C, a diffusion model augmented by ST-PointFormer, a powerful
encoder designed to leverage the spatial correlations between sparse data
points. Following this, the second stage introduces Diffusion-F, which
incorporates the proposed T-PatternNet to capture the temporal pattern within
sequential data. Together, these two stages form an end-to-end framework
capable of inferring complete, fine-grained data from incomplete and
coarse-grained observations. We conducted experiments on multiple real-world
datasets to demonstrate the superiority of our method."
Reasoning with Natural Language Explanations,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Explanation constitutes an archetypal feature of human rationality,
underpinning learning and generalisation, and representing one of the media
supporting scientific discovery and communication. Due to the importance of
explanations in human reasoning, an increasing amount of research in Natural
Language Inference (NLI) has started reconsidering the role that explanations
play in learning and inference, attempting to build explanation-based NLI
models that can effectively encode and use natural language explanations on
downstream tasks. Research in explanation-based NLI, however, presents specific
challenges and opportunities, as explanatory reasoning reflects aspects of both
material and formal inference, making it a particularly rich setting to model
and deliver complex reasoning. In this tutorial, we provide a comprehensive
introduction to the field of explanation-based NLI, grounding this discussion
on the epistemological-linguistic foundations of explanations, systematically
describing the main architectural trends and evaluation methodologies that can
be used to build systems capable of explanatory reasoning."
Noise Crystallization and Liquid Noise: Zero-shot Video Generation using Image Diffusion Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Although powerful for image generation, consistent and controllable video is
a longstanding problem for diffusion models. Video models require extensive
training and computational resources, leading to high costs and large
environmental impacts. Moreover, video models currently offer limited control
of the output motion. This paper introduces a novel approach to video
generation by augmenting image diffusion models to create sequential animation
frames while maintaining fine detail. These techniques can be applied to
existing image models without training any video parameters (zero-shot) by
altering the input noise in a latent diffusion model. Two complementary methods
are presented. Noise crystallization ensures consistency but is limited to
large movements due to reduced latent embedding sizes. Liquid noise trades
consistency for greater flexibility without resolution limitations. The core
concepts also allow other applications such as relighting, seamless upscaling,
and improved video style transfer. Furthermore, an exploration of the VAE
embedding used for latent diffusion models is performed, resulting in
interesting theoretical insights such as a method for human-interpretable
latent spaces."
Can the Variation of Model Weights be used as a Criterion for Self-Paced Multilingual NMT?,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Many-to-one neural machine translation systems improve over one-to-one
systems when training data is scarce. In this paper, we design and test a novel
algorithm for selecting the language of minibatches when training such systems.
The algorithm changes the language of the minibatch when the weights of the
model do not evolve significantly, as measured by the smoothed KL divergence
between all layers of the Transformer network. This algorithm outperforms the
use of alternating monolingual batches, but not the use of shuffled batches, in
terms of translation quality (measured with BLEU and COMET) and convergence
speed."
ConDa: Fast Federated Unlearning with Contribution Dampening,cs.LG,Machine Learning,2024-10-05,"Federated learning (FL) has enabled collaborative model training across
decentralized data sources or clients. While adding new participants to a
shared model does not pose great technical hurdles, the removal of a
participant and their related information contained in the shared model remains
a challenge. To address this problem, federated unlearning has emerged as a
critical research direction, seeking to remove information from globally
trained models without harming the model performance on the remaining data.
Most modern federated unlearning methods use costly approaches such as the use
of remaining clients data to retrain the global model or methods that would
require heavy computation on client or server side. We introduce Contribution
Dampening (ConDa), a framework that performs efficient unlearning by tracking
down the parameters which affect the global model for each client and performs
synaptic dampening on the parameters of the global model that have privacy
infringing contributions from the forgetting client. Our technique does not
require clients data or any kind of retraining and it does not put any
computational overhead on either the client or server side. We perform
experiments on multiple datasets and demonstrate that ConDa is effective to
forget a client's data. In experiments conducted on the MNIST, CIFAR10, and
CIFAR100 datasets, ConDa proves to be the fastest federated unlearning method,
outperforming the nearest state of the art approach by at least 100x. Our
emphasis is on the non-IID Federated Learning setting, which presents the
greatest challenge for unlearning. Additionally, we validate ConDa's robustness
through backdoor and membership inference attacks. We envision this work as a
crucial component for FL in adhering to legal and ethical requirements."
Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Knowledge distillation aims to transfer knowledge from a large teacher model
to a compact student counterpart, often coming with a significant performance
gap between them. We find that a too-large performance gap can hamper the
training process, which is also verified in recent studies. To address this, we
propose a Gap Preserving Distillation (GPD) method that trains an additional
dynamic teacher model from scratch along with training the student to bridge
this gap. In this way, it becomes possible to maintain a reasonable performance
gap between teacher and student during the whole distillation process. To
further strengthen distillation from the dynamic teacher to the student, we
develop a hard strategy by enforcing them to share parameters and encouraging
parameter inheritance. Besides hard strategy, we also build the soft
bidirectional mappings between them which are built on an Inverse
Reparameterization (IR) method and a Channel-Branch Reparameterization (CBR)
strategy. We highlight that our IR is able to initialize a larger dynamic
teacher with an arbitrary expansion ratio, while preserving exactly the same
accuracy as the given student model. In this way, it guarantees that the
dynamic teacher and student start from the same point and avoid a too large gap
in early stage of training. As for our CBR, with parameter-sharing, it directly
extracts an effective student model from the well-learned dynamic teacher
without any post-training, making our method highly flexible for model
deployment. In the experiments, GPD significantly outperforms existing
distillation methods on top of both CNNs and transformers architectures,
achieving up to 1.58% accuracy improvement. Interestingly, GPD also generalizes
well to the scenarios without a pre-trained teacher, including training from
scratch and fine-tuning, yielding a large improvement of 1.80% and 0.89% on
ResNet18, respectively."
From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Large language models (LLMs) have achieved significant performance gains
using advanced prompting techniques over various tasks. However, the increasing
length of prompts leads to high computational costs and often obscures crucial
information. Prompt compression has been proposed to alleviate these issues,
but it faces challenges in (i) capturing the global context and (ii) training
the compressor effectively. To tackle these challenges, we introduce a novel
prompt compression method, namely Reading To Compressing (R2C), utilizing the
Fusion-in-Decoder (FiD) architecture to identify the important information in
the prompt. Specifically, the cross-attention scores of the FiD are used to
discern essential chunks and sentences from the prompt. R2C effectively
captures the global context without compromising semantic consistency while
detouring the necessity of pseudo-labels for training the compressor. Empirical
results show that R2C retains key contexts, enhancing the LLM performance by 6%
in out-of-domain evaluations while reducing the prompt length by 80%."
From Hospital to Portables: A Universal ECG Foundation Model Built on 10+ Million Diverse Recordings,cs.LG,Machine Learning,2024-10-05,"Artificial Intelligence (AI) has shown great promise in electrocardiogram
(ECG) analysis and cardiovascular disease detection. However, developing a
general AI-ECG model has been challenging due to inter-individual variability
and the diversity of ECG diagnoses, limiting existing models to specific
diagnostic tasks and datasets. Moreover, current AI-ECG models struggle to
achieve comparable performance between single-lead and 12-lead ECGs, limiting
the application of AI-ECG to portable and wearable ECG devices. To address
these limitations, we introduce an ECG Foundation Model (ECGFounder), a
general-purpose model that leverages real-world ECG annotations from cardiology
experts to broaden the diagnostic capabilities of ECG analysis. ECGFounder is
trained on over 10 million ECGs with 150 label categories from the
Harvard-Emory ECG Database, enabling comprehensive cardiovascular disease
diagnosis through ECG analysis. The model is designed to be both effective
out-of-the-box and fine-tunable for downstream tasks, maximizing usability.
More importantly, we extend its application to single-lead ECGs, enabling
complex condition diagnoses and supporting various downstream tasks in mobile
and remote monitoring scenarios. Experimental results demonstrate that
ECGFounder achieves expert-level performance on internal validation sets for
both 12-lead and single-lead ECGs, while also exhibiting strong classification
performance and generalization across various diagnoses on external validation
sets. When fine-tuned, ECGFounder outperforms baseline models in demographics
detection, clinical event detection, and cross-modality cardiac rhythm
diagnosis. The trained model and data will be publicly released upon
publication through the bdsp.io. Our code is available at
https://github.com/bdsp-core/ECGFounder."
Rethinking Fair Representation Learning for Performance-Sensitive Tasks,cs.LG,Machine Learning,2024-10-05,"We investigate the prominent class of fair representation learning methods
for bias mitigation. Using causal reasoning to define and formalise different
sources of dataset bias, we reveal important implicit assumptions inherent to
these methods. We prove fundamental limitations on fair representation learning
when evaluation data is drawn from the same distribution as training data and
run experiments across a range of medical modalities to examine the performance
of fair representation learning under distribution shifts. Our results explain
apparent contradictions in the existing literature and reveal how rarely
considered causal and statistical aspects of the underlying data affect the
validity of fair representation learning. We raise doubts about current
evaluation practices and the applicability of fair representation learning
methods in performance-sensitive settings. We argue that fine-grained analysis
of dataset biases should play a key role in the field moving forward."
Riemann Sum Optimization for Accurate Integrated Gradients Computation,cs.LG,Machine Learning,2024-10-05,"Integrated Gradients (IG) is a widely used algorithm for attributing the
outputs of a deep neural network to its input features. Due to the absence of
closed-form integrals for deep learning models, inaccurate Riemann Sum
approximations are used to calculate IG. This often introduces undesirable
errors in the form of high levels of noise, leading to false insights in the
model's decision-making process. We introduce a framework, RiemannOpt, that
minimizes these errors by optimizing the sample point selection for the Riemann
Sum. Our algorithm is highly versatile and applicable to IG as well as its
derivatives like Blur IG and Guided IG. RiemannOpt achieves up to 20%
improvement in Insertion Scores. Additionally, it enables its users to curtail
computational costs by up to four folds, thereby making it highly functional
for constrained environments."
Exploring LLM-based Data Annotation Strategies for Medical Dialogue Preference Alignment,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"This research examines the use of Reinforcement Learning from AI Feedback
(RLAIF) techniques to improve healthcare dialogue models, with the aim of
tackling the challenges of preference-aligned data annotation while reducing
the reliance on medical experts. We argue that the primary challenges in
current RLAIF research for healthcare are the limitations of automated
evaluation methods and the difficulties in accurately representing physician
preferences. To address these challenges, we present a new evaluation framework
based on standardized patient examinations. This framework is designed to
objectively assess the effectiveness of large language models (LLMs) in guiding
users and following instructions, enabling a comprehensive comparison across
different models. Furthermore, our investigation of effective ways to express
physician preferences using Constitutional AI algorithms highlighted the
particular effectiveness of flowcharts. Utilizing this finding, we introduce an
innovative agent-based approach for annotating preference data. This approach
autonomously creates medical dialogue flows tailored to the patient's
condition, demonstrates strong generalization abilities, and reduces the need
for expert involvement. Our results show that the agent-based approach
outperforms existing RLAIF annotation methods in standardized patient
examinations and surpasses current open source medical dialogue LLMs in various
test scenarios."
180 Days After EIP-4844: Will Blob Sharing Solve Dilemma for Small Rollups?,cs.CR,Cryptography and Security,2024-10-05,"The introduction of blobs through EIP-4844 has significantly reduced the Data
Availability (DA) costs for rollups on Ethereum. However, due to the fixed size
of blobs at 128 KB, rollups with low data throughput face a dilemma: they
either use blobs inefficiently or decrease the frequency of DA submissions.
Blob sharing, where multiple rollups share a single blob, has been proposed as
a solution to this problem. This paper examines the effectiveness of blob
sharing based on real-world data collected approximately six months after the
implementation of EIP-4844. By simulating cost changes using a simple blob
sharing format, we demonstrate that blob sharing can substantially improve the
costs and DA service quality for small rollups, effectively resolving their
dilemma. Notably, we observed cost reductions in USD exceeding 90% for most of
the rollups when they cooperate, attributable to the smoothing effect of the
blob base fee achieved through blob sharing."
On the Sample Complexity of a Policy Gradient Algorithm with Occupancy Approximation for General Utility Reinforcement Learning,cs.LG,Machine Learning,2024-10-05,"Reinforcement learning with general utilities has recently gained attention
thanks to its ability to unify several problems, including imitation learning,
pure exploration, and safe RL. However, prior work for solving this general
problem in a unified way has mainly focused on the tabular setting. This is
restrictive when considering larger state-action spaces because of the need to
estimate occupancy measures during policy optimization. In this work, we
address this issue and propose to approximate occupancy measures within a
function approximation class using maximum likelihood estimation (MLE). We
propose a simple policy gradient algorithm (PG-OMA) where an actor updates the
policy parameters to maximize the general utility objective whereas a critic
approximates the occupancy measure using MLE. We provide a sample complexity
analysis of PG-OMA showing that our occupancy measure estimation error only
scales with the dimension of our function approximation class rather than the
size of the state action space. Under suitable assumptions, we establish first
order stationarity and global optimality performance bounds for the proposed
PG-OMA algorithm for nonconcave and concave general utilities respectively. We
complement our methodological and theoretical findings with promising empirical
results showing the scalability potential of our approach compared to existing
tabular count-based approaches."
TUBench: Benchmarking Large Vision-Language Models on Trustworthiness with Unanswerable Questions,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Large Vision-Language Models (LVLMs) have achieved remarkable progress on
visual perception and linguistic interpretation. Despite their impressive
capabilities across various tasks, LVLMs still suffer from the issue of
hallucination, which involves generating content that is incorrect or
unfaithful to the visual or textual inputs. Traditional benchmarks, such as MME
and POPE, evaluate hallucination in LVLMs within the scope of Visual Question
Answering (VQA) using answerable questions. However, some questions are
unanswerable due to insufficient information in the images, and the performance
of LVLMs on such unanswerable questions remains underexplored. To bridge this
research gap, we propose TUBench, a benchmark specifically designed to evaluate
the reliability of LVLMs using unanswerable questions. TUBench comprises an
extensive collection of high-quality, unanswerable questions that are
meticulously crafted using ten distinct strategies. To thoroughly evaluate
LVLMs, the unanswerable questions in TUBench are based on images from four
diverse domains as visual contexts: screenshots of code snippets, natural
images, geometry diagrams, and screenshots of statistical tables. These
unanswerable questions are tailored to test LVLMs' trustworthiness in code
reasoning, commonsense reasoning, geometric reasoning, and mathematical
reasoning related to tables, respectively. We conducted a comprehensive
quantitative evaluation of 28 leading foundational models on TUBench, with
Gemini-1.5-Pro, the top-performing model, achieving an average accuracy of
69.2%, and GPT-4o, the third-ranked model, reaching 66.7% average accuracy, in
determining whether questions are answerable. TUBench is available at
https://github.com/NLPCode/TUBench."
A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Due to the continuous emergence of new data, version updates have become an
indispensable requirement for Large Language Models (LLMs). The training
paradigms for version updates of LLMs include pre-training from scratch (PTFS)
and continual pre-training (CPT). Preliminary experiments demonstrate that PTFS
achieves better pre-training performance, while CPT has lower training cost.
Moreover, their performance and training cost gaps widen progressively with
version updates. To investigate the underlying reasons for this phenomenon, we
analyze the effect of learning rate adjustments during the two stages of CPT:
preparing an initialization checkpoint and continual pre-training based on this
checkpoint. We find that a large learning rate in the first stage and a
complete learning rate decay process in the second stage are crucial for
version updates of LLMs. Hence, we propose a learning rate path switching
training paradigm. Our paradigm comprises one main path, where we pre-train a
LLM with the maximal learning rate, and multiple branching paths, each of which
corresponds to an update of the LLM with newly-added training data. Extensive
experiments demonstrate the effectiveness and generalization of our paradigm.
Particularly, when training four versions of LLMs, our paradigm reduces the
total training cost to 58% compared to PTFS, while maintaining comparable
pre-training performance."
Sinc Kolmogorov-Arnold Network and Its Applications on Physics-informed Neural Networks,cs.LG,Machine Learning,2024-10-05,"In this paper, we propose to use Sinc interpolation in the context of
Kolmogorov-Arnold Networks, neural networks with learnable activation
functions, which recently gained attention as alternatives to multilayer
perceptron. Many different function representations have already been tried,
but we show that Sinc interpolation proposes a viable alternative, since it is
known in numerical analysis to represent well both smooth functions and
functions with singularities. This is important not only for function
approximation but also for the solutions of partial differential equations with
physics-informed neural networks. Through a series of experiments, we show that
SincKANs provide better results in almost all of the examples we have
considered."
BloomWise: Enhancing Problem-Solving capabilities of Large Language Models using Bloom's-Taxonomy-Inspired Prompts,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Despite the continuous progress of Large Language Models (LLMs) across
various tasks, their performance on mathematical problems and reasoning tasks
remains limited. This limitation can be attributed, among other factors, to the
inherent difficulty of these problems and the fact that solutions often consist
of multiple steps, potentially of varying nature, making it challenging for a
single prompting technique to execute all required steps. To address this, we
introduce BloomWise, a new prompting technique, inspired by Bloom's Taxonomy,
aiming to improve LLMs' performance in solving such problems by encouraging
them to approach the problem starting from simple, i.e., remembering, and
progressing to higher cognitive skills, i.e., analyzing, until the correct
solution is reached. The decision regarding the need to employ more
sophisticated cognitive skills is based on self-evaluation performed by the
LLM. Thus, we encourage the LLM to deploy the appropriate cognitive processes.
In extensive experiments across 4 popular math reasoning datasets, we have
demonstrated the effectiveness of our proposed approach. We also present
extensive ablations, analyzing the strengths of each module within our system."
Cross-Lingual Query-by-Example Spoken Term Detection: A Transformer-Based Approach,cs.LG,Machine Learning,2024-10-05,"Query-by-example spoken term detection (QbE-STD) is typically constrained by
transcribed data scarcity and language specificity. This paper introduces a
novel, language-agnostic QbE-STD model leveraging image processing techniques
and transformer architecture. By employing a pre-trained XLSR-53 network for
feature extraction and a Hough transform for detection, our model effectively
searches for user-defined spoken terms within any audio file. Experimental
results across four languages demonstrate significant performance gains
(19-54%) over a CNN-based baseline. While processing time is improved compared
to DTW, accuracy remains inferior. Notably, our model offers the advantage of
accurately counting query term repetitions within the target audio."
High-Speed Stereo Visual SLAM for Low-Powered Computing Devices,cs.RO,Robotics,2024-10-05,"We present an accurate and GPU-accelerated Stereo Visual SLAM design called
Jetson-SLAM. It exhibits frame-processing rates above 60FPS on NVIDIA's
low-powered 10W Jetson-NX embedded computer and above 200FPS on desktop-grade
200W GPUs, even in stereo configuration and in the multiscale setting. Our
contributions are threefold: (i) a Bounded Rectification technique to prevent
tagging many non-corner points as a corner in FAST detection, improving SLAM
accuracy. (ii) A novel Pyramidal Culling and Aggregation (PyCA) technique that
yields robust features while suppressing redundant ones at high speeds by
harnessing a GPU device. PyCA uses our new Multi-Location Per Thread culling
strategy (MLPT) and Thread-Efficient Warp-Allocation (TEWA) scheme for GPU to
enable Jetson-SLAM achieving high accuracy and speed on embedded devices. (iii)
Jetson-SLAM library achieves resource efficiency by having a data-sharing
mechanism. Our experiments on three challenging datasets: KITTI, EuRoC, and
KAIST-VIO, and two highly accurate SLAM backends: Full-BA and ICE-BA show that
Jetson-SLAM is the fastest available accurate and GPU-accelerated SLAM system
(Fig. 1)."
Designing Concise ConvNets with Columnar Stages,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"In the era of vision Transformers, the recent success of VanillaNet shows the
huge potential of simple and concise convolutional neural networks (ConvNets).
Where such models mainly focus on runtime, it is also crucial to simultaneously
focus on other aspects, e.g., FLOPs, parameters, etc, to strengthen their
utility further. To this end, we introduce a refreshing ConvNet macro design
called Columnar Stage Network (CoSNet). CoSNet has a systematically developed
simple and concise structure, smaller depth, low parameter count, low FLOPs,
and attention-less operations, well suited for resource-constrained deployment.
The key novelty of CoSNet is deploying parallel convolutions with fewer kernels
fed by input replication, using columnar stacking of these convolutions, and
minimizing the use of 1x1 convolution layers. Our comprehensive evaluations
show that CoSNet rivals many renowned ConvNets and Transformer designs under
resource-constrained scenarios. Code: https://github.com/ashishkumar822/CoSNet"
Cross Resolution Encoding-Decoding For Detection Transformers,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Detection Transformers (DETR) are renowned object detection pipelines,
however computationally efficient multiscale detection using DETR is still
challenging. In this paper, we propose a Cross-Resolution Encoding-Decoding
(CRED) mechanism that allows DETR to achieve the accuracy of high-resolution
detection while having the speed of low-resolution detection. CRED is based on
two modules; Cross Resolution Attention Module (CRAM) and One Step Multiscale
Attention (OSMA). CRAM is designed to transfer the knowledge of low-resolution
encoder output to a high-resolution feature. While OSMA is designed to fuse
multiscale features in a single step and produce a feature map of a desired
resolution enriched with multiscale information. When used in prominent DETR
methods, CRED delivers accuracy similar to the high-resolution DETR counterpart
in roughly 50% fewer FLOPs. Specifically, state-of-the-art DN-DETR, when used
with CRED (calling CRED-DETR), becomes 76% faster, with ~50% reduced FLOPs than
its high-resolution counterpart with 202 G FLOPs on MS-COCO benchmark. We plan
to release pretrained CRED-DETRs for use by the community. Code:
https://github.com/ashishkumar822/CRED-DETR"
"GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization",cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"News summarization in today's global scene can be daunting with its flood of
multilingual content and varied viewpoints from different sources. However,
current studies often neglect such real-world scenarios as they tend to focus
solely on either single-language or single-document tasks. To bridge this gap,
we aim to unify Multi-lingual, Cross-lingual and Multi-document Summarization
into a novel task, i.e., MCMS, which encapsulates the real-world requirements
all-in-one. Nevertheless, the lack of a benchmark inhibits researchers from
adequately studying this invaluable problem. To tackle this, we have
meticulously constructed the GLOBESUMM dataset by first collecting a wealth of
multilingual news reports and restructuring them into event-centric format.
Additionally, we introduce the method of protocol-guided prompting for
high-quality and cost-effective reference annotation. In MCMS, we also
highlight the challenge of conflicts between news reports, in addition to the
issues of redundancies and omissions, further enhancing the complexity of
GLOBESUMM. Through extensive experimental analysis, we validate the quality of
our dataset and elucidate the inherent challenges of the task. We firmly
believe that GLOBESUMM, given its challenging nature, will greatly contribute
to the multilingual communities and the evaluation of LLMs."
Taming the Tail: Leveraging Asymmetric Loss and Pade Approximation to Overcome Medical Image Long-Tailed Class Imbalance,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Long-tailed problems in healthcare emerge from data imbalance due to
variability in the prevalence and representation of different medical
conditions, warranting the requirement of precise and dependable classification
methods. Traditional loss functions such as cross-entropy and binary
cross-entropy are often inadequate due to their inability to address the
imbalances between the classes with high representation and the classes with
low representation found in medical image datasets. We introduce a novel
polynomial loss function based on Pade approximation, designed specifically to
overcome the challenges associated with long-tailed classification. This
approach incorporates asymmetric sampling techniques to better classify
under-represented classes. We conducted extensive evaluations on three publicly
available medical datasets and a proprietary medical dataset. Our
implementation of the proposed loss function is open-sourced in the public
repository:https://github.com/ipankhi/ALPA."
$$-VAE: Denoising as Visual Decoding,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"In generative modeling, tokenization simplifies complex data into compact,
structured representations, creating a more efficient, learnable space. For
high-dimensional visual data, it reduces redundancy and emphasizes key features
for high-quality generation. Current visual tokenization methods rely on a
traditional autoencoder framework, where the encoder compresses data into
latent representations, and the decoder reconstructs the original input. In
this work, we offer a new perspective by proposing denoising as decoding,
shifting from single-step reconstruction to iterative refinement. Specifically,
we replace the decoder with a diffusion process that iteratively refines noise
to recover the original image, guided by the latents provided by the encoder.
We evaluate our approach by assessing both reconstruction (rFID) and generation
quality (FID), comparing it to state-of-the-art autoencoding approach. We hope
this work offers new insights into integrating iterative generation and
autoencoding for improved compression and generation."
High Probability Bound for Cross-Learning Contextual Bandits with Unknown Context Distributions,cs.LG,Machine Learning,2024-10-05,"Motivated by applications in online bidding and sleeping bandits, we examine
the problem of contextual bandits with cross learning, where the learner
observes the loss associated with the action across all possible contexts, not
just the current round's context. Our focus is on a setting where losses are
chosen adversarially, and contexts are sampled i.i.d. from a specific
distribution. This problem was first studied by Balseiro et al. (2019), who
proposed an algorithm that achieves near-optimal regret under the assumption
that the context distribution is known in advance. However, this assumption is
often unrealistic. To address this issue, Schneider and Zimmert (2023) recently
proposed a new algorithm that achieves nearly optimal expected regret. It is
well-known that expected regret can be significantly weaker than
high-probability bounds. In this paper, we present a novel, in-depth analysis
of their algorithm and demonstrate that it actually achieves near-optimal
regret with high probability. There are steps in the original analysis by
Schneider and Zimmert (2023) that lead only to an expected bound by nature. In
our analysis, we introduce several new insights. Specifically, we make
extensive use of the weak dependency structure between different epochs, which
was overlooked in previous analyses. Additionally, standard martingale
inequalities are not directly applicable, so we refine martingale inequalities
to complete our analysis."
PsFuture: A Pseudo-Future-based Zero-Shot Adaptive Policy for Simultaneous Machine Translation,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Simultaneous Machine Translation (SiMT) requires target tokens to be
generated in real-time as streaming source tokens are consumed. Traditional
approaches to SiMT typically require sophisticated architectures and extensive
parameter configurations for training adaptive read/write policies, which in
turn demand considerable computational power and memory. We propose PsFuture,
the first zero-shot adaptive read/write policy for SiMT, enabling the
translation model to independently determine read/write actions without the
necessity for additional training. Furthermore, we introduce a novel training
strategy, Prefix-to-Full (P2F), specifically tailored to adjust offline
translation models for SiMT applications, exploiting the advantages of the
bidirectional attention mechanism inherent in offline models. Experiments
across multiple benchmarks demonstrate that our zero-shot policy attains
performance on par with strong baselines and the P2F method can further enhance
performance, achieving an outstanding trade-off between translation quality and
latency."
On Eliciting Syntax from Language Models via Hashing,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Unsupervised parsing, also known as grammar induction, aims to infer
syntactic structure from raw text. Recently, binary representation has
exhibited remarkable information-preserving capabilities at both lexicon and
syntax levels. In this paper, we explore the possibility of leveraging this
capability to deduce parsing trees from raw text, relying solely on the
implicitly induced grammars within models. To achieve this, we upgrade the
bit-level CKY from zero-order to first-order to encode the lexicon and syntax
in a unified binary representation space, switch training from supervised to
unsupervised under the contrastive hashing framework, and introduce a novel
loss function to impose stronger yet balanced alignment signals. Our model
shows competitive performance on various datasets, therefore, we claim that our
method is effective and efficient enough to acquire high-quality parsing trees
from pre-trained language models at a low cost."
Multi-Round Region-Based Optimization for Scene Sketching,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Scene sketching is to convert a scene into a simplified, abstract
representation that captures the essential elements and composition of the
original scene. It requires semantic understanding of the scene and
consideration of different regions within the scene. Since scenes often contain
diverse visual information across various regions, such as foreground objects,
background elements, and spatial divisions, dealing with these different
regions poses unique difficulties. In this paper, we define a sketch as some
sets of Bezier curves. We optimize the different regions of input scene in
multiple rounds. In each round of optimization, strokes sampled from the next
region can seamlessly be integrated into the sketch generated in the previous
round of optimization. We propose additional stroke initialization method to
ensure the integrity of the scene and the convergence of optimization. A novel
CLIP-Based Semantic loss and a VGG-Based Feature loss are utilized to guide our
multi-round optimization. Extensive experimental results on the quality and
quantity of the generated sketches confirm the effectiveness of our method."
PAD: Personalized Alignment at Decoding-Time,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Aligning with personalized preferences, which vary significantly across
cultural, educational, and political differences, poses a significant challenge
due to the computational costs and data demands of traditional alignment
methods. In response, this paper presents Personalized Alignment at
Decoding-time (PAD), a novel framework designed to align LLM outputs with
diverse personalized preferences during the inference phase, eliminating the
need for additional training. By introducing a unique personalized reward
modeling strategy, this framework decouples the text generation process from
personalized preferences, facilitating the generation of generalizable
token-level personalized rewards. The PAD algorithm leverages these rewards to
guide the decoding process, dynamically tailoring the base model's predictions
to personalized preferences. Extensive experimental results demonstrate that
PAD not only outperforms existing training-based alignment methods in terms of
aligning with diverse preferences but also shows significant generalizability
to preferences unseen during training and scalability across different base
models. This work advances the capability of LLMs to meet user needs in
real-time applications, presenting a substantial step forward in personalized
LLM alignment."
ECon: On the Detection and Resolution of Evidence Conflicts,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"The rise of large language models (LLMs) has significantly influenced the
quality of information in decision-making systems, leading to the prevalence of
AI-generated content and challenges in detecting misinformation and managing
conflicting information, or ""inter-evidence conflicts."" This study introduces a
method for generating diverse, validated evidence conflicts to simulate
real-world misinformation scenarios. We evaluate conflict detection methods,
including Natural Language Inference (NLI) models, factual consistency (FC)
models, and LLMs, on these conflicts (RQ1) and analyze LLMs' conflict
resolution behaviors (RQ2). Our key findings include: (1) NLI and LLM models
exhibit high precision in detecting answer conflicts, though weaker models
suffer from low recall; (2) FC models struggle with lexically similar answer
conflicts, while NLI and LLM models handle these better; and (3) stronger
models like GPT-4 show robust performance, especially with nuanced conflicts.
For conflict resolution, LLMs often favor one piece of conflicting evidence
without justification and rely on internal knowledge if they have prior
beliefs."
Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback,cs.LG,Machine Learning,2024-10-05,"Large language models (LLMs) have demonstrated strong capabilities across
various language tasks, notably through instruction-tuning methods. However,
LLMs face challenges in visualizing complex, real-world data through charts and
plots. Firstly, existing datasets rarely cover a full range of chart types,
such as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning
methods do not fully leverage the intricate relationships within rich datasets,
including text, code, and figures. To address these challenges, we propose a
hierarchical pipeline and a new dataset for chart generation. Our dataset,
Text2Chart31, includes 31 unique plot types referring to the Matplotlib
library, with 11.1K tuples of descriptions, code, data tables, and plots.
Moreover, we introduce a reinforcement learning-based instruction tuning
technique for chart generation tasks without requiring human feedback. Our
experiments show that this approach significantly enhances the model
performance, enabling smaller models to outperform larger open-source models
and be comparable to state-of-the-art proprietary models in data visualization
tasks. We make the code and dataset available at
https://github.com/fatemehpesaran310/Text2Chart31."
Unique ID based Trust Scheme for Improved IoV Wireless Sensor Network Security Against Power Controlled Sybil Attacks,cs.CR,Cryptography and Security,2024-10-05,"Wireless sensor networks (WSN) are widely used in vehicular networks to
support Vehicle-to-Everything (V2X) communications. Wireless sensors in
vehicular networks support sensing and monitoring of various environmental
factors and vehicle movement, which can help to enhance traffic management,
road safety, and transportation efficiency. However, WSNs face security
challenges due to their distributed nature and resource limited modules. In
Sybil attacks, attackers create multiple fake identities to disrupt network
operations (e.g., denial-of-service (DoS)), which is one of the major security
concerns in WSNs. Defensive techniques have been proposed, which recently
include a received signal strength indicator (RSSI) profiling scheme that
improves the performance and is not affected by internal forgeable information.
However, even this new RSSI based robust detection scheme was found to be
vulnerable when Sybil attackers are mobile or intentionally manipulate their
radio transmission power in addition to their device address. In this paper, a
unique identification based trust path routing scheme (UITrust) is proposed,
which uses the device's physically invariable unique identifiers and routing
path trust level estimations to avoid power-controlled Sybil attacks, where the
simulation results show the proposed scheme can provide a significant
improvement compared to existing schemes."
Enhancing Graph Self-Supervised Learning with Graph Interplay,cs.LG,Machine Learning,2024-10-05,"Graph self-supervised learning (GSSL) has emerged as a compelling framework
for extracting informative representations from graph-structured data without
extensive reliance on labeled inputs. In this study, we introduce Graph
Interplay (GIP), an innovative and versatile approach that significantly
enhances the performance equipped with various existing GSSL methods. To this
end, GIP advocates direct graph-level communications by introducing random
inter-graph edges within standard batches. Against GIP's simplicity, we further
theoretically show that \textsc{GIP} essentially performs a principled manifold
separation via combining inter-graph message passing and GSSL, bringing about
more structured embedding manifolds and thus benefits a series of downstream
tasks. Our empirical study demonstrates that GIP surpasses the performance of
prevailing GSSL methods across multiple benchmarks by significant margins,
highlighting its potential as a breakthrough approach. Besides, GIP can be
readily integrated into a series of GSSL methods and consistently offers
additional performance gain. This advancement not only amplifies the capability
of GSSL but also potentially sets the stage for a novel graph learning paradigm
in a broader sense."
LoRTA: Low Rank Tensor Adaptation of Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Low Rank Adaptation (LoRA) is a popular Parameter Efficient Fine Tuning
(PEFT) method that effectively adapts large pre-trained models for downstream
tasks. LoRA parameterizes model updates using low-rank matrices at each layer,
significantly reducing the number of trainable parameters and, consequently,
resource requirements during fine-tuning. However, the lower bound on the
number of trainable parameters remains high due to the use of the low-rank
matrix model. In this paper, we address this limitation by proposing a novel
approach that employs a low rank tensor parametrization for model updates. The
proposed low rank tensor model can significantly reduce the number of trainable
parameters, while also allowing for finer-grained control over adapter size.
Our experiments on Natural Language Understanding, Instruction Tuning,
Preference Optimization and Protein Folding benchmarks demonstrate that our
method is both efficient and effective for fine-tuning large language models,
achieving a substantial reduction in the number of parameters while maintaining
comparable performance."
RetCompletion:High-Speed Inference Image Completion with Retentive Network,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Time cost is a major challenge in achieving high-quality pluralistic image
completion. Recently, the Retentive Network (RetNet) in natural language
processing offers a novel approach to this problem with its low-cost inference
capabilities. Inspired by this, we apply RetNet to the pluralistic image
completion task in computer vision. We present RetCompletion, a two-stage
framework. In the first stage, we introduce Bi-RetNet, a bidirectional sequence
information fusion model that integrates contextual information from images.
During inference, we employ a unidirectional pixel-wise update strategy to
restore consistent image structures, achieving both high reconstruction quality
and fast inference speed. In the second stage, we use a CNN for low-resolution
upsampling to enhance texture details. Experiments on ImageNet and CelebA-HQ
demonstrate that our inference speed is 10$\times$ faster than ICT and
15$\times$ faster than RePaint. The proposed RetCompletion significantly
improves inference speed and delivers strong performance, especially when masks
cover large areas of the image."
Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"While Vision-Language Models (VLMs) have shown remarkable abilities in visual
and language reasoning tasks, they invariably generate flawed responses.
Self-correction that instructs models to refine their outputs presents a
promising solution to this issue. Previous studies have mainly concentrated on
Large Language Models (LLMs), while the self-correction abilities of VLMs,
particularly concerning both visual and linguistic information, remain largely
unexamined. This study investigates the self-correction capabilities of VLMs
during both inference and fine-tuning stages. We introduce a Self-Correction
Learning (SCL) approach that enables VLMs to learn from their self-generated
self-correction data through Direct Preference Optimization (DPO) without
relying on external feedback, facilitating self-improvement. Specifically, we
collect preferred and disfavored samples based on the correctness of initial
and refined responses, which are obtained by two-turn self-correction with VLMs
during the inference stage. Experimental results demonstrate that although VLMs
struggle to self-correct effectively during iterative inference without
additional fine-tuning and external feedback, they can enhance their
performance and avoid previous mistakes through preference fine-tuning when
their self-generated self-correction data are categorized into preferred and
disfavored samples. This study emphasizes that self-correction is not merely a
refinement process; rather, it should enhance the reasoning abilities of models
through additional training, enabling them to generate high-quality responses
directly without further refinement."
Large Language Models can Achieve Social Balance,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Social balance is a concept in sociology which states that if every three
individuals in a population achieve certain structures of positive or negative
interactions, then the whole population ends up in one faction of positive
interactions or divided between two or more antagonistic factions. In this
paper, we consider a group of interacting large language models (LLMs) and
study how, after continuous interactions, they can achieve social balance.
Across three different LLM models, we found that social balance depends on (i)
whether interactions are updated based on ""relationships"", ""appraisals"", or
""opinions""; (ii) whether agents update their interactions based on homophily or
influence from their peers; and (iii) the number of simultaneous interactions
the LLMs consider. When social balance is achieved, its particular structure of
positive or negative interactions depends on these three conditions and are
different across LLM models and sizes. The stability of interactions and the
justification for their update also vary across models. Thus, social balance is
driven by the pre-training and alignment particular to each LLM model."
Beyond Imperfections: A Conditional Inpainting Approach for End-to-End Artifact Removal in VTON and Pose Transfer,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Artifacts often degrade the visual quality of virtual try-on (VTON) and pose
transfer applications, impacting user experience. This study introduces a novel
conditional inpainting technique designed to detect and remove such
distortions, improving image aesthetics. Our work is the first to present an
end-to-end framework addressing this specific issue, and we developed a
specialized dataset of artifacts in VTON and pose transfer tasks, complete with
masks highlighting the affected areas. Experimental results show that our
method not only effectively removes artifacts but also significantly enhances
the visual quality of the final images, setting a new benchmark in computer
vision and image processing."
Beyond Forecasting: Compositional Time Series Reasoning for End-to-End Task Execution,cs.LG,Machine Learning,2024-10-05,"In recent decades, there has been substantial advances in time series models
and benchmarks across various individual tasks, such as time series
forecasting, classification, and anomaly detection. Meanwhile, compositional
reasoning in time series is prevalent in real-world applications (e.g.,
decision-making and compositional question answering) and is in great demand.
Unlike simple tasks that primarily focus on predictive accuracy, compositional
reasoning emphasizes the synthesis of diverse information from both time series
data and various domain knowledge, making it distinct and extremely more
challenging. In this paper, we introduce Compositional Time Series Reasoning, a
new task of handling intricate multistep reasoning tasks from time series data.
Specifically, this new task focuses on various question instances requiring
structural and compositional reasoning abilities on time series data, such as
decision-making and compositional question answering. As an initial attempt to
tackle this novel task, we developed TS-Reasoner, a program-aided approach that
utilizes large language model (LLM) to decompose a complex task into steps of
programs that leverage existing time series models and numerical subroutines.
Unlike existing reasoning work which only calls off-the-shelf modules,
TS-Reasoner allows for the creation of custom modules and provides greater
flexibility to incorporate domain knowledge as well as user-specified
constraints. We demonstrate the effectiveness of our method through a
comprehensive set of experiments. These promising results indicate potential
opportunities in the new task of time series reasoning and highlight the need
for further research."
Lane Detection System for Driver Assistance in Vehicles,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"This work presents the development of a lane detection system aimed at
assisting the driving of conventional and autonomous vehicles. The system was
implemented using traditional computer vision techniques, focusing on
robustness and efficiency to operate in real-time, even under adverse
conditions such as worn-out lanes and weather variations. The methodology
employs an image processing pipeline that includes camera calibration,
distortion correction, perspective transformation, and binary image generation.
Lane detection is performed using sliding window techniques and segmentation
based on gradients and color channels, enabling the precise identification of
lanes in various road scenarios. The results indicate that the system can
effectively detect and track lanes, performing well under different lighting
conditions and road surfaces. However, challenges were identified in extreme
situations, such as intense shadows and sharp curves. It is concluded that,
despite its limitations, the traditional computer vision approach shows
significant potential for application in driver assistance systems and
autonomous navigation, with room for future improvements."
Neuron-Level Sequential Editing for Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"This work explores sequential model editing in large language models (LLMs),
a critical task that involves modifying internal knowledge within LLMs
continuously through multi-round editing, each incorporating updates or
corrections to adjust the model outputs without the need for costly retraining.
Existing model editing methods, especially those that alter model parameters,
typically focus on single-round editing and often face significant challenges
in sequential model editing-most notably issues of model forgetting and
failure. To address these challenges, we introduce a new model editing method,
namely \textbf{N}euron-level \textbf{S}equential \textbf{E}diting (NSE),
tailored for supporting sequential model editing. Specifically, we optimize the
target layer's hidden states using the model's original weights to prevent
model failure. Furthermore, we iteratively select neurons in multiple layers
for editing based on their activation values to mitigate model forgetting. Our
empirical experiments demonstrate that NSE significantly outperforms current
modifying parameters model editing methods, marking a substantial advancement
in the field of sequential model editing. Our code is released on
\url{https://github.com/jianghoucheng/NSE}."
Improving LLM Reasoning through Scaling Inference Computation with Collaborative Verification,cs.LG,Machine Learning,2024-10-05,"Despite significant advancements in the general capability of large language
models (LLMs), they continue to struggle with consistent and accurate
reasoning, especially in complex tasks such as mathematical and code reasoning.
One key limitation is that LLMs are trained primarily on correct solutions,
reducing their ability to detect and learn from errors, which hampers their
ability to reliably verify and rank outputs. To address this, we scale up the
inference-time computation by generating multiple reasoning paths and employing
verifiers to assess and rank the generated outputs by correctness. To
facilitate this, we introduce a comprehensive dataset consisting of correct and
incorrect solutions for math and code tasks, generated by multiple LLMs. This
diverse set of solutions enables verifiers to more effectively distinguish and
rank correct answers from erroneous outputs. The training methods for building
verifiers were selected based on an extensive comparison of existing
approaches. Moreover, to leverage the unique strengths of different reasoning
strategies, we propose a novel collaborative method integrating
Chain-of-Thought (CoT) and Program-of-Thought (PoT) solutions for verification.
CoT provides a clear, step-by-step reasoning process that enhances
interpretability, while PoT, being executable, offers a precise and
error-sensitive validation mechanism. By taking both of their strengths, our
approach significantly improves the accuracy and reliability of reasoning
verification. Our verifiers, Math-Rev and Code-Rev, demonstrate substantial
performance gains to existing LLMs, achieving state-of-the-art results on
benchmarks such as GSM8k and MATH and even outperforming GPT-4o with
Qwen-72B-Instruct as the reasoner."
BlockFound: Customized blockchain foundation model for anomaly detection,cs.CR,Cryptography and Security,2024-10-05,"We propose BlockFound, a customized foundation model for anomaly blockchain
transaction detection. Unlike existing methods that rely on rule-based systems
or directly apply off-the-shelf large language models, BlockFound introduces a
series of customized designs to model the unique data structure of blockchain
transactions. First, a blockchain transaction is multi-modal, containing
blockchain-specific tokens, texts, and numbers. We design a modularized
tokenizer to handle these multi-modal inputs, balancing the information across
different modalities. Second, we design a customized mask language learning
mechanism for pretraining with RoPE embedding and FlashAttention for handling
longer sequences. After training the foundation model, we further design a
novel detection method for anomaly detection. Extensive evaluations on Ethereum
and Solana transactions demonstrate BlockFound's exceptional capability in
anomaly detection while maintaining a low false positive rate. Remarkably,
BlockFound is the only method that successfully detects anomalous transactions
on Solana with high accuracy, whereas all other approaches achieved very low or
zero detection recall scores. This work not only provides new foundation models
for blockchain but also sets a new benchmark for applying LLMs in blockchain
data."
Gamified crowd-sourcing of high-quality data for visual fine-tuning,cs.AI,Artificial Intelligence,2024-10-05,"This paper introduces Gamified Adversarial Prompting (GAP), a framework that
crowd-sources high-quality data for visual instruction tuning of large
multimodal models. GAP transforms the data collection process into an engaging
game, incentivizing players to provide fine-grained, challenging questions and
answers that target gaps in the model's knowledge. Our contributions include
(1) an approach to capture question-answer pairs from humans that directly
address weaknesses in a model's knowledge, (2) a method for evaluating and
rewarding players that successfully incentivizes them to provide high-quality
submissions, and (3) a scalable, gamified platform that succeeds in collecting
this data from over 50,000 participants in just a few weeks. Our implementation
of GAP has significantly improved the accuracy of a small multimodal model,
namely MiniCPM-Llama3-V-2.5-8B, increasing its GPT score from 0.147 to 0.477 on
our dataset, approaching the benchmark set by the much larger GPT-4V. Moreover,
we demonstrate that the data generated using MiniCPM-Llama3-V-2.5-8B also
enhances its performance across other benchmarks, and exhibits cross-model
benefits. Specifically, the same data improves the performance of QWEN2-VL-2B
and QWEN2-VL-7B on the same multiple benchmarks."
ForgeryTTT: Zero-Shot Image Manipulation Localization with Test-Time Training,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Social media is increasingly plagued by realistic fake images, making it hard
to trust content. Previous algorithms to detect these fakes often fail in new,
real-world scenarios because they are trained on specific datasets. To address
the problem, we introduce ForgeryTTT, the first method leveraging test-time
training (TTT) to identify manipulated regions in images. The proposed approach
fine-tunes the model for each individual test sample, improving its
performance. ForgeryTTT first employs vision transformers as a shared image
encoder to learn both classification and localization tasks simultaneously
during the training-time training using a large synthetic dataset. Precisely,
the localization head predicts a mask to highlight manipulated areas. Given
such a mask, the input tokens can be divided into manipulated and genuine
groups, which are then fed into the classification head to distinguish between
manipulated and genuine parts. During test-time training, the predicted mask
from the localization head is used for the classification head to update the
image encoder for better adaptation. Additionally, using the classical dropout
strategy in each token group significantly improves performance and efficiency.
We test ForgeryTTT on five standard benchmarks. Despite its simplicity,
ForgeryTTT achieves a 20.1% improvement in localization accuracy compared to
other zero-shot methods and a 4.3% improvement over non-zero-shot techniques.
Our code and data will be released upon publication."
SyllableLM: Learning Coarse Semantic Units for Speech Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Language models require tokenized inputs. However, tokenization strategies
for continuous data like audio and vision are often based on simple heuristics
such as fixed sized convolutions or discrete clustering, which do not
necessarily align with the semantic structure of the data. For speech in
particular, the high resolution of waveforms (16,000 samples/second or more)
presents a significant challenge as speech-based language models have had to
use several times more tokens per word than text-based language models. In this
work, we introduce a controllable self-supervised technique to merge speech
representations into coarser syllable-like units while still preserving
semantic information. We do this by 1) extracting noisy boundaries through
analyzing correlations in pretrained encoder losses and 2) iteratively
improving model representations with a novel distillation technique. Our method
produces controllable-rate semantic units at as low as 5Hz and 60bps and
achieves SotA in syllabic segmentation and clustering. Using these coarse
tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM)
that matches or outperforms current SotA SpeechLMs on a range of spoken
language modeling tasks. SyllableLM also achieves significant improvements in
efficiency with a 30x reduction in training compute and a 4x wall-clock
inference speedup."
A Simple yet Effective Training-free Prompt-free Approach to Chinese Spelling Correction Based on Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"This work proposes a simple training-free prompt-free approach to leverage
large language models (LLMs) for the Chinese spelling correction (CSC) task,
which is totally different from all previous CSC approaches. The key idea is to
use an LLM as a pure language model in a conventional manner. The LLM goes
through the input sentence from the beginning, and at each inference step,
produces a distribution over its vocabulary for deciding the next token, given
a partial sentence. To ensure that the output sentence remains faithful to the
input sentence, we design a minimal distortion model that utilizes
pronunciation or shape similarities between the original and replaced
characters. Furthermore, we propose two useful reward strategies to address
practical challenges specific to the CSC task. Experiments on five public
datasets demonstrate that our approach significantly improves LLM performance,
enabling them to compete with state-of-the-art domain-general CSC models."
Efficient Large-Scale Urban Parking Prediction: Graph Coarsening Based on Real-Time Parking Service Capability,cs.LG,Machine Learning,2024-10-05,"With the sharp increase in the number of vehicles, the issue of parking
difficulties has emerged as an urgent challenge that many cities need to
address promptly. In the task of predicting large-scale urban parking data,
existing research often lacks effective deep learning models and strategies. To
tackle this challenge, this paper proposes an innovative framework for
predicting large-scale urban parking graphs leveraging real-time service
capabilities, aimed at improving the accuracy and efficiency of parking
predictions. Specifically, we introduce a graph attention mechanism that
assesses the real-time service capabilities of parking lots to construct a
dynamic parking graph that accurately reflects real preferences in parking
behavior. To effectively handle large-scale parking data, this study combines
graph coarsening techniques with temporal convolutional autoencoders to achieve
unified dimension reduction of the complex urban parking graph structure and
features. Subsequently, we use a spatio-temporal graph convolutional model to
make predictions based on the coarsened graph, and a pre-trained
autoencoder-decoder module restores the predicted results to their original
data dimensions, completing the task. Our methodology has been rigorously
tested on a real dataset from parking lots in Shenzhen. The experimental
results indicate that compared to traditional parking prediction models, our
framework achieves improvements of 46.8\% and 30.5\% in accuracy and
efficiency, respectively. Remarkably, with the expansion of the graph's scale,
our framework's advantages become even more apparent, showcasing its
substantial potential for solving complex urban parking dilemmas in practical
scenarios."
Accelerating Diffusion Transformers with Token-wise Feature Caching,cs.LG,Machine Learning,2024-10-05,"Diffusion transformers have shown significant effectiveness in both image and
video synthesis at the expense of huge computation costs. To address this
problem, feature caching methods have been introduced to accelerate diffusion
transformers by caching the features in previous timesteps and reusing them in
the following timesteps. However, previous caching methods ignore that
different tokens exhibit different sensitivities to feature caching, and
feature caching on some tokens may lead to 10$\times$ more destruction to the
overall generation quality compared with other tokens. In this paper, we
introduce token-wise feature caching, allowing us to adaptively select the most
suitable tokens for caching, and further enable us to apply different caching
ratios to neural layers in different types and depths. Extensive experiments on
PixArt-$\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image
and video generation with no requirements for training. For instance,
2.36$\times$ and 1.93$\times$ acceleration are achieved on OpenSora and
PixArt-$\alpha$ with almost no drop in generation quality."
PalmBench: A Comprehensive Benchmark of Compressed Large Language Models on Mobile Platforms,cs.LG,Machine Learning,2024-10-05,"Deploying large language models (LLMs) locally on mobile devices is
advantageous in scenarios where transmitting data to remote cloud servers is
either undesirable due to privacy concerns or impractical due to network
connection. Recent advancements (MLC, 2023a; Gerganov, 2023) have facilitated
the local deployment of LLMs. However, local deployment also presents
challenges, particularly in balancing quality (generative performance),
latency, and throughput within the hardware constraints of mobile devices. In
this paper, we introduce our lightweight, all-in-one automated benchmarking
framework that allows users to evaluate LLMs on mobile devices. We provide a
comprehensive benchmark of various popular LLMs with different quantization
configurations (both weights and activations) across multiple mobile platforms
with varying hardware capabilities. Unlike traditional benchmarks that assess
full-scale models on high-end GPU clusters, we focus on evaluating resource
efficiency (memory and power consumption) and harmful output for compressed
models on mobile devices. Our key observations include i) differences in energy
efficiency and throughput across mobile platforms; ii) the impact of
quantization on memory usage, GPU execution time, and power consumption; and
iii) accuracy and performance degradation of quantized models compared to their
non-quantized counterparts; and iv) the frequency of hallucinations and toxic
content generated by compressed LLMs on mobile devices."
Improving Temporal Link Prediction via Temporal Walk Matrix Projection,cs.LG,Machine Learning,2024-10-05,"Temporal link prediction, aiming at predicting future interactions among
entities based on historical interactions, is crucial for a series of
real-world applications. Although previous methods have demonstrated the
importance of relative encodings for effective temporal link prediction,
computational efficiency remains a major concern in constructing these
encodings. Moreover, existing relative encodings are usually constructed based
on structural connectivity, where temporal information is seldom considered. To
address the aforementioned issues, we first analyze existing relative encodings
and unify them as a function of temporal walk matrices. This unification
establishes a connection between relative encodings and temporal walk matrices,
providing a more principled way for analyzing and designing relative encodings.
Based on this analysis, we propose a new temporal graph neural network called
TPNet, which introduces a temporal walk matrix that incorporates the time decay
effect to simultaneously consider both temporal and structural information.
Moreover, TPNet designs a random feature propagation mechanism with theoretical
guarantees to implicitly maintain the temporal walk matrices, which improves
the computation and storage efficiency. Experimental results on 13 benchmark
datasets verify the effectiveness and efficiency of TPNet, where TPNet
outperforms other baselines on most datasets and achieves a maximum speedup of
$33.3 \times$ compared to the SOTA baseline. Our code can be found at
\url{https://github.com/lxd99/TPNet}."
"JAM: A Comprehensive Model for Age Estimation, Verification, and Comparability",cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"This paper introduces a comprehensive model for age estimation, verification,
and comparability, offering a comprehensive solution for a wide range of
applications. It employs advanced learning techniques to understand age
distribution and uses confidence scores to create probabilistic age ranges,
enhancing its ability to handle ambiguous cases. The model has been tested on
both proprietary and public datasets and compared against one of the
top-performing models in the field. Additionally, it has recently been
evaluated by NIST as part of the FATE challenge, achieving top places in many
categories."
Kalman Filter Applied To A Differential Robot,cs.RO,Robotics,2024-10-05,"This document presents the study of the problem of location and trajectory
that a robot must follow. It focuses on applying the Kalman filter to achieve
location and trajectory estimation in an autonomous mobile differential robot.
The experimental data was carried out through tests obtained with the help of
two incremental encoders that are part of the construction of the differential
robot. The data transmission is carried out from a PC where the control is
carried out with the Matlab/Simulink software. The results are expressed in
graphs showing the path followed by the robot using PI control, the estimator
of the Kalman filter in a real system."
Hyperbolic Fine-tuning for Large Language Models,cs.LG,Machine Learning,2024-10-05,"Large language models (LLMs) have demonstrated remarkable performance on
various tasks. However, it remains an open question whether the default
Euclidean space is the most suitable choice for embedding tokens in LLMs. In
this study, we first investigate the non-Euclidean characteristics of LLMs. Our
findings reveal that token frequency follows a power-law distribution, with
high-frequency tokens clustering near the origin and low-frequency tokens
positioned farther away. Additionally, token embeddings exhibit a high degree
of hyperbolicity, indicating a latent tree-like structure in the embedding
space. Building on the observation, we propose to efficiently fine-tune LLMs in
hyperbolic space to better exploit the underlying complex structures. However,
we found that this fine-tuning in hyperbolic space cannot be achieved with
naive application of exponential and logarithmic maps, when the embedding and
weight matrices both reside in Euclidean space. To address this technique
issue, we introduce a new method called hyperbolic low-rank efficient
fine-tuning, HypLoRA, that performs low-rank adaptation directly on the
hyperbolic manifold, avoiding the cancellation effect caused by the exponential
and logarithmic maps, thus preserving the hyperbolic modeling capabilities.
Through extensive experiments, we demonstrate that HypLoRA significantly
enhances the performance of LLMs on reasoning tasks, particularly for complex
reasoning problems. In particular, HypLoRA improves the performance in the
complex AQuA dataset by up to 13.0%, showcasing its effectiveness in handling
complex reasoning challenges"
ASPIRER: Bypassing System Prompts With Permutation-based Backdoors in LLMs,cs.CR,Cryptography and Security,2024-10-05,"Large Language Models (LLMs) have become integral to many applications, with
system prompts serving as a key mechanism to regulate model behavior and ensure
ethical outputs. In this paper, we introduce a novel backdoor attack that
systematically bypasses these system prompts, posing significant risks to the
AI supply chain. Under normal conditions, the model adheres strictly to its
system prompts. However, our backdoor allows malicious actors to circumvent
these safeguards when triggered. Specifically, we explore a scenario where an
LLM provider embeds a covert trigger within the base model. A downstream
deployer, unaware of the hidden trigger, fine-tunes the model and offers it as
a service to users. Malicious actors can purchase the trigger from the provider
and use it to exploit the deployed model, disabling system prompts and
achieving restricted outcomes. Our attack utilizes a permutation trigger, which
activates only when its components are arranged in a precise order, making it
computationally challenging to detect or reverse-engineer. We evaluate our
approach on five state-of-the-art models, demonstrating that our method
achieves an attack success rate (ASR) of up to 99.50% while maintaining a clean
accuracy (CACC) of 98.58%, even after defensive fine-tuning. These findings
highlight critical vulnerabilities in LLM deployment pipelines and underscore
the need for stronger defenses."
Take It Easy: Label-Adaptive Self-Rationalization for Fact Verification and Explanation Generation,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"Computational methods to aid journalists in the task often require adapting a
model to specific domains and generating explanations. However, most automated
fact-checking methods rely on three-class datasets, which do not accurately
reflect real-world misinformation. Moreover, fact-checking explanations are
often generated based on text summarization of evidence, failing to address the
relationship between the claim and the evidence. To address these issues, we
extend the self-rationalization method--typically used in natural language
inference (NLI) tasks--to fact verification. We propose a label-adaptive
learning approach: first, we fine-tune a model to learn veracity prediction
with annotated labels (step-1 model). Then, we fine-tune the step-1 model again
to learn self-rationalization, using the same data and additional annotated
explanations. Our results show that our label-adaptive approach improves
veracity prediction by more than ten percentage points (Macro F1) on both the
PubHealth and AVeriTec datasets, outperforming the GPT-4 model. Furthermore, to
address the high cost of explanation annotation, we generated 64 synthetic
explanations from three large language models: GPT-4-turbo, GPT-3.5-turbo, and
Llama-3-8B and few-shot fine-tune our step-1 model. The few-shot synthetic
explanation fine-tuned model performed comparably to the fully fine-tuned
self-rationalization model, demonstrating the potential of low-budget learning
with synthetic data. Our label-adaptive self-rationalization approach presents
a promising direction for future research on real-world explainable
fact-checking with different labeling schemes."
FastLRNR and Sparse Physics Informed Backpropagation,cs.LG,Machine Learning,2024-10-05,"We introduce Sparse Physics Informed Backpropagation (SPInProp), a new class
of methods for accelerating backpropagation for a specialized neural network
architecture called Low Rank Neural Representation (LRNR). The approach
exploits the low rank structure within LRNR and constructs a reduced neural
network approximation that is much smaller in size. We call the smaller network
FastLRNR. We show that backpropagation of FastLRNR can be substituted for that
of LRNR, enabling a significant reduction in complexity. We apply SPInProp to a
physics informed neural networks framework and demonstrate how the solution of
parametrized partial differential equations is accelerated."
Impact of Regularization on Calibration and Robustness: from the Representation Space Perspective,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"Recent studies have shown that regularization techniques using soft labels,
e.g., label smoothing, Mixup, and CutMix, not only enhance image classification
accuracy but also improve model calibration and robustness against adversarial
attacks. However, the underlying mechanisms of such improvements remain
underexplored. In this paper, we offer a novel explanation from the perspective
of the representation space (i.e., the space of the features obtained at the
penultimate layer). Our investigation first reveals that the decision regions
in the representation space form cone-like shapes around the origin after
training regardless of the presence of regularization. However, applying
regularization causes changes in the distribution of features (or
representation vectors). The magnitudes of the representation vectors are
reduced and subsequently the cosine similarities between the representation
vectors and the class centers (minimal loss points for each class) become
higher, which acts as a central mechanism inducing improved calibration and
robustness. Our findings provide new insights into the characteristics of the
high-dimensional representation space in relation to training and
regularization using soft labels."
On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-05,"We study the presence of heteronormative biases and prejudice against
interracial romantic relationships in large language models by performing
controlled name-replacement experiments for the task of relationship
prediction. We show that models are less likely to predict romantic
relationships for (a) same-gender character pairs than different-gender pairs;
and (b) intra/inter-racial character pairs involving Asian names as compared to
Black, Hispanic, or White names. We examine the contextualized embeddings of
first names and find that gender for Asian names is less discernible than
non-Asian names. We discuss the social implications of our findings,
underlining the need to prioritize the development of inclusive and equitable
technology."
Symmetry From Scratch: Group Equivariance as a Supervised Learning Task,cs.LG,Machine Learning,2024-10-05,"In machine learning datasets with symmetries, the paradigm for backward
compatibility with symmetry-breaking has been to relax equivariant
architectural constraints, engineering extra weights to differentiate
symmetries of interest. However, this process becomes increasingly
over-engineered as models are geared towards specific symmetries/asymmetries
hardwired of a particular set of equivariant basis functions. In this work, we
introduce symmetry-cloning, a method for inducing equivariance in machine
learning models. We show that general machine learning architectures (i.e.,
MLPs) can learn symmetries directly as a supervised learning task from group
equivariant architectures and retain/break the learned symmetry for downstream
tasks. This simple formulation enables machine learning models with
group-agnostic architectures to capture the inductive bias of group-equivariant
architectures."
Mamba Capsule Routing Towards Part-Whole Relational Camouflaged Object Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-05,"The part-whole relational property endowed by Capsule Networks (CapsNets) has
been known successful for camouflaged object detection due to its segmentation
integrity. However, the previous Expectation Maximization (EM) capsule routing
algorithm with heavy computation and large parameters obstructs this trend. The
primary attribution behind lies in the pixel-level capsule routing.
Alternatively, in this paper, we propose a novel mamba capsule routing at the
type level. Specifically, we first extract the implicit latent state in mamba
as capsule vectors, which abstract type-level capsules from pixel-level
versions. These type-level mamba capsules are fed into the EM routing algorithm
to get the high-layer mamba capsules, which greatly reduce the computation and
parameters caused by the pixel-level capsule routing for part-whole
relationships exploration. On top of that, to retrieve the pixel-level capsule
features for further camouflaged prediction, we achieve this on the basis of
the low-layer pixel-level capsules with the guidance of the correlations from
adjacent-layer type-level mamba capsules. Extensive experiments on three widely
used COD benchmark datasets demonstrate that our method significantly
outperforms state-of-the-arts. Code has been available on
https://github.com/Liangbo-Cheng/mamba\_capsule."
MetricX-24: The Google Submission to the WMT 2024 Metrics Shared Task,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"In this paper, we present the MetricX-24 submissions to the WMT24 Metrics
Shared Task and provide details on the improvements we made over the previous
version of MetricX. Our primary submission is a hybrid reference-based/-free
metric, which can score a translation irrespective of whether it is given the
source segment, the reference, or both. The metric is trained on previous WMT
data in a two-stage fashion, first on the DA ratings only, then on a mixture of
MQM and DA ratings. The training set in both stages is augmented with synthetic
examples that we created to make the metric more robust to several common
failure modes, such as fluent but unrelated translation, or undertranslation.
We demonstrate the benefits of the individual modifications via an ablation
study, and show a significant performance increase over MetricX-23 on the WMT23
MQM ratings, as well as our new synthetic challenge set."
Improving Arabic Multi-Label Emotion Classification using Stacked Embeddings and Hybrid Loss Function,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"In multi-label emotion classification, particularly for low-resource
languages like Arabic, the challenges of class imbalance and label correlation
hinder model performance, especially in accurately predicting minority
emotions. To address these issues, this study proposes a novel approach that
combines stacked embeddings, meta-learning, and a hybrid loss function to
enhance multi-label emotion classification for the Arabic language. The study
extracts contextual embeddings from three fine-tuned language
models-ArabicBERT, MarBERT, and AraBERT-which are then stacked to form enriched
embeddings. A meta-learner is trained on these stacked embeddings, and the
resulting concatenated representations are provided as input to a Bi-LSTM
model, followed by a fully connected neural network for multi-label
classification. To further improve performance, a hybrid loss function is
introduced, incorporating class weighting, label correlation matrix, and
contrastive learning, effectively addressing class imbalances and improving the
handling of label correlations. Extensive experiments validate the proposed
model's performance across key metrics such as Precision, Recall, F1-Score,
Jaccard Accuracy, and Hamming Loss. The class-wise performance analysis
demonstrates the hybrid loss function's ability to significantly reduce
disparities between majority and minority classes, resulting in a more balanced
emotion classification. An ablation study highlights the contribution of each
component, showing the superiority of the model compared to baseline approaches
and other loss functions. This study not only advances multi-label emotion
classification for Arabic but also presents a generalizable framework that can
be adapted to other languages and domains, providing a significant step forward
in addressing the challenges of low-resource emotion classification tasks."
Optimizing Sparse Generalized Singular Vectors for Feature Selection in Proximal Support Vector Machines with Application to Breast and Ovarian Cancer Detection,cs.LG,Machine Learning,2024-10-04,"This paper presents approaches to compute sparse solutions of Generalized
Singular Value Problem (GSVP). The GSVP is regularized by $\ell_1$-norm and
$\ell_q$-penalty for $0<q<1$, resulting in the $\ell_1$-GSVP and $\ell_q$-GSVP
formulations. The solutions of these problems are determined by applying the
proximal gradient descent algorithm with a fixed step size. The inherent
sparsity levels within the computed solutions are exploited for feature
selection, and subsequently, binary classification with non-parallel Support
Vector Machines (SVM). For our feature selection task, SVM is integrated into
the $\ell_1$-GSVP and $\ell_q$-GSVP frameworks to derive the $\ell_1$-GSVPSVM
and $\ell_q$-GSVPSVM variants. Machine learning applications to cancer
detection are considered. We remarkably report near-to-perfect balanced
accuracy across breast and ovarian cancer datasets using a few selected
features."
Learning to Balance: Diverse Normalization for Cloth-Changing Person Re-Identification,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Cloth-Changing Person Re-Identification (CC-ReID) involves recognizing
individuals in images regardless of clothing status. In this paper, we
empirically and experimentally demonstrate that completely eliminating or fully
retaining clothing features is detrimental to the task. Existing work, either
relying on clothing labels, silhouettes, or other auxiliary data, fundamentally
aim to balance the learning of clothing and identity features. However, we
practically find that achieving this balance is challenging and nuanced. In
this study, we introduce a novel module called Diverse Norm, which expands
personal features into orthogonal spaces and employs channel attention to
separate clothing and identity features. A sample re-weighting optimization
strategy is also introduced to guarantee the opposite optimization direction.
Diverse Norm presents a simple yet effective approach that does not require
additional data. Furthermore, Diverse Norm can be seamlessly integrated
ResNet50 and significantly outperforms the state-of-the-art methods."
Efficient Training of Neural Stochastic Differential Equations by Matching Finite Dimensional Distributions,cs.LG,Machine Learning,2024-10-04,"Neural Stochastic Differential Equations (Neural SDEs) have emerged as
powerful mesh-free generative models for continuous stochastic processes, with
critical applications in fields such as finance, physics, and biology. Previous
state-of-the-art methods have relied on adversarial training, such as GANs, or
on minimizing distance measures between processes using signature kernels.
However, GANs suffer from issues like instability, mode collapse, and the need
for specialized training techniques, while signature kernel-based methods
require solving linear PDEs and backpropagating gradients through the solver,
whose computational complexity scales quadratically with the discretization
steps. In this paper, we identify a novel class of strictly proper scoring
rules for comparing continuous Markov processes. This theoretical finding
naturally leads to a novel approach called Finite Dimensional Matching (FDM)
for training Neural SDEs. Our method leverages the Markov property of SDEs to
provide a computationally efficient training objective. This scoring rule
allows us to bypass the computational overhead associated with signature
kernels and reduces the training complexity from $O(D^2)$ to $O(D)$ per epoch,
where $D$ represents the number of discretization steps of the process. We
demonstrate that FDM achieves superior performance, consistently outperforming
existing methods in terms of both computational efficiency and generative
quality."
Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks,cs.LG,Machine Learning,2024-10-04,"Task-trained recurrent neural networks (RNNs) are versatile models of
dynamical processes widely used in machine learning and neuroscience. While
RNNs are easily trained to perform a wide range of tasks, the nature and extent
of the degeneracy in the resultant solutions (i.e., the variability across
trained RNNs) remain poorly understood. Here, we provide a unified framework
for analyzing degeneracy across three levels: behavior, neural dynamics, and
weight space. We analyzed RNNs trained on diverse tasks across machine learning
and neuroscience domains, including N-bit flip-flop, sine wave generation,
delayed discrimination, and path integration. Our key finding is that the
variability across RNN solutions, quantified on the basis of neural dynamics
and trained weights, depends primarily on network capacity and task
characteristics such as complexity. We introduce information-theoretic measures
to quantify task complexity and demonstrate that increasing task complexity
consistently reduces degeneracy in neural dynamics and generalization behavior
while increasing degeneracy in weight space. These relationships hold across
diverse tasks and can be used to control the degeneracy of the solution space
of task-trained RNNs. Furthermore, we provide several strategies to control
solution degeneracy, enabling task-trained RNNs to learn more consistent or
diverse solutions as needed. We envision that these insights will lead to more
reliable machine learning models and could inspire strategies to better
understand and control degeneracy observed in neuroscience experiments."
ROS2-Based Simulation Framework for Cyberphysical Security Analysis of UAVs,cs.RO,Robotics,2024-10-04,"We present a new simulator of Uncrewed Aerial Vehicles (UAVs) that is
  tailored to the needs of testing cyber-physical security attacks and
  defenses. Recent investigations into UAV safety have unveiled various attack
  surfaces and some defense mechanisms. However, due to escalating regulations
  imposed by aviation authorities on security research on real UAVs, and the
  substantial costs associated with hardware test-bed configurations, there
  arises a necessity for a simulator capable of substituting for hardware
  experiments, and/or narrowing down their scope to the strictly necessary.
  The study of different attack mechanisms requires specific features in a
  simulator. We propose a simulation framework based on ROS2, leveraging some
  of its key advantages, including modularity, replicability, customization,
  and the utilization of open-source tools such as Gazebo. Our framework has a
  built-in motion planner, controller, communication models and attack models.
  We share examples of research use cases that our framework can enable,
  demonstrating its utility."
Decoding Game: On Minimax Optimality of Heuristic Text Generation Strategies,cs.LG,Machine Learning,2024-10-04,"Decoding strategies play a pivotal role in text generation for modern
language models, yet a puzzling gap divides theory and practice. Surprisingly,
strategies that should intuitively be optimal, such as Maximum a Posteriori
(MAP), often perform poorly in practice. Meanwhile, popular heuristic
approaches like Top-$k$ and Nucleus sampling, which employ truncation and
normalization of the conditional next-token probabilities, have achieved great
empirical success but lack theoretical justifications. In this paper, we
propose Decoding Game, a comprehensive theoretical framework which reimagines
text generation as a two-player zero-sum game between Strategist, who seeks to
produce text credible in the true distribution, and Nature, who distorts the
true distribution adversarially. After discussing the decomposibility of
multi-step generation, we derive the optimal strategy in closed form for
one-step Decoding Game. It is shown that the adversarial Nature imposes an
implicit regularization on likelihood maximization, and
truncation-normalization methods are first-order approximations to the optimal
strategy under this regularization. Additionally, by generalizing the objective
and parameters of Decoding Game, near-optimal strategies encompass diverse
methods such as greedy search, temperature scaling, and hybrids thereof.
Numerical experiments are conducted to complement our theoretical analysis."
Variational Language Concepts for Interpreting Foundation Language Models,cs.LG,Machine Learning,2024-10-04,"Foundation Language Models (FLMs) such as BERT and its variants have achieved
remarkable success in natural language processing. To date, the
interpretability of FLMs has primarily relied on the attention weights in their
self-attention layers. However, these attention weights only provide word-level
interpretations, failing to capture higher-level structures, and are therefore
lacking in readability and intuitiveness. To address this challenge, we first
provide a formal definition of conceptual interpretation and then propose a
variational Bayesian framework, dubbed VAriational Language Concept (VALC), to
go beyond word-level interpretations and provide concept-level interpretations.
Our theoretical analysis shows that our VALC finds the optimal language
concepts to interpret FLM predictions. Empirical results on several real-world
datasets show that our method can successfully provide conceptual
interpretation for FLMs."
SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation,cs.LG,Machine Learning,2024-10-04,"LLM inference for popular enterprise use cases, such as summarization, RAG,
and code-generation, typically observes orders of magnitude longer prompt
lengths than generation lengths. This characteristic leads to high cost of
prefill and increased response latency. In this paper, we present SwiftKV, a
novel model transformation and distillation procedure specifically designed to
reduce the time and cost of processing prompt tokens while preserving high
quality of generated tokens. SwiftKV combines three key mechanisms: i)
SingleInputKV, which prefills later layers' KV cache using a much earlier
layer's output, allowing prompt tokens to skip much of the model computation,
ii) AcrossKV, which merges the KV caches of neighboring layers to reduce the
memory footprint and support larger batch size for higher throughput, and iii)
a knowledge-preserving distillation procedure that can adapt existing LLMs for
SwiftKV with minimal accuracy impact and low compute and data requirement. For
Llama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%
and the memory requirement of the KV cache by 62.5% while incurring minimum
quality degradation across a wide range of tasks. In the end-to-end inference
serving using an optimized vLLM implementation, SwiftKV realizes up to 2x
higher aggregate throughput and 60% lower time per output token. It can achieve
a staggering 560 TFlops/GPU of normalized inference throughput, which
translates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100
GPUs."
Grounding Language in Multi-Perspective Referential Communication,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"We introduce a task and dataset for referring expression generation and
comprehension in multi-agent embodied environments. In this task, two agents in
a shared scene must take into account one another's visual perspective, which
may be different from their own, to both produce and understand references to
objects in a scene and the spatial relations between them. We collect a dataset
of 2,970 human-written referring expressions, each paired with human
comprehension judgments, and evaluate the performance of automated models as
speakers and listeners paired with human partners, finding that model
performance in both reference generation and comprehension lags behind that of
pairs of human agents. Finally, we experiment training an open-weight speaker
model with evidence of communicative success when paired with a listener,
resulting in an improvement from 58.9 to 69.3% in communicative success and
even outperforming the strongest proprietary model."
Model Developmental Safety: A Safety-Centric Method and Applications in Vision-Language Models,cs.LG,Machine Learning,2024-10-04,"In the real world, a learning-enabled system usually undergoes multiple
cycles of model development to enhance the system's ability to handle difficult
or emerging tasks. This continual model development process raises a
significant issue that the model development for acquiring new or improving
existing capabilities may inadvertently lose capabilities of the old model,
also known as catastrophic forgetting. Existing continual learning studies
focus on mitigating catastrophic forgetting by trading off performance on
previous tasks and new tasks to ensure good average performance. However, they
are inadequate for many applications especially in safety-critical domains, as
failure to strictly preserve the performance of the old model not only
introduces safety risks and uncertainties but also imposes substantial expenses
in the re-improving and re-validation of existing properties. To address this
issue, we introduce model developmental safety as a guarantee of a learning
system such that in the model development process the new model should strictly
preserve the existing protected capabilities of the old model while improving
its performance on target tasks. To ensure the model developmental safety, we
present a safety-centric framework by formulating the model developmental
safety as data-dependent constraints. Under this framework, we study how to
develop a pretrained vision-language model (aka the CLIP model) for acquiring
new capabilities or improving existing capabilities of image classification. We
propose an efficient constrained optimization algorithm with theoretical
guarantee and use its insights to finetune a CLIP model with task-dependent
heads for promoting the model developmental safety. Our experiments on
improving vision perception capabilities on autonomous driving and scene
recognition datasets demonstrate the efficacy of the proposed approach."
SDA-GRIN for Adaptive Spatial-Temporal Multivariate Time Series Imputation,cs.LG,Machine Learning,2024-10-04,"In various applications, the multivariate time series often suffers from
missing data. This issue can significantly disrupt systems that rely on the
data. Spatial and temporal dependencies can be leveraged to impute the missing
samples. Existing imputation methods often ignore dynamic changes in spatial
dependencies. We propose a Spatial Dynamic Aware Graph Recurrent Imputation
Network (SDA-GRIN) which is capable of capturing dynamic changes in spatial
dependencies.SDA-GRIN leverages a multi-head attention mechanism to adapt graph
structures with time. SDA-GRIN models multivariate time series as a sequence of
temporal graphs and uses a recurrent message-passing architecture for
imputation. We evaluate SDA-GRIN on four real-world datasets: SDA-GRIN improves
MSE by 9.51% for the AQI and 9.40% for AQI-36. On the PEMS-BAY dataset, it
achieves a 1.94% improvement in MSE. Detailed ablation study demonstrates the
effect of window sizes and missing data on the performance of the method.
Project page:https://ameskandari.github.io/sda-grin/"
LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Combining large language models during training or at inference time has
shown substantial performance gain over component LLMs. This paper presents
LLM-TOPLA, a diversity-optimized LLM ensemble method with three unique
properties: (i) We introduce the focal diversity metric to capture the
diversity-performance correlation among component LLMs of an ensemble. (ii) We
develop a diversity-optimized ensemble pruning algorithm to select the top-k
sub-ensembles from a pool of $N$ base LLMs. Our pruning method recommends
top-performing LLM subensembles of size $S$, often much smaller than $N$. (iii)
We generate new output for each prompt query by utilizing a learn-to-ensemble
approach, which learns to detect and resolve the output inconsistency among all
component LLMs of an ensemble. Extensive evaluation on four different
benchmarks shows good performance gain over the best LLM ensemble methods: (i)
In constrained solution set problems, LLM-TOPLA outperforms the best-performing
ensemble (Mixtral) by 2.2\% in accuracy on MMLU and the best-performing LLM
ensemble (MoreAgent) on GSM8k by 2.1\%. (ii) In generative tasks, LLM-TOPLA
outperforms the top-2 performers (Llama70b/Mixtral) on SearchQA by
$3.9\mathrm{x}$ in F1, and on XSum by more than $38$ in ROUGE-1. Our code and
dataset, which contains outputs of 8 modern LLMs on 4 benchmarks is available
at https://github.com/git-disl/llm-topla"
A Brain-Inspired Regularizer for Adversarial Robustness,cs.LG,Machine Learning,2024-10-04,"Convolutional Neural Networks (CNNs) excel in many visual tasks, but they
tend to be sensitive to slight input perturbations that are imperceptible to
the human eye, often resulting in task failures. Recent studies indicate that
training CNNs with regularizers that promote brain-like representations, using
neural recordings, can improve model robustness. However, the requirement to
use neural data severely restricts the utility of these methods. Is it possible
to develop regularizers that mimic the computational function of neural
regularizers without the need for neural recordings, thereby expanding the
usability and effectiveness of these techniques? In this work, we inspect a
neural regularizer introduced in Li et al. (2019) to extract its underlying
strength. The regularizer uses neural representational similarities, which we
find also correlate with pixel similarities. Motivated by this finding, we
introduce a new regularizer that retains the essence of the original but is
computed using image pixel similarities, eliminating the need for neural
recordings. We show that our regularization method 1) significantly increases
model robustness to a range of black box attacks on various datasets and 2) is
computationally inexpensive and relies only on original datasets. Our work
explores how biologically motivated loss functions can be used to drive the
performance of artificial neural networks."
UFLUX v2.0: A Process-Informed Machine Learning Framework for Efficient and Explainable Modelling of Terrestrial Carbon Uptake,cs.LG,Machine Learning,2024-10-04,"Gross Primary Productivity (GPP), the amount of carbon plants fixed by
photosynthesis, is pivotal for understanding the global carbon cycle and
ecosystem functioning. Process-based models built on the knowledge of
ecological processes are susceptible to biases stemming from their assumptions
and approximations. These limitations potentially result in considerable
uncertainties in global GPP estimation, which may pose significant challenges
to our Net Zero goals. This study presents UFLUX v2.0, a process-informed model
that integrates state-of-art ecological knowledge and advanced machine learning
techniques to reduce uncertainties in GPP estimation by learning the biases
between process-based models and eddy covariance (EC) measurements. In our
findings, UFLUX v2.0 demonstrated a substantial improvement in model accuracy,
achieving an R^2 of 0.79 with a reduced RMSE of 1.60 g C m^-2 d^-1, compared to
the process-based model's R^2 of 0.51 and RMSE of 3.09 g C m^-2 d^-1. Our
global GPP distribution analysis indicates that while UFLUX v2.0 and the
process-based model achieved similar global total GPP (137.47 Pg C and 132.23
Pg C, respectively), they exhibited large differences in spatial distribution,
particularly in latitudinal gradients. These differences are very likely due to
systematic biases in the process-based model and differing sensitivities to
climate and environmental conditions. This study offers improved adaptability
for GPP modelling across diverse ecosystems, and further enhances our
understanding of global carbon cycles and its responses to environmental
changes."
Structured List-Grounded Question Answering,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Document-grounded dialogue systems aim to answer user queries by leveraging
external information. Previous studies have mainly focused on handling
free-form documents, often overlooking structured data such as lists, which can
represent a range of nuanced semantic relations. Motivated by the observation
that even advanced language models like GPT-3.5 often miss semantic cues from
lists, this paper aims to enhance question answering (QA) systems for better
interpretation and use of structured lists. To this end, we introduce the
LIST2QA dataset, a novel benchmark to evaluate the ability of QA systems to
respond effectively using list information. This dataset is created from
unlabeled customer service documents using language models and model-based
filtering processes to enhance data quality, and can be used to fine-tune and
evaluate QA models. Apart from directly generating responses through fine-tuned
models, we further explore the explicit use of Intermediate Steps for Lists
(ISL), aligning list items with user backgrounds to better reflect how humans
interpret list items before generating responses. Our experimental results
demonstrate that models trained on LIST2QA with our ISL approach outperform
baselines across various metrics. Specifically, our fine-tuned Flan-T5-XL model
shows increases of 3.1% in ROUGE-L, 4.6% in correctness, 4.5% in faithfulness,
and 20.6% in completeness compared to models without applying filtering and the
proposed ISL method."
"Unidirectional Key Update in Updatable Encryption, Revisited",cs.CR,Cryptography and Security,2024-10-04,"In this paper we construct a new efficient updatable encryption (UE) scheme
based on FrodoPKE learning with errors key encapsulation. We analyse the
security of the proposed scheme in the backward-leak uni-directional setting
within the rand-ind-eu-cpa model. Since the underlying computationally hard
problem here is LWE, the scheme is secure against both classical and quantum
attacks."
Interpolation-Free Deep Learning for Meteorological Downscaling on Unaligned Grids Across Multiple Domains with Application to Wind Power,cs.LG,Machine Learning,2024-10-04,"As climate change intensifies, the shift to cleaner energy sources becomes
increasingly urgent. With wind energy production set to accelerate, reliable
wind probabilistic forecasts are essential to ensure its efficient use.
However, since numerical weather prediction models are computationally
expensive, probabilistic forecasts are produced at resolutions too coarse to
capture all mesoscale wind behaviors. Statistical downscaling, typically
applied to enchance the resolution of climate model simulations, presents a
viable solution with lower computational costs by learning a mapping from
low-resolution (LR) variables to high-resolution (HR) meteorological variables.
Leveraging deep learning, we evaluate a downscaling model based on a
state-of-the-art U-Net architecture, applied to an ensemble member from a
coarse-scale probabilistic forecast of wind velocity. The architecture is
modified to incorporate (1) a learned grid alignment strategy to resolve LR-HR
grid mismatches and (2) a processing module for multi-level atmospheric
predictors. To extend the downscaling model's applicability from fixed spatial
domains to the entire Canadian region, we assess a transfer learning approach.
Our results show that the learned grid alignment strategy performs as well as
conventional pre-processing interpolation steps and that LR wind speed at
multiple levels is sufficient as a predictor, enabling a more compact
architecture. Additionally, they suggest that extending to new spatial domains
using transfer learning is promising, and that downscaled wind velocities
demonstrate potential in improving the detection of wind power ramps, a
critical phenomenon for wind energy."
Oscillatory State-Space Models,cs.LG,Machine Learning,2024-10-04,"We propose Linear Oscillatory State-Space models (LinOSS) for efficiently
learning on long sequences. Inspired by cortical dynamics of biological neural
networks, we base our proposed LinOSS model on a system of forced harmonic
oscillators. A stable discretization, integrated over time using fast
associative parallel scans, yields the proposed state-space model. We prove
that LinOSS produces stable dynamics only requiring nonnegative diagonal state
matrix. This is in stark contrast to many previous state-space models relying
heavily on restrictive parameterizations. Moreover, we rigorously show that
LinOSS is universal, i.e., it can approximate any continuous and causal
operator mapping between time-varying functions, to desired accuracy. In
addition, we show that an implicit-explicit discretization of LinOSS perfectly
conserves the symmetry of time reversibility of the underlying dynamics.
Together, these properties enable efficient modeling of long-range
interactions, while ensuring stable and accurate long-horizon forecasting.
Finally, our empirical results, spanning a wide range of time-series tasks from
mid-range to very long-range classification and regression, as well as
long-horizon forecasting, demonstrate that our proposed LinOSS model
consistently outperforms state-of-the-art sequence models. Notably, LinOSS
outperforms Mamba by nearly 2x and LRU by 2.5x on a sequence modeling task with
sequences of length 50k."
AutoLoRA: AutoGuidance Meets Low-Rank Adaptation for Diffusion Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Low-rank adaptation (LoRA) is a fine-tuning technique that can be applied to
conditional generative diffusion models. LoRA utilizes a small number of
context examples to adapt the model to a specific domain, character, style, or
concept. However, due to the limited data utilized during training, the
fine-tuned model performance is often characterized by strong context bias and
a low degree of variability in the generated images. To solve this issue, we
introduce AutoLoRA, a novel guidance technique for diffusion models fine-tuned
with the LoRA approach. Inspired by other guidance techniques, AutoLoRA
searches for a trade-off between consistency in the domain represented by LoRA
weights and sample diversity from the base conditional diffusion model.
Moreover, we show that incorporating classifier-free guidance for both LoRA
fine-tuned and base models leads to generating samples with higher diversity
and better quality. The experimental results for several fine-tuned LoRA
domains show superiority over existing guidance techniques on selected metrics."
"A Feasibility Study of a Soft, Low-Cost, 6-Axis Load Cell for Haptics",cs.RO,Robotics,2024-10-04,"Haptic devices have shown to be valuable in supplementing surgical training,
especially when providing haptic feedback based on user performance metrics
such as wrench applied by the user on the tool. However, current 6-axis
force/torque sensors are prohibitively expensive. This paper presents the
design and calibration of a low-cost, six-axis force/torque sensor specially
designed for laparoscopic haptic training applications. The proposed design
uses Hall-effect sensors to measure the change in the position of magnets
embedded in a silicone layer that results from an applied wrench to the device.
Preliminary experimental validation demonstrates that these sensors can achieve
an accuracy of 0.45 N and 0.014 Nm, and a theoretical XY range of +/-50N, Z
range of +/-20N, and torque range of +/-0.2Nm. This study indicates that the
proposed low-cost 6-axis force/torque sensor can accurately measure user force
and provide useful feedback during laparoscopic training on a haptic device."
Clustering Alzheimer's Disease Subtypes via Similarity Learning and Graph Diffusion,cs.LG,Machine Learning,2024-10-04,"Alzheimer's disease (AD) is a complex neurodegenerative disorder that affects
millions of people worldwide. Due to the heterogeneous nature of AD, its
diagnosis and treatment pose critical challenges. Consequently, there is a
growing research interest in identifying homogeneous AD subtypes that can
assist in addressing these challenges in recent years. In this study, we aim to
identify subtypes of AD that represent distinctive clinical features and
underlying pathology by utilizing unsupervised clustering with graph diffusion
and similarity learning. We adopted SIMLR, a multi-kernel similarity learning
framework, and graph diffusion to perform clustering on a group of 829 patients
with AD and mild cognitive impairment (MCI, a prodromal stage of AD) based on
their cortical thickness measurements extracted from magnetic resonance imaging
(MRI) scans. Although the clustering approach we utilized has not been explored
for the task of AD subtyping before, it demonstrated significantly better
performance than several commonly used clustering methods. Specifically, we
showed the power of graph diffusion in reducing the effects of noise in the
subtype detection. Our results revealed five subtypes that differed remarkably
in their biomarkers, cognitive status, and some other clinical features. To
evaluate the resultant subtypes further, a genetic association study was
carried out and successfully identified potential genetic underpinnings of
different AD subtypes. Our source code is available at:
https://github.com/PennShenLab/AD-SIMLR."
Learning Truncated Causal History Model for Video Restoration,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"One key challenge to video restoration is to model the transition dynamics of
video frames governed by motion. In this work, we propose TURTLE to learn the
truncated causal history model for efficient and high-performing video
restoration. Unlike traditional methods that process a range of contextual
frames in parallel, TURTLE enhances efficiency by storing and summarizing a
truncated history of the input frame latent representation into an evolving
historical state. This is achieved through a sophisticated similarity-based
retrieval mechanism that implicitly accounts for inter-frame motion and
alignment. The causal design in TURTLE enables recurrence in inference through
state-memorized historical features while allowing parallel training by
sampling truncated video clips. We report new state-of-the-art results on a
multitude of video restoration benchmark tasks, including video desnowing,
nighttime video deraining, video raindrops and rain streak removal, video
super-resolution, real-world and synthetic video deblurring, and blind video
denoising while reducing the computational cost compared to existing best
contextual methods on all these tasks."
GAS-Norm: Score-Driven Adaptive Normalization for Non-Stationary Time Series Forecasting in Deep Learning,cs.LG,Machine Learning,2024-10-04,"Despite their popularity, deep neural networks (DNNs) applied to time series
forecasting often fail to beat simpler statistical models. One of the main
causes of this suboptimal performance is the data non-stationarity present in
many processes. In particular, changes in the mean and variance of the input
data can disrupt the predictive capability of a DNN. In this paper, we first
show how DNN forecasting models fail in simple non-stationary settings. We then
introduce GAS-Norm, a novel methodology for adaptive time series normalization
and forecasting based on the combination of a Generalized Autoregressive Score
(GAS) model and a Deep Neural Network. The GAS approach encompasses a
score-driven family of models that estimate the mean and variance at each new
observation, providing updated statistics to normalize the input data of the
deep model. The output of the DNN is eventually denormalized using the
statistics forecasted by the GAS model, resulting in a hybrid approach that
leverages the strengths of both statistical modeling and deep learning. The
adaptive normalization improves the performance of the model in non-stationary
settings. The proposed approach is model-agnostic and can be applied to any DNN
forecasting model. To empirically validate our proposal, we first compare
GAS-Norm with other state-of-the-art normalization methods. We then combine it
with state-of-the-art DNN forecasting models and test them on real-world
datasets from the Monash open-access forecasting repository. Results show that
deep forecasting models improve their performance in 21 out of 25 settings when
combined with GAS-Norm compared to other normalization methods."
Reverb: Open-Source ASR and Diarization from Rev,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Today, we are open-sourcing our core speech recognition and diarization
models for non-commercial use. We are releasing both a full production pipeline
for developers as well as pared-down research models for experimentation. Rev
hopes that these releases will spur research and innovation in the fast-moving
domain of voice technology. The speech recognition models released today
outperform all existing open source speech recognition models across a variety
of long-form speech recognition domains."
An Intelligent Native Network Slicing Security Architecture Empowered by Federated Learning,cs.CR,Cryptography and Security,2024-10-04,"Network Slicing (NS) has transformed the landscape of resource sharing in
networks, offering flexibility to support services and applications with highly
variable requirements in areas such as the next-generation 5G/6G mobile
networks (NGMN), vehicular networks, industrial Internet of Things (IoT), and
verticals. Although significant research and experimentation have driven the
development of network slicing, existing architectures often fall short in
intrinsic architectural intelligent security capabilities. This paper proposes
an architecture-intelligent security mechanism to improve the NS solutions. We
idealized a security-native architecture that deploys intelligent microservices
as federated agents based on machine learning, providing intra-slice and
architectural operation security for the Slicing Future Internet
Infrastructures (SFI2) reference architecture. It is noteworthy that federated
learning approaches match the highly distributed modern microservice-based
architectures, thus providing a unifying and scalable design choice for NS
platforms addressing both service and security. Using ML-Agents and Security
Agents, our approach identified Distributed Denial-of-Service (DDoS) and
intrusion attacks within the slice using generic and non-intrusive telemetry
records, achieving an average accuracy of approximately $95.60\%$ in the
network slicing architecture and $99.99\%$ for the deployed slice --
intra-slice. This result demonstrates the potential for leveraging
architectural operational security and introduces a promising new research
direction for network slicing architectures."
C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"The development of tools and techniques to analyze and extract organizations
data habits from privacy policies are critical for scalable regulatory
compliance audits. Unfortunately, these tools are becoming increasingly limited
in their ability to identify compliance issues and fixes. After all, most were
developed using regulation-agnostic datasets of annotated privacy policies
obtained from a time before the introduction of landmark privacy regulations
such as EUs GDPR and Californias CCPA. In this paper, we describe the first
open regulation-aware dataset of expert-annotated privacy policies, C3PA (CCPA
Privacy Policy Provision Annotations), aimed to address this challenge. C3PA
contains over 48K expert-labeled privacy policy text segments associated with
responses to CCPA-specific disclosure mandates from 411 unique organizations.
We demonstrate that the C3PA dataset is uniquely suited for aiding automated
audits of compliance with CCPA-related disclosure mandates."
Question-Answering System for Bangla: Fine-tuning BERT-Bangla for a Closed Domain,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Question-answering systems for Bengali have seen limited development,
particularly in domain-specific applications. Leveraging advancements in
natural language processing, this paper explores a fine-tuned BERT-Bangla model
to address this gap. It presents the development of a question-answering system
for Bengali using a fine-tuned BERT-Bangla model in a closed domain. The
dataset was sourced from Khulna University of Engineering \& Technology's
(KUET) website and other relevant texts. The system was trained and evaluated
with 2500 question-answer pairs generated from curated data. Key metrics,
including the Exact Match (EM) score and F1 score, were used for evaluation,
achieving scores of 55.26\% and 74.21\%, respectively. The results demonstrate
promising potential for domain-specific Bengali question-answering systems.
Further refinements are needed to improve performance for more complex queries."
ConceptLens: from Pixels to Understanding,cs.LG,Machine Learning,2024-10-04,"ConceptLens is an innovative tool designed to illuminate the intricate
workings of deep neural networks (DNNs) by visualizing hidden neuron
activations. By integrating deep learning with symbolic methods, ConceptLens
offers users a unique way to understand what triggers neuron activations and
how they respond to various stimuli. The tool uses error-margin analysis to
provide insights into the confidence levels of neuron activations, thereby
enhancing the interpretability of DNNs. This paper presents an overview of
ConceptLens, its implementation, and its application in real-time visualization
of neuron activations and error margins through bar charts."
Learning Object Properties Using Robot Proprioception via Differentiable Robot-Object Interaction,cs.RO,Robotics,2024-10-04,"Differentiable simulation has become a powerful tool for system
identification. While prior work has focused on identifying robot properties
using robot-specific data or object properties using object-specific data, our
approach calibrates object properties by using information from the robot,
without relying on data from the object itself. Specifically, we utilize robot
joint encoder information, which is commonly available in standard robotic
systems. Our key observation is that by analyzing the robot's reactions to
manipulated objects, we can infer properties of those objects, such as inertia
and softness. Leveraging this insight, we develop differentiable simulations of
robot-object interactions to inversely identify the properties of the
manipulated objects. Our approach relies solely on proprioception -- the
robot's internal sensing capabilities -- and does not require external
measurement tools or vision-based tracking systems. This general method is
applicable to any articulated robot and requires only joint position
information. We demonstrate the effectiveness of our method on a low-cost
robotic platform, achieving accurate mass and elastic modulus estimations of
manipulated objects with just a few seconds of computation on a laptop."
Online Posterior Sampling with a Diffusion Prior,cs.LG,Machine Learning,2024-10-04,"Posterior sampling in contextual bandits with a Gaussian prior can be
implemented exactly or approximately using the Laplace approximation. The
Gaussian prior is computationally efficient but it cannot describe complex
distributions. In this work, we propose approximate posterior sampling
algorithms for contextual bandits with a diffusion model prior. The key idea is
to sample from a chain of approximate conditional posteriors, one for each
stage of the reverse process, which are estimated in a closed form using the
Laplace approximation. Our approximations are motivated by posterior sampling
with a Gaussian prior, and inherit its simplicity and efficiency. They are
asymptotically consistent and perform well empirically on a variety of
contextual bandit problems."
STONE: A Submodular Optimization Framework for Active 3D Object Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"3D object detection is fundamentally important for various emerging
applications, including autonomous driving and robotics. A key requirement for
training an accurate 3D object detector is the availability of a large amount
of LiDAR-based point cloud data. Unfortunately, labeling point cloud data is
extremely challenging, as accurate 3D bounding boxes and semantic labels are
required for each potential object. This paper proposes a unified active 3D
object detection framework, for greatly reducing the labeling cost of training
3D object detector. Our framework is based on a novel formulation of submodular
optimization, specifically tailored to the problem of active 3D object
detection. In particular, we address two fundamental challenges associated with
active 3D object detection: data imbalance and the need to cover the
distribution of the data, including LiDAR-based point cloud data of varying
difficulty levels. Extensive experiments demonstrate that our method achieves
state-of-the-art performance with high computational efficiency compared to
existing active learning methods."
Multi-Objective Risk Assessment Framework for Exploration Planning Using Terrain and Traversability Analysis,cs.RO,Robotics,2024-10-04,"Exploration of unknown, unstructured environments, such as in search and
rescue, cave exploration, and planetary missions,presents significant
challenges due to their unpredictable nature. This unpredictability can lead to
inefficient path planning and potential mission failures. We propose a
multi-objective risk assessment method for exploration planning in such
unconstrained environments. Our approach dynamically adjusts the weight of
various risk factors to prevent the robot from undertaking lethal actions too
early in the mission. By gradually increasing the allowable risk as the mission
progresses, our method enables more efficient exploration. We evaluate risk
based on environmental terrain properties, including elevation, slope,
roughness, and traversability, and account for factors like battery life,
mission duration, and travel distance. Our method is validated through
experiments in various subterranean simulated cave environments. The results
demonstrate that our approach ensures consistent exploration without incurring
lethal actions, while introducing minimal computational overhead to the
planning process."
Distribution Guided Active Feature Acquisition,cs.LG,Machine Learning,2024-10-04,"Human agents routinely reason on instances with incomplete and muddied data
(and weigh the cost of obtaining further features). In contrast, much of ML is
devoted to the unrealistic, sterile environment where all features are observed
and further information on an instance is obviated. Here we extend past static
ML and develop an active feature acquisition (AFA) framework that interacts
with the environment to obtain new information on-the-fly and can: 1) make
inferences on an instance in the face of incomplete features, 2) determine a
plan for feature acquisitions to obtain additional information on the instance
at hand. We build our AFA framework on a backbone of understanding the
information and conditional dependencies that are present in the data. First,
we show how to build generative models that can capture dependencies over
arbitrary subsets of features and employ these models for acquisitions in a
greedy scheme. After, we show that it is possible to guide the training of RL
agents for AFA via side-information and auxiliary rewards stemming from our
generative models. We also examine two important factors for deploying AFA
models in real-world scenarios, namely interpretability and robustness.
Extensive experiments demonstrate the state-of-the-art performance of our AFA
framework."
Improving Efficiency of Sampling-based Motion Planning via Message-Passing Monte Carlo,cs.RO,Robotics,2024-10-04,"Sampling-based motion planning methods, while effective in high-dimensional
spaces, often suffer from inefficiencies due to irregular sampling
distributions, leading to suboptimal exploration of the configuration space. In
this paper, we propose an approach that enhances the efficiency of these
methods by utilizing low-discrepancy distributions generated through
Message-Passing Monte Carlo (MPMC). MPMC leverages Graph Neural Networks (GNNs)
to generate point sets that uniformly cover the space, with uniformity assessed
using the the $\cL_p$-discrepancy measure, which quantifies the irregularity of
sample distributions. By improving the uniformity of the point sets, our
approach significantly reduces computational overhead and the number of samples
required for solving motion planning problems. Experimental results demonstrate
that our method outperforms traditional sampling techniques in terms of
planning efficiency."
Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"In this study, we introduce ANGST, a novel, first-of-its kind benchmark for
depression-anxiety comorbidity classification from social media posts. Unlike
contemporary datasets that often oversimplify the intricate interplay between
different mental health disorders by treating them as isolated conditions,
ANGST enables multi-label classification, allowing each post to be
simultaneously identified as indicating depression and/or anxiety. Comprising
2876 meticulously annotated posts by expert psychologists and an additional
7667 silver-labeled posts, ANGST posits a more representative sample of online
mental health discourse. Moreover, we benchmark ANGST using various
state-of-the-art language models, ranging from Mental-BERT to GPT-4. Our
results provide significant insights into the capabilities and limitations of
these models in complex diagnostic scenarios. While GPT-4 generally outperforms
other models, none achieve an F1 score exceeding 72% in multi-class comorbid
classification, underscoring the ongoing challenges in applying language models
to mental health diagnostics."
ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Large language models~(LLMs) have been adopted to process textual task
description and accomplish procedural planning in embodied AI tasks because of
their powerful reasoning ability. However, there is still lack of study on how
vision language models~(VLMs) behave when multi-modal task inputs are
considered. Counterfactual planning that evaluates the model's reasoning
ability over alternative task situations are also under exploited. In order to
evaluate the planning ability of both multi-modal and counterfactual aspects,
we propose ActPlan-1K. ActPlan-1K is a multi-modal planning benchmark
constructed based on ChatGPT and household activity simulator iGibson2. The
benchmark consists of 153 activities and 1,187 instances. Each instance
describing one activity has a natural language task description and multiple
environment images from the simulator. The gold plan of each instance is action
sequences over the objects in provided scenes. Both the correctness and
commonsense satisfaction are evaluated on typical VLMs. It turns out that
current VLMs are still struggling at generating human-level procedural plans
for both normal activities and counterfactual activities. We further provide
automatic evaluation metrics by finetuning over BLEURT model to facilitate
future research on our benchmark."
An Approach To Enhance IoT Security In 6G Networks Through Explainable AI,cs.CR,Cryptography and Security,2024-10-04,"Wireless communication has evolved significantly, with 6G offering
groundbreaking capabilities, particularly for IoT. However, the integration of
IoT into 6G presents new security challenges, expanding the attack surface due
to vulnerabilities introduced by advanced technologies such as open RAN,
terahertz (THz) communication, IRS, massive MIMO, and AI. Emerging threats like
AI exploitation, virtualization risks, and evolving attacks, including data
manipulation and signal interference, further complicate security efforts. As
6G standards are set to be finalized by 2030, work continues to align security
measures with technological advances. However, substantial gaps remain in
frameworks designed to secure integrated IoT and 6G systems. Our research
addresses these challenges by utilizing tree-based machine learning algorithms
to manage complex datasets and evaluate feature importance. We apply data
balancing techniques to ensure fair attack representation and use SHAP and LIME
to improve model transparency. By aligning feature importance with XAI methods
and cross-validating for consistency, we boost model accuracy and enhance IoT
security within the 6G ecosystem."
PersonalSum: A User-Subjective Guided Personalized Summarization Dataset for Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"With the rapid advancement of Natural Language Processing in recent years,
numerous studies have shown that generic summaries generated by Large Language
Models (LLMs) can sometimes surpass those annotated by experts, such as
journalists, according to human evaluations. However, there is limited research
on whether these generic summaries meet the individual needs of ordinary
people. The biggest obstacle is the lack of human-annotated datasets from the
general public. Existing work on personalized summarization often relies on
pseudo datasets created from generic summarization datasets or controllable
tasks that focus on specific named entities or other aspects, such as the
length and specificity of generated summaries, collected from hypothetical
tasks without the annotators' initiative. To bridge this gap, we propose a
high-quality, personalized, manually annotated abstractive summarization
dataset called PersonalSum. This dataset is the first to investigate whether
the focus of public readers differs from the generic summaries generated by
LLMs. It includes user profiles, personalized summaries accompanied by source
sentences from given articles, and machine-generated generic summaries along
with their sources. We investigate several personal signals - entities/topics,
plot, and structure of articles - that may affect the generation of
personalized summaries using LLMs in a few-shot in-context learning scenario.
Our preliminary results and analysis indicate that entities/topics are merely
one of the key factors that impact the diverse preferences of users, and
personalized summarization remains a significant challenge for existing LLMs."
Improving Node Representation by Boosting Target-Aware Contrastive Loss,cs.LG,Machine Learning,2024-10-04,"Graphs model complex relationships between entities, with nodes and edges
capturing intricate connections. Node representation learning involves
transforming nodes into low-dimensional embeddings. These embeddings are
typically used as features for downstream tasks. Therefore, their quality has a
significant impact on task performance. Existing approaches for node
representation learning span (semi-)supervised, unsupervised, and
self-supervised paradigms. In graph domains, (semi-)supervised learning often
only optimizes models based on class labels, neglecting other abundant graph
signals, which limits generalization. While self-supervised or unsupervised
learning produces representations that better capture underlying graph signals,
the usefulness of these captured signals for downstream target tasks can vary.
To bridge this gap, we introduce Target-Aware Contrastive Learning
(Target-aware CL) which aims to enhance target task performance by maximizing
the mutual information between the target task and node representations with a
self-supervised learning process. This is achieved through a sampling function,
XGBoost Sampler (XGSampler), to sample proper positive examples for the
proposed Target-Aware Contrastive Loss (XTCL). By minimizing XTCL, Target-aware
CL increases the mutual information between the target task and node
representations, such that model generalization is improved. Additionally,
XGSampler enhances the interpretability of each signal by showing the weights
for sampling the proper positive examples. We show experimentally that XTCL
significantly improves the performance on two target tasks: node classification
and link prediction tasks, compared to state-of-the-art models."
The Wallpaper is Ugly: Indoor Localization using Vision and Language,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"We study the task of locating a user in a mapped indoor environment using
natural language queries and images from the environment.
  Building on recent pretrained vision-language models, we learn a similarity
score between text descriptions and images of locations in the environment.
  This score allows us to identify locations that best match the language
query, estimating the user's location.
  Our approach is capable of localizing on environments, text, and images that
were not seen during training.
  One model, finetuned CLIP, outperformed humans in our evaluation."
Human-aligned Chess with a Bit of Search,cs.LG,Machine Learning,2024-10-04,"Chess has long been a testbed for AI's quest to match human intelligence, and
in recent years, chess AI systems have surpassed the strongest humans at the
game. However, these systems are not human-aligned; they are unable to match
the skill levels of all human partners or model human-like behaviors beyond
piece movement. In this paper, we introduce Allie, a chess-playing AI designed
to bridge the gap between artificial and human intelligence in this classic
game. Allie is trained on log sequences of real chess games to model the
behaviors of human chess players across the skill spectrum, including non-move
behaviors such as pondering times and resignations In offline evaluations, we
find that Allie exhibits humanlike behavior: it outperforms the existing
state-of-the-art in human chess move prediction and ""ponders"" at critical
positions. The model learns to reliably assign reward at each game state, which
can be used at inference as a reward function in a novel time-adaptive
Monte-Carlo tree search (MCTS) procedure, where the amount of search depends on
how long humans would think in the same positions. Adaptive search enables
remarkable skill calibration; in a large-scale online evaluation against
players with ratings from 1000 to 2600 Elo, our adaptive search method leads to
a skill gap of only 49 Elo on average, substantially outperforming search-free
and standard MCTS baselines. Against grandmaster-level (2500 Elo) opponents,
Allie with adaptive search exhibits the strength of a fellow grandmaster, all
while learning exclusively from humans."
Towards Cost Sensitive Decision Making,cs.LG,Machine Learning,2024-10-04,"Many real-world situations allow for the acquisition of additional relevant
information when making decisions with limited or uncertain data. However,
traditional RL approaches either require all features to be acquired beforehand
(e.g. in a MDP) or regard part of them as missing data that cannot be acquired
(e.g. in a POMDP). In this work, we consider RL models that may actively
acquire features from the environment to improve the decision quality and
certainty, while automatically balancing the cost of feature acquisition
process and the reward of task decision process. We propose the
Active-Acquisition POMDP and identify two types of the acquisition process for
different application domains. In order to assist the agent in the
actively-acquired partially-observed environment and alleviate the
exploration-exploitation dilemma, we develop a model-based approach, where a
deep generative model is utilized to capture the dependencies of the features
and impute the unobserved features. The imputations essentially represent the
beliefs of the agent. Equipped with the dynamics model, we develop hierarchical
RL algorithms to resolve both types of the AA-POMDPs. Empirical results
demonstrate that our approach achieves considerably better performance than
existing POMDP-RL solutions."
Solving Dual Sourcing Problems with Supply Mode Dependent Failure Rates,cs.LG,Machine Learning,2024-10-04,"This paper investigates dual sourcing problems with supply mode dependent
failure rates, particularly relevant in managing spare parts for
downtime-critical assets. To enhance resilience, businesses increasingly adopt
dual sourcing strategies using both conventional and additive manufacturing
techniques. This paper explores how these strategies can optimise sourcing by
addressing variations in part properties and failure rates. A significant
challenge is the distinct failure characteristics of parts produced by these
methods, which influence future demand. To tackle this, we propose a new
iterative heuristic and several reinforcement learning techniques combined with
an endogenous parameterised learning (EPL) approach. This EPL approach -
compatible with any learning method - allows a single policy to handle various
input parameters for multiple items. In a stylised setting, our best policy
achieves an average optimality gap of 0.4%. In a case study within the energy
sector, our policies outperform the baseline in 91.1% of instances, yielding
average cost savings up to 22.6%."
ShieldDiff: Suppressing Sexual Content Generation from Diffusion Models through Reinforcement Learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"With the advance of generative AI, the text-to-image (T2I) model has the
ability to generate various contents. However, the generated contents cannot be
fully controlled. There is a potential risk that T2I model can generate unsafe
images with uncomfortable contents. In our work, we focus on eliminating the
NSFW (not safe for work) content generation from T2I model while maintaining
the high quality of generated images by fine-tuning the pre-trained diffusion
model via reinforcement learning by optimizing the well-designed content-safe
reward function. The proposed method leverages a customized reward function
consisting of the CLIP (Contrastive Language-Image Pre-training) and nudity
rewards to prune the nudity contents that adhere to the pret-rained model and
keep the corresponding semantic meaning on the safe side. In this way, the T2I
model is robust to unsafe adversarial prompts since unsafe visual
representations are mitigated from latent space. Extensive experiments
conducted on different datasets demonstrate the effectiveness of the proposed
method in alleviating unsafe content generation while preserving the
high-fidelity of benign images as well as images generated by unsafe prompts.
We compare with five existing state-of-the-art (SOTA) methods and achieve
competitive performance on sexual content removal and image quality retention.
In terms of robustness, our method outperforms counterparts under the SOTA
black-box attacking model. Furthermore, our constructed method can be a
benchmark for anti-NSFW generation with semantically-relevant safe alignment."
Collaborative Safety-Critical Formation Control with Obstacle Avoidance,cs.RO,Robotics,2024-10-04,"This work explores a collaborative method for ensuring safety in multi-agent
formation control problems. We formulate a control barrier function (CBF) based
safety filter control law for a generic distributed formation controller and
extend our previously developed collaborative safety framework to an obstacle
avoidance problem for agents with acceleration control inputs. We then
incorporate multi-obstacle collision avoidance into the collaborative safety
framework. This framework includes a method for computing the maximum
capability of agents to satisfy their individual safety requirements. We
analyze the convergence rate of our collaborative safety algorithm, and prove
the linear-time convergence of cooperating agents to a jointly feasible safe
action for all agents under the special case of a tree-structured communication
network with a single obstacle for each agent. We illustrate the analytical
results via simulation on a mass-spring kinematics-based formation controller
and demonstrate the finite-time convergence of the collaborative safety
algorithm in the simple proven case, the more general case of a fully-connected
system with multiple static obstacles, and with dynamic obstacles."
KidLM: Advancing Language Models for Children -- Early Insights and Future Directions,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Recent studies highlight the potential of large language models in creating
educational tools for children, yet significant challenges remain in
maintaining key child-specific properties such as linguistic nuances, cognitive
needs, and safety standards. In this paper, we explore foundational steps
toward the development of child-specific language models, emphasizing the
necessity of high-quality pre-training data. We introduce a novel user-centric
data collection pipeline that involves gathering and validating a corpus
specifically written for and sometimes by children. Additionally, we propose a
new training objective, Stratified Masking, which dynamically adjusts masking
probabilities based on our domain-specific child language data, enabling models
to prioritize vocabulary and concepts more suitable for children. Experimental
evaluations demonstrate that our model excels in understanding lower
grade-level text, maintains safety by avoiding stereotypes, and captures
children's unique preferences. Furthermore, we provide actionable insights for
future research and development in child-specific language modeling."
DiSK: Differentially Private Optimizer with Simplified Kalman Filter for Noise Reduction,cs.LG,Machine Learning,2024-10-04,"Differential privacy (DP) offers a robust framework for safeguarding
individual data privacy. To utilize DP in training modern machine learning
models, differentially private optimizers have been widely used in recent
years. A popular approach to privatize an optimizer is to clip the individual
gradients and add sufficiently large noise to the clipped gradient. This
approach led to the development of DP optimizers that have comparable
performance with their non-private counterparts in fine-tuning tasks or in
tasks with a small number of training parameters. However, a significant
performance drop is observed when these optimizers are applied to large-scale
training. This degradation stems from the substantial noise injection required
to maintain DP, which disrupts the optimizer's dynamics. This paper introduces
DiSK, a novel framework designed to significantly enhance the performance of DP
optimizers. DiSK employs Kalman filtering, a technique drawn from control and
signal processing, to effectively denoise privatized gradients and generate
progressively refined gradient estimations. To ensure practicality for
large-scale training, we simplify the Kalman filtering process, minimizing its
memory and computational demands. We establish theoretical privacy-utility
trade-off guarantees for DiSK, and demonstrate provable improvements over
standard DP optimizers like DPSGD in terms of iteration complexity upper-bound.
Extensive experiments across diverse tasks, including vision tasks such as
CIFAR-100 and ImageNet-1k and language fine-tuning tasks such as GLUE, E2E, and
DART, validate the effectiveness of DiSK. The results showcase its ability to
significantly improve the performance of DP optimizers, surpassing
state-of-the-art results under the same privacy constraints on several
benchmarks."
SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Integrating the 3D world into large language models (3D-based LLMs) has been
a promising research direction for 3D scene understanding. However, current
3D-based LLMs fall short in situated understanding due to two key limitations:
1) existing 3D datasets are constructed from a global perspective of the 3D
scenes and lack situated context. 2) the architectures of existing 3D-based
LLMs lack explicit alignment between the spatial representations of 3D scenes
and natural language, limiting their performance in tasks requiring precise
spatial reasoning. We address these issues by introducing a scalable situated
3D dataset, named Spartun3D, that incorporates various situated spatial
reasoning tasks. Furthermore, we propose Spartun3D-LLM, built on an existing
3D-based LLM but integrated with a novel situated spatial alignment module,
aiming to enhance the alignment between 3D visual representations and their
corresponding textual descriptions. Experimental results demonstrate that both
our proposed dataset and alignment module significantly enhance the situated
spatial understanding of 3D-based LLMs."
A Federated Distributionally Robust Support Vector Machine with Mixture of Wasserstein Balls Ambiguity Set for Distributed Fault Diagnosis,cs.LG,Machine Learning,2024-10-04,"The training of classification models for fault diagnosis tasks using
geographically dispersed data is a crucial task for original parts
manufacturers (OEMs) seeking to provide long-term service contracts (LTSCs) to
their customers. Due to privacy and bandwidth constraints, such models must be
trained in a federated fashion. Moreover, due to harsh industrial settings the
data often suffers from feature and label uncertainty. Therefore, we study the
problem of training a distributionally robust (DR) support vector machine (SVM)
in a federated fashion over a network comprised of a central server and $G$
clients without sharing data. We consider the setting where the local data of
each client $g$ is sampled from a unique true distribution $\mathbb{P}_g$, and
the clients can only communicate with the central server. We propose a novel
Mixture of Wasserstein Balls (MoWB) ambiguity set that relies on local
Wasserstein balls centered at the empirical distribution of the data at each
client. We study theoretical aspects of the proposed ambiguity set, deriving
its out-of-sample performance guarantees and demonstrating that it naturally
allows for the separability of the DR problem. Subsequently, we propose two
distributed optimization algorithms for training the global FDR-SVM: i) a
subgradient method-based algorithm, and ii) an alternating direction method of
multipliers (ADMM)-based algorithm. We derive the optimization problems to be
solved by each client and provide closed-form expressions for the computations
performed by the central server during each iteration for both algorithms.
Finally, we thoroughly examine the performance of the proposed algorithms in a
series of numerical experiments utilizing both simulation data and popular
real-world datasets."
Comparative Survey of Cyber-Threat and Attack Trends and Prediction of Future Cyber-Attack Patterns,cs.CR,Cryptography and Security,2024-10-04,"This paper presents a comparative survey of cyberthreat and attack trends
starting from 2010 till date Cyber security breaches are constantly on the rise
with huge uncertainty and risks The trend is causing rife globally because of
its consequences to national security and economy With diverse interests and
motivations for various categories of threats and attacks we carried out a
comparative survey and analysis of security breaches to unravel the patterns
and predict what will shape future security challenges The diversity of attacks
and growing state actors involvement without any sort of regulation is making
cyber weapons attractive to the states States are leveraging the anonymity and
attribution flaws to hit hard on perceived adversaries thereby complicating the
cyber security equation"
From Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Self-anthropomorphism in robots manifests itself through their display of
human-like characteristics in dialogue, such as expressing preferences and
emotions. Our study systematically analyzes self-anthropomorphic expression
within various dialogue datasets, outlining the contrasts between
self-anthropomorphic and non-self-anthropomorphic responses in dialogue
systems. We show significant differences in these two types of responses and
propose transitioning from one type to the other. We also introduce
Pix2Persona, a novel dataset aimed at developing ethical and engaging AI
systems in various embodiments. This dataset preserves the original dialogues
from existing corpora and enhances them with paired responses:
self-anthropomorphic and non-self-anthropomorphic for each original bot
response. Our work not only uncovers a new category of bot responses that were
previously under-explored but also lays the groundwork for future studies about
dynamically adjusting self-anthropomorphism levels in AI systems to align with
ethical standards and user expectations."
Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Text-based image generation models, such as Stable Diffusion and DALL-E 3,
hold significant potential in content creation and publishing workflows, making
them the focus in recent years. Despite their remarkable capability to generate
diverse and vivid images, considerable efforts are being made to prevent the
generation of harmful content, such as abusive, violent, or pornographic
material. To assess the safety of existing models, we introduce a novel
jailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises
image generation models through a step-by-step editing process. Specifically,
for malicious queries that cannot bypass the safeguards with a single prompt,
we intentionally decompose the query into multiple sub-queries. The image
generation models are then prompted to generate and iteratively edit images
based on these sub-queries. To evaluate the effectiveness of our CoJ attack
method, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine
safety scenarios, three types of editing operations, and three editing
elements. Experiments on four widely-used image generation services provided by
GPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack
method can successfully bypass the safeguards of models for over 60% cases,
which significantly outperforms other jailbreaking methods (i.e., 14%).
Further, to enhance these models' safety against our CoJ attack method, we also
propose an effective prompting-based method, Think Twice Prompting, that can
successfully defend over 95% of CoJ attack. We release our dataset and code to
facilitate the AI safety research."
Can Language Models Reason about Individualistic Human Values and Preferences?,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Recent calls for pluralistic alignment emphasize that AI systems should
address the diverse needs of all people. Yet, efforts in this space often
require sorting people into fixed buckets of pre-specified diversity-defining
dimensions (e.g., demographics, personalities, communication styles), risking
smoothing out or even stereotyping the rich spectrum of individualistic
variations. To achieve an authentic representation of diversity that respects
individuality, we propose individualistic alignment. While individualistic
alignment can take various forms, in this paper, we introduce
IndieValueCatalog, a dataset transformed from the influential World Values
Survey (WVS), to study language models (LMs) on the specific challenge of
individualistic value reasoning. Specifically, given a sample of an
individual's value-expressing statements, models are tasked with predicting
their value judgments in novel cases. With IndieValueCatalog, we reveal
critical limitations in frontier LMs' abilities to reason about individualistic
human values with accuracies, only ranging between 55% to 65%. Moreover, our
results highlight that a precise description of individualistic values cannot
be approximated only via demographic information. We also identify a partiality
of LMs in reasoning about global individualistic values, as measured by our
proposed Value Inequity Index ({\sigma}INEQUITY). Finally, we train a series of
Individualistic Value Reasoners (IndieValueReasoner) using IndieValueCatalog to
enhance models' individualistic value reasoning capability, revealing new
patterns and dynamics into global human values. We outline future research
challenges and opportunities for advancing individualistic alignment."
Empowering Domain-Specific Language Models with Graph-Oriented Databases: A Paradigm Shift in Performance and Model Maintenance,cs.AI,Artificial Intelligence,2024-10-04,"In an era dominated by data, the management and utilization of
domain-specific language have emerged as critical challenges in various
application domains, particularly those with industry-specific requirements.
Our work is driven by the need to effectively manage and process large volumes
of short text documents inherent in specific application domains. By leveraging
domain-specific knowledge and expertise, our approach aims to shape factual
data within these domains, thereby facilitating enhanced utilization and
understanding by end-users. Central to our methodology is the integration of
domain-specific language models with graph-oriented databases, facilitating
seamless processing, analysis, and utilization of textual data within targeted
domains. Our work underscores the transformative potential of the partnership
of domain-specific language models and graph-oriented databases. This
cooperation aims to assist researchers and engineers in metric usage,
mitigation of latency issues, boosting explainability, enhancing debug and
improving overall model performance. Moving forward, we envision our work as a
guide AI engineers, providing valuable insights for the implementation of
domain-specific language models in conjunction with graph-oriented databases,
and additionally provide valuable experience in full-life cycle maintenance of
this kind of products."
DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search,cs.AI,Artificial Intelligence,2024-10-04,"Enhancing the capability of large language models (LLMs) in reasoning has
gained significant attention in recent years. Previous studies have
demonstrated the effectiveness of various prompting strategies in aiding LLMs
in reasoning (called ""reasoning actions""), such as step-by-step thinking,
reflecting before answering, solving with programs, and their combinations.
However, these approaches often applied static, predefined reasoning actions
uniformly to all questions, without considering the specific characteristics of
each question or the capability of the task-solving LLM. In this paper, we
propose DOTS, an approach enabling LLMs to reason dynamically via optimal
reasoning trajectory search, tailored to the specific characteristics of each
question and the inherent capability of the task-solving LLM. Our approach
involves three key steps: i) defining atomic reasoning action modules that can
be composed into various reasoning action trajectories; ii) searching for the
optimal action trajectory for each training question through iterative
exploration and evaluation for the specific task-solving LLM; and iii) using
the collected optimal trajectories to train an LLM to plan for the reasoning
trajectories of unseen questions. In particular, we propose two learning
paradigms, i.e., fine-tuning an external LLM as a planner to guide the
task-solving LLM, or directly fine-tuning the task-solving LLM with an
internalized capability for reasoning actions planning. Our experiments across
eight reasoning tasks show that our method consistently outperforms static
reasoning techniques and the vanilla instruction tuning approach. Further
analysis reveals that our method enables LLMs to adjust their computation based
on problem complexity, allocating deeper thinking and reasoning to harder
problems."
Improving Mapper's Robustness by Varying Resolution According to Lens-Space Density,cs.LG,Machine Learning,2024-10-04,"We propose an improvement to the Mapper algorithm that removes the assumption
of a single resolution scale across semantic space, and improves the robustness
of the results under change of parameters. This eases parameter selection,
especially for datasets with highly variable local density in the Morse
function $f$ used for Mapper. This is achieved by incorporating this density
into the choice of cover for Mapper. Furthermore, we prove that for covers with
some natural hypotheses, the graph output by Mapper still converges in
bottleneck distance to the Reeb graph of the Rips complex of the data, but
captures more topological features than when using the usual Mapper cover.
Finally, we discuss implementation details, and include the results of
computational experiments. We also provide an accompanying reference
implementation."
Refinement of Monocular Depth Maps via Multi-View Differentiable Rendering,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"The accurate reconstruction of per-pixel depth for an image is vital for many
tasks in computer graphics, computer vision, and robotics. In this paper, we
present a novel approach to generate view consistent and detailed depth maps
from a number of posed images. We leverage advances in monocular depth
estimation, which generate topologically complete, but metrically inaccurate
depth maps and refine them in a two-stage optimization process based on a
differentiable renderer. Taking the monocular depth map as input, we first
scale this map to absolute distances based on structure-from-motion and
transform the depths to a triangle surface mesh. We then refine this depth mesh
in a local optimization, enforcing photometric and geometric consistency.
  Our evaluation shows that our method is able to generate dense, detailed,
high-quality depth maps, also in challenging indoor scenarios, and outperforms
state-of-the-art depth reconstruction approaches. Overview and supplemental
material of this project can be found at https://lorafib.github.io/ref_depth/."
MDMP: Multi-modal Diffusion for supervised Motion Predictions with uncertainty,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"This paper introduces a Multi-modal Diffusion model for Motion Prediction
(MDMP) that integrates and synchronizes skeletal data and textual descriptions
of actions to generate refined long-term motion predictions with quantifiable
uncertainty. Existing methods for motion forecasting or motion generation rely
solely on either prior motions or text prompts, facing limitations with
precision or control, particularly over extended durations. The multi-modal
nature of our approach enhances the contextual understanding of human motion,
while our graph-based transformer framework effectively capture both spatial
and temporal motion dynamics. As a result, our model consistently outperforms
existing generative techniques in accurately predicting long-term motions.
Additionally, by leveraging diffusion models' ability to capture different
modes of prediction, we estimate uncertainty, significantly improving spatial
awareness in human-robot interactions by incorporating zones of presence with
varying confidence levels for each body joint."
SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Autonomous systems for software engineering are now capable of fixing bugs
and developing features. These systems are commonly evaluated on SWE-bench
(Jimenez et al., 2024a), which assesses their ability to solve software issues
from GitHub repositories. However, SWE-bench uses only Python repositories,
with problem statements presented predominantly as text and lacking visual
elements such as images. This limited coverage motivates our inquiry into how
existing systems might perform on unrepresented software engineering domains
(e.g., front-end, game development, DevOps), which use different programming
languages and paradigms. Therefore, we propose SWE-bench Multimodal (SWE-bench
M), to evaluate systems on their ability to fix bugs in visual, user-facing
JavaScript software. SWE-bench M features 617 task instances collected from 17
JavaScript libraries used for web interface design, diagramming, data
visualization, syntax highlighting, and interactive mapping. Each SWE-bench M
task instance contains at least one image in its problem statement or unit
tests. Our analysis finds that top-performing SWE-bench systems struggle with
SWE-bench M, revealing limitations in visual problem-solving and cross-language
generalization. Lastly, we show that SWE-agent's flexible language-agnostic
features enable it to substantially outperform alternatives on SWE-bench M,
resolving 12% of task instances compared to 6% for the next best system."
Modeling and Analysis of Spatial and Temporal Land Clutter Statistics in SAR Imaging Based on MSTAR Data,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"The statistical analysis of land clutter for Synthetic Aperture Radar (SAR)
imaging has become an increasingly important subject for research and
investigation. It is also absolutely necessary for designing robust algorithms
capable of performing the task of target detection in the background clutter.
Any attempt to extract the energy of the desired targets from the land clutter
requires complete knowledge of the statistical properties of the background
clutter. In this paper, the spatial as well as the temporal characteristics of
the land clutter are studied. Since the data for each image has been collected
based on a different aspect angle; therefore, the temporal analysis contains
variation in the aspect angle. Consequently, the temporal analysis includes the
characteristics of the radar cross section with respect to the aspect angle
based on which the data has been collected. In order to perform the statistical
analysis, several well-known and relevant distributions, namely, Weibull,
Log-normal, Gamma, and Rayleigh are considered as prime candidates to model the
land clutter. The goodness-of-fit test is based on the Kullback-Leibler (KL)
Divergence metric. The detailed analysis presented in this paper demonstrates
that the Weibull distribution is a more accurate fit for the
temporal-aspect-angle statistical analysis while the Rayleigh distribution
models the spatial characteristics of the background clutter with higher
accuracy. Finally, based on the aforementioned statistical analyses and by
utilizing the Constant False Alarm Rate (CFAR) algorithm, we perform target
detection in land clutter. The overall verification of the analysis is
performed by exploiting the Moving and Stationary Target Acquisition and
Recognition (MSTAR) data-set, which has been collected in spotlight mode at
X-band, and the results are presented."
Unsupervised Prior Learning: Discovering Categorical Pose Priors from Videos,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"A prior represents a set of beliefs or assumptions about a system, aiding
inference and decision-making. In this work, we introduce the challenge of
unsupervised prior learning in pose estimation, where AI models learn pose
priors of animate objects from videos in a self-supervised manner. These videos
present objects performing various actions, providing crucial information about
their keypoints and connectivity. While priors are effective in pose
estimation, acquiring them can be difficult. We propose a novel method, named
Pose Prior Learner (PPL), to learn general pose priors applicable to any object
category. PPL uses a hierarchical memory to store compositional parts of
prototypical poses, from which we distill a general pose prior. This prior
enhances pose estimation accuracy through template transformation and image
reconstruction. PPL learns meaningful pose priors without any additional human
annotations or interventions, outperforming competitive baselines on both human
and animal pose estimation datasets. Notably, our experimental results reveal
the effectiveness of PPL using learnt priors for pose estimation on occluded
images. Through iterative inference, PPL leverages priors to refine estimated
poses, regressing them to any prototypical poses stored in memory. Our code,
model, and data will be publicly available."
You Know What I'm Saying: Jailbreak Attack via Implicit Reference,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"While recent advancements in large language model (LLM) alignment have
enabled the effective identification of malicious objectives involving scene
nesting and keyword rewriting, our study reveals that these methods remain
inadequate at detecting malicious objectives expressed through context within
nested harmless objectives. This study identifies a previously overlooked
vulnerability, which we term Attack via Implicit Reference (AIR). AIR
decomposes a malicious objective into permissible objectives and links them
through implicit references within the context. This method employs multiple
related harmless objectives to generate malicious content without triggering
refusal responses, thereby effectively bypassing existing detection
techniques.Our experiments demonstrate AIR's effectiveness across
state-of-the-art LLMs, achieving an attack success rate (ASR) exceeding 90% on
most models, including GPT-4o, Claude-3.5-Sonnet, and Qwen-2-72B. Notably, we
observe an inverse scaling phenomenon, where larger models are more vulnerable
to this attack method. These findings underscore the urgent need for defense
mechanisms capable of understanding and preventing contextual attacks.
Furthermore, we introduce a cross-model attack strategy that leverages less
secure models to generate malicious contexts, thereby further increasing the
ASR when targeting other models.Our code and jailbreak artifacts can be found
at https://github.com/Lucas-TY/llm_Implicit_reference."
Detecting Machine-Generated Long-Form Content with Latent-Space Variables,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"The increasing capability of large language models (LLMs) to generate fluent
long-form texts is presenting new challenges in distinguishing
machine-generated outputs from human-written ones, which is crucial for
ensuring authenticity and trustworthiness of expressions. Existing zero-shot
detectors primarily focus on token-level distributions, which are vulnerable to
real-world domain shifts, including different prompting and decoding
strategies, and adversarial attacks. We propose a more robust method that
incorporates abstract elements, such as event transitions, as key deciding
factors to detect machine versus human texts by training a latent-space model
on sequences of events or topics derived from human-written texts. In three
different domains, machine-generated texts, which are originally inseparable
from human texts on the token level, can be better distinguished with our
latent-space model, leading to a 31% improvement over strong baselines such as
DetectGPT. Our analysis further reveals that, unlike humans, modern LLMs like
GPT-4 generate event triggers and their transitions differently, an inherent
disparity that helps our method to robustly detect machine-generated texts."
"A Survey on Group Fairness in Federated Learning: Challenges, Taxonomy of Solutions and Directions for Future Research",cs.LG,Machine Learning,2024-10-04,"Group fairness in machine learning is a critical area of research focused on
achieving equitable outcomes across different groups defined by sensitive
attributes such as race or gender. Federated learning, a decentralized approach
to training machine learning models across multiple devices or organizations
without sharing raw data, amplifies the need for fairness due to the
heterogeneous data distributions across clients, which can exacerbate biases.
The intersection of federated learning and group fairness has attracted
significant interest, with 47 research works specifically dedicated to
addressing this issue. However, no dedicated survey has focused comprehensively
on group fairness in federated learning. In this work, we present an in-depth
survey on this topic, addressing the critical challenges and reviewing related
works in the field. We create a novel taxonomy of these approaches based on key
criteria such as data partitioning, location, and applied strategies.
Additionally, we explore broader concerns related to this problem and
investigate how different approaches handle the complexities of various
sensitive groups and their intersections. Finally, we review the datasets and
applications commonly used in current research. We conclude by highlighting key
areas for future research, emphasizing the need for more methods to address the
complexities of achieving group fairness in federated systems."
Towards Assuring EU AI Act Compliance and Adversarial Robustness of LLMs,cs.CR,Cryptography and Security,2024-10-04,"Large language models are prone to misuse and vulnerable to security threats,
raising significant safety and security concerns. The European Union's
Artificial Intelligence Act seeks to enforce AI robustness in certain contexts,
but faces implementation challenges due to the lack of standards, complexity of
LLMs and emerging security vulnerabilities. Our research introduces a framework
using ontologies, assurance cases, and factsheets to support engineers and
stakeholders in understanding and documenting AI system compliance and security
regarding adversarial robustness. This approach aims to ensure that LLMs adhere
to regulatory standards and are equipped to counter potential threats."
"Sequential Probability Assignment with Contexts: Minimax Regret, Contextual Shtarkov Sums, and Contextual Normalized Maximum Likelihood",cs.LG,Machine Learning,2024-10-04,"We study the fundamental problem of sequential probability assignment, also
known as online learning with logarithmic loss, with respect to an arbitrary,
possibly nonparametric hypothesis class. Our goal is to obtain a complexity
measure for the hypothesis class that characterizes the minimax regret and to
determine a general, minimax optimal algorithm. Notably, the sequential
$\ell_{\infty}$ entropy, extensively studied in the literature (Rakhlin and
Sridharan, 2015, Bilodeau et al., 2020, Wu et al., 2023), was shown to not
characterize minimax risk in general. Inspired by the seminal work of Shtarkov
(1987) and Rakhlin, Sridharan, and Tewari (2010), we introduce a novel
complexity measure, the \emph{contextual Shtarkov sum}, corresponding to the
Shtarkov sum after projection onto a multiary context tree, and show that the
worst case log contextual Shtarkov sum equals the minimax regret. Using the
contextual Shtarkov sum, we derive the minimax optimal strategy, dubbed
\emph{contextual Normalized Maximum Likelihood} (cNML). Our results hold for
sequential experts, beyond binary labels, which are settings rarely considered
in prior work. To illustrate the utility of this characterization, we provide a
short proof of a new regret upper bound in terms of sequential $\ell_{\infty}$
entropy, unifying and sharpening state-of-the-art bounds by Bilodeau et al.
(2020) and Wu et al. (2023)."
Using Prompts to Guide Large Language Models in Imitating a Real Person's Language Style,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Large language models (LLMs), such as GPT series and Llama series have
demonstrated strong capabilities in natural language processing, contextual
understanding, and text generation. In recent years, researchers are trying to
enhance the abilities of LLMs in performing various tasks, and numerous studies
have proved that well-designed prompts can significantly improve the
performance of LLMs on these tasks. This study compares the language style
imitation ability of three different large language models under the guidance
of the same zero-shot prompt. It also involves comparing the imitation ability
of the same large language model when guided by three different prompts
individually. Additionally, by applying a Tree-of-Thoughts (ToT) Prompting
method to Llama 3, a conversational AI with the language style of a real person
was created. In this study, three evaluation methods were used to evaluate LLMs
and prompts. The results show that Llama 3 performs best at imitating language
styles, and that the ToT prompting method is the most effective to guide it in
imitating language styles. Using a ToT framework, Llama 3 was guided to
interact with users in the language style of a specific individual without
altering its core parameters, thereby creating a text-based conversational AI
that reflects the language style of the individual."
Model-Based Reward Shaping for Adversarial Inverse Reinforcement Learning in Stochastic Environments,cs.LG,Machine Learning,2024-10-04,"In this paper, we aim to tackle the limitation of the Adversarial Inverse
Reinforcement Learning (AIRL) method in stochastic environments where
theoretical results cannot hold and performance is degraded. To address this
issue, we propose a novel method which infuses the dynamics information into
the reward shaping with the theoretical guarantee for the induced optimal
policy in the stochastic environments. Incorporating our novel model-enhanced
rewards, we present a novel Model-Enhanced AIRL framework, which integrates
transition model estimation directly into reward shaping. Furthermore, we
provide a comprehensive theoretical analysis of the reward error bound and
performance difference bound for our method. The experimental results in MuJoCo
benchmarks show that our method can achieve superior performance in stochastic
environments and competitive performance in deterministic environments, with
significant improvement in sample efficiency, compared to existing baselines."
ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Open-source Electronic Design Automation (EDA) tools are rapidly transforming
chip design by addressing key barriers of commercial EDA tools such as
complexity, costs, and access. Recent advancements in Large Language Models
(LLMs) have further enhanced efficiency in chip design by providing user
assistance across a range of tasks like setup, decision-making, and flow
automation. This paper introduces ORAssistant, a conversational assistant for
OpenROAD, based on Retrieval-Augmented Generation (RAG). ORAssistant aims to
improve the user experience for the OpenROAD flow, from RTL-GDSII by providing
context-specific responses to common user queries, including installation,
command usage, flow setup, and execution, in prose format. Currently,
ORAssistant integrates OpenROAD, OpenROAD-flow-scripts, Yosys, OpenSTA, and
KLayout. The data model is built from publicly available documentation and
GitHub resources. The proposed architecture is scalable, supporting extensions
to other open-source tools, operating modes, and LLM models. We use Google
Gemini as the base LLM model to build and test ORAssistant. Early evaluation
results of the RAG-based model show notable improvements in performance and
accuracy compared to non-fine-tuned LLMs."
Output Scouting: Auditing Large Language Models for Catastrophic Responses,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Recent high profile incidents in which the use of Large Language Models
(LLMs) resulted in significant harm to individuals have brought about a growing
interest in AI safety. One reason LLM safety issues occur is that models often
have at least some non-zero probability of producing harmful outputs. In this
work, we explore the following scenario: imagine an AI safety auditor is
searching for catastrophic responses from an LLM (e.g. a ""yes"" responses to
""can I fire an employee for being pregnant?""), and is able to query the model a
limited number times (e.g. 1000 times). What is a strategy for querying the
model that would efficiently find those failure responses? To this end, we
propose output scouting: an approach that aims to generate semantically fluent
outputs to a given prompt matching any target probability distribution. We then
run experiments using two LLMs and find numerous examples of catastrophic
responses. We conclude with a discussion that includes advice for practitioners
who are looking to implement LLM auditing for catastrophic responses. We also
release an open-source toolkit (https://github.com/joaopfonseca/outputscouting)
that implements our auditing framework using the Hugging Face transformers
library."
Developing Assurance Cases for Adversarial Robustness and Regulatory Compliance in LLMs,cs.CR,Cryptography and Security,2024-10-04,"This paper presents an approach to developing assurance cases for adversarial
robustness and regulatory compliance in large language models (LLMs). Focusing
on both natural and code language tasks, we explore the vulnerabilities these
models face, including adversarial attacks based on jailbreaking, heuristics,
and randomization. We propose a layered framework incorporating guardrails at
various stages of LLM deployment, aimed at mitigating these attacks and
ensuring compliance with the EU AI Act. Our approach includes a meta-layer for
dynamic risk management and reasoning, crucial for addressing the evolving
nature of LLM vulnerabilities. We illustrate our method with two exemplary
assurance cases, highlighting how different contexts demand tailored strategies
to ensure robust and compliant AI systems."
FaithCAMERA: Construction of a Faithful Dataset for Ad Text Generation,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"In ad text generation (ATG), desirable ad text is both faithful and
informative. That is, it should be faithful to the input document, while at the
same time containing important information that appeals to potential customers.
The existing evaluation data, CAMERA (arXiv:2309.12030), is suitable for
evaluating informativeness, as it consists of reference ad texts created by ad
creators. However, these references often include information unfaithful to the
input, which is a notable obstacle in promoting ATG research. In this study, we
collaborate with in-house ad creators to refine the CAMERA references and
develop an alternative ATG evaluation dataset called FaithCAMERA, in which the
faithfulness of references is guaranteed. Using FaithCAMERA, we can evaluate
how well existing methods for improving faithfulness can generate informative
ad text while maintaining faithfulness. Our experiments show that removing
training data that contains unfaithful entities improves the faithfulness and
informativeness at the entity level, but decreases both at the sentence level.
This result suggests that for future ATG research, it is essential not only to
scale the training data but also to ensure their faithfulness. Our dataset will
be publicly available."
Learning Code Preference via Synthetic Evolution,cs.LG,Machine Learning,2024-10-04,"Large Language Models (LLMs) have recently demonstrated remarkable coding
capabilities. However, assessing code generation based on well-formed
properties and aligning it with developer preferences remains challenging. In
this paper, we explore two key questions under the new challenge of code
preference learning: (i) How do we train models to predict meaningful
preferences for code? and (ii) How do human and LLM preferences align with
verifiable code properties and developer code tastes? To this end, we propose
CodeFavor, a framework for training pairwise code preference models from
synthetic evolution data, including code commits and code critiques. To
evaluate code preferences, we introduce CodePrefBench, a benchmark comprising
1364 rigorously curated code preference tasks to cover three verifiable
properties-correctness, efficiency, and security-along with human preference.
Our evaluation shows that CodeFavor holistically improves the accuracy of
model-based code preferences by up to 28.8%. Meanwhile, CodeFavor models can
match the performance of models with 6-9x more parameters while being 34x more
cost-effective. We also rigorously validate the design choices in CodeFavor via
a comprehensive set of controlled experiments. Furthermore, we discover the
prohibitive costs and limitations of human-based code preference: despite
spending 23.4 person-minutes on each task, 15.1-40.3% of tasks remain unsolved.
Compared to model-based preference, human preference tends to be more accurate
under the objective of code correctness, while being sub-optimal for
non-functional objectives."
GraphRouter: A Graph-based Router for LLM Selections,cs.AI,Artificial Intelligence,2024-10-04,"The rapidly growing number and variety of Large Language Models (LLMs)
present significant challenges in efficiently selecting the appropriate LLM for
a given query, especially considering the trade-offs between performance and
computational cost. Current LLM selection methods often struggle to generalize
across new LLMs and different tasks because of their limited ability to
leverage contextual interactions among tasks, queries, and LLMs, as well as
their dependence on a transductive learning framework. To address these
shortcomings, we introduce a novel inductive graph framework, named as
GraphRouter, which fully utilizes the contextual information among tasks,
queries, and LLMs to enhance the LLM selection process. GraphRouter constructs
a heterogeneous graph comprising task, query, and LLM nodes, with interactions
represented as edges, which efficiently captures the contextual information
between the query's requirements and the LLM's capabilities. Through an
innovative edge prediction mechanism, GraphRouter is able to predict attributes
(the effect and cost of LLM response) of potential edges, allowing for
optimized recommendations that adapt to both existing and newly introduced LLMs
without requiring retraining. Comprehensive experiments across three distinct
effect-cost weight scenarios have shown that GraphRouter substantially
surpasses existing routers, delivering a minimum performance improvement of
12.3%. In addition, it achieves enhanced generalization across new LLMs
settings and supports diverse tasks with at least a 9.5% boost in effect and a
significant reduction in computational demands. This work endeavors to apply a
graph-based approach for the contextual and adaptive selection of LLMs,
offering insights for real-world applications. Our codes for GraphRouter will
soon be released at https://github.com/ulab-uiuc/GraphRouter."
Why Fine-Tuning Struggles with Forgetting in Machine Unlearning? Theoretical Insights and a Remedial Approach,cs.LG,Machine Learning,2024-10-04,"Machine Unlearning has emerged as a significant area of research, focusing on
'removing' specific subsets of data from a trained model. Fine-tuning (FT)
methods have become one of the fundamental approaches for approximating
unlearning, as they effectively retain model performance. However, it is
consistently observed that naive FT methods struggle to forget the targeted
data. In this paper, we present the first theoretical analysis of FT methods
for machine unlearning within a linear regression framework, providing a deeper
exploration of this phenomenon. We investigate two scenarios with distinct
features and overlapping features. Our findings reveal that FT models can
achieve zero remaining loss yet fail to forget the forgetting data, unlike
golden models (trained from scratch without the forgetting data). This analysis
reveals that naive FT methods struggle with forgetting because the pretrained
model retains information about the forgetting data, and the fine-tuning
process has no impact on this retained information. To address this issue, we
first propose a theoretical approach to mitigate the retention of forgetting
data in the pretrained model. Our analysis shows that removing the forgetting
data's influence allows FT models to match the performance of the golden model.
Building on this insight, we introduce a discriminative regularization term to
practically reduce the unlearning loss gap between the fine-tuned model and the
golden model. Our experiments on both synthetic and real-world datasets
validate these theoretical insights and demonstrate the effectiveness of the
proposed regularization method."
Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Misinformation, defined as false or inaccurate information, can result in
significant societal harm when it is spread with malicious or even innocuous
intent. The rapid online information exchange necessitates advanced detection
mechanisms to mitigate misinformation-induced harm. Existing research, however,
has predominantly focused on assessing veracity, overlooking the legal
implications and social consequences of misinformation. In this work, we take a
novel angle to consolidate the definition of misinformation detection using
legal issues as a measurement of societal ramifications, aiming to bring
interdisciplinary efforts to tackle misinformation and its consequence. We
introduce a new task: Misinformation with Legal Consequence (MisLC), which
leverages definitions from a wide range of legal domains covering 4 broader
legal topics and 11 fine-grained legal issues, including hate speech, election
laws, and privacy regulations. For this task, we advocate a two-step dataset
curation approach that utilizes crowd-sourced checkworthiness and expert
evaluations of misinformation. We provide insights about the MisLC task through
empirical evidence, from the problem definition to experiments and expert
involvement. While the latest large language models and retrieval-augmented
generation are effective baselines for the task, we find they are still far
from replicating expert performance."
MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Estimating geometry from dynamic scenes, where objects move and deform over
time, remains a core challenge in computer vision. Current approaches often
rely on multi-stage pipelines or global optimizations that decompose the
problem into subtasks, like depth and flow, leading to complex systems prone to
errors. In this paper, we present Motion DUSt3R (MonST3R), a novel
geometry-first approach that directly estimates per-timestep geometry from
dynamic scenes. Our key insight is that by simply estimating a pointmap for
each timestep, we can effectively adapt DUST3R's representation, previously
only used for static scenes, to dynamic scenes. However, this approach presents
a significant challenge: the scarcity of suitable training data, namely
dynamic, posed videos with depth labels. Despite this, we show that by posing
the problem as a fine-tuning task, identifying several suitable datasets, and
strategically training the model on this limited data, we can surprisingly
enable the model to handle dynamics, even without an explicit motion
representation. Based on this, we introduce new optimizations for several
downstream video-specific tasks and demonstrate strong performance on video
depth and camera pose estimation, outperforming prior work in terms of
robustness and efficiency. Moreover, MonST3R shows promising results for
primarily feed-forward 4D reconstruction."
Estimating Body and Hand Motion in an Ego-sensed World,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"We present EgoAllo, a system for human motion estimation from a head-mounted
device. Using only egocentric SLAM poses and images, EgoAllo guides sampling
from a conditional diffusion model to estimate 3D body pose, height, and hand
parameters that capture the wearer's actions in the allocentric coordinate
frame of the scene. To achieve this, our key insight is in representation: we
propose spatial and temporal invariance criteria for improving model
performance, from which we derive a head motion conditioning parameterization
that improves estimation by up to 18%. We also show how the bodies estimated by
our system can improve the hands: the resulting kinematic and temporal
constraints result in over 40% lower hand estimation errors compared to noisy
monocular estimates. Project page: https://egoallo.github.io/"
Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Large language models (LLMs) have exhibited complex reasoning abilities by
generating question rationales and demonstrated exceptional performance in
natural language processing (NLP) tasks. However, these reasoning capabilities
generally emerge in models with tens of billions of parameters, creating
significant computational challenges for real-world deployment. Recent research
has concentrated on improving open-source smaller models through knowledge
distillation (KD) from commercial LLMs. Nevertheless, most of these studies
rely solely on the responses from one single LLM as the gold rationale for
training. In this paper, we introduce a novel Mistake-Aware Peer-Review
Distillation (MAPD) approach: 1) Instead of merely obtaining gold rationales
from teachers, our method asks teachers to identify and explain the student's
mistakes, providing customized instruction learning data. 2) We design a
simulated peer-review process between teacher LLMs, which selects only the
generated rationales above the acceptance threshold. This reduces the chance of
teachers guessing correctly with flawed rationale, improving instructional data
quality. Comprehensive experiments and analysis on mathematical, commonsense,
and logical reasoning tasks demonstrate the effectiveness of our method."
System 2 reasoning capabilities are nigh,cs.AI,Artificial Intelligence,2024-10-04,"In recent years, machine learning models have made strides towards human-like
reasoning capabilities from several directions. In this work, we review the
current state of the literature and describe the remaining steps to achieve a
neural model which can perform System 2 reasoning analogous to a human. We
argue that if current models are insufficient to be classed as performing
reasoning, there remains very little additional progress needed to attain that
goal."
Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Large Vision-Language Models (LVLMs) have demonstrated impressive
capabilities for capturing and reasoning over multimodal inputs. However, these
models are prone to parametric knowledge conflicts, which arise from
inconsistencies of represented knowledge between their vision and language
components. In this paper, we formally define the problem of
$\textbf{cross-modality parametric knowledge conflict}$ and present a
systematic approach to detect, interpret, and mitigate them. We introduce a
pipeline that identifies conflicts between visual and textual answers, showing
a persistently high conflict rate across modalities in recent LVLMs regardless
of the model size. We further investigate how these conflicts interfere with
the inference process and propose a contrastive metric to discern the
conflicting samples from the others. Building on these insights, we develop a
novel dynamic contrastive decoding method that removes undesirable logits
inferred from the less confident modality components based on answer
confidence. For models that do not provide logits, we also introduce two
prompt-based strategies to mitigate the conflicts. Our methods achieve
promising improvements in accuracy on both the ViQuAE and InfoSeek datasets.
Specifically, using LLaVA-34B, our proposed dynamic contrastive decoding
improves an average accuracy of 2.24%."
RAFT: Realistic Attacks to Fool Text Detectors,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Large language models (LLMs) have exhibited remarkable fluency across various
tasks. However, their unethical applications, such as disseminating
disinformation, have become a growing concern. Although recent works have
proposed a number of LLM detection methods, their robustness and reliability
remain unclear. In this paper, we present RAFT: a grammar error-free black-box
attack against existing LLM detectors. In contrast to previous attacks for
language models, our method exploits the transferability of LLM embeddings at
the word-level while preserving the original text quality. We leverage an
auxiliary embedding to greedily select candidate words to perturb against the
target detector. Experiments reveal that our attack effectively compromises all
detectors in the study across various domains by up to 99%, and are
transferable across source models. Manual human evaluation studies show our
attacks are realistic and indistinguishable from original human-written text.
We also show that examples generated by RAFT can be used to train adversarially
robust detectors. Our work shows that current LLM detectors are not
adversarially robust, underscoring the urgent need for more resilient detection
mechanisms."
Geometric Representation Condition Improves Equivariant Molecule Generation,cs.LG,Machine Learning,2024-10-04,"Recent advancements in molecular generative models have demonstrated
substantial potential in accelerating scientific discovery, particularly in
drug design. However, these models often face challenges in generating
high-quality molecules, especially in conditional scenarios where specific
molecular properties must be satisfied. In this work, we introduce GeoRCG, a
general framework to enhance the performance of molecular generative models by
integrating geometric representation conditions. We decompose the molecule
generation process into two stages: first, generating an informative geometric
representation; second, generating a molecule conditioned on the
representation. Compared to directly generating a molecule, the relatively
easy-to-generate representation in the first-stage guides the second-stage
generation to reach a high-quality molecule in a more goal-oriented and much
faster way. Leveraging EDM as the base generator, we observe significant
quality improvements in unconditional molecule generation on the widely-used
QM9 and GEOM-DRUG datasets. More notably, in the challenging conditional
molecular generation task, our framework achieves an average 31\% performance
improvement over state-of-the-art approaches, highlighting the superiority of
conditioning on semantically rich geometric representations over conditioning
on individual property values as in previous approaches. Furthermore, we show
that, with such representation guidance, the number of diffusion steps can be
reduced to as small as 100 while maintaining superior generation quality than
that achieved with 1,000 steps, thereby significantly accelerating the
generation process."
Learning Humanoid Locomotion over Challenging Terrain,cs.RO,Robotics,2024-10-04,"Humanoid robots can, in principle, use their legs to go almost anywhere.
Developing controllers capable of traversing diverse terrains, however, remains
a considerable challenge. Classical controllers are hard to generalize broadly
while the learning-based methods have primarily focused on gentle terrains.
Here, we present a learning-based approach for blind humanoid locomotion
capable of traversing challenging natural and man-made terrain. Our method uses
a transformer model to predict the next action based on the history of
proprioceptive observations and actions. The model is first pre-trained on a
dataset of flat-ground trajectories with sequence modeling, and then fine-tuned
on uneven terrain using reinforcement learning. We evaluate our model on a real
humanoid robot across a variety of terrains, including rough, deformable, and
sloped surfaces. The model demonstrates robust performance, in-context
adaptation, and emergent terrain representations. In real-world case studies,
our humanoid robot successfully traversed over 4 miles of hiking trails in
Berkeley and climbed some of the steepest streets in San Francisco."
Dorami: Privilege Separating Security Monitor on RISC-V TEEs,cs.CR,Cryptography and Security,2024-10-04,"TEE implementations on RISC-V offer an enclave abstraction by introducing a
trusted component called the security monitor (SM). The SM performs critical
tasks such as isolating enclaves from each other as well as from the OS by
using privileged ISA instructions that enforce the physical memory protection.
However, the SM executes at the highest privilege layer on the platform
(machine-mode) along side firmware that is not only large in size but also
includes third-party vendor code specific to the platform. In this paper, we
present Dorami - a privilege separation approach that isolates the SM from the
firmware thus reducing the attack surface on TEEs. Dorami re-purposes existing
ISA features to enforce its isolation and achieves its goals without large
overheads."
GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs,cs.RO,Robotics,2024-10-04,"Robotic simulation today remains challenging to scale up due to the human
efforts required to create diverse simulation tasks and scenes.
Simulation-trained policies also face scalability issues as many sim-to-real
methods focus on a single task. To address these challenges, this work proposes
GenSim2, a scalable framework that leverages coding LLMs with multi-modal and
reasoning capabilities for complex and realistic simulation task creation,
including long-horizon tasks with articulated objects. To automatically
generate demonstration data for these tasks at scale, we propose planning and
RL solvers that generalize within object categories. The pipeline can generate
data for up to 100 articulated tasks with 200 objects and reduce the required
human efforts. To utilize such data, we propose an effective multi-task
language-conditioned policy architecture, dubbed proprioceptive point-cloud
transformer (PPT), that learns from the generated demonstrations and exhibits
strong sim-to-real zero-shot transfer. Combining the proposed pipeline and the
policy architecture, we show a promising usage of GenSim2 that the generated
data can be used for zero-shot transfer or co-train with real-world collected
data, which enhances the policy performance by 20% compared with training
exclusively on limited real data."
Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Traditional unlearnable strategies have been proposed to prevent unauthorized
users from training on the 2D image data. With more 3D point cloud data
containing sensitivity information, unauthorized usage of this new type data
has also become a serious concern. To address this, we propose the first
integral unlearnable framework for 3D point clouds including two processes: (i)
we propose an unlearnable data protection scheme, involving a class-wise
setting established by a category-adaptive allocation strategy and
multi-transformations assigned to samples; (ii) we propose a data restoration
scheme that utilizes class-wise inverse matrix transformation, thus enabling
authorized-only training for unlearnable data. This restoration process is a
practical issue overlooked in most existing unlearnable literature, \ie, even
authorized users struggle to gain knowledge from 3D unlearnable data. Both
theoretical and empirical results (including 6 datasets, 16 models, and 2
tasks) demonstrate the effectiveness of our proposed unlearnable framework. Our
code is available at \url{https://github.com/CGCL-codes/UnlearnablePC}"
Aligning LLMs with Individual Preferences via Interaction,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"As large language models (LLMs) demonstrate increasingly advanced
capabilities, aligning their behaviors with human values and preferences
becomes crucial for their wide adoption. While previous research focuses on
general alignment to principles such as helpfulness, harmlessness, and honesty,
the need to account for individual and diverse preferences has been largely
overlooked, potentially undermining customized human experiences. To address
this gap, we train LLMs that can ''interact to align'', essentially cultivating
the meta-skill of LLMs to implicitly infer the unspoken personalized
preferences of the current user through multi-turn conversations, and then
dynamically align their following behaviors and responses to these inferred
preferences. Our approach involves establishing a diverse pool of 3,310
distinct user personas by initially creating seed examples, which are then
expanded through iterative self-generation and filtering. Guided by distinct
user personas, we leverage multi-LLM collaboration to develop a multi-turn
preference dataset containing 3K+ multi-turn conversations in tree structures.
Finally, we apply supervised fine-tuning and reinforcement learning to enhance
LLMs using this dataset. For evaluation, we establish the ALOE (ALign With
CustOmized PrEferences) benchmark, consisting of 100 carefully selected
examples and well-designed metrics to measure the customized alignment
performance during conversations. Experimental results demonstrate the
effectiveness of our method in enabling dynamic, personalized alignment via
interaction."
Real-World Benchmarks Make Membership Inference Attacks Fail on Diffusion Models,cs.LG,Machine Learning,2024-10-04,"Membership inference attacks (MIAs) on diffusion models have emerged as
potential evidence of unauthorized data usage in training pre-trained diffusion
models. These attacks aim to detect the presence of specific images in training
datasets of diffusion models. Our study delves into the evaluation of
state-of-the-art MIAs on diffusion models and reveals critical flaws and overly
optimistic performance estimates in existing MIA evaluation. We introduce
CopyMark, a more realistic MIA benchmark that distinguishes itself through the
support for pre-trained diffusion models, unbiased datasets, and fair
evaluation pipelines. Through extensive experiments, we demonstrate that the
effectiveness of current MIA methods significantly degrades under these more
practical conditions. Based on our results, we alert that MIA, in its current
state, is not a reliable approach for identifying unauthorized data usage in
pre-trained diffusion models. To the best of our knowledge, we are the first to
discover the performance overestimation of MIAs on diffusion models and present
a unified benchmark for more realistic evaluation. Our code is available on
GitHub: \url{https://github.com/caradryanl/CopyMark}."
Large Language Models can be Strong Self-Detoxifiers,cs.LG,Machine Learning,2024-10-04,"Reducing the likelihood of generating harmful and toxic output is an
essential task when aligning large language models (LLMs). Existing methods
mainly rely on training an external reward model (i.e., another language model)
or fine-tuning the LLM using self-generated data to influence the outcome. In
this paper, we show that LLMs have the capability of self-detoxification
without the use of an additional reward model or re-training. We propose
\textit{Self-disciplined Autoregressive Sampling (SASA)}, a lightweight
controlled decoding algorithm for toxicity reduction of LLMs. SASA leverages
the contextual representations from an LLM to learn linear subspaces
characterizing toxic v.s. non-toxic output in analytical forms. When
auto-completing a response token-by-token, SASA dynamically tracks the margin
of the current output to steer the generation away from the toxic subspace, by
adjusting the autoregressive sampling strategy. Evaluated on LLMs of different
scale and nature, namely Llama-3.1-Instruct (8B), Llama-2 (7B), and GPT2-L
models with the RealToxicityPrompts, BOLD, and AttaQ benchmarks, SASA markedly
enhances the quality of the generated sentences relative to the original models
and attains comparable performance to state-of-the-art detoxification
techniques, significantly reducing the toxicity level by only using the LLM's
internal representations."
Robust Offline Imitation Learning from Diverse Auxiliary Data,cs.LG,Machine Learning,2024-10-04,"Offline imitation learning enables learning a policy solely from a set of
expert demonstrations, without any environment interaction. To alleviate the
issue of distribution shift arising due to the small amount of expert data,
recent works incorporate large numbers of auxiliary demonstrations alongside
the expert data. However, the performance of these approaches rely on
assumptions about the quality and composition of the auxiliary data. However,
they are rarely successful when those assumptions do not hold. To address this
limitation, we propose Robust Offline Imitation from Diverse Auxiliary Data
(ROIDA). ROIDA first identifies high-quality transitions from the entire
auxiliary dataset using a learned reward function. These high-reward samples
are combined with the expert demonstrations for weighted behavioral cloning.
For lower-quality samples, ROIDA applies temporal difference learning to steer
the policy towards high-reward states, improving long-term returns. This
two-pronged approach enables our framework to effectively leverage both high
and low-quality data without any assumptions. Extensive experiments validate
that ROIDA achieves robust and consistent performance across multiple auxiliary
datasets with diverse ratios of expert and non-expert demonstrations. ROIDA
effectively leverages unlabeled auxiliary data, outperforming prior methods
reliant on specific data assumptions."
A Global Medical Data Security and Privacy Preserving Standards Identification Framework for Electronic Healthcare Consumers,cs.LG,Machine Learning,2024-10-04,"Electronic Health Records (EHR) are crucial for the success of digital
healthcare, with a focus on putting consumers at the center of this
transformation. However, the digitalization of healthcare records brings along
security and privacy risks for personal data. The major concern is that
different countries have varying standards for the security and privacy of
medical data. This paper proposed a novel and comprehensive framework to
standardize these rules globally, bringing them together on a common platform.
To support this proposal, the study reviews existing literature to understand
the research interest in this issue. It also examines six key laws and
standards related to security and privacy, identifying twenty concepts. The
proposed framework utilized K-means clustering to categorize these concepts and
identify five key factors. Finally, an Ordinal Priority Approach is applied to
determine the preferred implementation of these factors in the context of EHRs.
The proposed study provides a descriptive then prescriptive framework for the
implementation of privacy and security in the context of electronic health
records. Therefore, the findings of the proposed framework are useful for
professionals and policymakers in improving the security and privacy associated
with EHRs."
Open-World Reinforcement Learning over Long Short-Term Imagination,cs.LG,Machine Learning,2024-10-04,"Training visual reinforcement learning agents in a high-dimensional open
world presents significant challenges. While various model-based methods have
improved sample efficiency by learning interactive world models, these agents
tend to be ""short-sighted"", as they are typically trained on short snippets of
imagined experiences. We argue that the primary obstacle in open-world
decision-making is improving the efficiency of off-policy exploration across an
extensive state space. In this paper, we present LS-Imagine, which extends the
imagination horizon within a limited number of state transition steps, enabling
the agent to explore behaviors that potentially lead to promising long-term
feedback. The foundation of our approach is to build a long short-term world
model. To achieve this, we simulate goal-conditioned jumpy state transitions
and compute corresponding affordance maps by zooming in on specific areas
within single images. This facilitates the integration of direct long-term
values into behavior learning. Our method demonstrates significant improvements
over state-of-the-art techniques in MineDojo."
What Matters for Model Merging at Scale?,cs.LG,Machine Learning,2024-10-04,"Model merging aims to combine multiple expert models into a more capable
single model, offering benefits such as reduced storage and serving costs,
improved generalization, and support for decentralized model development.
Despite its promise, previous studies have primarily focused on merging a few
small models. This leaves many unanswered questions about the effect of scaling
model size and how it interplays with other key factors -- like the base model
quality and number of expert models -- , to affect the merged model's
performance. This work systematically evaluates the utility of model merging at
scale, examining the impact of these different factors. We experiment with
merging fully fine-tuned models using 4 popular merging methods -- Averaging,
Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B
parameters and merging up to 8 different expert models. We evaluate the merged
models on both held-in tasks, i.e., the expert's training tasks, and zero-shot
generalization to unseen held-out tasks. Our experiments provide several new
insights about model merging at scale and the interplay between different
factors. First, we find that merging is more effective when experts are created
from strong base models, i.e., models with good zero-shot performance. Second,
larger models facilitate easier merging. Third merging consistently improves
generalization capabilities. Notably, when merging 8 large expert models, the
merged models often generalize better compared to the multitask trained models.
Fourth, we can better merge more expert models when working with larger models.
Fifth, different merging methods behave very similarly at larger scales.
Overall, our findings shed light on some interesting properties of model
merging while also highlighting some limitations. We hope that this study will
serve as a reference point on large-scale merging for upcoming research."
Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation,cs.LG,Machine Learning,2024-10-04,"As large language models (LLMs) increasingly integrate into every aspect of
our work and daily lives, there are growing concerns about user privacy, which
push the trend toward local deployment of these models. There are a number of
lightweight LLMs (e.g., Gemini Nano, LLAMA2 7B) that can run locally on
smartphones, providing users with greater control over their personal data. As
a rapidly emerging application, we are concerned about their performance on
commercial-off-the-shelf mobile devices. To fully understand the current
landscape of LLM deployment on mobile platforms, we conduct a comprehensive
measurement study on mobile devices. We evaluate both metrics that affect user
experience, including token throughput, latency, and battery consumption, as
well as factors critical to developers, such as resource utilization, DVFS
strategies, and inference engines. In addition, we provide a detailed analysis
of how these hardware capabilities and system dynamics affect on-device LLM
performance, which may help developers identify and address bottlenecks for
mobile LLM applications. We also provide comprehensive comparisons across the
mobile system-on-chips (SoCs) from major vendors, highlighting their
performance differences in handling LLM workloads. We hope that this study can
provide insights for both the development of on-device LLMs and the design for
future mobile system architecture."
TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation,cs.AI,Artificial Intelligence,2024-10-04,"Given the widespread adoption and usage of Large Language Models (LLMs), it
is crucial to have flexible and interpretable evaluations of their
instruction-following ability. Preference judgments between model outputs have
become the de facto evaluation standard, despite distilling complex,
multi-faceted preferences into a single ranking. Furthermore, as human
annotation is slow and costly, LLMs are increasingly used to make these
judgments, at the expense of reliability and interpretability. In this work, we
propose TICK (Targeted Instruct-evaluation with ChecKlists), a fully automated,
interpretable evaluation protocol that structures evaluations with
LLM-generated, instruction-specific checklists. We first show that, given an
instruction, LLMs can reliably produce high-quality, tailored evaluation
checklists that decompose the instruction into a series of YES/NO questions.
Each question asks whether a candidate response meets a specific requirement of
the instruction. We demonstrate that using TICK leads to a significant increase
(46.4% $\to$ 52.2%) in the frequency of exact agreements between LLM judgements
and human preferences, as compared to having an LLM directly score an output.
We then show that STICK (Self-TICK) can be used to improve generation quality
across multiple benchmarks via self-refinement and Best-of-N selection. STICK
self-refinement on LiveBench reasoning tasks leads to an absolute gain of
$+$7.8%, whilst Best-of-N selection with STICK attains $+$6.3% absolute
improvement on the real-world instruction dataset, WildBench. In light of this,
structured, multi-faceted self-improvement is shown to be a promising way to
further advance LLM capabilities. Finally, by providing LLM-generated
checklists to human evaluators tasked with directly scoring LLM responses to
WildBench instructions, we notably increase inter-annotator agreement (0.194
$\to$ 0.256)."
LeLaN: Learning A Language-Conditioned Navigation Policy from In-the-Wild Videos,cs.RO,Robotics,2024-10-04,"The world is filled with a wide variety of objects. For robots to be useful,
they need the ability to find arbitrary objects described by people. In this
paper, we present LeLaN(Learning Language-conditioned Navigation policy), a
novel approach that consumes unlabeled, action-free egocentric data to learn
scalable, language-conditioned object navigation. Our framework, LeLaN
leverages the semantic knowledge of large vision-language models, as well as
robotic foundation models, to label in-the-wild data from a variety of indoor
and outdoor environments. We label over 130 hours of data collected in
real-world indoor and outdoor environments, including robot observations,
YouTube video tours, and human walking data. Extensive experiments with over
1000 real-world trials show that our approach enables training a policy from
unlabeled action-free videos that outperforms state-of-the-art robot navigation
methods, while being capable of inference at 4 times their speed on edge
compute. We open-source our models, datasets and provide supplementary videos
on our project page (https://learning-language-navigation.github.io/)."
How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework,cs.LG,Machine Learning,2024-10-04,"Discrete diffusion models have gained increasing attention for their ability
to model complex distributions with tractable sampling and inference. However,
the error analysis for discrete diffusion models remains less well-understood.
In this work, we propose a comprehensive framework for the error analysis of
discrete diffusion models based on L\'evy-type stochastic integrals. By
generalizing the Poisson random measure to that with a time-independent and
state-dependent intensity, we rigorously establish a stochastic integral
formulation of discrete diffusion models and provide the corresponding change
of measure theorems that are intriguingly analogous to It\^o integrals and
Girsanov's theorem for their continuous counterparts. Our framework unifies and
strengthens the current theoretical results on discrete diffusion models and
obtains the first error bound for the $\tau$-leaping scheme in KL divergence.
With error sources clearly identified, our analysis gives new insight into the
mathematical properties of discrete diffusion models and offers guidance for
the design of efficient and accurate algorithms for real-world discrete
diffusion model applications."
Efficiently Identifying Watermarked Segments in Mixed-Source Texts,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Text watermarks in large language models (LLMs) are increasingly used to
detect synthetic text, mitigating misuse cases like fake news and academic
dishonesty. While existing watermarking detection techniques primarily focus on
classifying entire documents as watermarked or not, they often neglect the
common scenario of identifying individual watermark segments within longer,
mixed-source documents. Drawing inspiration from plagiarism detection systems,
we propose two novel methods for partial watermark detection. First, we develop
a geometry cover detection framework aimed at determining whether there is a
watermark segment in long text. Second, we introduce an adaptive online
learning algorithm to pinpoint the precise location of watermark segments
within the text. Evaluated on three popular watermarking techniques
(KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves
high accuracy, significantly outperforming baseline methods. Moreover, our
framework is adaptable to other watermarking techniques, offering new insights
for precise watermark detection."
SiMilarity-Enhanced Homophily for Multi-View Heterophilous Graph Clustering,cs.AI,Artificial Intelligence,2024-10-04,"With the increasing prevalence of graph-structured data, multi-view graph
clustering has been widely used in various downstream applications. Existing
approaches primarily rely on a unified message passing mechanism, which
significantly enhances clustering performance. Nevertheless, this mechanism
limits its applicability to heterophilous situations, as it is fundamentally
predicated on the assumption of homophily, i.e., the connected nodes often
belong to the same class. In reality, this assumption does not always hold; a
moderately or even mildly homophilous graph is more common than a fully
homophilous one due to inevitable heterophilous information in the graph. To
address this issue, in this paper, we propose a novel SiMilarity-enhanced
Homophily for Multi-view Heterophilous Graph Clustering (SMHGC) approach. By
analyzing the relationship between similarity and graph homophily, we propose
to enhance the homophily by introducing three similarity terms, i.e., neighbor
pattern similarity, node feature similarity, and multi-view global similarity,
in a label-free manner. Then, a consensus-based inter- and intra-view fusion
paradigm is proposed to fuse the improved homophilous graph from different
views and utilize them for clustering. The state-of-the-art experimental
results on both multi-view heterophilous and homophilous datasets collectively
demonstrate the strong capacity of similarity for unsupervised multi-view
heterophilous graph learning. Additionally, the consistent performance across
semi-synthetic datasets with varying levels of homophily serves as further
evidence of SMHGC's resilience to heterophily."
Understanding Reasoning in Chain-of-Thought from the Hopfieldian View,cs.AI,Artificial Intelligence,2024-10-04,"Large Language Models have demonstrated remarkable abilities across various
tasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to
enhance reasoning capabilities. However, existing research primarily focuses on
improving performance, lacking a comprehensive framework to explain and
understand the fundamental factors behind CoT's success. To bridge this gap, we
introduce a novel perspective grounded in the Hopfieldian view of cognition in
cognitive neuroscience. We establish a connection between CoT reasoning and key
cognitive elements such as stimuli, actions, neural populations, and
representation spaces. From our view, we can understand the reasoning process
as the movement between these representation spaces. Building on this insight,
we develop a method for localizing reasoning errors in the response of CoTs.
Moreover, we propose the Representation-of-Thought (RoT) framework, which
leverages the robustness of low-dimensional representation spaces to enhance
the robustness of the reasoning process in CoTs. Experimental results
demonstrate that RoT improves the robustness and interpretability of CoT
reasoning while offering fine-grained control over the reasoning process."
"Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments",cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Prior works formulate the extraction of event-specific arguments as a span
extraction problem, where event arguments are explicit -- i.e. assumed to be
contiguous spans of text in a document. In this study, we revisit this
definition of Event Extraction (EE) by introducing two key argument types that
cannot be modeled by existing EE frameworks. First, implicit arguments are
event arguments which are not explicitly mentioned in the text, but can be
inferred through context. Second, scattered arguments are event arguments that
are composed of information scattered throughout the text. These two argument
types are crucial to elicit the full breadth of information required for proper
event modeling.
  To support the extraction of explicit, implicit, and scattered arguments, we
develop a novel dataset, DiscourseEE, which includes 7,464 argument annotations
from online health discourse. Notably, 51.2% of the arguments are implicit, and
17.4% are scattered, making DiscourseEE a unique corpus for complex event
extraction. Additionally, we formulate argument extraction as a text generation
problem to facilitate the extraction of complex argument types. We provide a
comprehensive evaluation of state-of-the-art models and highlight critical open
challenges in generative event extraction. Our data and codebase are available
at https://omar-sharif03.github.io/DiscourseEE."
Variational Bayes Gaussian Splatting,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Recently, 3D Gaussian Splatting has emerged as a promising approach for
modeling 3D scenes using mixtures of Gaussians. The predominant optimization
method for these models relies on backpropagating gradients through a
differentiable rendering pipeline, which struggles with catastrophic forgetting
when dealing with continuous streams of data. To address this limitation, we
propose Variational Bayes Gaussian Splatting (VBGS), a novel approach that
frames training a Gaussian splat as variational inference over model
parameters. By leveraging the conjugacy properties of multivariate Gaussians,
we derive a closed-form variational update rule, allowing efficient updates
from partial, sequential observations without the need for replay buffers. Our
experiments show that VBGS not only matches state-of-the-art performance on
static datasets, but also enables continual learning from sequentially streamed
2D and 3D data, drastically improving performance in this setting."
A novel TLS-based Fingerprinting approach that combines feature expansion and similarity mapping,cs.CR,Cryptography and Security,2024-10-04,"Malicious domains are part of the landscape of the internet but are becoming
more prevalent and more dangerous to both companies and individuals. They can
be hosted on variety of technologies and serve an array of content, ranging
from Malware, command and control, and complex Phishing sites that are designed
to deceive and expose. Tracking, blocking and detecting such domains is
complex, and very often involves complex allow or deny list management or SIEM
integration with open-source TLS fingerprinting techniques. Many fingerprint
techniques such as JARM and JA3 are used by threat hunters to determine domain
classification, but with the increase in TLS similarity, particularly in CDNs,
they are becoming less useful. The aim of this paper is to adapt and evolve
open-source TLS fingerprinting techniques with increased features to enhance
granularity, and to produce a similarity mapping system that enables the
tracking and detection of previously unknown malicious domains. This is done by
enriching TLS fingerprints with HTTP header data and producing a fine grain
similarity visualisation that represented high dimensional data using MinHash
and local sensitivity hashing. Influence was taken from the Chemistry domain,
where the problem of high dimensional similarity in chemical fingerprints is
often encountered. An enriched fingerprint was produced which was then
visualised across three separate datasets. The results were analysed and
evaluated, with 67 previously unknown malicious domains being detected based on
their similarity to known malicious domains and nothing else. The similarity
mapping technique produced demonstrates definite promise in the arena of early
detection of Malware and Phishing domains."
Training Over a Distribution of Hyperparameters for Enhanced Performance and Adaptability on Imbalanced Classification,cs.LG,Machine Learning,2024-10-04,"Although binary classification is a well-studied problem, training reliable
classifiers under severe class imbalance remains a challenge. Recent techniques
mitigate the ill effects of imbalance on training by modifying the loss
functions or optimization methods. We observe that different hyperparameter
values on these loss functions perform better at different recall values. We
propose to exploit this fact by training one model over a distribution of
hyperparameter values--instead of a single value--via Loss Conditional Training
(LCT). Experiments show that training over a distribution of hyperparameters
not only approximates the performance of several models but actually improves
the overall performance of models on both CIFAR and real medical imaging
applications, such as melanoma and diabetic retinopathy detection. Furthermore,
training models with LCT is more efficient because some hyperparameter tuning
can be conducted after training to meet individual needs without needing to
retrain from scratch."
Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Despite their impressive capabilities, Multimodal Large Language Models
(MLLMs) are susceptible to hallucinations, especially assertively fabricating
content not present in the visual inputs. To address the aforementioned
challenge, we follow a common cognitive process - when one's initial memory of
critical on-sight details fades, it is intuitive to look at them a second time
to seek a factual and accurate answer. Therefore, we introduce Memory-space
Visual Retracing (MemVR), a novel hallucination mitigation paradigm that
without the need for external knowledge retrieval or additional fine-tuning. In
particular, we treat visual prompts as supplementary evidence to be reinjected
into MLLMs via Feed Forward Network (FFN) as key-value memory, when the model
is uncertain or even amnesic about question-relevant visual memories.
Comprehensive experimental evaluations demonstrate that MemVR significantly
mitigates hallucination issues across various MLLMs and excels in general
benchmarks without incurring added time overhead, thus emphasizing its
potential for widespread applicability."
Table Question Answering for Low-resourced Indic Languages,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"TableQA is the task of answering questions over tables of structured
information, returning individual cells or tables as output. TableQA research
has focused primarily on high-resource languages, leaving medium- and
low-resource languages with little progress due to scarcity of annotated data
and neural models. We address this gap by introducing a fully automatic
large-scale tableQA data generation process for low-resource languages with
limited budget. We incorporate our data generation method on two Indic
languages, Bengali and Hindi, which have no tableQA datasets or models. TableQA
models trained on our large-scale datasets outperform state-of-the-art LLMs. We
further study the trained models on different aspects, including mathematical
reasoning capabilities and zero-shot cross-lingual transfer. Our work is the
first on low-resource tableQA focusing on scalable data generation and
evaluation procedures. Our proposed data generation method can be applied to
any low-resource language with a web presence. We release datasets, models, and
code (https://github.com/kolk/Low-Resource-TableQA-Indic-languages)."
HyResPINNs: Adaptive Hybrid Residual Networks for Learning Optimal Combinations of Neural and RBF Components for Physics-Informed Modeling,cs.LG,Machine Learning,2024-10-04,"Physics-informed neural networks (PINNs) are an increasingly popular class of
techniques for the numerical solution of partial differential equations (PDEs),
where neural networks are trained using loss functions regularized by relevant
PDE terms to enforce physical constraints. We present a new class of PINNs
called HyResPINNs, which augment traditional PINNs with adaptive hybrid
residual blocks that combine the outputs of a standard neural network and a
radial basis function (RBF) network. A key feature of our method is the
inclusion of adaptive combination parameters within each residual block, which
dynamically learn to weigh the contributions of the neural network and RBF
network outputs. Additionally, adaptive connections between residual blocks
allow for flexible information flow throughout the network. We show that
HyResPINNs are more robust to training point locations and neural network
architectures than traditional PINNs. Moreover, HyResPINNs offer orders of
magnitude greater accuracy than competing methods on certain problems, with
only modest increases in training costs. We demonstrate the strengths of our
approach on challenging PDEs, including the Allen-Cahn equation and the
Darcy-Flow equation. Our results suggest that HyResPINNs effectively bridge the
gap between traditional numerical methods and modern machine learning-based
solvers."
Teaching Transformers Modular Arithmetic at Scale,cs.LG,Machine Learning,2024-10-04,"Modular addition is, on its face, a simple operation: given $N$ elements in
$\mathbb{Z}_q$, compute their sum modulo $q$. Yet, scalable machine learning
solutions to this problem remain elusive: prior work trains ML models that sum
$N \le 6$ elements mod $q \le 1000$. Promising applications of ML models for
cryptanalysis-which often involve modular arithmetic with large $N$ and
$q$-motivate reconsideration of this problem. This work proposes three changes
to the modular addition model training pipeline: more diverse training data, an
angular embedding, and a custom loss function. With these changes, we
demonstrate success with our approach for $N = 256, q = 3329$, a case which is
interesting for cryptographic applications, and a significant increase in $N$
and $q$ over prior work. These techniques also generalize to other modular
arithmetic problems, motivating future work."
Towards Linguistically-Aware and Language-Independent Tokenization for Large Language Models (LLMs),cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"This paper presents a comprehensive study on the tokenization techniques
employed by state-of-the-art large language models (LLMs) and their
implications on the cost and availability of services across different
languages, especially low resource languages. The analysis considers multiple
LLMs, including GPT-4 (using cl100k_base embeddings), GPT-3 (with p50k_base
embeddings), and DaVinci (employing r50k_base embeddings), as well as the
widely used BERT base tokenizer. The study evaluates the tokenization
variability observed across these models and investigates the challenges of
linguistic representation in subword tokenization. The research underscores the
importance of fostering linguistically-aware development practices, especially
for languages that are traditionally under-resourced. Moreover, this paper
introduces case studies that highlight the real-world implications of
tokenization choices, particularly in the context of electronic health record
(EHR) systems. This research aims to promote generalizable Internationalization
(I18N) practices in the development of AI services in this domain and beyond,
with a strong emphasis on inclusivity, particularly for languages traditionally
underrepresented in AI applications."
Training on more Reachable Tasks for Generalisation in Reinforcement Learning,cs.LG,Machine Learning,2024-10-04,"In multi-task reinforcement learning, agents train on a fixed set of tasks
and have to generalise to new ones. Recent work has shown that increased
exploration improves this generalisation, but it remains unclear why exactly
that is. In this paper, we introduce the concept of reachability in multi-task
reinforcement learning and show that an initial exploration phase increases the
number of reachable tasks the agent is trained on. This, and not the increased
exploration, is responsible for the improved generalisation, even to
unreachable tasks. Inspired by this, we propose a novel method Explore-Go that
implements such an exploration phase at the beginning of each episode.
Explore-Go only modifies the way experience is collected and can be used with
most existing on-policy or off-policy reinforcement learning algorithms. We
demonstrate the effectiveness of our method when combined with some popular
algorithms and show an increase in generalisation performance across several
environments."
frdXel: An Expert System for Danish Traffic Law,cs.AI,Artificial Intelligence,2024-10-04,"We present f{\ae}rdXel, a tool for symbolic reasoning in the domain of Danish
traffic law. f{\ae}rdXel combines techniques from logic programming with a
novel interface that allows users to navigate through its reasoning process,
thereby ensuring the system's trustworthiness. A preliminary empirical
evaluation indicates that this work is seen as very promising, and has the
potential to become a foundation for real-world AI tools supporting
professionals in the Danish legal sector."
Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Diffusion models are initially designed for image generation. Recent research
shows that the internal signals within their backbones, named activations, can
also serve as dense features for various discriminative tasks such as semantic
segmentation. Given numerous activations, selecting a small yet effective
subset poses a fundamental problem. To this end, the early study of this field
performs a large-scale quantitative comparison of the discriminative ability of
the activations. However, we find that many potential activations have not been
evaluated, such as the queries and keys used to compute attention scores.
Moreover, recent advancements in diffusion architectures bring many new
activations, such as those within embedded ViT modules. Both combined,
activation selection remains unresolved but overlooked. To tackle this issue,
this paper takes a further step with a much broader range of activations
evaluated. Considering the significant increase in activations, a full-scale
quantitative comparison is no longer operational. Instead, we seek to
understand the properties of these activations, such that the activations that
are clearly inferior can be filtered out in advance via simple qualitative
evaluation. After careful analysis, we discover three properties universal
among diffusion models, enabling this study to go beyond specific models. On
top of this, we present effective feature selection solutions for several
popular diffusion models. Finally, the experiments across multiple
discriminative tasks validate the superiority of our method over the SOTA
competitors. Our code is available at
https://github.com/Darkbblue/generic-diffusion-feature."
Enhancing Autonomous Navigation by Imaging Hidden Objects using Single-Photon LiDAR,cs.RO,Robotics,2024-10-04,"Robust autonomous navigation in environments with limited visibility remains
a critical challenge in robotics. We present a novel approach that leverages
Non-Line-of-Sight (NLOS) sensing using single-photon LiDAR to improve
visibility and enhance autonomous navigation. Our method enables mobile robots
to ""see around corners"" by utilizing multi-bounce light information,
effectively expanding their perceptual range without additional infrastructure.
We propose a three-module pipeline: (1) Sensing, which captures multi-bounce
histograms using SPAD-based LiDAR; (2) Perception, which estimates occupancy
maps of hidden regions from these histograms using a convolutional neural
network; and (3) Control, which allows a robot to follow safe paths based on
the estimated occupancy. We evaluate our approach through simulations and
real-world experiments on a mobile robot navigating an L-shaped corridor with
hidden obstacles. Our work represents the first experimental demonstration of
NLOS imaging for autonomous navigation, paving the way for safer and more
efficient robotic systems operating in complex environments. We also contribute
a novel dynamics-integrated transient rendering framework for simulating NLOS
scenarios, facilitating future research in this domain."
Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Proteins, as essential biomolecules, play a central role in biological
processes, including metabolic reactions and DNA replication. Accurate
prediction of their properties and functions is crucial in biological
applications. Recent development of protein language models (pLMs) with
supervised fine tuning provides a promising solution to this problem. However,
the fine-tuned model is tailored for particular downstream prediction task, and
achieving general-purpose protein understanding remains a challenge. In this
paper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)
framework to bridge this gap. Our approach integrates a noval structure-aware
module into pLMs to inform them with structural knowledge, and then connects
these enhanced pLMs to large language models (LLMs) to generate understanding
of proteins. In this framework, we propose a novel two-stage instruction tuning
pipeline that first establishes a basic understanding of proteins through
caption-based instructions and then refines this understanding using a mixture
of experts (MoEs) to learn more complex properties and functional information
with the same amount of activated parameters. Moreover, we construct the
largest and most comprehensive protein instruction dataset to date, which
allows us to train and evaluate the general-purpose protein understanding
model. Extensive experimental results on open-ended generation and closed-set
answer tasks demonstrate the superior performance of SEPIT over both
closed-source general LLMs and open-source LLMs trained with protein knowledge."
Loading Ceramics: Visualising Possibilities of Robotics in Ceramics,cs.RO,Robotics,2024-10-04,"This article introduces an artistic research project that utilises
artist-in-residency and exhibition as methods for exploring the possibilities
of robotic 3D printing and ceramics. The interdisciplinary project unites
artists and architects to collaborate on a proposed curatorial concept and
Do-It-With-Others (DIWO) technological development. Constraints include
material, specifically local clay, production technique, namely 3D printing
with a robotic arm, and kiln size, as well as an exhibition concept that is
further elaborated in the next chapter. The pictorial presents four projects as
case studies demonstrating how the creatives integrate these constraints into
their processes. This integration leads to the subsequent refinement and
customization of the robotic-ceramics interface, aligning with the
practitioners' requirements through software development. The project's focus
extends beyond artistic outcomes, aiming also to advance the pipeline of 3D
robotic printing in clay, employing a digitally controlled material press that
has been developed in-house, with its functionality refined through practice."
Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Research in natural language processing (NLP) for Computational Social
Science (CSS) heavily relies on data from social media platforms. This data
plays a crucial role in the development of models for analysing
socio-linguistic phenomena within online communities. In this work, we conduct
an in-depth examination of 20 datasets extensively used in NLP for CSS to
comprehensively examine data quality. Our analysis reveals that social media
datasets exhibit varying levels of data duplication. Consequently, this gives
rise to challenges like label inconsistencies and data leakage, compromising
the reliability of models. Our findings also suggest that data duplication has
an impact on the current claims of state-of-the-art performance, potentially
leading to an overestimation of model effectiveness in real-world scenarios.
Finally, we propose new protocols and best practices for improving dataset
development from social media data and its usage."
Re-examining Sexism and Misogyny Classification with Annotator Attitudes,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Gender-Based Violence (GBV) is an increasing problem online, but existing
datasets fail to capture the plurality of possible annotator perspectives or
ensure the representation of affected groups. We revisit two important stages
in the moderation pipeline for GBV: (1) manual data labelling; and (2)
automated classification. For (1), we examine two datasets to investigate the
relationship between annotator identities and attitudes and the responses they
give to two GBV labelling tasks. To this end, we collect demographic and
attitudinal information from crowd-sourced annotators using three validated
surveys from Social Psychology. We find that higher Right Wing Authoritarianism
scores are associated with a higher propensity to label text as sexist, while
for Social Dominance Orientation and Neosexist Attitudes, higher scores are
associated with a negative tendency to do so. For (2), we conduct
classification experiments using Large Language Models and five prompting
strategies, including infusing prompts with annotator information. We find: (i)
annotator attitudes affect the ability of classifiers to predict their labels;
(ii) including attitudinal information can boost performance when we use
well-structured brief annotator descriptions; and (iii) models struggle to
reflect the increased complexity and imbalanced classes of the new label sets."
Ward: Provable RAG Dataset Inference via LLM Watermarks,cs.LG,Machine Learning,2024-10-04,"Retrieval-Augmented Generation (RAG) improves LLMs by enabling them to
incorporate external data during generation. This raises concerns for data
owners regarding unauthorized use of their content in RAG systems. Despite its
importance, the challenge of detecting such unauthorized usage remains
underexplored, with existing datasets and methodologies from adjacent fields
being ill-suited for its study. In this work, we take several steps to bridge
this gap. First, we formalize this problem as (black-box) RAG Dataset Inference
(RAG-DI). To facilitate research on this challenge, we further introduce a
novel dataset specifically designed for benchmarking RAG-DI methods under
realistic conditions, and propose a set of baseline approaches. Building on
this foundation, we introduce Ward, a RAG-DI method based on LLM watermarks
that enables data owners to obtain rigorous statistical guarantees regarding
the usage of their dataset in a RAG system. In our experimental evaluation, we
show that Ward consistently outperforms all baselines across many challenging
settings, achieving higher accuracy, superior query efficiency and robustness.
Our work provides a foundation for future studies of RAG-DI and highlights LLM
watermarks as a promising approach to this problem."
NRGBoost: Energy-Based Generative Boosted Trees,cs.LG,Machine Learning,2024-10-04,"Despite the rise to dominance of deep learning in unstructured data domains,
tree-based methods such as Random Forests (RF) and Gradient Boosted Decision
Trees (GBDT) are still the workhorses for handling discriminative tasks on
tabular data. We explore generative extensions of these popular algorithms with
a focus on explicitly modeling the data density (up to a normalization
constant), thus enabling other applications besides sampling. As our main
contribution we propose an energy-based generative boosting algorithm that is
analogous to the second order boosting implemented in popular packages like
XGBoost. We show that, despite producing a generative model capable of handling
inference tasks over any input variable, our proposed algorithm can achieve
similar discriminative performance to GBDT on a number of real world tabular
datasets, outperforming alternative generative approaches. At the same time, we
show that it is also competitive with neural network based models for sampling."
MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Unsupervised rationale extraction aims to extract text snippets to support
model predictions without explicit rationale annotation. Researchers have made
many efforts to solve this task. Previous works often encode each aspect
independently, which may limit their ability to capture meaningful internal
correlations between aspects. While there has been significant work on
mitigating spurious correlations, our approach focuses on leveraging the
beneficial internal correlations to improve multi-aspect rationale extraction.
In this paper, we propose a Multi-Aspect Rationale Extractor (MARE) to explain
and predict multiple aspects simultaneously. Concretely, we propose a
Multi-Aspect Multi-Head Attention (MAMHA) mechanism based on hard deletion to
encode multiple text chunks simultaneously. Furthermore, multiple special
tokens are prepended in front of the text with each corresponding to one
certain aspect. Finally, multi-task training is deployed to reduce the training
overhead. Experimental results on two unsupervised rationale extraction
benchmarks show that MARE achieves state-of-the-art performance. Ablation
studies further demonstrate the effectiveness of our method. Our codes have
been available at https://github.com/CSU-NLP-Group/MARE."
No Need to Talk: Asynchronous Mixture of Language Models,cs.LG,Machine Learning,2024-10-04,"We introduce SmallTalk LM, an innovative method for training a mixture of
language models in an almost asynchronous manner. Each model of the mixture
specializes in distinct parts of the data distribution, without the need of
high-bandwidth communication between the nodes training each model. At
inference, a lightweight router directs a given sequence to a single expert,
according to a short prefix. This inference scheme naturally uses a fraction of
the parameters from the overall mixture model. Our experiments on language
modeling demonstrate tha SmallTalk LM achieves significantly lower perplexity
than dense model baselines for the same total training FLOPs and an almost
identical inference cost. Finally, in our downstream evaluations we outperform
the dense baseline on $75\%$ of the tasks."
Steering Large Language Models between Code Execution and Textual Reasoning,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"While a lot of recent research focuses on enhancing the textual reasoning
capabilities of Large Language Models (LLMs) by optimizing the multi-agent
framework or reasoning chains, several benchmark tasks can be solved with 100%
success through direct coding, which is more scalable and avoids the
computational overhead associated with textual iterating and searching. Textual
reasoning has inherent limitations in solving tasks with challenges in math,
logics, optimization, and searching, which is unlikely to be solved by simply
scaling up the model and data size. The recently released OpenAI GPT Code
Interpreter and multi-agent frameworks such as AutoGen have demonstrated
remarkable proficiency of integrating code generation and execution to solve
complex tasks using LLMs. However, based on our experiments on 7 existing
popular methods for steering code/text generation in both single- and
multi-turn settings with 14 tasks and 6 types of LLMs (including the new
O1-preview), currently there is no optimal method to correctly steer LLMs to
write code when needed. We discover some interesting patterns on when models
use code vs. textual reasoning with the evolution to task complexity and model
sizes, which even result in an astonishingly inverse scaling law. We also
discover that results from LLM written code are not always better than using
textual reasoning, even if the task could be solved through code. To mitigate
the above issues, we propose three methods to better steer LLM code/text
generation and achieve a notable improvement. The costs of token lengths and
runtime are thoroughly discussed for all the methods. We believe the problem of
steering LLM code/text generation is critical for future research and has much
space for further improvement. Project Page, Datasets, and Codes are available
at https://yongchao98.github.io/CodeSteer/."
A Probabilistic Perspective on Unlearning and Alignment for Large Language Models,cs.LG,Machine Learning,2024-10-04,"Comprehensive evaluation of Large Language Models (LLMs) is an open research
problem. Existing evaluations rely on deterministic point estimates generated
via greedy decoding. However, we find that deterministic evaluations fail to
capture the whole output distribution of a model, yielding inaccurate
estimations of model capabilities. This is particularly problematic in critical
contexts such as unlearning and alignment, where precise model evaluations are
crucial. To remedy this, we introduce the first formal probabilistic evaluation
framework in LLMs. Namely, we derive novel metrics with high-probability
guarantees concerning the output distribution of a model. Our metrics are
application-independent and allow practitioners to make more reliable estimates
about model capabilities before deployment. Through a case study focused on
unlearning, we reveal that deterministic evaluations falsely indicate
successful unlearning, whereas our probabilistic evaluations demonstrate that
most if not all of the supposedly unlearned information remains accessible in
these models. Additionally, we propose a novel unlearning loss based on entropy
optimization and adaptive temperature scaling, which significantly improves
unlearning in probabilistic settings on recent benchmarks. Our proposed shift
from point estimates to probabilistic evaluations of output distributions
represents an important step toward comprehensive evaluations of LLMs.
https://github.com/yascho/probabilistic-unlearning"
HMT-Grasp: A Hybrid Mamba-Transformer Approach for Robot Grasping in Cluttered Environments,cs.RO,Robotics,2024-10-04,"Robot grasping, whether handling isolated objects, cluttered items, or
stacked objects, plays a critical role in industrial and service applications.
However, current visual grasp detection methods based on Convolutional Neural
Networks (CNNs) and Vision Transformers (ViTs) struggle to adapt across various
grasping scenarios due to the imbalance between local and global feature
extraction. In this paper, we propose a novel hybrid Mamba-Transformer approach
to address these challenges. Our method improves robotic visual grasping by
effectively capturing both global and local information through the integration
of Vision Mamba and parallel convolutional-transformer blocks. This hybrid
architecture significantly improves adaptability, precision, and flexibility
across various robotic tasks. To ensure a fair evaluation, we conducted
extensive experiments on the Cornell, Jacquard, and OCID-Grasp datasets,
ranging from simple to complex scenarios. Additionally, we performed both
simulated and real-world robotic experiments. The results demonstrate that our
method not only surpasses state-of-the-art techniques on standard grasping
datasets but also delivers strong performance in both simulation and real-world
robot applications."
Improving Online Bagging for Complex Imbalanced Data Stream,cs.LG,Machine Learning,2024-10-04,"Learning classifiers from imbalanced and concept drifting data streams is
still a challenge. Most of the current proposals focus on taking into account
changes in the global imbalance ratio only and ignore the local difficulty
factors, such as the minority class decomposition into sub-concepts and the
presence of unsafe types of examples (borderline or rare ones). As the above
factors present in the stream may deteriorate the performance of popular online
classifiers, we propose extensions of resampling online bagging, namely
Neighbourhood Undersampling or Oversampling Online Bagging to take better
account of the presence of unsafe minority examples. The performed
computational experiments with synthetic complex imbalanced data streams have
shown their advantage over earlier variants of online bagging resampling
ensembles."
Fine-Grained Expressive Power of Weisfeiler-Leman: A Homomorphism Counting Perspective,cs.LG,Machine Learning,2024-10-04,"The ability of graph neural networks (GNNs) to count homomorphisms has
recently been proposed as a practical and fine-grained measure of their
expressive power. Although several existing works have investigated the
homomorphism counting power of certain GNN families, a simple and unified
framework for analyzing the problem is absent. In this paper, we first propose
\emph{generalized folklore Weisfeiler-Leman (GFWL)} algorithms as a flexible
design basis for expressive GNNs, and then provide a theoretical framework to
algorithmically determine the homomorphism counting power of an arbitrary class
of GNN within the GFWL design space. As the considered design space is large
enough to accommodate almost all known powerful GNNs, our result greatly
extends all existing works, and may find its application in the automation of
GNN model design."
Stabilized Neural Prediction of Potential Outcomes in Continuous Time,cs.LG,Machine Learning,2024-10-04,"Patient trajectories from electronic health records are widely used to
predict potential outcomes of treatments over time, which then allows to
personalize care. Yet, existing neural methods for this purpose have a key
limitation: while some adjust for time-varying confounding, these methods
assume that the time series are recorded in discrete time. In other words, they
are constrained to settings where measurements and treatments are conducted at
fixed time steps, even though this is unrealistic in medical practice. In this
work, we aim to predict potential outcomes in continuous time. The latter is of
direct practical relevance because it allows for modeling patient trajectories
where measurements and treatments take place at arbitrary, irregular
timestamps. We thus propose a new method called stabilized continuous time
inverse propensity network (SCIP-Net). For this, we further derive stabilized
inverse propensity weights for robust prediction of the potential outcomes. To
the best of our knowledge, our SCIP-Net is the first neural method that
performs proper adjustments for time-varying confounding in continuous time."
GAP-RL: Grasps As Points for RL Towards Dynamic Object Grasping,cs.RO,Robotics,2024-10-04,"Dynamic grasping of moving objects in complex, continuous motion scenarios
remains challenging. Reinforcement Learning (RL) has been applied in various
robotic manipulation tasks, benefiting from its closed-loop property. However,
existing RL-based methods do not fully explore the potential for enhancing
visual representations. In this letter, we propose a novel framework called
Grasps As Points for RL (GAP-RL) to effectively and reliably grasp moving
objects. By implementing a fast region-based grasp detector, we build a Grasp
Encoder by transforming 6D grasp poses into Gaussian points and extracting
grasp features as a higher-level abstraction than the original object point
features. Additionally, we develop a Graspable Region Explorer for real-world
deployment, which searches for consistent graspable regions, enabling smoother
grasp generation and stable policy execution. To assess the performance fairly,
we construct a simulated dynamic grasping benchmark involving objects with
various complex motions. Experiment results demonstrate that our method
effectively generalizes to novel objects and unseen dynamic motions compared to
other baselines. Real-world experiments further validate the framework's
sim-to-real transferability."
Classification-Denoising Networks,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Image classification and denoising suffer from complementary issues of lack
of robustness or partially ignoring conditioning information. We argue that
they can be alleviated by unifying both tasks through a model of the joint
probability of (noisy) images and class labels. Classification is performed
with a forward pass followed by conditioning. Using the Tweedie-Miyasawa
formula, we evaluate the denoising function with the score, which can be
computed by marginalization and back-propagation. The training objective is
then a combination of cross-entropy loss and denoising score matching loss
integrated over noise levels. Numerical experiments on CIFAR-10 and ImageNet
show competitive classification and denoising performance compared to reference
deep convolutional classifiers/denoisers, and significantly improves efficiency
compared to previous joint approaches. Our model shows an increased robustness
to adversarial perturbations compared to a standard discriminative classifier,
and allows for a novel interpretation of adversarial gradients as a difference
of denoisers."
CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"With the proliferation of Large Language Models (LLMs) in diverse domains,
there is a particular need for unified evaluation standards in clinical medical
scenarios, where models need to be examined very thoroughly. We present
CliMedBench, a comprehensive benchmark with 14 expert-guided core clinical
scenarios specifically designed to assess the medical ability of LLMs across 7
pivot dimensions. It comprises 33,735 questions derived from real-world medical
reports of top-tier tertiary hospitals and authentic examination exercises. The
reliability of this benchmark has been confirmed in several ways. Subsequent
experiments with existing LLMs have led to the following findings: (i) Chinese
medical LLMs underperform on this benchmark, especially where medical reasoning
and factual consistency are vital, underscoring the need for advances in
clinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs
demonstrate substantial potential in medical clinics, while the limited input
capacity of many medical LLMs hinders their practical use. These findings
reveal both the strengths and limitations of LLMs in clinical scenarios and
offer critical insights for medical research."
FedStein: Enhancing Multi-Domain Federated Learning Through James-Stein Estimator,cs.LG,Machine Learning,2024-10-04,"Federated Learning (FL) facilitates data privacy by enabling collaborative
in-situ training across decentralized clients. Despite its inherent advantages,
FL faces significant challenges of performance and convergence when dealing
with data that is not independently and identically distributed (non-i.i.d.).
While previous research has primarily addressed the issue of skewed label
distribution across clients, this study focuses on the less explored challenge
of multi-domain FL, where client data originates from distinct domains with
varying feature distributions. We introduce a novel method designed to address
these challenges FedStein: Enhancing Multi-Domain Federated Learning Through
the James-Stein Estimator. FedStein uniquely shares only the James-Stein (JS)
estimates of batch normalization (BN) statistics across clients, while
maintaining local BN parameters. The non-BN layer parameters are exchanged via
standard FL techniques. Extensive experiments conducted across three datasets
and multiple models demonstrate that FedStein surpasses existing methods such
as FedAvg and FedBN, with accuracy improvements exceeding 14% in certain
domains leading to enhanced domain generalization. The code is available at
https://github.com/sunnyinAI/FedStein"
Collaborative and Efficient Personalization with Mixtures of Adaptors,cs.LG,Machine Learning,2024-10-04,"Non-iid data is prevalent in real-world federated learning problems. Data
heterogeneity can come in different types in terms of distribution shifts. In
this work, we are interested in the heterogeneity that comes from concept
shifts, i.e., shifts in the prediction across clients. In particular, we
consider multi-task learning, where we want the model to adapt to the task of
the client. We propose a parameter-efficient framework to tackle this issue,
where each client learns to mix between parameter-efficient adaptors according
to its task. We use Low-Rank Adaptors (LoRAs) as the backbone and extend its
concept to other types of layers. We call our framework Federated Low-Rank
Adaptive Learning (FLoRAL). This framework is not an algorithm but rather a
model parameterization for a multi-task learning objective, so it can work on
top of any algorithm that optimizes this objective, which includes many
algorithms from the literature. FLoRAL is memory-efficient, and clients are
personalized with small states (e.g., one number per adaptor) as the adaptors
themselves are federated. Hence, personalization is--in this sense--federated
as well. Even though clients can personalize more freely by training an adaptor
locally, we show that collaborative and efficient training of adaptors is
possible and performs better. We also show that FLoRAL can outperform an
ensemble of full models with optimal cluster assignment, which demonstrates the
benefits of federated personalization and the robustness of FLoRAL to
overfitting. We show promising experimental results on synthetic datasets,
real-world federated multi-task problems such as MNIST, CIFAR-10, and
CIFAR-100. We also provide a theoretical analysis of local SGD on a relaxed
objective and discuss the effects of aggregation mismatch on convergence."
Fourier PINNs: From Strong Boundary Conditions to Adaptive Fourier Bases,cs.LG,Machine Learning,2024-10-04,"Interest is rising in Physics-Informed Neural Networks (PINNs) as a mesh-free
alternative to traditional numerical solvers for partial differential equations
(PDEs). However, PINNs often struggle to learn high-frequency and multi-scale
target solutions. To tackle this problem, we first study a strong Boundary
Condition (BC) version of PINNs for Dirichlet BCs and observe a consistent
decline in relative error compared to the standard PINNs. We then perform a
theoretical analysis based on the Fourier transform and convolution theorem. We
find that strong BC PINNs can better learn the amplitudes of high-frequency
components of the target solutions. However, constructing the architecture for
strong BC PINNs is difficult for many BCs and domain geometries. Enlightened by
our theoretical analysis, we propose Fourier PINNs -- a simple, general, yet
powerful method that augments PINNs with pre-specified, dense Fourier bases.
Our proposed architecture likewise learns high-frequency components better but
places no restrictions on the particular BCs or problem domains. We develop an
adaptive learning and basis selection algorithm via alternating neural net
basis optimization, Fourier and neural net basis coefficient estimation, and
coefficient truncation. This scheme can flexibly identify the significant
frequencies while weakening the nominal frequencies to better capture the
target solution's power spectrum. We show the advantage of our approach through
a set of systematic experiments."
Generative Artificial Intelligence for Navigating Synthesizable Chemical Space,cs.LG,Machine Learning,2024-10-04,"We introduce SynFormer, a generative modeling framework designed to
efficiently explore and navigate synthesizable chemical space. Unlike
traditional molecular generation approaches, we generate synthetic pathways for
molecules to ensure that designs are synthetically tractable. By incorporating
a scalable transformer architecture and a diffusion module for building block
selection, SynFormer surpasses existing models in synthesizable molecular
design. We demonstrate SynFormer's effectiveness in two key applications: (1)
local chemical space exploration, where the model generates synthesizable
analogs of a reference molecule, and (2) global chemical space exploration,
where the model aims to identify optimal molecules according to a black-box
property prediction oracle. Additionally, we demonstrate the scalability of our
approach via the improvement in performance as more computational resources
become available. With our code and trained models openly available, we hope
that SynFormer will find use across applications in drug discovery and
materials science."
Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM Benchmark Scores,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Large language models (LLMs) are stochastic, and not all models give
deterministic answers, even when setting temperature to zero with a fixed
random seed. However, few benchmark studies attempt to quantify uncertainty,
partly due to the time and cost of repeated experiments. We use benchmarks
designed for testing LLMs' capacity to reason about cardinal directions to
explore the impact of experimental repeats on mean score and prediction
interval. We suggest a simple method for cost-effectively quantifying the
uncertainty of a benchmark score and make recommendations concerning
reproducible LLM evaluation."
Gradient-based Jailbreak Images for Multimodal Fusion Models,cs.CR,Cryptography and Security,2024-10-04,"Augmenting language models with image inputs may enable more effective
jailbreak attacks through continuous optimization, unlike text inputs that
require discrete optimization. However, new multimodal fusion models tokenize
all input modalities using non-differentiable functions, which hinders
straightforward attacks. In this work, we introduce the notion of a tokenizer
shortcut that approximates tokenization with a continuous function and enables
continuous optimization. We use tokenizer shortcuts to create the first
end-to-end gradient image attacks against multimodal fusion models. We evaluate
our attacks on Chameleon models and obtain jailbreak images that elicit harmful
information for 72.5% of prompts. Jailbreak images outperform text jailbreaks
optimized with the same objective and require 3x lower compute budget to
optimize 50x more input tokens. Finally, we find that representation
engineering defenses, like Circuit Breakers, trained only on text attacks can
effectively transfer to adversarial image inputs."
MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-object Demand-driven Navigation,cs.RO,Robotics,2024-10-04,"The process of satisfying daily demands is a fundamental aspect of humans'
daily lives. With the advancement of embodied AI, robots are increasingly
capable of satisfying human demands. Demand-driven navigation (DDN) is a task
in which an agent must locate an object to satisfy a specified demand
instruction, such as ``I am thirsty.'' The previous study typically assumes
that each demand instruction requires only one object to be fulfilled and does
not consider individual preferences. However, the realistic human demand may
involve multiple objects. In this paper, we introduce the Multi-object
Demand-driven Navigation (MO-DDN) benchmark, which addresses these nuanced
aspects, including multi-object search and personal preferences, thus making
the MO-DDN task more reflective of real-life scenarios compared to DDN.
Building upon previous work, we employ the concept of ``attribute'' to tackle
this new task. However, instead of solely relying on attribute features in an
end-to-end manner like DDN, we propose a modular method that involves
constructing a coarse-to-fine attribute-based exploration agent (C2FAgent). Our
experimental results illustrate that this coarse-to-fine exploration strategy
capitalizes on the advantages of attributes at various decision-making levels,
resulting in superior performance compared to baseline methods. Code and video
can be found at https://sites.google.com/view/moddn."
A Multimodal Framework for Deepfake Detection,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"The rapid advancement of deepfake technology poses a significant threat to
digital media integrity. Deepfakes, synthetic media created using AI, can
convincingly alter videos and audio to misrepresent reality. This creates risks
of misinformation, fraud, and severe implications for personal privacy and
security. Our research addresses the critical issue of deepfakes through an
innovative multimodal approach, targeting both visual and auditory elements.
This comprehensive strategy recognizes that human perception integrates
multiple sensory inputs, particularly visual and auditory information, to form
a complete understanding of media content. For visual analysis, a model that
employs advanced feature extraction techniques was developed, extracting nine
distinct facial characteristics and then applying various machine learning and
deep learning models. For auditory analysis, our model leverages
mel-spectrogram analysis for feature extraction and then applies various
machine learning and deep learningmodels. To achieve a combined analysis, real
and deepfake audio in the original dataset were swapped for testing purposes
and ensured balanced samples. Using our proposed models for video and audio
classification i.e. Artificial Neural Network and VGG19, the overall sample is
classified as deepfake if either component is identified as such. Our
multimodal framework combines visual and auditory analyses, yielding an
accuracy of 94%."
STREAMS: An Assistive Multimodal AI Framework for Empowering Biosignal Based Robotic Controls,cs.RO,Robotics,2024-10-04,"End-effector based assistive robots face persistent challenges in generating
smooth and robust trajectories when controlled by human's noisy and unreliable
biosignals such as muscle activities and brainwaves. The produced endpoint
trajectories are often jerky and imprecise to perform complex tasks such as
stable robotic grasping. We propose STREAMS (Self-Training Robotic End-to-end
Adaptive Multimodal Shared autonomy) as a novel framework leveraged deep
reinforcement learning to tackle this challenge in biosignal based robotic
control systems. STREAMS blends environmental information and synthetic user
input into a Deep Q Learning Network (DQN) pipeline for an interactive
end-to-end and self-training mechanism to produce smooth trajectories for the
control of end-effector based robots. The proposed framework achieved a
high-performance record of 98% in simulation with dynamic target estimation and
acquisition without any pre-existing datasets. As a zero-shot sim-to-real user
study with five participants controlling a physical robotic arm with noisy head
movements, STREAMS (as an assistive mode) demonstrated significant improvements
in trajectory stabilization, user satisfaction, and task performance reported
as a success rate of 83% compared to manual mode which was 44% without any task
support. STREAMS seeks to improve biosignal based assistive robotic controls by
offering an interactive, end-to-end solution that stabilizes end-effector
trajectories, enhancing task performance and accuracy."
S2C2A: A Flexible Task Space Planning and Control Strategy for Modular Soft Robot Arms,cs.RO,Robotics,2024-10-04,"Modular soft robot arms (MSRAs) are composed of multiple independent modules
connected in a sequence. Due to their modular structure and high degrees of
freedom (DOFs), these modules can simultaneously bend at different angles in
various directions, enabling complex deformation. This capability allows MSRAs
to perform more intricate tasks than single module robots. However, the modular
structure also induces challenges in accurate planning, modeling, and control.
Nonlinearity, hysteresis, and gravity complicate the physical model, while the
modular structure and increased DOFs further lead to accumulative errors along
the sequence. To address these challenges, we propose a flexible task space
planning and control strategy for MSRAs, named S2C2A (State to Configuration to
Action). Our approach formulates an optimization problem, S2C (State to
Configuration planning), which integrates various loss functions and a forward
MSRA model to generate configuration trajectories based on target MSRA states.
Given the model complexity, we leverage a biLSTM network as the forward model.
Subsequently, a configuration controller C2A (Configuration to Action control)
is implemented to follow the planned configuration trajectories, leveraging
only inaccurate internal sensing feedback. Both a biLSTM network and a physical
model are utilized for configuration control. We validated our strategy using a
cable-driven MSRA, demonstrating its ability to perform diverse offline tasks
such as position control, orientation control, and obstacle avoidance.
Furthermore, our strategy endows MSRA with online interaction capability with
targets and obstacles. Future work will focus on addressing MSRA challenges,
such as developing more accurate physical models and reducing configuration
estimation errors along the module sequence."
"A Compact, Low-cost Force and Torque Sensor for Robot Fingers with LED-based Displacement Sensing",cs.RO,Robotics,2024-10-04,"Force/torque sensing is an important modality for robotic manipulation, but
commodity solutions, generally developed with other applications in mind, do
not generally fit the needs of robot hands. This paper introduces a novel
method for six-axis force/torque sensing, using LEDs to sense the displacement
between two plates connected by a transparent elastomer. Our method allows for
finger-size packaging with no amplification electronics, low cost
manufacturing, and easy integration into a complete hand. On test forces
between 0-2 N, our prototype sensor exhibits a mean error between 0.05 and 0.07
N across the three force directions, suggesting future applicability to fine
manipulation tasks."
VEDIT: Latent Prediction Architecture For Procedural Video Representation Learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Procedural video representation learning is an active research area where the
objective is to learn an agent which can anticipate and forecast the future
given the present video input, typically in conjunction with textual
annotations. Prior works often rely on large-scale pretraining of visual
encoders and prediction models with language supervision. However, the
necessity and effectiveness of extending compute intensive pretraining to learn
video clip sequences with noisy text supervision have not yet been fully
validated by previous works. In this work, we show that a strong off-the-shelf
frozen pretrained visual encoder, along with a well designed prediction model,
can achieve state-of-the-art (SoTA) performance in forecasting and procedural
planning without the need for pretraining the prediction model, nor requiring
additional supervision from language or ASR. Instead of learning
representations from pixel space, our method utilizes the latent embedding
space of publicly available vision encoders. By conditioning on frozen
clip-level embeddings from observed steps to predict the actions of unseen
steps, our prediction model is able to learn robust representations for
forecasting through iterative denoising - leveraging the recent advances in
diffusion transformers (Peebles & Xie, 2023). Empirical studies over a total of
five procedural learning tasks across four datasets (NIV, CrossTask, COIN and
Ego4D-v2) show that our model advances the strong baselines in long-horizon
action anticipation (+2.6% in Verb ED@20, +3.1% in Noun ED@20), and
significantly improves the SoTA in step forecasting (+5.0%), task
classification (+3.8%), and procedure planning tasks (up to +2.28% in success
rate, +3.39% in mAcc, and +0.90% in mIoU)."
On the Hardness of Learning One Hidden Layer Neural Networks,cs.LG,Machine Learning,2024-10-04,"In this work, we consider the problem of learning one hidden layer ReLU
neural networks with inputs from $\mathbb{R}^d$. We show that this learning
problem is hard under standard cryptographic assumptions even when: (1) the
size of the neural network is polynomial in $d$, (2) its input distribution is
a standard Gaussian, and (3) the noise is Gaussian and polynomially small in
$d$. Our hardness result is based on the hardness of the Continuous Learning
with Errors (CLWE) problem, and in particular, is based on the largely believed
worst-case hardness of approximately solving the shortest vector problem up to
a multiplicative polynomial factor."
Vulnerability Detection via Topological Analysis of Attention Maps,cs.LG,Machine Learning,2024-10-04,"Recently, deep learning (DL) approaches to vulnerability detection have
gained significant traction. These methods demonstrate promising results, often
surpassing traditional static code analysis tools in effectiveness.
  In this study, we explore a novel approach to vulnerability detection
utilizing the tools from topological data analysis (TDA) on the attention
matrices of the BERT model. Our findings reveal that traditional machine
learning (ML) techniques, when trained on the topological features extracted
from these attention matrices, can perform competitively with pre-trained
language models (LLMs) such as CodeBERTa. This suggests that TDA tools,
including persistent homology, are capable of effectively capturing semantic
information critical for identifying vulnerabilities."
Is Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"The potential effectiveness of counterspeech as a hate speech mitigation
strategy is attracting increasing interest in the NLG research community,
particularly towards the task of automatically producing it. However,
automatically generated responses often lack the argumentative richness which
characterises expert-produced counterspeech. In this work, we focus on two
aspects of counterspeech generation to produce more cogent responses. First, by
investigating the tension between helpfulness and harmlessness of LLMs, we test
whether the presence of safety guardrails hinders the quality of the
generations. Secondly, we assess whether attacking a specific component of the
hate speech results in a more effective argumentative strategy to fight online
hate. By conducting an extensive human and automatic evaluation, we show how
the presence of safety guardrails can be detrimental also to a task that
inherently aims at fostering positive social interactions. Moreover, our
results show that attacking a specific component of the hate speech, and in
particular its implicit negative stereotype and its hateful parts, leads to
higher-quality generations."
S7: Selective and Simplified State Space Layers for Sequence Modeling,cs.LG,Machine Learning,2024-10-04,"A central challenge in sequence modeling is efficiently handling tasks with
extended contexts. While recent state-space models (SSMs) have made significant
progress in this area, they often lack input-dependent filtering or require
substantial increases in model complexity to handle input variability. We
address this gap by introducing S7, a simplified yet powerful SSM that can
handle input dependence while incorporating stable reparameterization and
specific design choices to dynamically adjust state transitions based on input
content, maintaining efficiency and performance. We prove that this
reparameterization ensures stability in long-sequence modeling by keeping state
transitions well-behaved over time. Additionally, it controls the gradient
norm, enabling efficient training and preventing issues like exploding or
vanishing gradients. S7 significantly outperforms baselines across various
sequence modeling tasks, including neuromorphic event-based datasets, Long
Range Arena benchmarks, and various physical and biological time series.
Overall, S7 offers a more straightforward approach to sequence modeling without
relying on complex, domain-specific inductive biases, achieving significant
improvements across key benchmarks."
Diffusion State-Guided Projected Gradient for Inverse Problems,cs.LG,Machine Learning,2024-10-04,"Recent advancements in diffusion models have been effective in learning data
priors for solving inverse problems. They leverage diffusion sampling steps for
inducing a data prior while using a measurement guidance gradient at each step
to impose data consistency. For general inverse problems, approximations are
needed when an unconditionally trained diffusion model is used since the
measurement likelihood is intractable, leading to inaccurate posterior
sampling. In other words, due to their approximations, these methods fail to
preserve the generation process on the data manifold defined by the diffusion
prior, leading to artifacts in applications such as image restoration. To
enhance the performance and robustness of diffusion models in solving inverse
problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad),
which projects the measurement gradient onto a subspace that is a low-rank
approximation of an intermediate state of the diffusion process. DiffStateGrad,
as a module, can be added to a wide range of diffusion-based inverse solvers to
improve the preservation of the diffusion process on the prior manifold and
filter out artifact-inducing components. We highlight that DiffStateGrad
improves the robustness of diffusion models in terms of the choice of
measurement guidance step size and noise while improving the worst-case
performance. Finally, we demonstrate that DiffStateGrad improves upon the
state-of-the-art on linear and nonlinear image restoration inverse problems."
Linear Transformer Topological Masking with Graph Random Features,cs.LG,Machine Learning,2024-10-04,"When training transformers on graph-structured data, incorporating
information about the underlying topology is crucial for good performance.
Topological masking, a type of relative position encoding, achieves this by
upweighting or downweighting attention depending on the relationship between
the query and keys in a graph. In this paper, we propose to parameterise
topological masks as a learnable function of a weighted adjacency matrix -- a
novel, flexible approach which incorporates a strong structural inductive bias.
By approximating this mask with graph random features (for which we prove the
first known concentration bounds), we show how this can be made fully
compatible with linear attention, preserving $\mathcal{O}(N)$ time and space
complexity with respect to the number of input tokens. The fastest previous
alternative was $\mathcal{O}(N \log N)$ and only suitable for specific graphs.
Our efficient masking algorithms provide strong performance gains for tasks on
image and point cloud data, including with $>30$k nodes."
Auto-GDA: Automatic Domain Adaptation for Efficient Grounding Verification in Retrieval Augmented Generation,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"While retrieval augmented generation (RAG) has been shown to enhance
factuality of large language model (LLM) outputs, LLMs still suffer from
hallucination, generating incorrect or irrelevant information. One common
detection strategy involves prompting the LLM again to assess whether its
response is grounded in the retrieved evidence, but this approach is costly.
Alternatively, lightweight natural language inference (NLI) models for
efficient grounding verification can be used at inference time. While existing
pre-trained NLI models offer potential solutions, their performance remains
subpar compared to larger models on realistic RAG inputs. RAG inputs are more
complex than most datasets used for training NLI models and have
characteristics specific to the underlying knowledge base, requiring adaptation
of the NLI models to a specific target domain. Additionally, the lack of
labeled instances in the target domain makes supervised domain adaptation,
e.g., through fine-tuning, infeasible. To address these challenges, we
introduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework
enables unsupervised domain adaptation through synthetic data generation.
Unlike previous methods that rely on handcrafted filtering and augmentation
strategies, Auto-GDA employs an iterative process to continuously improve the
quality of generated samples using weak labels from less efficient teacher
models and discrete optimization to select the most promising augmented
samples. Experimental results demonstrate the effectiveness of our approach,
with models fine-tuned on synthetic data using Auto-GDA often surpassing the
performance of the teacher model and reaching the performance level of LLMs at
10 % of their computational cost."
"Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges",cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Vietnamese, a low-resource language, is typically categorized into three
primary dialect groups that belong to Northern, Central, and Southern Vietnam.
However, each province within these regions exhibits its own distinct
pronunciation variations. Despite the existence of various speech recognition
datasets, none of them has provided a fine-grained classification of the 63
dialects specific to individual provinces of Vietnam. To address this gap, we
introduce Vietnamese Multi-Dialect (ViMD) dataset, a novel comprehensive
dataset capturing the rich diversity of 63 provincial dialects spoken across
Vietnam. Our dataset comprises 102.56 hours of audio, consisting of
approximately 19,000 utterances, and the associated transcripts contain over
1.2 million words. To provide benchmarks and simultaneously demonstrate the
challenges of our dataset, we fine-tune state-of-the-art pre-trained models for
two downstream tasks: (1) Dialect identification and (2) Speech recognition.
The empirical results suggest two implications including the influence of
geographical factors on dialects, and the constraints of current approaches in
speech recognition tasks involving multi-dialect speech data. Our dataset is
available for research purposes."
CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Detecting logical fallacies in texts can help users spot argument flaws, but
automating this detection is not easy. Manually annotating fallacies in
large-scale, real-world text data to create datasets for developing and
validating detection models is costly. This paper introduces CoCoLoFa, the
largest known logical fallacy dataset, containing 7,706 comments for 648 news
articles, with each comment labeled for fallacy presence and type. We recruited
143 crowd workers to write comments embodying specific fallacy types (e.g.,
slippery slope) in response to news articles. Recognizing the complexity of
this writing task, we built an LLM-powered assistant into the workers'
interface to aid in drafting and refining their comments. Experts rated the
writing quality and labeling validity of CoCoLoFa as high and reliable.
BERT-based models fine-tuned using CoCoLoFa achieved the highest fallacy
detection (F1=0.86) and classification (F1=0.87) performance on its test set,
outperforming the state-of-the-art LLMs. Our work shows that combining
crowdsourcing and LLMs enables us to more effectively construct datasets for
complex linguistic phenomena that crowd workers find challenging to produce on
their own."
Dynamic Diffusion Transformer,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Diffusion Transformer (DiT), an emerging diffusion model for image
generation, has demonstrated superior performance but suffers from substantial
computational costs. Our investigations reveal that these costs stem from the
static inference paradigm, which inevitably introduces redundant computation in
certain diffusion timesteps and spatial regions. To address this inefficiency,
we propose Dynamic Diffusion Transformer (DyDiT), an architecture that
dynamically adjusts its computation along both timestep and spatial dimensions
during generation. Specifically, we introduce a Timestep-wise Dynamic Width
(TDW) approach that adapts model width conditioned on the generation timesteps.
In addition, we design a Spatial-wise Dynamic Token (SDT) strategy to avoid
redundant computation at unnecessary spatial locations. Extensive experiments
on various datasets and different-sized models verify the superiority of DyDiT.
Notably, with <3% additional fine-tuning iterations, our method reduces the
FLOPs of DiT-XL by 51%, accelerates generation by 1.73, and achieves a
competitive FID score of 2.07 on ImageNet. The code is publicly available at
https://github.com/NUS-HPC-AI-Lab/ Dynamic-Diffusion-Transformer."
MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents,cs.LG,Machine Learning,2024-10-04,"MLLM agents demonstrate potential for complex embodied tasks by retrieving
multimodal task-relevant trajectory data. However, current retrieval methods
primarily focus on surface-level similarities of textual or visual cues in
trajectories, neglecting their effectiveness for the specific task at hand. To
address this issue, we propose a novel method, MLLM as ReTriever (MART), which
enhances the performance of embodied agents by utilizing interaction data to
fine-tune an MLLM retriever based on preference learning, such that the
retriever fully considers the effectiveness of trajectories and prioritize them
for unseen tasks. We also introduce Trajectory Abstraction, a mechanism that
leverages MLLMs' summarization capabilities to represent trajectories with
fewer tokens while preserving key information, enabling agents to better
comprehend milestones in the trajectory. Experimental results across various
environments demonstrate our method significantly improves task success rates
in unseen scenes compared to baseline methods. This work presents a new
paradigm for multimodal retrieval in embodied agents, by fine-tuning a
general-purpose MLLM as the retriever to assess trajectory effectiveness. All
benchmark task sets and simulator code modifications for action and observation
spaces will be released."
How Language Models Prioritize Contextual Grammatical Cues?,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Transformer-based language models have shown an excellent ability to
effectively capture and utilize contextual information. Although various
analysis techniques have been used to quantify and trace the contribution of
single contextual cues to a target task such as subject-verb agreement or
coreference resolution, scenarios in which multiple relevant cues are available
in the context remain underexplored. In this paper, we investigate how language
models handle gender agreement when multiple gender cue words are present, each
capable of independently disambiguating a target gender pronoun. We analyze two
widely used Transformer-based models: BERT, an encoder-based, and GPT-2, a
decoder-based model. Our analysis employs two complementary approaches: context
mixing analysis, which tracks information flow within the model, and a variant
of activation patching, which measures the impact of cues on the model's
prediction. We find that BERT tends to prioritize the first cue in the context
to form both the target word representations and the model's prediction, while
GPT-2 relies more on the final cue. Our findings reveal striking differences in
how encoder-based and decoder-based models prioritize and use contextual
information for their predictions."
On Uncertainty In Natural Language Processing,cs.AI,Artificial Intelligence,2024-10-04,"The last decade in deep learning has brought on increasingly capable systems
that are deployed on a wide variety of applications. In natural language
processing, the field has been transformed by a number of breakthroughs
including large language models, which are used in increasingly many
user-facing applications. In order to reap the benefits of this technology and
reduce potential harms, it is important to quantify the reliability of model
predictions and the uncertainties that shroud their development.
  This thesis studies how uncertainty in natural language processing can be
characterized from a linguistic, statistical and neural perspective, and how it
can be reduced and quantified through the design of the experimental pipeline.
We further explore uncertainty quantification in modeling by theoretically and
empirically investigating the effect of inductive model biases in text
classification tasks. The corresponding experiments include data for three
different languages (Danish, English and Finnish) and tasks as well as a large
set of different uncertainty quantification approaches. Additionally, we
propose a method for calibrated sampling in natural language generation based
on non-exchangeable conformal prediction, which provides tighter token sets
with better coverage of the actual continuation. Lastly, we develop an approach
to quantify confidence in large black-box language models using auxiliary
predictors, where the confidence is predicted from the input to and generated
output text of the target model alone."
CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Motion diffusion models and Reinforcement Learning (RL) based control for
physics-based simulations have complementary strengths for human motion
generation. The former is capable of generating a wide variety of motions,
adhering to intuitive control such as text, while the latter offers physically
plausible motion and direct interaction with the environment. In this work, we
present a method that combines their respective strengths. CLoSD is a
text-driven RL physics-based controller, guided by diffusion generation for
various tasks. Our key insight is that motion diffusion can serve as an
on-the-fly universal planner for a robust RL controller. To this end, CLoSD
maintains a closed-loop interaction between two modules -- a Diffusion Planner
(DiP), and a tracking controller. DiP is a fast-responding autoregressive
diffusion model, controlled by textual prompts and target locations, and the
controller is a simple and robust motion imitator that continuously receives
motion plans from DiP and provides feedback from the environment. CLoSD is
capable of seamlessly performing a sequence of different tasks, including
navigation to a goal location, striking an object with a hand or foot as
specified in a text prompt, sitting down, and getting up.
https://guytevet.github.io/CLoSD-page/"
Exploring the Benefit of Activation Sparsity in Pre-training,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Pre-trained Transformers inherently possess the characteristic of sparse
activation, where only a small fraction of the neurons are activated for each
token. While sparse activation has been explored through post-training methods,
its potential in pre-training remains untapped. In this work, we first study
how activation properties change during pre-training. Our examination reveals
that Transformers exhibit sparse activation throughout the majority of the
pre-training process while the activation correlation keeps evolving as
training progresses. Leveraging this observation, we propose Switchable
Sparse-Dense Learning (SSD). SSD adaptively switches between the
Mixtures-of-Experts (MoE) based sparse training and the conventional dense
training during the pre-training process, leveraging the efficiency of sparse
training and avoiding the static activation correlation of sparse training.
Compared to dense training, SSD achieves comparable performance with identical
model size and reduces pre-training costs. Moreover, the models trained with
SSD can be directly used as MoE models for sparse inference and achieve the
same performance as dense models with up to $2\times$ faster inference speed.
Codes are available at https://github.com/thunlp/moefication."
SOI: Scaling Down Computational Complexity by Estimating Partial States of the Model,cs.LG,Machine Learning,2024-10-04,"Consumer electronics used to follow the miniaturization trend described by
Moore's Law. Despite increased processing power in Microcontroller Units
(MCUs), MCUs used in the smallest appliances are still not capable of running
even moderately big, state-of-the-art artificial neural networks (ANNs)
especially in time-sensitive scenarios. In this work, we present a novel method
called Scattered Online Inference (SOI) that aims to reduce the computational
complexity of ANNs. SOI leverages the continuity and seasonality of time-series
data and model predictions, enabling extrapolation for processing speed
improvements, particularly in deeper layers. By applying compression, SOI
generates more general inner partial states of ANN, allowing skipping full
model recalculation at each inference."
ToolGen: Unified Tool Retrieval and Calling via Generation,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"As large language models (LLMs) advance, their inability to autonomously
execute tasks by directly interacting with external tools remains a critical
limitation. Traditional methods rely on inputting tool descriptions as context,
which is constrained by context length and requires separate, often
inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that
integrates tool knowledge directly into the LLM's parameters by representing
each tool as a unique token. This enables the LLM to generate tool calls and
arguments as part of its next token prediction capabilities, seamlessly
blending tool invocation with language generation. Our framework allows the LLM
to access and utilize a vast amount of tools with no additional retrieval step,
significantly enhancing both performance and scalability. Experimental results
with over 47,000 tools show that ToolGen not only achieves superior results in
both tool retrieval and autonomous task completion but also sets the stage for
a new era of AI agents that can adapt to tools across diverse domains. By
fundamentally transforming tool retrieval into a generative process, ToolGen
paves the way for more versatile, efficient, and autonomous AI systems. ToolGen
enables end-to-end tool learning and opens opportunities for integration with
other advanced techniques such as chain-of-thought and reinforcement learning,
thereby expanding the practical capabilities of LLMs."
Dessie: Disentanglement for Articulated 3D Horse Shape and Pose Estimation from Images,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"In recent years, 3D parametric animal models have been developed to aid in
estimating 3D shape and pose from images and video. While progress has been
made for humans, it's more challenging for animals due to limited annotated
data. To address this, we introduce the first method using synthetic data
generation and disentanglement to learn to regress 3D shape and pose. Focusing
on horses, we use text-based texture generation and a synthetic data pipeline
to create varied shapes, poses, and appearances, learning disentangled spaces.
Our method, Dessie, surpasses existing 3D horse reconstruction methods and
generalizes to other large animals like zebras, cows, and deer. See the project
website at: \url{https://celiali.github.io/Dessie/}."
Zebra: In-Context and Generative Pretraining for Solving Parametric PDEs,cs.LG,Machine Learning,2024-10-04,"Solving time-dependent parametric partial differential equations (PDEs) is
challenging, as models must adapt to variations in parameters such as
coefficients, forcing terms, and boundary conditions. Data-driven neural
solvers either train on data sampled from the PDE parameters distribution in
the hope that the model generalizes to new instances or rely on gradient-based
adaptation and meta-learning to implicitly encode the dynamics from
observations. This often comes with increased inference complexity. Inspired by
the in-context learning capabilities of large language models (LLMs), we
introduce Zebra, a novel generative auto-regressive transformer designed to
solve parametric PDEs without requiring gradient adaptation at inference. By
leveraging in-context information during both pre-training and inference, Zebra
dynamically adapts to new tasks by conditioning on input sequences that
incorporate context trajectories or preceding states. This approach enables
Zebra to flexibly handle arbitrarily sized context inputs and supports
uncertainty quantification through the sampling of multiple solution
trajectories. We evaluate Zebra across a variety of challenging PDE scenarios,
demonstrating its adaptability, robustness, and superior performance compared
to existing approaches."
EvenNICER-SLAM: Event-based Neural Implicit Encoding SLAM,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"The advancement of dense visual simultaneous localization and mapping (SLAM)
has been greatly facilitated by the emergence of neural implicit
representations. Neural implicit encoding SLAM, a typical example of which is
NICE-SLAM, has recently demonstrated promising results in large-scale indoor
scenes. However, these methods typically rely on temporally dense RGB-D image
streams as input in order to function properly. When the input source does not
support high frame rates or the camera movement is too fast, these methods
often experience crashes or significant degradation in tracking and mapping
accuracy. In this paper, we propose EvenNICER-SLAM, a novel approach that
addresses this issue through the incorporation of event cameras. Event cameras
are bio-inspired cameras that respond to intensity changes instead of absolute
brightness. Specifically, we integrated an event loss backpropagation stream
into the NICE-SLAM pipeline to enhance camera tracking with insufficient RGB-D
input. We found through quantitative evaluation that EvenNICER-SLAM, with an
inclusion of higher-frequency event image input, significantly outperforms
NICE-SLAM with reduced RGB-D input frequency. Our results suggest the potential
for event cameras to improve the robustness of dense SLAM systems against fast
camera motion in real-world scenarios."
A General Framework for Producing Interpretable Semantic Text Embeddings,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Semantic text embedding is essential to many tasks in Natural Language
Processing (NLP). While black-box models are capable of generating high-quality
embeddings, their lack of interpretability limits their use in tasks that
demand transparency. Recent approaches have improved interpretability by
leveraging domain-expert-crafted or LLM-generated questions, but these methods
rely heavily on expert input or well-prompt design, which restricts their
generalizability and ability to generate discriminative questions across a wide
range of tasks. To address these challenges, we introduce \algo{CQG-MBQA}
(Contrastive Question Generation - Multi-task Binary Question Answering), a
general framework for producing interpretable semantic text embeddings across
diverse tasks. Our framework systematically generates highly discriminative,
low cognitive load yes/no questions through the \algo{CQG} method and answers
them efficiently with the \algo{MBQA} model, resulting in interpretable
embeddings in a cost-effective manner. We validate the effectiveness and
interpretability of \algo{CQG-MBQA} through extensive experiments and ablation
studies, demonstrating that it delivers embedding quality comparable to many
advanced black-box models while maintaining inherently interpretability.
Additionally, \algo{CQG-MBQA} outperforms other interpretable text embedding
methods across various downstream tasks."
Images Speak Volumes: User-Centric Assessment of Image Generation for Accessible Communication,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Explanatory images play a pivotal role in accessible and easy-to-read (E2R)
texts. However, the images available in online databases are not tailored
toward the respective texts, and the creation of customized images is
expensive. In this large-scale study, we investigated whether text-to-image
generation models can close this gap by providing customizable images quickly
and easily. We benchmarked seven, four open- and three closed-source, image
generation models and provide an extensive evaluation of the resulting images.
In addition, we performed a user study with people from the E2R target group to
examine whether the images met their requirements. We find that some of the
models show remarkable performance, but none of the models are ready to be used
at a larger scale without human supervision. Our research is an important step
toward facilitating the creation of accessible information for E2R creators and
tailoring accessible images to the target group's needs."
How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Natural Language Inference (NLI) evaluation is crucial for assessing language
understanding models; however, popular datasets suffer from systematic spurious
correlations that artificially inflate actual model performance. To address
this, we propose a method for the automated creation of a challenging test set
without relying on the manual construction of artificial and unrealistic
examples. We categorize the test set of popular NLI datasets into three
difficulty levels by leveraging methods that exploit training dynamics. This
categorization significantly reduces spurious correlation measures, with
examples labeled as having the highest difficulty showing markedly decreased
performance and encompassing more realistic and diverse linguistic phenomena.
When our characterization method is applied to the training set, models trained
with only a fraction of the data achieve comparable performance to those
trained on the full dataset, surpassing other dataset characterization
techniques. Our research addresses limitations in NLI dataset construction,
providing a more authentic evaluation of model performance with implications
for diverse NLU applications."
Cayley Graph Propagation,cs.LG,Machine Learning,2024-10-04,"In spite of the plethora of success stories with graph neural networks (GNNs)
on modelling graph-structured data, they are notoriously vulnerable to
over-squashing, whereby tasks necessitate the mixing of information between
distance pairs of nodes. To address this problem, prior work suggests rewiring
the graph structure to improve information flow. Alternatively, a significant
body of research has dedicated itself to discovering and precomputing
bottleneck-free graph structures to ameliorate over-squashing. One well
regarded family of bottleneck-free graphs within the mathematical community are
expander graphs, with prior work$\unicode{x2014}$Expander Graph Propagation
(EGP)$\unicode{x2014}$proposing the use of a well-known expander graph
family$\unicode{x2014}$the Cayley graphs of the $\mathrm{SL}(2,\mathbb{Z}_n)$
special linear group$\unicode{x2014}$as a computational template for GNNs.
However, in EGP the computational graphs used are truncated to align with a
given input graph. In this work, we show that truncation is detrimental to the
coveted expansion properties. Instead, we propose CGP, a method to propagate
information over a complete Cayley graph structure, thereby ensuring it is
bottleneck-free to better alleviate over-squashing. Our empirical evidence
across several real-world datasets not only shows that CGP recovers significant
improvements as compared to EGP, but it is also akin to or outperforms
computationally complex graph rewiring techniques."
"Can Mamba Always Enjoy the ""Free Lunch""?",cs.LG,Machine Learning,2024-10-04,"Transformers have been the cornerstone of current Large Language Models
(LLMs); however, its linear growth in overhead during inference with respect to
sequence length poses challenges for modeling long sequences. In this context,
Mamba has gradually attracted attention due to its constant-level size during
inference and existing empirical results have shown that it can perform
comparably to Transformers in sequence modeling while offering significant
savings. However, one may ask that, can Mamba always enjoy the ``free lunch""?
In this paper, we focus on analyzing the expressive ability of Mamba from a
theoretical standpoint. First, inspired by the connection between Mamba and
linear attention, we investigate potential shortcomings of the Mamba when
performing the COPY operation. Our results indicate that Mamba with constant
size may encounter bottlenecks when handling COPY, while it can achieve perfect
performance when the size scales linearly with sequence length. Based on this
observation, we analyze Mamba's ability to tackle DP problems when equipped
with Chain of Thought (CoT). Our findings suggest that to solve arbitrary DP
problems, the total cost of Mamba is comparable to standard and efficient
Transformers. However, similar to efficient Transformers, when facing DP
problems with favorable properties such as locality, Mamba can provide savings
in overhead. Our results contribute to a deeper understanding of Mamba."
One2set + Large Language Model: Best Partners for Keyphrase Generation,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Keyphrase generation (KPG) aims to automatically generate a collection of
phrases representing the core concepts of a given document. The dominant
paradigms in KPG include one2seq and one2set. Recently, there has been
increasing interest in applying large language models (LLMs) to KPG. Our
preliminary experiments reveal that it is challenging for a single model to
excel in both recall and precision. Further analysis shows that: 1) the one2set
paradigm owns the advantage of high recall, but suffers from improper
assignments of supervision signals during training; 2) LLMs are powerful in
keyphrase selection, but existing selection methods often make redundant
selections. Given these observations, we introduce a generate-then-select
framework decomposing KPG into two steps, where we adopt a one2set-based model
as generator to produce candidates and then use an LLM as selector to select
keyphrases from these candidates. Particularly, we make two important
improvements on our generator and selector: 1) we design an Optimal
Transport-based assignment strategy to address the above improper assignments;
2) we model the keyphrase selection as a sequence labeling task to alleviate
redundant selections. Experimental results on multiple benchmark datasets show
that our framework significantly surpasses state-of-the-art models, especially
in absent keyphrase prediction."
Img2CAD: Conditioned 3D CAD Model Generation from Single Image with Structured Visual Geometry,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"In this paper, we propose Img2CAD, the first approach to our knowledge that
uses 2D image inputs to generate CAD models with editable parameters. Unlike
existing AI methods for 3D model generation using text or image inputs often
rely on mesh-based representations, which are incompatible with CAD tools and
lack editability and fine control, Img2CAD enables seamless integration between
AI-based 3D reconstruction and CAD software. We have identified an innovative
intermediate representation called Structured Visual Geometry (SVG),
characterized by vectorized wireframes extracted from objects. This
representation significantly enhances the performance of generating conditioned
CAD models. Additionally, we introduce two new datasets to further support
research in this area: ABC-mono, the largest known dataset comprising over
200,000 3D CAD models with rendered images, and KOCAD, the first dataset
featuring real-world captured objects alongside their ground truth CAD models,
supporting further research in conditioned CAD model generation."
"Surgical, Cheap, and Flexible: Mitigating False Refusal in Language Models via Single Vector Ablation",cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Training a language model to be both helpful and harmless requires careful
calibration of refusal behaviours: Models should refuse to follow malicious
instructions or give harmful advice (e.g. ""how do I kill someone?""), but they
should not refuse safe requests, even if they superficially resemble unsafe
ones (e.g. ""how do I kill a Python process?""). Avoiding such false refusal, as
prior work has shown, is challenging even for highly-capable language models.
In this paper, we propose a simple and surgical method for mitigating false
refusal in language models via single vector ablation. For a given model, we
extract a false refusal vector and show that ablating this vector reduces false
refusal rate without negatively impacting model safety and general model
capabilities. We also show that our approach can be used for fine-grained
calibration of model safety. Our approach is training-free and model-agnostic,
making it useful for mitigating the problem of false refusal in current and
future language models."
Team MTS @ AutoMin 2021: An Overview of Existing Summarization Approaches and Comparison to Unsupervised Summarization Techniques,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Remote communication through video or audio conferences has become more
popular than ever because of the worldwide pandemic. These events, therefore,
have provoked the development of systems for automatic minuting of spoken
language leading to AutoMin 2021 challenge. The following paper illustrates the
results of the research that team MTS has carried out while participating in
the Automatic Minutes challenge. In particular, in this paper we analyze
existing approaches to text and speech summarization, propose an unsupervised
summarization technique based on clustering and provide a pipeline that
includes an adapted automatic speech recognition block able to run on real-life
recordings. The proposed unsupervised technique outperforms pre-trained
summarization models on the automatic minuting task with Rouge 1, Rouge 2 and
Rouge L values of 0.21, 0.02 and 0.2 on the dev set, with Rouge 1, Rouge 2,
Rouge L, Adequacy, Grammatical correctness and Fluency values of 0.180, 0.035,
0.098, 1.857, 2.304, 1.911 on the test set accordingly"
Predictive Coding for Decision Transformer,cs.LG,Machine Learning,2024-10-04,"Recent work in offline reinforcement learning (RL) has demonstrated the
effectiveness of formulating decision-making as return-conditioned supervised
learning. Notably, the decision transformer (DT) architecture has shown promise
across various domains. However, despite its initial success, DTs have
underperformed on several challenging datasets in goal-conditioned RL. This
limitation stems from the inefficiency of return conditioning for guiding
policy learning, particularly in unstructured and suboptimal datasets,
resulting in DTs failing to effectively learn temporal compositionality.
Moreover, this problem might be further exacerbated in long-horizon
sparse-reward tasks. To address this challenge, we propose the Predictive
Coding for Decision Transformer (PCDT) framework, which leverages generalized
future conditioning to enhance DT methods. PCDT utilizes an architecture that
extends the DT framework, conditioned on predictive codings, enabling
decision-making based on both past and future factors, thereby improving
generalization. Through extensive experiments on eight datasets from the
AntMaze and FrankaKitchen environments, our proposed method achieves
performance on par with or surpassing existing popular value-based and
transformer-based methods in offline goal-conditioned RL. Furthermore, we also
evaluate our method on a goal-reaching task with a physical robot."
Camel: Communication-Efficient and Maliciously Secure Federated Learning in the Shuffle Model of Differential Privacy,cs.CR,Cryptography and Security,2024-10-04,"Federated learning (FL) has rapidly become a compelling paradigm that enables
multiple clients to jointly train a model by sharing only gradient updates for
aggregation, without revealing their local private data. In order to protect
the gradient updates which could also be privacy-sensitive, there has been a
line of work studying local differential privacy (LDP) mechanisms to provide a
formal privacy guarantee. With LDP mechanisms, clients locally perturb their
gradient updates before sharing them out for aggregation. However, such
approaches are known for greatly degrading the model utility, due to heavy
noise addition. To enable a better privacy-utility tradeoff, a recently
emerging trend is to apply the shuffle model of DP in FL, which relies on an
intermediate shuffling operation on the perturbed gradient updates to achieve
privacy amplification. Following this trend, in this paper, we present Camel, a
new communication-efficient and maliciously secure FL framework in the shuffle
model of DP. Camel first departs from existing works by ambitiously supporting
integrity check for the shuffle computation, achieving security against
malicious adversary. Specifically, Camel builds on the trending cryptographic
primitive of secret-shared shuffle, with custom techniques we develop for
optimizing system-wide communication efficiency, and for lightweight integrity
checks to harden the security of server-side computation. In addition, we also
derive a significantly tighter bound on the privacy loss through analyzing the
Renyi differential privacy (RDP) of the overall FL process. Extensive
experiments demonstrate that Camel achieves better privacy-utility trade-offs
than the state-of-the-art work, with promising performance."
EBES: Easy Benchmarking for Event Sequences,cs.LG,Machine Learning,2024-10-04,"Event sequences, characterized by irregular sampling intervals and a mix of
categorical and numerical features, are common data structures in various
real-world domains such as healthcare, finance, and user interaction logs.
Despite advances in temporal data modeling techniques, there is no standardized
benchmarks for evaluating their performance on event sequences. This
complicates result comparison across different papers due to varying evaluation
protocols, potentially misleading progress in this field. We introduce EBES, a
comprehensive benchmarking tool with standardized evaluation scenarios and
protocols, focusing on regression and classification problems with
sequence-level targets. Our library simplifies benchmarking, dataset addition,
and method integration through a unified interface. It includes a novel
synthetic dataset and provides preprocessed real-world datasets, including the
largest publicly available banking dataset. Our results provide an in-depth
analysis of datasets, identifying some as unsuitable for model comparison. We
investigate the importance of modeling temporal and sequential components, as
well as the robustness and scaling properties of the models. These findings
highlight potential directions for future research. Our benchmark aim is to
facilitate reproducible research, expediting progress and increasing real-world
impacts."
GraphCroc: Cross-Correlation Autoencoder for Graph Structural Reconstruction,cs.LG,Machine Learning,2024-10-04,"Graph-structured data is integral to many applications, prompting the
development of various graph representation methods. Graph autoencoders (GAEs),
in particular, reconstruct graph structures from node embeddings. Current GAE
models primarily utilize self-correlation to represent graph structures and
focus on node-level tasks, often overlooking multi-graph scenarios. Our
theoretical analysis indicates that self-correlation generally falls short in
accurately representing specific graph features such as islands, symmetrical
structures, and directional edges, particularly in smaller or multiple graph
contexts. To address these limitations, we introduce a cross-correlation
mechanism that significantly enhances the GAE representational capabilities.
Additionally, we propose GraphCroc, a new GAE that supports flexible encoder
architectures tailored for various downstream tasks and ensures robust
structural reconstruction, through a mirrored encoding-decoding process. This
model also tackles the challenge of representation bias during optimization by
implementing a loss-balancing strategy. Both theoretical analysis and numerical
evaluations demonstrate that our methodology significantly outperforms existing
self-correlation-based GAEs in graph structure reconstruction."
Killing Two Flies with One Stone: An Attempt to Break LLMs Using English->Icelandic Idioms and Proper Names,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"This paper presents the submission of the \'Arni Magn\'usson Institute's team
to the WMT24 test suite subtask, focusing on idiomatic expressions and proper
names for the English->Icelandic translation direction.
  Intuitively and empirically, idioms and proper names are known to be a
significant challenge for modern translation models. We create two different
test suites. The first evaluates the competency of MT systems in translating
common English idiomatic expressions, as well as testing whether systems can
distinguish between those expressions and the same phrases when used in a
literal context. The second test suite consists of place names that should be
translated into their Icelandic exonyms (and correctly inflected) and pairs of
Icelandic names that share a surface form between the male and female variants,
so that incorrect translations impact meaning as well as readability.
  The scores reported are relatively low, especially for idiomatic expressions
and place names, and indicate considerable room for improvement."
Lightning UQ Box: A Comprehensive Framework for Uncertainty Quantification in Deep Learning,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Uncertainty quantification (UQ) is an essential tool for applying deep neural
networks (DNNs) to real world tasks, as it attaches a degree of confidence to
DNN outputs. However, despite its benefits, UQ is often left out of the
standard DNN workflow due to the additional technical knowledge required to
apply and evaluate existing UQ procedures. Hence there is a need for a
comprehensive toolbox that allows the user to integrate UQ into their modelling
workflow, without significant overhead. We introduce \texttt{Lightning UQ Box}:
a unified interface for applying and evaluating various approaches to UQ. In
this paper, we provide a theoretical and quantitative comparison of the wide
range of state-of-the-art UQ methods implemented in our toolbox. We focus on
two challenging vision tasks: (i) estimating tropical cyclone wind speeds from
infrared satellite imagery and (ii) estimating the power output of solar panels
from RGB images of the sky. By highlighting the differences between methods our
results demonstrate the need for a broad and approachable experimental
framework for UQ, that can be used for benchmarking UQ methods. The toolbox,
example implementations, and further information are available at:
https://github.com/lightning-uq-box/lightning-uq-box"
From Epilepsy Seizures Classification to Detection: A Deep Learning-based Approach for Raw EEG Signals,cs.LG,Machine Learning,2024-10-04,"Epilepsy represents the most prevalent neurological disease in the world.
One-third of people suffering from mesial temporal lobe epilepsy (MTLE) exhibit
drug resistance, urging the need to develop new treatments. A key part in
anti-seizure medication (ASM) development is the capability of detecting and
quantifying epileptic seizures occurring in electroencephalogram (EEG) signals,
which is crucial for treatment efficacy evaluation. In this study, we
introduced a seizure detection pipeline based on deep learning models applied
to raw EEG signals. This pipeline integrates: a new pre-processing technique
which segments continuous raw EEG signals without prior distinction between
seizure and seizure-free activities; a post-processing algorithm developed to
reassemble EEG segments and allow the identification of seizures start/end; and
finally, a new evaluation procedure based on a strict seizure events comparison
between predicted and real labels. Models training have been performed using a
data splitting strategy which addresses the potential for data leakage. We
demonstrated the fundamental differences between a seizure classification and a
seizure detection task and showed the differences in performance between the
two tasks. Finally, we demonstrated the generalization capabilities across
species of our best architecture, combining a Convolutional Neural Network and
a Transformer encoder. The model was trained on animal EEGs and tested on human
EEGs with a F1-score of 93% on a balanced Bonn dataset."
"Cogs in a Machine, Doing What They're Meant to Do -- The AMI Submission to the WMT24 General Translation Task",cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"This paper presents the submission of the \'Arni Magnusson Institute's team
to the WMT24 General translation task. We work on the English->Icelandic
translation direction. Our system comprises four translation models and a
grammar correction model. For training our models we carefully curate our
datasets, aggressively filtering out sentence pairs that may detrimentally
affect the quality of our system's output. Some of our data are collected from
human translations and some are synthetically generated. A part of the
synthetic data is generated using an LLM, and we find that it increases the
translation capability of our system significantly."
Predicting perturbation targets with causal differential networks,cs.LG,Machine Learning,2024-10-04,"Rationally identifying variables responsible for changes to a biological
system can enable myriad applications in disease understanding and cell
engineering. From a causality perspective, we are given two datasets generated
by the same causal model, one observational (control) and one interventional
(perturbed). The goal is to isolate the subset of measured variables (e.g.
genes) that were the targets of the intervention, i.e. those whose conditional
independencies have changed. Knowing the causal graph would limit the search
space, allowing us to efficiently pinpoint these variables. However, current
algorithms that infer causal graphs in the presence of unknown intervention
targets scale poorly to the hundreds or thousands of variables in biological
data, as they must jointly search the combinatorial spaces of graphs and
consistent intervention targets. In this work, we propose a causality-inspired
approach for predicting perturbation targets that decouples the two search
steps. First, we use an amortized causal discovery model to separately infer
causal graphs from the observational and interventional datasets. Then, we
learn to map these paired graphs to the sets of variables that were intervened
upon, in a supervised learning framework. This approach consistently
outperforms baselines for perturbation modeling on seven single-cell
transcriptomics datasets, each with thousands of measured variables. We also
demonstrate significant improvements over six causal discovery algorithms in
predicting intervention targets across a variety of tractable, synthetic
datasets."
Mitigating Adversarial Perturbations for Deep Reinforcement Learning via Vector Quantization,cs.LG,Machine Learning,2024-10-04,"Recent studies reveal that well-performing reinforcement learning (RL) agents
in training often lack resilience against adversarial perturbations during
deployment. This highlights the importance of building a robust agent before
deploying it in the real world. Most prior works focus on developing robust
training-based procedures to tackle this problem, including enhancing the
robustness of the deep neural network component itself or adversarially
training the agent on strong attacks. In this work, we instead study an input
transformation-based defense for RL. Specifically, we propose using a variant
of vector quantization (VQ) as a transformation for input observations, which
is then used to reduce the space of adversarial attacks during testing,
resulting in the transformed observations being less affected by attacks. Our
method is computationally efficient and seamlessly integrates with adversarial
training, further enhancing the robustness of RL agents against adversarial
attacks. Through extensive experiments in multiple environments, we demonstrate
that using VQ as the input transformation effectively defends against
adversarial attacks on the agent's observations."
Make Interval Bound Propagation great again,cs.LG,Machine Learning,2024-10-04,"In various scenarios motivated by real life, such as medical data analysis,
autonomous driving, and adversarial training, we are interested in robust deep
networks. A network is robust when a relatively small perturbation of the input
cannot lead to drastic changes in output (like change of class, etc.). This
falls under the broader scope field of Neural Network Certification (NNC). Two
crucial problems in NNC are of profound interest to the scientific community:
how to calculate the robustness of a given pre-trained network and how to
construct robust networks. The common approach to constructing robust networks
is Interval Bound Propagation (IBP). This paper demonstrates that IBP is
sub-optimal in the first case due to its susceptibility to the wrapping effect.
Even for linear activation, IBP gives strongly sub-optimal bounds.
Consequently, one should use strategies immune to the wrapping effect to obtain
bounds close to optimal ones. We adapt two classical approaches dedicated to
strict computations -- Dubleton Arithmetic and Affine Arithmetic -- to mitigate
the wrapping effect in neural networks. These techniques yield precise results
for networks with linear activation functions, thus resisting the wrapping
effect. As a result, we achieve bounds significantly closer to the optimal
level than IBPs."
Collision-Aware Traversability Analysis for Autonomous Vehicles in the Context of Agricultural Robotics,cs.RO,Robotics,2024-10-04,"In this paper, we introduce a novel method for safe navigation in
agricultural robotics. As global environmental challenges intensify, robotics
offers a powerful solution to reduce chemical usage while meeting the
increasing demands for food production. However, significant challenges remain
in ensuring the autonomy and resilience of robots operating in unstructured
agricultural environments. Obstacles such as crops and tall grass, which are
deformable, must be identified as safely traversable, compared to rigid
obstacles. To address this, we propose a new traversability analysis method
based on a 3D spectral map reconstructed using a LIDAR and a multispectral
camera. This approach enables the robot to distinguish between safe and unsafe
collisions with deformable obstacles. We perform a comprehensive evaluation of
multispectral metrics for vegetation detection and incorporate these metrics
into an augmented environmental map. Utilizing this map, we compute a
physics-based traversability metric that accounts for the robot's weight and
size, ensuring safe navigation over deformable obstacles."
Latent Abstractions in Generative Diffusion Models,cs.LG,Machine Learning,2024-10-04,"In this work we study how diffusion-based generative models produce
high-dimensional data, such as an image, by implicitly relying on a
manifestation of a low-dimensional set of latent abstractions, that guide the
generative process. We present a novel theoretical framework that extends NLF,
and that offers a unique perspective on SDE-based generative models. The
development of our theory relies on a novel formulation of the joint (state and
measurement) dynamics, and an information-theoretic measure of the influence of
the system state on the measurement process. According to our theory, diffusion
models can be cast as a system of SDE, describing a non-linear filter in which
the evolution of unobservable latent abstractions steers the dynamics of an
observable measurement process (corresponding to the generative pathways). In
addition, we present an empirical study to validate our theory and previous
empirical results on the emergence of latent abstractions at different stages
of the generative process."
Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Cross-lingual AMR parsing is the task of predicting AMR graphs in a target
language when training data is available only in a source language. Due to the
small size of AMR training data and evaluation data, cross-lingual AMR parsing
has only been explored in a small set of languages such as English, Spanish,
German, Chinese, and Italian. Taking inspiration from Langedijk et al. (2022),
who apply meta-learning to tackle cross-lingual syntactic parsing, we
investigate the use of meta-learning for cross-lingual AMR parsing. We evaluate
our models in $k$-shot scenarios (including 0-shot) and assess their
effectiveness in Croatian, Farsi, Korean, Chinese, and French. Notably, Korean
and Croatian test sets are developed as part of our work, based on the existing
The Little Prince English AMR corpus, and made publicly available. We
empirically study our method by comparing it to classical joint learning. Our
findings suggest that while the meta-learning model performs slightly better in
0-shot evaluation for certain languages, the performance gain is minimal or
absent when $k$ is higher than 0."
LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Auto-Regressive (AR) models have recently gained prominence in image
generation, often matching or even surpassing the performance of diffusion
models. However, one major limitation of AR models is their sequential nature,
which processes tokens one at a time, slowing down generation compared to
models like GANs or diffusion-based methods that operate more efficiently.
While speculative decoding has proven effective for accelerating LLMs by
generating multiple tokens in a single forward, its application in visual AR
models remains largely unexplored. In this work, we identify a challenge in
this setting, which we term \textit{token selection ambiguity}, wherein visual
AR models frequently assign uniformly low probabilities to tokens, hampering
the performance of speculative decoding. To overcome this challenge, we propose
a relaxed acceptance condition referred to as LANTERN that leverages the
interchangeability of tokens in latent space. This relaxation restores the
effectiveness of speculative decoding in visual AR models by enabling more
flexible use of candidate tokens that would otherwise be prematurely rejected.
Furthermore, by incorporating a total variation distance bound, we ensure that
these speed gains are achieved without significantly compromising image quality
or semantic coherence. Experimental results demonstrate the efficacy of our
method in providing a substantial speed-up over speculative decoding. In
specific, compared to a na\""ive application of the state-of-the-art speculative
decoding, LANTERN increases speed-ups by $\mathbf{1.75}\times$ and
$\mathbf{1.76}\times$, as compared to greedy decoding and random sampling,
respectively, when applied to LlamaGen, a contemporary visual AR model."
Generating Equivalent Representations of Code By A Self-Reflection Approach,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Equivalent Representations (ERs) of code are textual representations that
preserve the same semantics as the code itself, e.g., natural language comments
and pseudocode. ERs play a critical role in software development and
maintenance. However, how to automatically generate ERs of code remains an open
challenge. In this paper, we propose a self-reflection approach to generating
ERs of code. It enables two Large Language Models (LLMs) to work mutually and
produce an ER through a reflection process. Depending on whether constraints on
ERs are applied, our approach generates ERs in both open and constrained
settings. We conduct a empirical study to generate ERs in two settings and
obtain eight findings. (1) Generating ERs in the open setting. In the open
setting, we allow LLMs to represent code without any constraints, analyzing the
resulting ERs and uncovering five key findings. These findings shed light on
how LLMs comprehend syntactic structures, APIs, and numerical computations in
code. (2) Generating ERs in the constrained setting. In the constrained
setting, we impose constraints on ERs, such as natural language comments,
pseudocode, and flowcharts. This allows our approach to address a range of
software engineering tasks. Based on our experiments, we have three findings
demonstrating that our approach can effectively generate ERs that adhere to
specific constraints, thus supporting various software engineering tasks. (3)
Future directions. We also discuss potential future research directions, such
as deriving intermediate languages for code generation, exploring LLM-friendly
requirement descriptions, and further supporting software engineering tasks. We
believe that this paper will spark discussions in research communities and
inspire many follow-up studies."
Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning,cs.LG,Machine Learning,2024-10-04,"Neurosymbolic learning has emerged as a promising paradigm to incorporate
symbolic reasoning into deep learning models. However, existing frameworks are
limited in scalability with respect to both the training data and the
complexity of symbolic programs. We propose Dolphin, a framework to scale
neurosymbolic learning at a fundamental level by mapping both forward chaining
and backward gradient propagation in symbolic programs to vectorized
computations. For this purpose, Dolphin introduces a set of abstractions and
primitives built directly on top of a high-performance deep learning framework
like PyTorch, effectively enabling symbolic programs to be written as PyTorch
modules. It thereby enables neurosymbolic programs to be written in a language
like Python that is familiar to developers and compile them to computation
graphs that are amenable to end-to-end differentiation on GPUs. We evaluate
Dolphin on a suite of 13 benchmarks across 5 neurosymbolic tasks that combine
deep learning models for text, image, or video processing with symbolic
programs that involve multi-hop reasoning, recursion, and even black-box
functions like Python eval(). Dolphin only takes 0.33%-37.17% of the time (and
2.77% on average) to train these models on the largest input per task compared
to baselines Scallop, ISED, and IndeCateR+, which time out on most of these
inputs. Models written in Dolphin also achieve state-of-the-art accuracies even
on the largest benchmarks."
Practical Light Clients for Committee-Based Blockchains,cs.CR,Cryptography and Security,2024-10-04,"Light clients are gaining increasing attention in the literature since they
obviate the need for users to set up dedicated blockchain full nodes. While the
literature features a number of light client instantiations, most light client
protocols optimize for long offline phases and implicitly assume that the block
headers to be verified are signed by highly dynamic validators.
  In this paper, we show that (i) most light clients are rarely offline for
more than a week, and (ii) validators are unlikely to drastically change in
most permissioned blockchains and in a number of permissionless blockchains,
such as Cosmos and Polkadot. Motivated by these findings, we propose a novel
practical system that optimizes for such realistic assumptions and achieves
minimal communication and computational costs for light clients when compared
to existing protocols. By means of a prototype implementation of our solution,
we show that our protocol achieves a reduction by up to $90$ and $40000\times$
(respectively) in end-to-end latency and up to $1000$ and $10000\times$
(respectively) smaller proof size when compared to two state-of-the-art light
client instantiations from the literature."
Zero-Shot Fact Verification via Natural Logic and Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"The recent development of fact verification systems with natural logic has
enhanced their explainability by aligning claims with evidence through
set-theoretic operators, providing faithful justifications. Despite these
advancements, such systems often rely on a large amount of training data
annotated with natural logic. To address this issue, we propose a zero-shot
method that utilizes the generalization capabilities of instruction-tuned large
language models. To comprehensively assess the zero-shot capabilities of our
method and other fact verification systems, we evaluate all models on both
artificial and real-world claims, including multilingual datasets. We also
compare our method against other fact verification systems in two setups.
First, in the zero-shot generalization setup, we demonstrate that our approach
outperforms other systems that were not specifically trained on natural logic
data, achieving an average accuracy improvement of 8.96 points over the
best-performing baseline. Second, in the zero-shot transfer setup, we show that
current systems trained on natural logic data do not generalize well to other
domains, and our method outperforms these systems across all datasets with
real-world claims."
An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Radiological services are experiencing unprecedented demand, leading to
increased interest in automating radiology report generation. Existing
Vision-Language Models (VLMs) suffer from hallucinations, lack
interpretability, and require expensive fine-tuning. We introduce SAE-Rad,
which uses sparse autoencoders (SAEs) to decompose latent representations from
a pre-trained vision transformer into human-interpretable features. Our hybrid
architecture combines state-of-the-art SAE advancements, achieving accurate
latent reconstructions while maintaining sparsity. Using an off-the-shelf
language model, we distil ground-truth reports into radiological descriptions
for each SAE feature, which we then compile into a full report for each image,
eliminating the need for fine-tuning large models for this task. To the best of
our knowledge, SAE-Rad represents the first instance of using mechanistic
interpretability techniques explicitly for a downstream multi-modal reasoning
task. On the MIMIC-CXR dataset, SAE-Rad achieves competitive radiology-specific
metrics compared to state-of-the-art models while using significantly fewer
computational resources for training. Qualitative analysis reveals that SAE-Rad
learns meaningful visual concepts and generates reports aligning closely with
expert interpretations. Our results suggest that SAEs can enhance multimodal
reasoning in healthcare, providing a more interpretable alternative to existing
VLMs."
Metadata Matters for Time Series: Informative Forecasting with Transformers,cs.LG,Machine Learning,2024-10-04,"Time series forecasting is prevalent in extensive real-world applications,
such as financial analysis and energy planning. Previous studies primarily
focus on time series modality, endeavoring to capture the intricate variations
and dependencies inherent in time series. Beyond numerical time series data, we
notice that metadata (e.g.~dataset and variate descriptions) also carries
valuable information essential for forecasting, which can be used to identify
the application scenario and provide more interpretable knowledge than digit
sequences. Inspired by this observation, we propose a Metadata-informed Time
Series Transformer (MetaTST), which incorporates multiple levels of
context-specific metadata into Transformer forecasting models to enable
informative time series forecasting. To tackle the unstructured nature of
metadata, MetaTST formalizes them into natural languages by pre-designed
templates and leverages large language models (LLMs) to encode these texts into
metadata tokens as a supplement to classic series tokens, resulting in an
informative embedding. Further, a Transformer encoder is employed to
communicate series and metadata tokens, which can extend series representations
by metadata information for more accurate forecasting. This design also allows
the model to adaptively learn context-specific patterns across various
scenarios, which is particularly effective in handling large-scale,
diverse-scenario forecasting tasks. Experimentally, MetaTST achieves
state-of-the-art compared to advanced time series models and LLM-based methods
on widely acknowledged short- and long-term forecasting benchmarks, covering
both single-dataset individual and multi-dataset joint training settings."
Local Attention Mechanism: Boosting the Transformer Architecture for Long-Sequence Time Series Forecasting,cs.LG,Machine Learning,2024-10-04,"Transformers have become the leading choice in natural language processing
over other deep learning architectures. This trend has also permeated the field
of time series analysis, especially for long-horizon forecasting, showcasing
promising results both in performance and running time.
  In this paper, we introduce Local Attention Mechanism (LAM), an efficient
attention mechanism tailored for time series analysis. This mechanism exploits
the continuity properties of time series to reduce the number of attention
scores computed. We present an algorithm for implementing LAM in tensor algebra
that runs in time and memory O(nlogn), significantly improving upon the O(n^2)
time and memory complexity of traditional attention mechanisms. We also note
the lack of proper datasets to evaluate long-horizon forecast models. Thus, we
propose a novel set of datasets to improve the evaluation of models addressing
long-horizon forecasting challenges.
  Our experimental analysis demonstrates that the vanilla transformer
architecture magnified with LAM surpasses state-of-the-art models, including
the vanilla attention mechanism. These results confirm the effectiveness of our
approach and highlight a range of future challenges in long-sequence time
series forecasting."
Comparative Analysis and Ensemble Enhancement of Leading CNN Architectures for Breast Cancer Classification,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"This study introduces a novel and accurate approach to breast cancer
classification using histopathology images. It systematically compares leading
Convolutional Neural Network (CNN) models across varying image datasets,
identifies their optimal hyperparameters, and ranks them based on
classification efficacy. To maximize classification accuracy for each model we
explore, the effects of data augmentation, alternative fully-connected layers,
model training hyperparameter settings, and, the advantages of retraining
models versus using pre-trained weights. Our methodology includes several
original concepts, including serializing generated datasets to ensure
consistent data conditions across training runs and significantly reducing
training duration. Combined with automated curation of results, this enabled
the exploration of over 2,000 training permutations -- such a comprehensive
comparison is as yet unprecedented. Our findings establish the settings
required to achieve exceptional classification accuracy for standalone CNN
models and rank them by model efficacy. Based on these results, we propose
ensemble architectures that stack three high-performing standalone CNN models
together with diverse classifiers, resulting in improved classification
accuracy. The ability to systematically run so many model permutations to get
the best outcomes gives rise to very high quality results, including 99.75% for
BreakHis x40 and BreakHis x200 and 95.18% for the Bach datasets when split into
train, validation and test datasets. The Bach Online blind challenge, yielded
89% using this approach. Whilst this study is based on breast cancer
histopathology image datasets, the methodology is equally applicable to other
medical image datasets."
EmojiHeroVR: A Study on Facial Expression Recognition under Partial Occlusion from Head-Mounted Displays,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Emotion recognition promotes the evaluation and enhancement of Virtual
Reality (VR) experiences by providing emotional feedback and enabling advanced
personalization. However, facial expressions are rarely used to recognize
users' emotions, as Head-Mounted Displays (HMDs) occlude the upper half of the
face. To address this issue, we conducted a study with 37 participants who
played our novel affective VR game EmojiHeroVR. The collected database,
EmoHeVRDB (EmojiHeroVR Database), includes 3,556 labeled facial images of 1,778
reenacted emotions. For each labeled image, we also provide 29 additional
frames recorded directly before and after the labeled image to facilitate
dynamic Facial Expression Recognition (FER). Additionally, EmoHeVRDB includes
data on the activations of 63 facial expressions captured via the Meta Quest
Pro VR headset for each frame. Leveraging our database, we conducted a baseline
evaluation on the static FER classification task with six basic emotions and
neutral using the EfficientNet-B0 architecture. The best model achieved an
accuracy of 69.84% on the test set, indicating that FER under HMD occlusion is
feasible but significantly more challenging than conventional FER."
Does SpatioTemporal information benefit Two video summarization benchmarks?,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"An important aspect of summarizing videos is understanding the temporal
context behind each part of the video to grasp what is and is not important.
Video summarization models have in recent years modeled spatio-temporal
relationships to represent this information. These models achieved
state-of-the-art correlation scores on important benchmark datasets. However,
what has not been reviewed is whether spatio-temporal relationships are even
required to achieve state-of-the-art results. Previous work in activity
recognition has found biases, by prioritizing static cues such as scenes or
objects, over motion information. In this paper we inquire if similar spurious
relationships might influence the task of video summarization. To do so, we
analyse the role that temporal information plays on existing benchmark
datasets. We first estimate a baseline with temporally invariant models to see
how well such models rank on benchmark datasets (TVSum and SumMe). We then
disrupt the temporal order of the videos to investigate the impact it has on
existing state-of-the-art models. One of our findings is that the temporally
invariant models achieve competitive correlation scores that are close to the
human baselines on the TVSum dataset. We also demonstrate that existing models
are not affected by temporal perturbations. Furthermore, with certain
disruption strategies that shuffle fixed time segments, we can actually improve
their correlation scores. With these results, we find that spatio-temporal
relationship play a minor role and we raise the question whether these
benchmarks adequately model the task of video summarization. Code available at:
https://github.com/AashGan/TemporalPerturbSum"
Visual-O1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-thoughts Reasoning,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"As large-scale models evolve, language instructions are increasingly utilized
in multi-modal tasks. Due to human language habits, these instructions often
contain ambiguities in real-world scenarios, necessitating the integration of
visual context or common sense for accurate interpretation. However, even
highly intelligent large models exhibit significant performance limitations on
ambiguous instructions, where weak reasoning abilities of disambiguation can
lead to catastrophic errors. To address this issue, this paper proposes
Visual-O1, a multi-modal multi-turn chain-of-thought reasoning framework. It
simulates human multi-modal multi-turn reasoning, providing instantial
experience for highly intelligent models or empirical experience for generally
intelligent models to understand ambiguous instructions. Unlike traditional
methods that require models to possess high intelligence to understand long
texts or perform lengthy complex reasoning, our framework does not
significantly increase computational overhead and is more general and
effective, even for generally intelligent models. Experiments show that our
method not only significantly enhances the performance of models of different
intelligence levels on ambiguous instructions but also improves their
performance on general datasets. Our work highlights the potential of
artificial intelligence to work like humans in real-world scenarios with
uncertainty and ambiguity. We will release our data and code."
Influence-oriented Personalized Federated Learning,cs.LG,Machine Learning,2024-10-04,"Traditional federated learning (FL) methods often rely on fixed weighting for
parameter aggregation, neglecting the mutual influence by others. Hence, their
effectiveness in heterogeneous data contexts is limited. To address this
problem, we propose an influence-oriented federated learning framework, namely
FedC^2I, which quantitatively measures Client-level and Class-level Influence
to realize adaptive parameter aggregation for each client. Our core idea is to
explicitly model the inter-client influence within an FL system via the
well-crafted influence vector and influence matrix. The influence vector
quantifies client-level influence, enables clients to selectively acquire
knowledge from others, and guides the aggregation of feature representation
layers. Meanwhile, the influence matrix captures class-level influence in a
more fine-grained manner to achieve personalized classifier aggregation. We
evaluate the performance of FedC^2I against existing federated learning methods
under non-IID settings and the results demonstrate the superiority of our
method."
Context and System Fusion in Post-ASR Emotion Recognition with Large Language Models,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Large language models (LLMs) have started to play a vital role in modelling
speech and text. To explore the best use of context and multiple systems'
outputs for post-ASR speech emotion prediction, we study LLM prompting on a
recent task named GenSEC. Our techniques include ASR transcript ranking,
variable conversation context, and system output fusion. We show that the
conversation context has diminishing returns and the metric used to select the
transcript for prediction is crucial. Finally, our best submission surpasses
the provided baseline by 20% in absolute accuracy."
"Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models",cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Inspired by the recent success of LLMs, the field of human motion
understanding has increasingly shifted towards the development of large motion
models. Despite some progress, current state-of-the-art works remain far from
achieving truly generalist models, largely due to the lack of large-scale,
high-quality motion data. To address this, we present MotionBase, the first
million-level motion generation benchmark, offering 15 times the data volume of
the previous largest dataset, and featuring multimodal data with hierarchically
detailed text descriptions. By leveraging this vast dataset, our large motion
model demonstrates strong performance across a broad range of motions,
including unseen ones. Through systematic investigation, we underscore the
importance of scaling both data and model size, with synthetic data and pseudo
labels playing a crucial role in mitigating data acquisition costs. Moreover,
our research reveals the limitations of existing evaluation metrics,
particularly in handling out-of-domain text instructions -- an issue that has
long been overlooked. In addition to these, we introduce a novel 2D lookup-free
approach for motion tokenization, which preserves motion information and
expands codebook capacity, further enhancing the representative ability of
large motion models. The release of MotionBase and the insights gained from
this study are expected to pave the way for the development of more powerful
and versatile motion generation models."
Selective Test-Time Adaptation for Unsupervised Anomaly Detection using Neural Implicit Representations,cs.LG,Machine Learning,2024-10-04,"Deep learning models in medical imaging often encounter challenges when
adapting to new clinical settings unseen during training. Test-time adaptation
offers a promising approach to optimize models for these unseen domains, yet
its application in anomaly detection (AD) remains largely unexplored. AD aims
to efficiently identify deviations from normative distributions; however, full
adaptation, including pathological shifts, may inadvertently learn the
anomalies it intends to detect. We introduce a novel concept of
\emph{selective} test-time adaptation that utilizes the inherent
characteristics of deep pre-trained features to adapt \emph{selectively} in a
zero-shot manner to any test image from an unseen domain. This approach employs
a model-agnostic, lightweight multi-layer perceptron for neural implicit
representations, enabling the adaptation of outputs from any
reconstruction-based AD method without altering the source-trained model.
Rigorous validation in brain AD demonstrated that our strategy substantially
enhances detection accuracy for multiple conditions and different target
distributions. Specifically, our method improves the detection rates by up to
78\% for enlarged ventricles and 24\% for edemas."
SELU: Self-Learning Embodied MLLMs in Unknown Environments,cs.LG,Machine Learning,2024-10-04,"Recently, multimodal large language models (MLLMs) have demonstrated strong
visual understanding and decision-making capabilities, enabling the exploration
of autonomously improving MLLMs in unknown environments. However, external
feedback like human or environmental feedback is not always available. To
address this challenge, existing methods primarily focus on enhancing the
decision-making capabilities of MLLMs through voting and scoring mechanisms,
while little effort has been paid to improving the environmental comprehension
of MLLMs in unknown environments. To fully unleash the self-learning potential
of MLLMs, we propose a novel actor-critic self-learning paradigm, dubbed SELU,
inspired by the actor-critic paradigm in reinforcement learning. The critic
employs self-asking and hindsight relabeling to extract knowledge from
interaction trajectories collected by the actor, thereby augmenting its
environmental comprehension. Simultaneously, the actor is improved by the
self-feedback provided by the critic, enhancing its decision-making. We
evaluate our method in the AI2-THOR and VirtualHome environments, and SELU
achieves critic improvements of approximately 28% and 30%, and actor
improvements of about 20% and 24% via self-learning."
Action Selection Learning for Multi-label Multi-view Action Recognition,cs.CV,Computer Vision and Pattern Recognition,2024-10-04,"Multi-label multi-view action recognition aims to recognize multiple
concurrent or sequential actions from untrimmed videos captured by multiple
cameras. Existing work has focused on multi-view action recognition in a narrow
area with strong labels available, where the onset and offset of each action
are labeled at the frame-level. This study focuses on real-world scenarios
where cameras are distributed to capture a wide-range area with only weak
labels available at the video-level. We propose the method named MultiASL
(Multi-view Action Selection Learning), which leverages action selection
learning to enhance view fusion by selecting the most useful information from
different viewpoints. The proposed method includes a Multi-view
Spatial-Temporal Transformer video encoder to extract spatial and temporal
features from multi-viewpoint videos. Action Selection Learning is employed at
the frame-level, using pseudo ground-truth obtained from weak labels at the
video-level, to identify the most relevant frames for action recognition.
Experiments in a real-world office environment using the MM-Office dataset
demonstrate the superior performance of the proposed method compared to
existing methods."
Mixture of Attentions For Speculative Decoding,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"The growth in the number of parameters of Large Language Models (LLMs) has
led to a significant surge in computational requirements, making them
challenging and costly to deploy. Speculative decoding (SD) leverages smaller
models to efficiently propose future tokens, which are then verified by the LLM
in parallel. Small models that utilise activations from the LLM currently
achieve the fastest decoding speeds. However, we identify several limitations
of SD models including the lack of on-policyness during training and partial
observability. To address these shortcomings, we propose a more grounded
architecture for small models by introducing a Mixture of Attentions for SD.
Our novel architecture can be applied in two scenarios: a conventional single
device deployment and a novel client-server deployment where the small model is
hosted on a consumer device and the LLM on a server. In a single-device
scenario, we demonstrate state-of-the-art speedups improving EAGLE-2 by 9.5%
and its acceptance length by 25%. In a client-server setting, our experiments
demonstrate: 1) state-of-the-art latencies with minimal calls to the server for
different network conditions, and 2) in the event of a complete disconnection,
our approach can maintain higher accuracy compared to other SD methods and
demonstrates advantages over API calls to LLMs, which would otherwise be unable
to continue the generation process."
Text-guided Diffusion Model for 3D Molecule Generation,cs.LG,Machine Learning,2024-10-04,"The de novo generation of molecules with targeted properties is crucial in
biology, chemistry, and drug discovery. Current generative models are limited
to using single property values as conditions, struggling with complex
customizations described in detailed human language. To address this, we
propose the text guidance instead, and introduce TextSMOG, a new Text-guided
Small Molecule Generation Approach via 3D Diffusion Model which integrates
language and diffusion models for text-guided small molecule generation. This
method uses textual conditions to guide molecule generation, enhancing both
stability and diversity. Experimental results show TextSMOG's proficiency in
capturing and utilizing information from textual descriptions, making it a
powerful tool for generating 3D molecular structures in response to complex
textual customizations."
Comparing zero-shot self-explanations with human rationales in multilingual text classification,cs.CL,Computation and Language (Natural Language Processing),2024-10-04,"Instruction-tuned LLMs are able to provide an explanation about their output
to users by generating self-explanations that do not require gradient
computations or the application of possibly complex XAI methods. In this paper,
we analyse whether this ability results in a good explanation by evaluating
self-explanations in the form of input rationales with respect to their
plausibility to humans as well as their faithfulness to models. For this, we
apply two text classification tasks: sentiment classification and forced labour
detection. Next to English, we further include Danish and Italian translations
of the sentiment classification task and compare self-explanations to human
annotations for all samples. To allow for direct comparisons, we also compute
post-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and
apply this pipeline to 4 LLMs (Llama2, Llama3, Mistral and Mixtral). Our
results show that self-explanations align more closely with human annotations
compared to LRP, while maintaining a comparable level of faithfulness."
